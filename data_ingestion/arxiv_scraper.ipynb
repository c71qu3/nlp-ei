{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92c8678-3034-4449-bc96-ce577387b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests as rq\n",
    "from urllib.parse import urljoin\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bcf7ce8-c974-4dba-8079-0f99f900afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_urls(navigation: bs4.element.ResultSet, n: int) -> list:\n",
    "    paper_urls = []\n",
    "    \n",
    "    for item in navigation[0:n_papers]:\n",
    "        id_ = item.select_one(\"a[title='Abstract'][id]\")\n",
    "        html_tag = item.select_one(\"a[title='View HTML'][id^='html-']\")\n",
    "        html_url = urljoin(arxiv_url, html_tag[\"href\"]) if html_tag else None\n",
    "    \n",
    "        if not html_url:\n",
    "            continue\n",
    "        else:\n",
    "            paper_urls.append(html_url)\n",
    "\n",
    "    return paper_urls\n",
    "\n",
    "\n",
    "def get_title(soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Gets the text content of the title tag.\n",
    "    '''\n",
    "    title = soup.find(\"title\").get_text()\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def get_abstract(soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Gets the text content of the abstract tag.\n",
    "    '''\n",
    "    abstract_h6 = soup.find(\"h6\", class_=\"ltx_title ltx_title_abstract\")\n",
    "    p = abstract_h6.find_next(\"p\", class_=\"ltx_p\")\n",
    "    abstract_text = p.get_text()\n",
    "    \n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "def check_file_exists(path: str) -> bool:\n",
    "    '''\n",
    "    Check file exists.\n",
    "    '''\n",
    "    return os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffb4161-ec2c-4c48-9d0f-b16f50a37137",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924b9819-9e99-428f-af99-1d3e85338a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_url = \"https://arxiv.org/list/cs.AI/recent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d3dff3-7725-473c-8a48-27f6c3fe2004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query the recently uploaded papers list\n",
    "response = rq.get(arxiv_url, headers = {\"User-Agent\": \"Mozilla/5.0\"})\n",
    "response_html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afddfa54-4099-47d9-b0ca-7fa1a5a7465e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the list of items from the papers list\n",
    "main_soup = BeautifulSoup(response_html, \"html.parser\")\n",
    "navigation = main_soup.select(\"dl > dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5554e5-d750-4d86-a7cb-67d6ced25674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_papers = 50\n",
    "paper_urls = get_paper_urls(navigation=navigation, n=n_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10191187-4cf3-4968-b39b-088c37b13044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both files for 2510.20809v1 already downloaded, skipping.\n",
      "Both files for 2510.20784v1 already downloaded, skipping.\n",
      "Both files for 2510.20691v1 already downloaded, skipping.\n",
      "Both files for 2510.20665v1 already downloaded, skipping.\n",
      "\u001b[93m Skipping https://arxiv.org/html/2510.20641v1, not enough characters in HTML content. \u001b[0m\n",
      "Both files for 2510.20636v1 already downloaded, skipping.\n",
      "Both files for 2510.20621v1 already downloaded, skipping.\n",
      "Both files for 2510.20604v1 already downloaded, skipping.\n",
      "Both files for 2510.20603v1 already downloaded, skipping.\n",
      "Both files for 2510.20591v1 already downloaded, skipping.\n",
      "Both files for 2510.20457v1 already downloaded, skipping.\n",
      "Both files for 2510.20377v1 already downloaded, skipping.\n",
      "Both files for 2510.20345v1 already downloaded, skipping.\n",
      "Both files for 2510.20337v1 already downloaded, skipping.\n",
      "Both files for 2510.20332v1 already downloaded, skipping.\n",
      "Both files for 2510.20310v1 already downloaded, skipping.\n",
      "Both files for 2510.20275v1 already downloaded, skipping.\n",
      "Both files for 2510.20258v1 already downloaded, skipping.\n",
      "Both files for 2510.20252v1 already downloaded, skipping.\n",
      "Both files for 2510.20205v1 already downloaded, skipping.\n",
      "Both files for 2510.20190v1 already downloaded, skipping.\n",
      "\u001b[93m Skipping 2510.20188v1 as it does not have an abstract. \u001b[0m\n",
      "Both files for 2510.20099v1 already downloaded, skipping.\n",
      "Both files for 2510.20075v1 already downloaded, skipping.\n",
      "Both files for 2510.19954v1 already downloaded, skipping.\n",
      "Both files for 2510.19949v1 already downloaded, skipping.\n",
      "Both files for 2510.19842v1 already downloaded, skipping.\n",
      "Both files for 2510.19838v1 already downloaded, skipping.\n",
      "Both files for 2510.19835v1 already downloaded, skipping.\n",
      "Both files for 2510.20819v1 already downloaded, skipping.\n",
      "Both files for 2510.20818v1 already downloaded, skipping.\n",
      "Both files for 2510.20813v1 already downloaded, skipping.\n",
      "Both files for 2510.20812v1 already downloaded, skipping.\n",
      "Both files for 2510.20810v1 already downloaded, skipping.\n",
      "Both files for 2510.20808v1 already downloaded, skipping.\n",
      "Both files for 2510.20800v1 already downloaded, skipping.\n",
      "Both files for 2510.20797v1 already downloaded, skipping.\n",
      "Both files for 2510.20795v1 already downloaded, skipping.\n",
      "Both files for 2510.20782v1 already downloaded, skipping.\n",
      "Both files for 2510.20780v1 already downloaded, skipping.\n",
      "Both files for 2510.20774v1 already downloaded, skipping.\n"
     ]
    }
   ],
   "source": [
    "paper_contents = []\n",
    "\n",
    "for url in paper_urls:\n",
    "    paper_html_content = rq.get(url, headers = {\"User-Agent\": \"Mozilla/5.0\"}).text\n",
    "\n",
    "    if len(paper_html_content) > 3000:\n",
    "        html_id = url.split(\"/\")[-1]\n",
    "        \n",
    "        # Check if we already downloaded both files, otherwise skip it to reduce processing time.\n",
    "        full_content_file = f\"{data_path}/raw/htmls/{html_id}.txt\"\n",
    "        abstract_content_file = f\"{data_path}/raw/abstracts/{html_id}.txt\"\n",
    "        full_file_already_exists = check_file_exists(path=full_content_file)\n",
    "        abs_file_already_exists = check_file_exists(path=abstract_content_file)\n",
    "\n",
    "        if full_file_already_exists and abs_file_already_exists:\n",
    "            print(f\"Both files for {html_id} already downloaded, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get content of paper url\n",
    "        paper_soup = BeautifulSoup(paper_html_content, \"html.parser\")\n",
    "        title = get_title(soup=paper_soup)\n",
    "\n",
    "        # Obtain the abstract from the paper content\n",
    "        try:\n",
    "            abstract = get_abstract(paper_soup)\n",
    "        except AttributeError as e:\n",
    "            print(f\"\\033[93m Skipping {html_id} as it does not have an abstract. \\033[0m\")\n",
    "            continue\n",
    "\n",
    "        # Write files\n",
    "        print(f\"Downloading {url} \\n\\t Title: {title}\")\n",
    "        with open(full_content_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(paper_html_content)\n",
    "            print(f\"Wrote file {full_content_file}\")\n",
    "        with open(abstract_content_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(abstract)\n",
    "            print(f\"Wrote file {abstract_content_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\033[93m Skipping {url}, not enough characters in HTML content. \\033[0m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
