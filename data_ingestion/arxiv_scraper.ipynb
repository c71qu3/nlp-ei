{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c92c8678-3034-4449-bc96-ce577387b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests as rq\n",
    "from urllib.parse import urljoin\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bcf7ce8-c974-4dba-8079-0f99f900afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_urls(navigation: bs4.element.ResultSet, n: int) -> list:\n",
    "    '''\n",
    "    Obtains the url link to each paper which appears in the navigation list.\n",
    "    '''\n",
    "    paper_urls = []\n",
    "    \n",
    "    for i, item in enumerate(navigation):\n",
    "        id_ = item.select_one(\"a[title='Abstract'][id]\")\n",
    "        html_tag = item.select_one(\"a[title='View HTML'][id^='html-']\")\n",
    "        html_url = urljoin(arxiv_url, html_tag[\"href\"]) if html_tag else None\n",
    "    \n",
    "        if not html_url:\n",
    "            continue\n",
    "        else:\n",
    "            paper_urls.append(html_url)\n",
    "\n",
    "        if len(paper_urls) >= n_papers:\n",
    "            break\n",
    "\n",
    "    return paper_urls\n",
    "\n",
    "\n",
    "def get_title(soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Gets the text content of the title tag.\n",
    "    '''\n",
    "    title = soup.find(\"title\").get_text()\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def get_abstract(soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Gets the text content of the abstract tag.\n",
    "    '''\n",
    "    abstract_h6 = soup.find(\"h6\", class_=\"ltx_title ltx_title_abstract\")\n",
    "    p = abstract_h6.find_next(\"p\", class_=\"ltx_p\")\n",
    "    abstract_text = p.get_text()\n",
    "    \n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "def check_file_exists(path: str) -> bool:\n",
    "    '''\n",
    "    Check file exists.\n",
    "    '''\n",
    "    return os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ffb4161-ec2c-4c48-9d0f-b16f50a37137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path if necessary\n",
    "data_path = \"../data\"\n",
    "\n",
    "if not Path(data_path).exists():\n",
    "    Path(data_path).mkdir()\n",
    "    \n",
    "    (Path(data_path)/\"raw\").mkdir()\n",
    "    (Path(data_path)/\"raw\"/\"htmls\").mkdir()\n",
    "    (Path(data_path)/\"raw\"/\"abstracts\").mkdir()\n",
    "    (Path(data_path)/\"raw\"/\"parsed_sections\").mkdir()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "924b9819-9e99-428f-af99-1d3e85338a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying https://arxiv.org/list/cs.AI/recent?skip=0&show=1000\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for scraping the website\n",
    "ARXIV_BASE_URL = \"https://arxiv.org/list\"\n",
    "arxiv_topic = \"/cs.AI/recent?skip=0&show=1000\"\n",
    "arxiv_url = f\"{ARXIV_BASE_URL}{arxiv_topic}\"\n",
    "print(f\"Querying {arxiv_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60d3dff3-7725-473c-8a48-27f6c3fe2004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query the recently uploaded papers list\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = rq.get(arxiv_url, headers = headers)\n",
    "response_html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afddfa54-4099-47d9-b0ca-7fa1a5a7465e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the list of items from the papers list\n",
    "main_soup = BeautifulSoup(response_html, \"html.parser\")\n",
    "navigation = main_soup.select(\"dl > dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c5554e5-d750-4d86-a7cb-67d6ced25674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain a list of n links to the recent papers for the chosen topic\n",
    "# TO DO: Implement a handler for when the amount of documents is less than n due to issues with the HTML content of the linked page.\n",
    "n_papers = 60\n",
    "paper_urls = get_paper_urls(navigation=navigation, n=n_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afae102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively extract sections from soup representation of html\n",
    "def get_sections(tag: BeautifulSoup):\n",
    "    section_tags = tag.find_all(\"section\", recursive=False)\n",
    "    sections = []\n",
    "\n",
    "\n",
    "    for section_tag in section_tags:\n",
    "\n",
    "        paragraphs = [\n",
    "        p.get_text(\" \", strip=True)\n",
    "        for p in section_tag.find_all(\"p\")\n",
    "        ]\n",
    "        section_dict = {\n",
    "            \"title\": section_tag.find([\"h1\",\n",
    "                                       \"h2\",\n",
    "                                       \"h3\",\n",
    "                                       \"h4\",\n",
    "                                       \"h5\",\n",
    "                                       \"h6\",\n",
    "                                       \"h7\"]).get_text().strip(),\n",
    "            \"paragraphs\": \"\\n\".join(paragraphs),\n",
    "            \"subsections\": get_sections(section_tag)\n",
    "        }\n",
    "        sections.append(section_dict)\n",
    "    return sections\n",
    "\n",
    "    return get_sections(get_sections(\n",
    "        tag.find(\"html\", recursive=False).find(\"body\",recursive=False).find(\"div\",class_=\"ltx_page_main\", recursive=False).find(\"div\",class_=\"ltx_page_content\", recursive=False).find(\"article\", recursive=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10191187-4cf3-4968-b39b-088c37b13044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "paper_contents = []\n",
    "\n",
    "for url in paper_urls:\n",
    "    paper_html_content = rq.get(url, headers = headers).text\n",
    "\n",
    "    if len(paper_html_content) > 3000:\n",
    "        html_id = url.split(\"/\")[-1]\n",
    "        print(url)\n",
    "        \n",
    "        # Check if we already downloaded both files, otherwise skip it to reduce processing time.\n",
    "        full_content_file = f\"{data_path}/raw/htmls/{html_id}.txt\"\n",
    "        abstract_content_file = f\"{data_path}/raw/abstracts/{html_id}.txt\"\n",
    "        parsed_sections_file = f\"{data_path}/raw/parsed_sections/{html_id}.json\"\n",
    "        full_file_already_exists = check_file_exists(path=full_content_file)\n",
    "        abs_file_already_exists = check_file_exists(path=abstract_content_file)\n",
    "\n",
    "        if full_file_already_exists and abs_file_already_exists:\n",
    "            print(f\"Both files for {html_id} already downloaded, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get content of paper url\n",
    "        paper_soup = BeautifulSoup(paper_html_content, \"html.parser\")\n",
    "        title = get_title(soup=paper_soup)\n",
    "\n",
    "        # Obtain the abstract from the paper content\n",
    "        try:\n",
    "            abstract = get_abstract(paper_soup)\n",
    "        except AttributeError as e:\n",
    "            print(f\"\\033[93m Skipping {html_id} as it does not have an abstract. \\033[0m\")\n",
    "            continue\n",
    "        try:\n",
    "            sections = get_sections(paper_soup.find(\"html\",recursive=False).find(\"body\", recursive=False).find(\"div\",class_=\"ltx_page_main\", recursive=False).find(\"div\",class_=\"ltx_page_content\", recursive=False).find(\"article\", recursive=False))\n",
    "        except Exception as e:\n",
    "            print(f\"\\033[93m Error parsing html sections \\033[0m\")\n",
    "            continue\n",
    "        # Write files\n",
    "        print(f\"Downloading {url} \\n\\t Title: {title}\")\n",
    "        with open(full_content_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(paper_html_content)\n",
    "            print(f\"Wrote file {full_content_file}\")\n",
    "        with open(abstract_content_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(abstract)\n",
    "            print(f\"Wrote file {abstract_content_file}\")\n",
    "        with open(parsed_sections_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(sections, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(f\"\\033[93m Skipping {url}, not enough characters in HTML content. \\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb1c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a4350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
