{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c92c8678-3034-4449-bc96-ce577387b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests as rq\n",
    "from urllib.parse import urljoin\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bcf7ce8-c974-4dba-8079-0f99f900afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_urls(navigation: bs4.element.ResultSet, n: int) -> list:\n",
    "    '''\n",
    "    Obtains the url link to each paper which appears in the navigation list.\n",
    "    '''\n",
    "    paper_urls = []\n",
    "    \n",
    "    for i, item in enumerate(navigation):\n",
    "        id_ = item.select_one(\"a[title='Abstract'][id]\")\n",
    "        html_tag = item.select_one(\"a[title='View HTML'][id^='html-']\")\n",
    "        html_url = urljoin(arxiv_url, html_tag[\"href\"]) if html_tag else None\n",
    "    \n",
    "        if not html_url:\n",
    "            continue\n",
    "        else:\n",
    "            paper_urls.append(html_url)\n",
    "\n",
    "        if len(paper_urls) >= n_papers:\n",
    "            break\n",
    "\n",
    "    return paper_urls\n",
    "\n",
    "\n",
    "def get_title(soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Gets the text content of the title tag.\n",
    "    '''\n",
    "    title = soup.find(\"title\").get_text()\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def get_abstract(soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Gets the text content of the abstract tag.\n",
    "    '''\n",
    "    abstract_h6 = soup.find(\"h6\", class_=\"ltx_title ltx_title_abstract\")\n",
    "    p = abstract_h6.find_next(\"p\", class_=\"ltx_p\")\n",
    "    abstract_text = p.get_text()\n",
    "    \n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "def check_file_exists(path: str) -> bool:\n",
    "    '''\n",
    "    Check file exists.\n",
    "    '''\n",
    "    return os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ffb4161-ec2c-4c48-9d0f-b16f50a37137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path if necessary\n",
    "data_path = \"../data\"\n",
    "\n",
    "if not Path(data_path).exists():\n",
    "    Path(data_path).mkdir()\n",
    "    \n",
    "    (Path(data_path)/\"raw\").mkdir()\n",
    "    (Path(data_path)/\"raw\"/\"htmls\").mkdir()\n",
    "    (Path(data_path)/\"raw\"/\"abstracts\").mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "924b9819-9e99-428f-af99-1d3e85338a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying https://arxiv.org/list/cs.AI/recent?skip=0&show=1000\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for scraping the website\n",
    "ARXIV_BASE_URL = \"https://arxiv.org/list\"\n",
    "arxiv_topic = \"/cs.AI/recent?skip=0&show=1000\"\n",
    "arxiv_url = f\"{ARXIV_BASE_URL}{arxiv_topic}\"\n",
    "print(f\"Querying {arxiv_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60d3dff3-7725-473c-8a48-27f6c3fe2004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query the recently uploaded papers list\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = rq.get(arxiv_url, headers = headers)\n",
    "response_html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afddfa54-4099-47d9-b0ca-7fa1a5a7465e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the list of items from the papers list\n",
    "main_soup = BeautifulSoup(response_html, \"html.parser\")\n",
    "navigation = main_soup.select(\"dl > dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c5554e5-d750-4d86-a7cb-67d6ced25674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain a list of n links to the recent papers for the chosen topic\n",
    "# TO DO: Implement a handler for when the amount of documents is less than n due to issues with the HTML content of the linked page.\n",
    "n_papers = 60\n",
    "paper_urls = get_paper_urls(navigation=navigation, n=n_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10191187-4cf3-4968-b39b-088c37b13044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both files for 2510.25758v1 already downloaded, skipping.\n",
      "Both files for 2510.25724v1 already downloaded, skipping.\n",
      "Both files for 2510.25679v1 already downloaded, skipping.\n",
      "Both files for 2510.25668v1 already downloaded, skipping.\n",
      "Both files for 2510.25612v1 already downloaded, skipping.\n",
      "Both files for 2510.25588v1 already downloaded, skipping.\n",
      "Both files for 2510.25518v1 already downloaded, skipping.\n",
      "Both files for 2510.25517v1 already downloaded, skipping.\n",
      "Both files for 2510.25510v1 already downloaded, skipping.\n",
      "Both files for 2510.25504v1 already downloaded, skipping.\n",
      "Both files for 2510.25445v1 already downloaded, skipping.\n",
      "Both files for 2510.25388v1 already downloaded, skipping.\n",
      "Both files for 2510.25320v1 already downloaded, skipping.\n",
      "Both files for 2510.25232v1 already downloaded, skipping.\n",
      "Both files for 2510.25223v1 already downloaded, skipping.\n",
      "Both files for 2510.25205v1 already downloaded, skipping.\n",
      "Both files for 2510.25179v1 already downloaded, skipping.\n",
      "Both files for 2510.25101v1 already downloaded, skipping.\n",
      "Both files for 2510.25091v1 already downloaded, skipping.\n",
      "Both files for 2510.25065v1 already downloaded, skipping.\n",
      "Both files for 2510.25014v1 already downloaded, skipping.\n",
      "Both files for 2510.25007v1 already downloaded, skipping.\n",
      "Both files for 2510.25005v1 already downloaded, skipping.\n",
      "Both files for 2510.24832v1 already downloaded, skipping.\n",
      "Both files for 2510.25770v1 already downloaded, skipping.\n",
      "Both files for 2510.25732v1 already downloaded, skipping.\n",
      "Both files for 2510.25731v1 already downloaded, skipping.\n",
      "Both files for 2510.25729v1 already downloaded, skipping.\n",
      "Both files for 2510.25694v1 already downloaded, skipping.\n",
      "Both files for 2510.25683v1 already downloaded, skipping.\n",
      "Both files for 2510.25662v1 already downloaded, skipping.\n",
      "Both files for 2510.25634v1 already downloaded, skipping.\n",
      "Both files for 2510.25626v1 already downloaded, skipping.\n",
      "Both files for 2510.25616v1 already downloaded, skipping.\n",
      "Both files for 2510.25609v1 already downloaded, skipping.\n",
      "Both files for 2510.25602v1 already downloaded, skipping.\n",
      "Both files for 2510.25595v1 already downloaded, skipping.\n",
      "Both files for 2510.25590v1 already downloaded, skipping.\n",
      "Both files for 2510.25577v1 already downloaded, skipping.\n",
      "Both files for 2510.25563v1 already downloaded, skipping.\n",
      "Both files for 2510.25557v1 already downloaded, skipping.\n",
      "\u001b[93m Skipping 2510.25531v1 as it does not have an abstract. \u001b[0m\n",
      "Downloading https://arxiv.org/html/2510.25512v1 \n",
      "\t Title: FaCT: Faithful Concept Traces for Explaining Neural Network Decisions\n",
      "Wrote file ../data/raw/htmls/2510.25512v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25512v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25506v1 \n",
      "\t Title: Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies\n",
      "Wrote file ../data/raw/htmls/2510.25506v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25506v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25502v1 \n",
      "\t Title: TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting\n",
      "Wrote file ../data/raw/htmls/2510.25502v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25502v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25458v1 \n",
      "\t Title: 1 INTRODUCTION\n",
      "Wrote file ../data/raw/htmls/2510.25458v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25458v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25441v1 \n",
      "\t Title: Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs\n",
      "Wrote file ../data/raw/htmls/2510.25441v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25441v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25428v1 \n",
      "\t Title: Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report\n",
      "Wrote file ../data/raw/htmls/2510.25428v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25428v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25426v1 \n",
      "\t Title: Implicature in Interaction: Understanding Implicature Improves Alignment in Humanâ€“LLM InteractionPre-print article, Manuscript under review\n",
      "Wrote file ../data/raw/htmls/2510.25426v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25426v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25420v1 \n",
      "\t Title: Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models\n",
      "Wrote file ../data/raw/htmls/2510.25420v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25420v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25416v1 \n",
      "\t Title: Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems\n",
      "Wrote file ../data/raw/htmls/2510.25416v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25416v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25409v1 \n",
      "\t Title: BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains\n",
      "Wrote file ../data/raw/htmls/2510.25409v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25409v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25404v1 \n",
      "\t Title: GPTOpt: Towards Efficient LLM-Based Black-Box Optimization\n",
      "Wrote file ../data/raw/htmls/2510.25404v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25404v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25386v1 \n",
      "\t Title: Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods\n",
      "Wrote file ../data/raw/htmls/2510.25386v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25386v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25378v1 \n",
      "\t Title: Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy\n",
      "Wrote file ../data/raw/htmls/2510.25378v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25378v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25368v1 \n",
      "\t Title: Position: Biology is the Challenge Physics-Informed ML Needs to Evolve\n",
      "Wrote file ../data/raw/htmls/2510.25368v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25368v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25366v1 \n",
      "\t Title: A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks\n",
      "Wrote file ../data/raw/htmls/2510.25366v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25366v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25340v1 \n",
      "\t Title: Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork\n",
      "Wrote file ../data/raw/htmls/2510.25340v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25340v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25327v1 \n",
      "\t Title: MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding\n",
      "Wrote file ../data/raw/htmls/2510.25327v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25327v1.txt\n",
      "Downloading https://arxiv.org/html/2510.25319v1 \n",
      "\t Title: 4-Doodle: Text to 3D Sketches that Move!\n",
      "Wrote file ../data/raw/htmls/2510.25319v1.txt\n",
      "Wrote file ../data/raw/abstracts/2510.25319v1.txt\n"
     ]
    }
   ],
   "source": [
    "paper_contents = []\n",
    "\n",
    "for url in paper_urls:\n",
    "    paper_html_content = rq.get(url, headers = headers).text\n",
    "\n",
    "    if len(paper_html_content) > 3000:\n",
    "        html_id = url.split(\"/\")[-1]\n",
    "        \n",
    "        # Check if we already downloaded both files, otherwise skip it to reduce processing time.\n",
    "        full_content_file = f\"{data_path}/raw/htmls/{html_id}.txt\"\n",
    "        abstract_content_file = f\"{data_path}/raw/abstracts/{html_id}.txt\"\n",
    "        full_file_already_exists = check_file_exists(path=full_content_file)\n",
    "        abs_file_already_exists = check_file_exists(path=abstract_content_file)\n",
    "\n",
    "        if full_file_already_exists and abs_file_already_exists:\n",
    "            print(f\"Both files for {html_id} already downloaded, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get content of paper url\n",
    "        paper_soup = BeautifulSoup(paper_html_content, \"html.parser\")\n",
    "        title = get_title(soup=paper_soup)\n",
    "\n",
    "        # Obtain the abstract from the paper content\n",
    "        try:\n",
    "            abstract = get_abstract(paper_soup)\n",
    "        except AttributeError as e:\n",
    "            print(f\"\\033[93m Skipping {html_id} as it does not have an abstract. \\033[0m\")\n",
    "            continue\n",
    "\n",
    "        # Write files\n",
    "        print(f\"Downloading {url} \\n\\t Title: {title}\")\n",
    "        with open(full_content_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(paper_html_content)\n",
    "            print(f\"Wrote file {full_content_file}\")\n",
    "        with open(abstract_content_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(abstract)\n",
    "            print(f\"Wrote file {abstract_content_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\033[93m Skipping {url}, not enough characters in HTML content. \\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb1c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
