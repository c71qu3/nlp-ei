{"..\\data\\raw\\htmls\\2510.24832v1.txt": {"abstract": "Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large Language Models (LLMs) can be conceptualized as progressively editing a query's 'Reasoning Tree'. This process involves exploring nodes (tokens) and dynamically modifying the model's policy at each node. When combined with data scheduling, this process yields further gains in data efficiency and accuracy.\n\nHowever, existing RLVR data scheduling methods typically rely on path-based metrics to rank queries, overlooking the reasoning tree structures of these queries.\n\nIn this paper, we introduce a novel metric, namely Reasoning Score (r-score), which measures the query's learning difficulty based on the structure of its reasoning tree.\n\nBased on the r-score, we propose the R easoning Tr e e Schedule (Re-Schedule), a scheduling algorithm that constructs a curriculum progressing from structurally simple (high r-score) to complex (low r-score) queries.\n\nExperiments on six math-reasoning benchmarks show that Re-Schedule significantly improves average accuracy, achieving gains of up to 3.2%.\n\nThese strong results validate our approach and demonstrate that a structural understanding of the reasoning tree provides a more powerful and principled foundation for RLVR data scheduling\n\n1 1 1 Our code is available at https://github.com/zz-haooo/Re-Schedule. .", "v1_summary": "Reinforcement-learning-with-verifiable-reward training for large language models suffers from a critical inefficiency: current data-scheduling methods rely on final solution accuracy as a proxy for problem difficulty, overlooking the structural complexity of a query\u2019s reasoning tree and thus misprioritizing examples with low initial accuracy but simple structure and under-weighting complex cases with high initial accuracy. To address this, the Reasoning Score (r-score) quantifies a query\u2019s true learning potential by measuring the maximum accuracy gain achievable under a fixed node-editing budget, and the Re-Schedule algorithm leverages this metric to construct approximate reasoning trees offline, simulate edits to compute r-scores, and dynamically weight training samples according to an easy-to-hard curriculum integrated into standard GRPO-based RLVR fine-tuning. Evaluation on six standard math-reasoning benchmarks, including DAPA-Math-17K with Qwen2.5-Math-7B and Qwen2.5-7B, adopts a fixed-structure k-ary tree (k=4, depth=4, token interval=200) and key hyperparameters (budget M=4, epochs=10, dynamic weight range 0.5\u20132.0) with linear and sigmoid scheduling functions. Compared to accuracy-based scheduling variants, random sampling, and OPO/GRPO baselines, results indicate that single-node fixes under the Re-Schedule curriculum achieve 47.1% versus 46.9% under a linear schedule and 48.3% versus 47.4% under a sigmoid schedule, consistently surpassing pruning-based approaches. Consequently, replacing path-based scheduling with a structurally grounded r-score leads to faster convergence and higher reasoning accuracy, demonstrating that a reasoning-tree perspective is a superior proxy for sample difficulty. Although the evaluation is confined to RLVR benchmarks, this work shifts curriculum design toward structurally informed metrics, offering a principled path to more efficient and robust training in reasoning-focused learning tasks.", "refined_summary": "Reinforcement-learning-with-verifiable-reward (RLVR) training for large language models is hindered by data-scheduling methods that use final solution accuracy as a proxy for problem difficulty, ignoring reasoning-tree structure and misprioritizing samples. We propose the Reasoning Score (r-score), defined as the maximum accuracy gain under a fixed node-editing budget, and introduce Re-Schedule, which constructs approximate k-ary reasoning trees (k=4, depth=4, token interval=200) offline, simulates node edits (budget M=4) to compute r-scores and applies an easy-to-hard curriculum with dynamic weights (0.5\u20132.0) in standard GRPO-based RLVR fine-tuning over 10 epochs. We evaluate on six math-reasoning benchmarks, including DAPA-Math-17K with Qwen2.5-Math-7B and Qwen2.5-7B, comparing accuracy-based scheduling, random sampling and OPO/GRPO baselines. Under a linear schedule, Re-Schedule yields 47.1% versus 46.9%; under a sigmoid schedule, 48.3% versus 47.4%, consistently outperforming pruning-based approaches. Replacing path-based scheduling with structure-driven r-score accelerates convergence and raises reasoning accuracy, establishing a reasoning-tree perspective as a superior proxy for sample difficulty and paving the way for structurally grounded curriculum design in RLVR."}, "..\\data\\raw\\htmls\\2510.25005v1.txt": {"abstract": "Most counterfactual inference frameworks traditionally assume acyclic structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However, many real-world systems (e.g. biological systems) contain feedback loops or cyclic dependencies that violate acyclicity. In this work, we study counterfactual inference in cyclic SCMs under *shift-scale interventions* ,\n\ni.e., soft, policy-style changes\n\nthat rescale and/or shift a variable's mechanism.", "v1_summary": "Addressing the critical gap that prevailing counterfactual frameworks require acyclic causal graphs and hence fail in systems with feedback loops, this work introduces a theoretical framework for counterfactual inference in cyclic structural causal models under soft shift-scale interventions. Specifically, by imposing a global \u2113\u1d56-contraction condition on the causal mechanisms and invoking Banach\u2019s fixed-point theorem, the authors guarantee unique solvability for both the original and intervened twin SCMs. The core methodology develops abstract SCMs over endogenous variables X and exogenous Gaussian noise E, extends them to twin copies sharing E, and defines shift-scale interventions f\u1d62\u02e2\u02e2(x,e)=a\u1d62 f\u1d62(x,e)+b\u1d62 with |a\u1d62|\u22641, while perfect do-interventions arise as a special case. An illustrative two\u2010variable linear cyclic SCM modeling consumption and income under Gaussian noise highlights the contraction condition\u2019s applicability. Under an added Lipschitz regularity on the noise dependence, they derive sub-Gaussian concentration bounds for counterfactual outcomes with variance proxy (1\u2013\u03ba)\u207b\u00b2\u03c3\u00b2. Consequently, bounded shift-scale interventions preserve contractivity and are closed under composition, yielding an algebraically stable foundation for cyclic counterfactual reasoning. Although no empirical datasets or comparative baselines are evaluated, this contribution provides the first mathematically rigorous treatment of counterfactuals in cyclic SCMs, establishing contraction-based models as a tractable yet expressive class for analyzing interventions in feedback systems and paving the way for future algorithmic and empirical developments.", "refined_summary": "Prevailing counterfactual frameworks assume acyclic graphs, failing in systems with feedback loops. We introduce a theoretical framework for counterfactual inference in cyclic SCMs under soft shift-scale interventions. Imposing a global \u2113\u1d56-contraction (\u03ba < 1) on causal mechanisms and applying Banach\u2019s fixed-point theorem yields unique solvability for original and intervened twin SCMs. We formalize SCMs over endogenous X and exogenous Gaussian noise E, extend to twin copies sharing E, and define shift-scale interventions f\u1d62\u02e2\u02e2(x,e) = a\u1d62 f\u1d62(x,e) + b\u1d62 with |a\u1d62| \u2264 1 (perfect do-interventions as a special case). In a two-variable linear cyclic SCM of consumption and income under Gaussian noise, the \u2113\u1d56-contraction condition holds. Adding a Lipschitz condition on noise dependence produces sub-Gaussian concentration bounds for counterfactual outcomes with variance proxy (1 \u2013 \u03ba)\u207b\u00b2 \u03c3\u00b2. Bounded shift-scale interventions preserve contractivity and close under composition, yielding an algebraically stable structure for cyclic counterfactual reasoning. By casting cyclic counterfactual reasoning in contraction-based algebra, we enable future algorithmic and empirical advances in feedback systems."}, "..\\data\\raw\\htmls\\2510.25007v1.txt": {"abstract": "Evaluation and Management (E/M) coding, under the Current Procedural Terminology (CPT) taxonomy, documents medical services provided to patients by physicians. Used primarily for billing purposes, it is in physicians' best interest to provide accurate CPT E/M codes. Automating this coding task will help alleviate physicians' documentation burden, improve billing efficiency, and ultimately enable better patient care. However, a number of real-world complexities have made E/M encoding automation a challenging task. In this paper, we elaborate some of the key complexities and present ProFees, our LLM-based framework that tackles them, followed by a systematic evaluation. On an expert-curated real-world dataset, ProFees achieves an increase in coding accuracy of more than 36% over a commercial CPT E/M coding system and almost 5% over our strongest single-prompt baseline, demonstrating its effectiveness in addressing the real-world complexities.\n\nTaming the Real-world Complexities in CPT E/M Coding with Large Language Models\n\nIslam Nassar, Yang Lin, Yuan Jin, Rongxin Zhu, Chang Wei Tan, Zenan Zhai, Nitika Mathur , Thanh Tien Vu , Xu Zhong , Long Duong , Yuan-Fang Li Oracle Health &amp; AI {islam.nassar, yang.y.lin, yuan.j.jin, rongxin.zhu, changwei.tan, zenan.zhai, nitika.mathur, thanh.v.vu, peter.zhong, long.duong, yuanfang.li}@oracle.com", "v1_summary": "Manual CPT E/M coding remains an error-prone, resource-intensive process characterized by coder variability, complex guideline adherence, and substantial financial and compliance risks. Prior automated approaches relying on classical machine learning with heavy feature engineering, deep learning with data-hungry architectures, and na\u00efve LLM prompting have failed to deliver consistent, guideline-aligned performance. To overcome these limitations, we introduce ProFees, a production-ready, modular LLM-based framework combining dynamic few-shot prompting tailored to each encounter, an LLM-based critic for explicit Medical Decision Making validation, and a self-consistency strategy to counteract generative stochasticity. We used 216 de-identified outpatient and preventive SOAP-note encounters from hospital production databases, each annotated with physician-assigned CPT E/M codes, professional coder final codes, and, for 117 development cases, fine-grained internal-expert MDM element complexities. Specifically, development data were partitioned into Platinum and Disagreement subsets with expert-crafted chain-of-thought justifications indexed in a vector database for exemplar retrieval. ProFees comprises an Encounter-Type Classifier, an MDM Complexity Classifier employing retrieval-augmented chain-of-thought prompting over top-N exemplars, Recursive Criticism & Improvement critics aligned to AMA guidelines, and a self-consistency mechanism with K parallel inferences and majority voting, followed by a deterministic rule-based CPT decision tree. Consequently, ProFees achieves a CPT accuracy gain of 36.85% over a commercial tool and 33.73% over a single-prompt baseline, with an additional 4.73% improvement versus the strongest LLM baseline. These results underscore the promise of chain-of-thought\u2013driven architectures for complex clinical automation while acknowledging the limitation of a relatively small, de-identified expert-annotated test set. Future work will explore multi-code support and synthetic edge-case generation to enhance generalizability.", "refined_summary": "Manual CPT E/M coding is error-prone and resource-intensive, subject to coder variability, complex guidelines and compliance risk. Prior automated methods\u2014classical machine learning with extensive feature engineering, data-hungry deep learning and na\u00efve LLM prompting\u2014lack consistent, guideline-aligned performance. We introduce ProFees, a modular LLM framework integrating dynamic few-shot prompting, an LLM-based MDM critic and self-consistency via K parallel inferences with majority voting. We used 216 de-identified outpatient and preventive SOAP-note encounters annotated with physician-assigned and coder CPT E/M codes, including 117 development cases with internal-expert MDM complexities, then partitioned data into Platinum and Disagreement subsets, indexing expert-crafted chain-of-thought justifications in a vector database. ProFees comprises an Encounter-Type Classifier; an MDM Complexity Classifier using retrieval-augmented chain-of-thought over top-N exemplars; Recursive Criticism & Improvement critics aligned to AMA guidelines; and a deterministic CPT decision tree. ProFees achieves a CPT accuracy gain of 36.85% over a commercial tool, 33.73% over a single-prompt baseline and 4.73% over the strongest LLM baseline. These results validate chain-of-thought\u2013driven architectures for complex clinical automation. Future work will extend multi-code support and synthetic edge-case generation to improve generalizability."}, "..\\data\\raw\\htmls\\2510.25014v1.txt": {"abstract": "Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust. This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading (browse-offer-review-confirm). To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations. Evaluation across 300 trading dialogues demonstrates &gt; &gt; 99% state compliance\n\nand 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models' (Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games. The code will be publicly available upon acceptance.\n\n\u2020 \u2020 footnotetext: \u00a9 2025 Copyright held by the owner/author(s).", "v1_summary": "While large language models enable rich natural\u2010language NPC interactions, they struggle to respect the semi\u2010structured procedures essential for safe in\u2010game trading\u2014browse, offer, review, confirm\u2014often skipping steps, triggering unwanted purchases, and undermining player trust, a challenge that extends to any rule\u2010governed human\u2013AI dialogue requiring both conversational flexibility and strict procedural compliance. Consequently, we introduce Autoregressive State\u2010Tracking Prompting (ASTP), a Prime-Guide-Enforce prompting workflow that requires the model to explicitly infer and emit the previous dialogue state before each turn\u2014ensuring verifiable, step\u2010by\u2010step adherence\u2014and pair it with a placeholder\u2010based post\u2010processing mechanism that dramatically boosts numerical accuracy, enabling smaller LLMs to match larger models\u2019 performance while maintaining conversational naturalness. Specifically, we evaluated ASTP on a corpus of 300 in\u2010game trading dialogues drawn from two conversational scenarios, using JSON\u2010formatted game world data comprising 52 total items and a 20\u2010item merchant inventory as prompt placeholders. Our architecture centers on a single unified prompt that embeds system instructions, game context, dialogue history and four Dialogue Guidelines elements\u2014explicit state definitions, transition conditions, a directive to identify the previous state, and a requirement to output that state in JSON\u2014enabling the LLM to infer and report dialogue states such as SHOW_ITEMS, OFFER_SELL, FINAL_CHECK, COMMIT_SALE before generating a response. State\u2010specific placeholder post\u2010processing replaces a \u201c__PRICE__\u201d token with a programmatically computed total at OFFER_SELL, ensuring arithmetic precision. We benchmarked ASTP (with and without placeholder post\u2010processing, and with structured\u2010output variants) against Baseline1, Baseline2, ZS\u2010CoT, AutoTOD, and a DFI\u2010inspired prompt across Gemini\u20102.0-Flash, Gemini-2.5-Flash, and Gemini-2.5-Pro models, measuring state\u2010compliance (>99%), price\u2010accuracy (up to 100%), token usage, and response latency (2.4 s vs. 21.2 s). We observe that ASTP achieved a 99.64% State Transition Compliance Rate and 99.3% price accuracy\u2014corresponding to just 0.36% and 0.7% failure rates, respectively\u2014representing a substantial improvement over baseline and comparative prompt methods. These results demonstrate near\u2010perfect adherence to the required dialogue flow and highly accurate price calculations. The authors show that ASTP\u2019s explicit state\u2010tracking dramatically boosts procedural compliance from 78.1% to 99.6% and price\u2010calculation accuracy from 84.3% to 99.3% even on smaller, faster models, a crucial advancement for any application where strict rules and conversational flexibility must coexist. Although scalability to more numerous or complex states and transition rules remains to be demonstrated, ASTP represents a significant advance toward making large language models reliably rule\u2010bound yet expressive, with clear implications for in\u2010game trading and broader domains demanding transactional integrity.", "refined_summary": "Large language models (LLMs) enable rich natural\u2010language NPC interactions but fail to enforce semi\u2010structured trading procedures\u2014browse, offer, review, confirm\u2014skipping steps and triggering unwanted purchases. We introduce Autoregressive State\u2010Tracking Prompting (ASTP), a Prime-Guide-Enforce workflow requiring explicit inference and emission of the previous dialogue state before each turn, paired with placeholder-based post-processing for numeric precision. We evaluated ASTP on 300 in-game trading dialogues from two scenarios, using JSON-formatted world data with 52 items and a 20-item merchant inventory. A single unified prompt embeds system instructions, game context, dialogue history and four Dialogue Guidelines\u2014explicit state definitions, transition conditions, previous-state identification and JSON output\u2014enabling the LLM to report states (e.g., SHOW_ITEMS, OFFER_SELL, FINAL_CHECK, COMMIT_SALE) before responding. State-specific post-processing replaces \u201c__PRICE__\u201d with a computed total at OFFER_SELL, ensuring arithmetic accuracy. We benchmarked ASTP (with/without post-processing, structured-output variants) against Baseline1, Baseline2, ZS-CoT, AutoTOD and a DFI-inspired prompt across Gemini-2.0-Flash, Gemini-2.5-Flash and Gemini-2.5-Pro, measuring state-compliance (>99%), price-accuracy (up to 100%), token usage and response latency (2.4 s vs 21.2 s). ASTP achieved a 99.64% State Transition Compliance Rate (0.36% failure) and 99.3% price accuracy (0.7% failure), improving procedural compliance from 78.1% to 99.6% and price-calculation accuracy from 84.3% to 99.3% on smaller, faster models."}, "..\\data\\raw\\htmls\\2510.25065v1.txt": {"abstract": "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs),\n\nyet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO)\n\nthat augments standard answer/format rewards with signals over the reasoning procedure.\n\nTo this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's\n\nreasoning aligns with the pretrained teacher model.\n\nThe empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms\n\nexisting methodologies for GRPO-based post-training. These results highlight\n\nthat leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.\n\n*K* eywords Process Mining \u22c5 \\cdot Large Reasoning Model \u22c5 \\cdot Group Relative Policy Optimization\n\nCode: [Thrillcrazyer/THIP](https://github.com/Thrillcrazyer/THIP) Models: [THIP-7B](https://huggingface.co/Thrillcrazyer/Qwen-7B_THIP)\n\n[Uncaptioned image]\n\n<!-- image -->", "v1_summary": "Current GRPO-based RL post-training methods for large reasoning models optimize only final answers or surface-level text features, neglecting the chain-of-thought process and thereby encouraging verbosity, speculative leaps, or accidental correctness without genuine understanding. To address this gap, PM4GRPO introduces a reasoning-aware GRPO framework that leverages process mining to extract event-log traces of a teacher model\u2019s reasoning and computes a conformance-based reward measuring the alignment between student-generated and teacher traces. Specifically, the approach assumes access to a corpus of problem\u2013query pairs and a pretrained teacher model generating reference reasoning traces. PM4GRPO augments Group Sequence Policy Optimization by integrating the Inductive Miner to discover a process model from the policy\u2019s generated reasoning sequence and alignment-based conformance checking to compare it against the teacher trace, yielding a sequence-level conformance reward defined as the F1 score of fitness and precision. Consequently, this reward is combined with standard format and answer rewards and optimized under the GSPO objective using sequence-level importance ratios and clipping hyperparameter \u03b5. Under settings mirroring GSPO\u2019s group size and advantage normalization, PM4GRPO achieves 91.1% on MATH 500 and 61.1% on the Olympiad Bench in the 7B-parameter setting, outperforming strong open baselines such as R1-Distill-Qwen-7B and Skywork-OR1-7B. We observe that explicitly rewarding alignment between reasoning trajectories and a high-quality teacher substantially boosts post-training performance on complex benchmarks. Although dependence on a pretrained teacher model can limit applicability when such a model is unavailable or imperfect, this work underscores the promise of process-aware reward design and paves the way for more robust reinforcement learning strategies for large reasoning models.", "refined_summary": "Current post-training methods based on GRPO for large reasoning models optimize final answers or surface text, neglecting chain-of-thought and encouraging verbosity, speculation or accidental correctness. We introduce PM4GRPO, a reasoning-aware GRPO framework that applies process mining to teacher model event logs and derives a conformance-based reward measuring alignment between student and teacher reasoning traces. Given a corpus of problem\u2013query pairs and a pretrained teacher generating reference traces, PM4GRPO extends Group Sequence Policy Optimization by using the Inductive Miner to infer a process model from policy-generated reasoning sequences and alignment-based conformance checking to compute an F1 reward over fitness and precision. We integrate this sequence-level conformance reward with standard format and answer rewards under the GSPO objective with sequence-level importance ratios and clipping hyperparameter \u03b5. In the 7B-parameter setting, with GSPO\u2019s group size and advantage normalization, PM4GRPO attains 91.1% on MATH 500 and 61.1% on the Olympiad Bench, surpassing R1-Distill-Qwen-7B and Skywork-OR1-7B. These results demonstrate that explicitly rewarding reasoning trajectory alignment with a high-quality teacher improves post-training performance on complex benchmarks, highlighting process-aware reward design as a path toward robust RL strategies for large reasoning models, albeit with reliance on pretrained teacher availability."}, "..\\data\\raw\\htmls\\2510.25091v1.txt": {"abstract": ".\n\nStock movement prediction remains fundamentally challenging due to complex temporal dependencies, heterogeneous modalities, and dynamically evolving inter-stock relationships. Existing approaches often fail to unify structural, semantic, and regime-adaptive modeling within a scalable framework. This work introduces H3M-SSMoEs, a novel H ypergraph-based M ulti M odal architecture with LL M reasoning and S tyle- S tructured M ixture o f E xpert s , integrating three key innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph (LCH) and persistent inter-stock dependencies through a Global Context Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon Divergence weighting mechanism for adaptive relational learning and cross-modal alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities, enriching representations with domain-specific financial knowledge; and (3) a Style-Structured Mixture of Experts (SSMoEs) that combines shared market experts and industry-specialized experts, each parameterized by learnable style vectors enabling regime-aware specialization under sparse activation. Extensive experiments on three major stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in both superior predictive accuracy and investment performance, while exhibiting effective risk control. Datasets, source code, and model weights are available at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs .\n\nStock Prediction,\n\nHypergraph Neural Network,\n\nLarge Language Model,\n\nMixture of Experts\n\n\u2020 \u2020 copyright: acmlicensed \u2020 \u2020 journalyear: 2018 \u2020 \u2020 doi: XXXXXXX.XXXXXXX \u2020 \u2020 conference: Make sure to enter the correct\n\nconference title from your rights confirmation email; June 03-05,\n\n2018; Woodstock, NY\n\n\u2020 \u2020 isbn: 978-1-4503-XXXX-X/2018/06", "v1_summary": "Persistent difficulty in stock movement prediction arises from low signal-to-noise ratios, non-stationarity, dynamic inter-stock group dependencies and multimodal data heterogeneity, compounded by the inefficiency of existing models that treat graph relations, textual information and time series in isolation using pairwise GNNs and rudimentary fusion methods. To overcome these limitations, H3M-SSMoEs introduces a unified architecture featuring a hierarchical multi-context hypergraph that jointly encodes quantitative and textual modalities via shared hyperedges, a frozen Llama-3.2-1B large language model with lightweight adapters for deep semantic alignment and a Style-Structured Mixture-of-Experts module that sparsely activates specialized experts according to market regimes. Specifically, each modality is projected into a shared latent space through modality-specific feedforward networks, temporal context is injected into quantitative streams, and Local and Global hypergraphs with intra- and inter-modal sub-hyperedges, JSD-based weighting and hypergraph convolution capture fine- and coarse-grained market dependencies. The fused hypergraph outputs are concatenated and processed by the frozen LLM for semantic reasoning, after which representations traverse a sparse top-K gating ensemble of shared market experts and industry-specialized experts before final classification via a feedforward network and softmax. Training optimizes cross-entropy with auxiliary losses for balanced expert utilization, and benchmarking against 15 state-of-the-art baselines under a realistic backtesting regime demonstrates that on the DJIA H3M-SSMoEs achieved a 50.00 % annual return and a Sharpe ratio of 1.585\u2014a 57.7 % improvement over the strongest baseline\u2014thereby establishing that integrated structural, semantic and stylistic modeling can significantly enhance return profiles and risk-adjusted performance, although the framework\u2019s complexity and data diversity requirements may challenge real-time deployment and generalization across markets.", "refined_summary": "Stock movement prediction is hindered by low signal-to-noise ratios, non-stationarity, dynamic inter-stock dependencies and multimodal heterogeneity; existing models process graph, text and time-series data in isolation via pairwise GNNs and basic fusion. We introduce H3M-SSMoEs, which unifies structural, semantic and stylistic modeling through a hierarchical multi-context hypergraph, a frozen Llama-3.2-1B LLM with lightweight adapters and a Style-Structured Mixture-of-Experts module. Quantitative and textual inputs project into a shared latent space via modality-specific feedforward networks; temporal context is injected into quantitative streams. Local and Global hypergraphs with intra- and inter-modal sub-hyperedges, JSD-based weighting and hypergraph convolution capture fine- and coarse-grained market dependencies. Embeddings are concatenated, processed by the frozen LLM for semantic reasoning and routed through a sparse top-K gating ensemble of market and industry experts before classification via feedforward and softmax layers. Training minimizes cross-entropy with auxiliary losses to balance expert utilization. Backtesting on DJIA yields a 50.00 % annual return and Sharpe ratio of 1.585\u2014a 57.7 % improvement over the strongest baseline. These results demonstrate that integrated structural, semantic and stylistic modeling enhances returns and risk-adjusted performance, though complexity and data demands may hinder real-time deployment and cross-market generalization."}, "..\\data\\raw\\htmls\\2510.25101v1.txt": {"abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1 , an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.", "v1_summary": "Current agentic KBQA methods exhibit brittleness and rigidity due to reliance on process-supervised, idealized, gold-logical-form reasoning trajectories that are singular and error-free, resulting in poor robustness to noisy tool interactions and limited flexibility to discover alternative reasoning paths. To overcome these issues, KnowCoder-A1 employs an LLM-based KBQA agent trained exclusively with outcome supervision via a multi-stage curriculum reinforcement learning framework. Specifically, outcome-based rejection sampling first generates a seed of high-quality exploratory trajectories, then progressive curriculum scheduling of reward criteria strengthens autonomous exploration, tool use, query refinement and error recovery. Consequently, KnowCoder-A1 operates over a structured knowledge graph accessed via SearchTypes, SearchGraphPatterns and ExecuteSPARQL tools and is evaluated on WebQSP, CWQ and GrailQA, using a ReAct-style agent built on Qwen2.5-Coder-7B that interleaves natural-language reasoning tokens, tool-call tokens and environment observations to form trajectories under a learned policy. The two-stage training regime comprises cold-start finetuning on masked high-quality trajectories and subsequent on-policy Group Relative Policy Optimization with a KL penalty toward the cold-start policy, employing a composite reward capped at 1.0\u2014including a 0.1-weighted format correctness subreward and an F\u03b2-based accuracy subreward\u2014while scheduling the KL penalty weight from 0.5 to 1.0. The agent achieves an F1 score of 80.5% on the generalization-focused GrailQA dataset and delivers a 3.3% relative improvement over the prior state-of-the-art KBQA-o1 baseline. By demonstrating end-to-end training with only outcome-based feedback and staged curriculum, KnowCoder-A1 breaks free of process supervision constraints, yielding a system that robustly recovers from mistakes and explores novel reasoning paths. Although residual error types remain that may benefit from advanced reflection mechanisms, this work establishes curriculum reinforcement learning with outcome-only supervision as a powerful paradigm for flexible, agentic reasoning systems and suggests its applicability to other complex interactive AI tasks.", "refined_summary": "Relying on process-supervised gold logical-form trajectories, agentic KBQA methods suffer brittleness and limited flexibility; we introduce KnowCoder-A1, an LLM-based KBQA agent trained solely with outcome supervision via a multi-stage curriculum reinforcement learning framework. First, outcome-based rejection sampling generates high-quality exploratory trajectories; next, progressive curriculum scheduling strengthens autonomous exploration, tool use, query refinement and error recovery. KnowCoder-A1 operates over a knowledge graph using SearchTypes, SearchGraphPatterns and ExecuteSPARQL tools, and employs a ReAct-style agent built on Qwen2.5-Coder-7B that interleaves natural-language reasoning tokens, tool-call tokens and environment observations. We evaluate on WebQSP, CWQ and GrailQA. Training comprises cold-start fine-tuning on masked high-quality trajectories followed by on-policy Group Relative Policy Optimization with a KL penalty toward the cold-start policy and a composite reward capped at 1.0\u2014including a 0.1-weighted format correctness subreward and an F\u03b2-based accuracy subreward\u2014while scheduling the KL penalty weight from 0.5 to 1.0. KnowCoder-A1 achieves an F1 score of 80.5% on GrailQA, delivering a 3.3% relative improvement over the KBQA-o1 baseline. Our results establish curriculum reinforcement learning with outcome-only supervision as an effective paradigm for robust, flexible agentic reasoning systems."}, "..\\data\\raw\\htmls\\2510.25179v1.txt": {"abstract": "Agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning, collaboration, and adaptive control, enabling systems to coordinate and independently solve complex tasks. We extend this paradigm to safety alignment by introducing Agentic Moderation, a model-agnostic framework that leverages specialized agents to defend multimodal systems against jailbreak attacks. Unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications(safe or unsafe), our method integrates dynamic, cooperative agents,including Shield, Responder, Evaluator, and Reflector,to achieve context-aware and interpretable moderation. Extensive experiments across five datasets and four representative large vision-language models (LVLMs) demonstrate that our approach reduces the Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF), and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable, and well-balanced safety performance. By harnessing the flexibility and reasoning capacity of agentic architectures, Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the broader potential of agentic systems as a foundation for automated safety governance.", "v1_summary": "Large vision-language models (LVLMs) are increasingly susceptible to sophisticated cross-modal adversarial attacks, including pixel-level perturbations, hidden intent in benign text\u2013image combinations, and ensemble strategies that evade existing defenses due to high computational cost, rigid rule-based categorizations, or lack of context-aware reasoning. To address these challenges, Agentic Moderation is proposed as a model-agnostic, multi-agent framework that transforms safety alignment into a collaborative workflow in which a SHIELD agent performs fine-grained policy checks, a Responder executes tasks, an Evaluator assesses outputs, and a Reflection agent iteratively refines responses. Consequently, multimodal inputs are processed through a Coordinator-driven loop without retraining the underlying LVLM. Specifically, inputs are categorized into 45 predefined policy classes and evaluated via the SeeThreats framework in DSPy with Likert-scale harmfulness ratings. The SHIELD agent appends \u201cshould do/should not do\u201d cues to guide the Responder, whose outputs are vetted by the Evaluator\u2019s safety and utility rubrics, and the Reflector generates structured \u201cIssue\u2010Fix\u201d feedback for regeneration until convergence. Benchmarking against static rule-based and classification baselines demonstrates a 17% increase in refusal rate on LLaMA, with only 0.015 seconds of preprocessing overhead per query. These results indicate that Agentic Moderation delivers substantial safety gains with negligible latency cost. Furthermore, by treating safety alignment as a collaborative process among specialized agents, LVLMs can dynamically filter, evaluate, and revise outputs to markedly reduce harmful multimodal responses without undermining instruction following. A key limitation is the absence of adaptive coordination among agents, which the authors identify as a direction for future development. Overall, Agentic Moderation establishes a modular, extensible approach to multimodal safety that can be tailored to diverse policies and latency requirements, offering a promising foundation for robust and flexible defense strategies in the field.", "refined_summary": "Large vision-language models (LVLMs) remain vulnerable to pixel-level adversarial perturbations, hidden intent in benign text\u2013image pairs and ensemble attacks that bypass rule-based defenses or demand costly computation. We introduce Agentic Moderation, a model-agnostic, multi-agent framework that executes safety alignment as a collaborative workflow without retraining the underlying LVLM. A Coordinator orchestrates four specialists: SHIELD applies fine-grained checks across 45 predefined policy classes using DSPy\u2019s SeeThreats framework with Likert-scale harmfulness ratings; Responder generates outputs guided by \u201cshould do/should not do\u201d cues; Evaluator vets safety and utility via structured rubrics; Reflector issues \u201cIssue-Fix\u201d feedback and triggers iterative refinement until convergence. On LLaMA, Agentic Moderation boosts refusal rates by 17% over static rule-based and classifier baselines while incurring just 0.015 s preprocessing overhead per query. By treating safety as a modular, extensible process\u2014filtering, evaluating and revising multimodal outputs\u2014this approach achieves substantial risk reduction with negligible latency impact. The primary limitation is the lack of adaptive coordination among agents, which we identify as future work. Agentic Moderation offers a scalable foundation for robust, policy-tailored defense strategies in multimodal systems."}, "..\\data\\raw\\htmls\\2510.25205v1.txt": {"abstract": "Autonomous driving is an emerging technology that is expected to bring significant social, economic, and environmental benefits. However, these benefits come with rising energy consumption by computation engines, limiting the driving range of vehicles, especially electric ones. Perception computing is typically the most power-intensive component, as it relies on large-scale deep learning models to extract environmental features. Recently, numerous studies have employed model compression techniques, such as sparsification, quantization, and distillation, to reduce computational consumption. However, these methods often result in either a substantial model size or a significant drop in perception accuracy compared to high-computation models. To address these challenges, we propose an energy-efficient autonomous driving framework, called EneAD , which includes an adaptive perception and a robust decision module. In the adaptive perception module, a perception optimization strategy is designed from the perspective of data management and tuning. Firstly, we manage multiple perception models with different computational consumption and adjust the execution framerate dynamically. Then, we define them as knobs and design a transferable tuning method based on Bayesian optimization to identify promising knob values that achieve low computation while maintaining desired accuracy. To adaptively switch the knob values in various traffic scenarios, a lightweight classification model is proposed to distinguish the perception difficulty in different scenarios. In the robust decision module, we propose a decision model based on reinforcement learning and design a regularization term to enhance driving stability in the face of perturbed perception results. Extensive experiments evidence the superiority of our framework in both energy consumption and driving performance. EneAD can reduce perception consumption by 1.9 \u00d7 1.9\\times to 3.5 \u00d7 3.5\\times and thus improve driving range by 3.9 % 3.9\\% to 8.5 % 8.5\\% .", "v1_summary": "State-of-the-art modular autonomous driving perception systems consume hundreds to over a thousand watts, severely reducing electric vehicle range, and existing model compression or gate-based methods either still demand high computation, degrade accuracy in complex scenarios, or are challenging to train; additionally, there is no scalable way to adapt perception configurations to diverse traffic conditions or to ensure robust decision-making under perturbed perception outputs. EneAD addresses these challenges with an energy-efficient autonomous driving framework featuring an adaptive perception module that treats model choice, framerate, and interpolation as tunable knobs. Specifically, a lightweight uncertainty-aware classifier built on a Swin-T encoder plus a three-layer MLP with MC dropout, trained on six-camera 360\u00b0 images and one LiDAR from Nuscenes-R and Nuscenes-S (4:1 train/test split) and labeled by NDS scores from a SparseBEV detector, groups traffic scenarios into four difficulty levels. Consequently, multi-objective Bayesian optimization with an SMAC surrogate and EHVI acquisition, enhanced by a meta-surrogate warm-start, identifies low-compute yet accurate configurations across SparseBEV, SparseFusion, BevFusion, and BevFusion-e models, framerate skips, and interpolation methods, populating a lookup table for target NDS thresholds. All perception tuning runs on an RTX 3090 at 101.71 GFLOPs/J, exploring up to 80 configurations per difficulty level over a 5.5-hour dataset in approximately 14 hours. For decision making, driving is formulated as a regularized Markov decision process solved by P-DQN-style actor and value networks trained under a hybrid reward combining safety via time-to-collision, efficiency, comfort, and impact metrics, with a regularization term to penalize large deviations and curb Q-value extrapolation. We observe that EneAD yields a 1.9\u00d7\u20133.5\u00d7 reduction in perception energy consumption and a 3.9%\u20138.5% increase in driving range compared to the standard Auto baseline, matching Auto\u2019s safety performance while surpassing it in comfort and impact through minimal acceleration change and deceleration, with only a minor decrease in average velocity. These results demonstrate that jointly tuning perception \u201cknobs\u201d via Bayesian optimization and integrating a regularized reinforcement-learning decision module can reconcile energy efficiency and perception accuracy without sacrificing autonomy, signaling a shift toward energy-aware, component-wise design for sustainable autonomous driving. Acknowledging that experiments were limited to selected real-world and synthetic datasets, further validation is required to assess generalizability to diverse, unseen driving conditions.", "refined_summary": "State-of-the-art modular autonomous-driving perception systems consume hundreds to >1,000 W, limiting EV range, while existing compression or gate-based methods remain compute-intensive, degrade accuracy in complex scenes or lack scalable adaptation to diverse traffic and perturbed inputs. We introduce EneAD, an energy-efficient framework that adaptively configures model choice, framerate and interpolation. A lightweight uncertainty-aware classifier built on Swin-T plus a three-layer MLP with MC dropout, trained on six-camera 360\u00b0 images and one LiDAR from NuScenes-R/S (4:1 train/test) with NDS labels from SparseBEV, categorizes scenarios into four difficulty levels. Multi-objective Bayesian optimization with an SMAC surrogate and EHVI acquisition, warm-started by a meta-surrogate, selects low-compute yet accurate configurations across SparseBEV, SparseFusion, BEVFusion and BEVFusion-e, framerate skips and interpolation methods, storing results in a lookup table for target NDS thresholds. Tuning runs on an RTX 3090 at 101.71 GFLOPs/J, evaluating up to 80 configurations per level over a 5.5-hour dataset in ~14 h. For decision making, we formalize driving as a regularized MDP solved by P-DQN-style actor and value networks trained with a hybrid reward combining safety (time-to-collision), efficiency, comfort and impact plus a regularizer to limit Q-value extrapolation. EneAD achieves a 1.9\u00d7\u20133.5\u00d7 reduction in perception energy and a 3.9%\u20138.5% range increase versus the Auto baseline, matching its safety and surpassing comfort and impact with minimal acceleration changes at only minor velocity loss. Further validation is needed for unseen conditions."}, "..\\data\\raw\\htmls\\2510.25223v1.txt": {"abstract": "Event log data, recording fine-grained user actions and system events, represent one of the most valuable assets for modern digital services. However, the complexity and heterogeneity of industrial event logs-characterized by large scale, high dimensionality, diverse data types, and intricate temporal or relational structures-make feature engineering extremely challenging. Existing automatic feature engineering approaches, such as AutoML or genetic methods, often suffer from limited explainability, rigid predefined operations, and poor adaptability to complicated heterogeneous data.\n\nIn this paper, we propose FELA (Feature Engineering LLM Agents), a multi-agent evolutionary system that autonomously extracts meaningful and high-performing features from complex industrial event log data. FELA integrates the reasoning and coding capabilities of large language models (LLMs) with an insight-guided self-evolution paradigm. Specifically, FELA employs specialized agents-Idea Agents, Code Agents, and Critic Agents-to collaboratively generate, validate, and implement novel feature ideas. An Evaluation Agent summarizes feedback and updates a hierarchical knowledge base and dual-memory system to enable continual improvement. Moreover, FELA introduces an agentic evolution algorithm, combining reinforcement learning and genetic algorithm principles to balance exploration and exploitation across the idea space.\n\nExtensive experiments on real industrial datasets demonstrate that FELA can generate explainable, domain-relevant features that significantly improve model performance while reducing manual effort. Our results highlight the potential of LLM-based multi-agent systems as a general framework for automated, interpretable, and adaptive feature engineering in complex real-world environments.", "v1_summary": "Industrial event logs are massive, heterogeneous, and temporally and relationally complex, rendering manual feature engineering prohibitively time\u2010consuming and existing automated methods ill\u2010suited due to their opacity, limited generality, and inability to scale with sequential dependencies. We introduce Feature Engineering LLM Agents (FELA), a multi\u2010agent system in which specialized idea agents generate human\u2010readable feature hypotheses, code agents translate these hypotheses into executable transformations, and critic agents verify schema consistency and correctness; all are orchestrated by a hierarchical idea\u2013feature knowledge structure and an agentic evolution algorithm that integrates genetic and reinforcement\u2010learning principles with dual\u2010memory mechanisms for insight\u2010guided self\u2010evolution. Specifically, FELA operates on generic tabular and event\u2010log data D together with a data schema H and optional prior\u2010knowledge text, employing a three\u2010step ReAct/Reflexion reasoning loop in idea agents to propose, synthesize, and create structured feature concepts that include rationale, summary, and pseudocode. Code agents compile these concepts into executable transformations, critic agents enforce correctness through a bounded number of refinements, and an evaluation agent updates long\u2010 and short\u2010term memories based on downstream model metrics. Consequently, FELA outperforms fixed\u2010operation baselines and unguided free\u2010coding methods by enabling unrestricted code generation grounded in evolving high\u2010level insights. We observe clear AUC gains on industrial benchmarks: from 0.641 to 0.653 on Taobao conversion prediction and from 0.686 to 0.701 on Tencent user churn prediction. Thus, FELA demonstrates that a collaborative ecosystem of LLM\u2010based agents can meaningfully automate and elevate feature engineering for complex industrial event logs, delivering novel, high\u2010performing, and explainable features with minimal human oversight. Ultimately, this work signals a significant shift toward autonomous, adaptive feature engineering systems and lays a foundation for future extensions aimed at enhancing controllability, scalability, and cross\u2010domain applicability, despite current limitations in human\u2010in\u2010the\u2010loop interaction and validation on multimodal or dynamically changing datasets.", "refined_summary": "Manual feature engineering for massive, heterogeneous industrial event logs is time\u2010consuming; automated methods lack transparency, generality, and scalability with sequential dependencies. We present Feature Engineering LLM Agents (FELA), a multi\u2010agent framework where idea agents use a ReAct/Reflexion loop to generate human\u2010readable feature hypotheses (rationale, summary, pseudocode), code agents translate them into executable transformations, and critic agents verify schema consistency within bounded refinements. These agents are orchestrated by a hierarchical idea\u2013feature knowledge structure and an agentic evolution algorithm that fuses genetic programming with reinforcement learning and dual\u2010memory self\u2010evolution. FELA operates on generic tabular and event\u2010log data D, schema H, and optional prior\u2010knowledge text, executing a three\u2010step loop to propose, synthesize, and formalize feature concepts. A dedicated evaluation agent updates long\u2010 and short\u2010term memories based on downstream model metrics. On industrial benchmarks, FELA raises AUC from 0.641 to 0.653 on Taobao conversion prediction and from 0.686 to 0.701 on Tencent user churn prediction, surpassing fixed\u2010operation baselines and unguided free\u2010coding methods. FELA thus automates feature engineering for complex event logs, producing explainable, high\u2010performing features with minimal oversight and paving the way toward autonomous, adaptive feature engineering systems with enhanced controllability, scalability, and cross\u2010domain applicability."}}