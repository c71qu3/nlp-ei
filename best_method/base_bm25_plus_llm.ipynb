{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f3093a",
   "metadata": {},
   "source": [
    "# RAG Summarization with BM25 and LLMs\n",
    "\n",
    "### Implementation Description\n",
    "\n",
    "| Step | Description | Rationale | Comment |\n",
    "|------|-------------|-----------|---------|\n",
    "| Process source document | Break paper into sentences. Encode the full document together, and each sentence independently. Tokenize and lemmatize text. |  | Using `verbatim_rag` to read HTML files. |\n",
    "| Select relevant sentences | Calculate **BM25** similarity score between sentences and full document. Request an LLM model to extract the theme | This was the most reliable method for selecting sentences from the baseline results. |  |\n",
    "| Shape summary from sentences | Use **LLM** to shape selected sentences into summary. | Plain `rank_bm25` results are disjointed and out of order. |  |\n",
    "| Append supporting citation | Use `verbatim_rag` to identify supporting material in the original paper. |  |  |\n",
    "\n",
    "#### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed50a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/guaya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import os\n",
    "import re\n",
    "from verbatim_rag.schema import DocumentSchema\n",
    "from IPython.display import display, Markdown\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from verbatim_rag.chunker_providers import MarkdownChunkerProvider\n",
    "from verbatim_rag.embedding_providers import SentenceTransformersProvider\n",
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag import VerbatimIndex, VerbatimRAG\n",
    "from verbatim_rag.core import LLMClient\n",
    "\n",
    "\n",
    "assert nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fbf9a",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "\n",
    "Using `verbatim_rag` to load documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474179e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_URL = \"https://arxiv.org/pdf/\"\n",
    "HTML_PATH = os.path.join(\"..\", \"data\", \"raw\", \"htmls\")\n",
    "\n",
    "DOCUMENT_ID = [\n",
    "    '2511.21398v1',\n",
    "    '2511.21444v1',\n",
    "    '2511.21460v1',\n",
    "    '2511.21471v1',\n",
    "    '2511.21522v1',\n",
    "    '2511.21569v1',\n",
    "    '2511.21570v1',\n",
    "    '2511.21591v1',\n",
    "    '2511.21636v1',\n",
    "    '2511.21678v1',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7eb15e",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "\n",
    "Method to extract abstract using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b79473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_from_(paper: str) -> Optional[str]:\n",
    "    \"\"\"Get abstract from Markdown text.\"\"\"\n",
    "    match = re.search(r'## Abstract\\s*(.+?)(?=\\n##)', paper, re.DOTALL)\n",
    "    if match:\n",
    "        abstract = match.group(1).strip()\n",
    "        # abstract = re.sub(r\"^\\s*\\.\\s*\\n*\", \"\", abstract)\n",
    "        return abstract\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee831430",
   "metadata": {},
   "source": [
    "Method to display Markdown strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c980334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_markdown_(text: str) -> None:\n",
    "    \"\"\"Print Markdown string.\"\"\"\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf5e2c",
   "metadata": {},
   "source": [
    "## Process Source Documents\n",
    "\n",
    "The `verbatim_rag` library captures the abstract correctly in all examples, unlike the processed used for the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181993e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:37:15,921 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:16,008 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:16,009 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:16,023 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-06 13:37:16,025 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-06 13:37:16,026 - INFO - Processing document 2511.21398v1.html\n",
      "2026-01-06 13:37:16,135 - INFO - Finished converting document 2511.21398v1.html in 0.22 sec.\n",
      "2026-01-06 13:37:16,186 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:16,203 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:16,204 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:16,204 - INFO - Processing document 2511.21444v1.html\n",
      "2026-01-06 13:37:16,232 - INFO - Finished converting document 2511.21444v1.html in 0.05 sec.\n",
      "2026-01-06 13:37:16,249 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:16,543 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:16,543 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:16,544 - INFO - Processing document 2511.21460v1.html\n",
      "2026-01-06 13:37:16,635 - INFO - Finished converting document 2511.21460v1.html in 0.39 sec.\n",
      "2026-01-06 13:37:16,679 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:16,744 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:16,744 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:16,745 - INFO - Processing document 2511.21471v1.html\n",
      "2026-01-06 13:37:16,825 - INFO - Finished converting document 2511.21471v1.html in 0.15 sec.\n",
      "2026-01-06 13:37:16,855 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:17,266 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:17,267 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:17,267 - INFO - Processing document 2511.21522v1.html\n",
      "2026-01-06 13:37:17,577 - INFO - Finished converting document 2511.21522v1.html in 0.73 sec.\n",
      "2026-01-06 13:37:17,658 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:18,186 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:18,186 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:18,187 - INFO - Processing document 2511.21569v1.html\n",
      "2026-01-06 13:37:18,373 - INFO - Finished converting document 2511.21569v1.html in 0.72 sec.\n",
      "2026-01-06 13:37:18,432 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:18,445 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:18,445 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:18,446 - INFO - Processing document 2511.21570v1.html\n",
      "2026-01-06 13:37:18,475 - INFO - Finished converting document 2511.21570v1.html in 0.05 sec.\n",
      "2026-01-06 13:37:18,496 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:18,544 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:18,544 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:18,545 - INFO - Processing document 2511.21591v1.html\n",
      "2026-01-06 13:37:18,615 - INFO - Finished converting document 2511.21591v1.html in 0.12 sec.\n",
      "2026-01-06 13:37:18,690 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:18,839 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:18,840 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:18,840 - INFO - Processing document 2511.21636v1.html\n",
      "2026-01-06 13:37:19,089 - INFO - Finished converting document 2511.21636v1.html in 0.40 sec.\n",
      "2026-01-06 13:37:19,154 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-06 13:37:19,225 - INFO - Going to convert document batch...\n",
      "2026-01-06 13:37:19,226 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-06 13:37:19,226 - INFO - Processing document 2511.21678v1.html\n",
      "2026-01-06 13:37:19,326 - INFO - Finished converting document 2511.21678v1.html in 0.17 sec.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "papers = []\n",
    "abstracts = []\n",
    "\n",
    "for document in DOCUMENT_ID:\n",
    "    # paper = DocumentSchema.from_url(url=ARXIV_URL + document)\n",
    "    document = DocumentSchema.from_url(url=os.path.join(HTML_PATH, document + '.html'))\n",
    "    documents.append(document)\n",
    "    papers.append(document.content)\n",
    "    abstracts.append(abstract_from_(papers[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d119cb",
   "metadata": {},
   "source": [
    "#### Tokenize & Lemmatize\n",
    "\n",
    "Method to pre-process papers for BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee7a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_(paper: str) -> Tuple[List[List[str]], List[str]]:\n",
    "    \"\"\"Process paper into plain and lemmatized sentences.\"\"\"\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    sentence_split = nltk.sent_tokenize(paper)\n",
    "    word_split = [\n",
    "        nltk.word_tokenize(sentence) for\n",
    "        sentence in sentence_split]\n",
    "\n",
    "    plain = []\n",
    "    lemmatized = []\n",
    "    for i, sentence in enumerate(word_split):\n",
    "        lemmatized.append([])\n",
    "        plain.append(sentence_split[i].replace('\\n', ''))\n",
    "\n",
    "        for word in sentence:\n",
    "            token = word.lower()\n",
    "            if token.isalpha() and token not in stop_words:\n",
    "                lemmatized[-1].append(lemmatizer.lemmatize(token))\n",
    "\n",
    "        # Discard sentences where\n",
    "        # lemmatization returns nothing\n",
    "        if not lemmatized[-1]:\n",
    "            lemmatized.pop()\n",
    "            plain.pop()\n",
    "\n",
    "    return lemmatized, plain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777bf31",
   "metadata": {},
   "source": [
    "Pre-process each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f944fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_papers = []\n",
    "plain_sentences = []\n",
    "\n",
    "for paper in papers:\n",
    "    lemmatized, plain = pre_process_(paper)\n",
    "    lemmatized_papers.append(lemmatized)\n",
    "    plain_sentences.append(plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1886e",
   "metadata": {},
   "source": [
    "## Select Relevant Sentences\n",
    "\n",
    "Method to rank sentences using BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7492e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rank_(lemmatized: str) -> List[int]:\n",
    "    \"\"\"Order index of sentences by BM25 similarity to whole document.\"\"\"\n",
    "    sentences = BM25Okapi(lemmatized)\n",
    "    scores = sentences.get_scores(sum(lemmatized, []))\n",
    "\n",
    "    indexes = sorted(\n",
    "        range(len(scores)),\n",
    "        key=lambda i: scores[i],\n",
    "        reverse=True)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492dca9",
   "metadata": {},
   "source": [
    "Calculate sentence ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597c301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_rankings = []\n",
    "\n",
    "for sentences in lemmatized_papers:\n",
    "    bm25_rankings.append(bm25_rank_(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cac554",
   "metadata": {},
   "source": [
    "Method to gather plain sentences selected by BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff89f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plain_(sentences: List[str], indexes: List[int], n: int=15) -> List[str]:\n",
    "    \"\"\"Get top n sentences according to indexes.\"\"\"\n",
    "    result = []\n",
    "    for i in indexes[:n]:\n",
    "        result.append(sentences[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "# print(get_plain_(plain_sentences[2], bm25_rankings[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91c858",
   "metadata": {},
   "source": [
    "## Shape Summary from Sentences\n",
    "\n",
    "Query used to format selected sentences into a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3324526",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = \"\"\"\n",
    "These are 15 sentences selected from a scientific paper:\n",
    "\n",
    "{sentences}\n",
    "\n",
    "Please format a concise summary of this paper by rewriting these sentences.\n",
    "The summary can re-order the sentences.\n",
    "The summary can discard least relevant sentences.\n",
    "The summary has to be 5 to 8 sentences long.\n",
    "Write from the perspective of the reader using phrases like:\n",
    "  - \"the paper claims to ...\"\n",
    "  - \"the authors state that ...\".\n",
    "  - \"the article asserts ...\".\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "OPENAI_MODEL = 'o4-mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd7869",
   "metadata": {},
   "source": [
    "Method to query LLM model to shape summary.\n",
    "\n",
    "**Note:** Requires OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647aaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert key\n",
    "\n",
    "\n",
    "def build_summary_(sentences: List[str]) -> str:\n",
    "    client = OpenAI(api_key=key)\n",
    "    response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        instructions=\"Only reply with the rewritten paragraph.\",\n",
    "        input=QUERY_PROMPT.format(sentences=sentences)\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6ea89",
   "metadata": {},
   "source": [
    "Generate all summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1896f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:37:35,701 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:37:44,020 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:38:00,420 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:38:13,114 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:38:38,137 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:38:49,248 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:38:59,194 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:39:05,718 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:39:22,822 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:39:32,447 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "for s, r in zip(plain_sentences, bm25_rankings):\n",
    "    summaries.append(build_summary_(get_plain_(s, r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1faff93",
   "metadata": {},
   "source": [
    "Save temporary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a23120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output.json', 'w') as f:\n",
    "#     json.dump(summaries, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c92e7",
   "metadata": {},
   "source": [
    "## Append Supporting Citation\n",
    "\n",
    "Method to find supporting evidence for summary using `verbatim_rag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05dad21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evidence_(summary: str, document: DocumentSchema, n: int) -> str:\n",
    "\n",
    "    chunker = MarkdownChunkerProvider(\n",
    "        min_chunk_size=500,\n",
    "        max_chunk_size=5000)\n",
    "    dense_provider = SentenceTransformersProvider(\n",
    "        model_name=\"ibm-granite/granite-embedding-english-r2\",\n",
    "        device='cpu')\n",
    "    vector_store = LocalMilvusStore(\n",
    "        db_path=f\"./rag_test_{n}.db\",\n",
    "        collection_name=f'rag_test_{n}',\n",
    "        dense_dim=dense_provider.get_dimension(),\n",
    "        enable_dense=True,\n",
    "        enable_sparse=False,\n",
    "        nlist=16384)\n",
    "    index = VerbatimIndex(\n",
    "        vector_store=vector_store,\n",
    "        dense_provider=dense_provider,\n",
    "        chunker_provider=chunker)\n",
    "    index.add_documents([document])\n",
    "\n",
    "    llm_client = LLMClient(model=OPENAI_MODEL, temperature=1.0)\n",
    "    rag = VerbatimRAG(index, llm_client=llm_client)\n",
    "    query_string = f\"Find supporting evidence for this summary:\\n {summary}\"\n",
    "\n",
    "    response = rag.query(query_string)\n",
    "    return response.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf77a1d",
   "metadata": {},
   "source": [
    "Find all citations for the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04e316d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 14:27:09,325 - INFO - Load pretrained SentenceTransformer: ibm-granite/granite-embedding-english-r2\n",
      "2026-01-06 14:27:13,036 - INFO - Loaded SentenceTransformers model: ibm-granite/granite-embedding-english-r2\n",
      "2026-01-06 14:27:14,180 - INFO - Created indexes for collection: rag_test_9\n",
      "2026-01-06 14:27:14,204 - INFO - Created documents collection: rag_test_9_documents\n",
      "2026-01-06 14:27:14,204 - INFO - Connected to Milvus Lite: ./rag_test_9.db\n",
      "Batches: 100%|██████████| 1/1 [00:22<00:00, 22.19s/it]]\n",
      "2026-01-06 14:27:36,485 - INFO - Added 25 vectors to Milvus\n",
      "2026-01-06 14:27:36,512 - INFO - Added 1 documents to Milvus\n",
      "Adding documents: 100%|██████████| 1/1 [00:22<00:00, 22.31s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relevant spans...\n",
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 14:28:38,926 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Span not found verbatim in document: 'Finally, the solver generates the answer by conditioning on both the original inputs and the retriev...'\n",
      "Warning: Span not found verbatim in document: 'Problem Formulation. Consider a sequence of multimodal inputs (x₁, x₂, ..., xₙ), where each input xᵢ...'\n",
      "Warning: Span not found verbatim in document: 'As illustrated in Figure 2 (a), the cycle operates as follows: given problem xᵢ, the system performs...'\n",
      "Processing spans...\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 14:28:44,593 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "n = 9\n",
    "citation = find_evidence_(summaries[n], documents[n], n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf5772",
   "metadata": {},
   "source": [
    "Preview summary and evidence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d76d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "MLLMs exhibit strong reasoning on isolated queries, yet they operate *de novo* -solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated , preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem , a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge-preserving stable, generalizable strategies while avoiding catastrophic forgetting.\n",
       "\n",
       "Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction-hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at\n",
       "\n",
       "[https://weihao-bo.github.io/ViLoMeo-page/](https://weihao-bo.github.io/ViLoMeo-page/) ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The paper claims to introduce a dual-stream memory architecture for multimodal LLMs that separately stores textual reasoning guidelines and visual priors. The authors state that each memory bank is queried via cosine similarity thresholds to retrieve top-k relevant entries, followed by a two-stage filtering process that first ranks visual candidates by perceptual embedding and then by textual relevance. The article asserts that an LLM-based error analysis module classifies reasoning failures and generates abstracted logic and visual guidelines whenever the model’s prediction diverges from the ground truth. The authors state that these new guidelines are merged into the respective memory banks through replace-or-merge operations, ensuring that both streams evolve with fresh, context-specific knowledge. The paper claims that final answers are generated by conditioning on the original image and question along with the retrieved dual-stream memories, integrating perception, question understanding, and guided reasoning. The article asserts that this cyclical process of retrieval, error analysis, guideline synthesis, and memory update systematically addresses reasoning errors and common visual pitfalls. The authors conclude that their approach yields more accurate, explainable solutions by continually refining the model’s perceptual and logical priors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here is the supporting evidence for the summary: \n",
       "- **Memory Retrieval**: [1] (c) Memory Retrieval : Specialized dual-stream retrieval mechanism. Visual memories undergo a two-stage process involving image-embedding retrieval followed by question-specific retrieval, since visual information must be conditioned on both image content and the textual query. Logical memories are retrieved through problem analysis and text-embedding similarity. \n",
       "- **Memory Generation**: [2] (b) Memory Generation : An error-attribution framework that employs an LLM for logical analysis and an MLLM for visual analysis, producing structured memory schemas through similarity-based merge and create operations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_markdown_(abstracts[n])\n",
    "print_markdown_(summaries[n])\n",
    "print_markdown_(citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cbf47",
   "metadata": {},
   "source": [
    "### Conclusions on Base Approach\n",
    "\n",
    "In general, the summaries capture more details on what the paper attempts to accomplish.\n",
    "There is often a enumeration of the steps followed in the research to a deeper level than discussed in the abstract.\n",
    "The level of detail can sometimes render statements that are missing context, but the context can often be found in the evidence section.\n",
    "Evidence generation from the whole summary often results in clear citations for the first half of the summary while ignoring the second.\n",
    "\n",
    "| Paper Index | Accurate Summary | Corroborating Evidence | Observation |\n",
    "|-------------|------------------|------------------------|-------------|\n",
    "| 0 | Yes. | No. | Consistently returns only 1 piece of evidence, often from the conclusion. |\n",
    "| 1 | Yes. | Yes. | Evidence is relevant and with high coverage. |\n",
    "| 2 | Yes. | Yes. |  |\n",
    "| 3 | Yes. | Yes. | Evidence focused only on first half of summary. |\n",
    "| 4 | - | - | Somehow VSCodium freezes everytime this runs. |\n",
    "| 5 | No, minor omission mixing different statistics. | Yes. | Evidence offers insight into missing statements in summary. |\n",
    "| 6 | Yes. | Yes. | Accurate summary and complete evidence. |\n",
    "| 7 | No. | Yes. | The summary looks into details of the steps in the research missing the overall point stated in the abstract. |\n",
    "| 8 | Yes. | No. | Summary goes deep is disjointed and goes into details while failing to describe the general idea. No evidence was found. |\n",
    "| 9 | Yes | Yes, but incomplete. |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab70717",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
