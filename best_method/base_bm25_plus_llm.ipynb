{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f3093a",
   "metadata": {},
   "source": [
    "# RAG Summarization with BM25 and LLMs\n",
    "\n",
    "### Implementation Description\n",
    "\n",
    "| Step | Description | Rationale | Comment |\n",
    "|------|-------------|-----------|---------|\n",
    "| Process source document | Break paper into sentences. Encode the full document together, and each sentence independently. Tokenize and lemmatize text. |  | Using `verbatim_rag` to read HTML files. |\n",
    "| Select relevant sentences | Calculate **BM25** similarity score between sentences and full document. Request an LLM model to extract the theme | This was the most reliable method for selecting sentences from the baseline results. |  |\n",
    "| Shape summary from sentences | Use **LLM** to shape selected sentences into summary. | Plain `rank_bm25` results are disjointed and out of order. |  |\n",
    "| Append supporting citation | Use `verbatim_rag` to identify supporting material in the original paper. |  |  |\n",
    "\n",
    "#### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed50a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from verbatim_rag.schema import DocumentSchema\n",
    "from verbatim_rag.chunker_providers import MarkdownChunkerProvider\n",
    "from verbatim_rag.embedding_providers import SentenceTransformersProvider\n",
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag import VerbatimIndex, VerbatimRAG\n",
    "from verbatim_rag.core import LLMClient\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "assert nltk.download('wordnet')\n",
    "assert nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32d57975-5d8b-45a3-910d-a0cc8de942aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bf7c8e3-7099-49c9-8c5c-6909ec9e1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fbf9a",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "\n",
    "Using `verbatim_rag` to load documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "474179e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_URL = \"https://arxiv.org/pdf/\"\n",
    "HTML_PATH = os.path.join(\"..\", \"data\", \"raw\", \"htmls\")\n",
    "\n",
    "documents = glob.glob(f\"{HTML_PATH}/*.txt\")\n",
    "documents\n",
    "\n",
    "DOCUMENT_ID = [\n",
    "    '2511.21398v1',\n",
    "    '2511.21444v1',\n",
    "    '2511.21460v1',\n",
    "    '2511.21471v1',\n",
    "    '2511.21522v1',\n",
    "    '2511.21569v1',\n",
    "    '2511.21570v1',\n",
    "    '2511.21591v1',\n",
    "    '2511.21636v1',\n",
    "    '2511.21678v1',\n",
    "]\n",
    "\n",
    "DOCUMENT_ID = documents[0:10]\n",
    "\n",
    "#DOCUMENT_ID = [\"..\\\\data\\\\raw\\\\htmls\\\\2510.25320v1.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7eb15e",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "\n",
    "Method to extract abstract using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b79473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_from_(paper: str) -> Optional[str]:\n",
    "    \"\"\"Get abstract from Markdown text.\"\"\"\n",
    "    match = re.search(r'## Abstract\\s*(.+?)(?=\\n##)', paper, re.DOTALL)\n",
    "    if match:\n",
    "        abstract = match.group(1).strip()\n",
    "        # abstract = re.sub(r\"^\\s*\\.\\s*\\n*\", \"\", abstract)\n",
    "        return abstract\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee831430",
   "metadata": {},
   "source": [
    "Method to display Markdown strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c980334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_markdown_(text: str) -> None:\n",
    "    \"\"\"Print Markdown string.\"\"\"\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf5e2c",
   "metadata": {},
   "source": [
    "## Process Source Documents\n",
    "\n",
    "The `verbatim_rag` library captures the abstract correctly in all examples, unlike the processed used for the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "181993e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:07:35,448 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:35,568 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:35,570 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:35,571 - INFO - Processing document 2510.24832v1.txt\n",
      "2026-01-12 20:07:35,773 - INFO - Finished converting document 2510.24832v1.txt in 0.33 sec.\n",
      "2026-01-12 20:07:35,884 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:36,204 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:36,205 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:36,205 - INFO - Processing document 2510.25005v1.txt\n",
      "2026-01-12 20:07:36,800 - INFO - Finished converting document 2510.25005v1.txt in 0.92 sec.\n",
      "2026-01-12 20:07:36,966 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:37,180 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:37,180 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:37,181 - INFO - Processing document 2510.25007v1.txt\n",
      "2026-01-12 20:07:37,352 - INFO - Finished converting document 2510.25007v1.txt in 0.39 sec.\n",
      "2026-01-12 20:07:37,444 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:37,853 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:37,854 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:37,854 - INFO - Processing document 2510.25014v1.txt\n",
      "2026-01-12 20:07:38,080 - INFO - Finished converting document 2510.25014v1.txt in 0.64 sec.\n",
      "2026-01-12 20:07:38,170 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:38,222 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:38,223 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:38,224 - INFO - Processing document 2510.25065v1.txt\n",
      "2026-01-12 20:07:38,306 - INFO - Finished converting document 2510.25065v1.txt in 0.14 sec.\n",
      "2026-01-12 20:07:38,358 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:38,606 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:38,607 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:38,608 - INFO - Processing document 2510.25091v1.txt\n",
      "2026-01-12 20:07:39,016 - INFO - Finished converting document 2510.25091v1.txt in 0.66 sec.\n",
      "2026-01-12 20:07:39,170 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:39,355 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:39,356 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:39,357 - INFO - Processing document 2510.25101v1.txt\n",
      "2026-01-12 20:07:39,609 - INFO - Finished converting document 2510.25101v1.txt in 0.44 sec.\n",
      "2026-01-12 20:07:39,729 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:39,821 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:39,822 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:39,823 - INFO - Processing document 2510.25179v1.txt\n",
      "2026-01-12 20:07:39,903 - INFO - Finished converting document 2510.25179v1.txt in 0.18 sec.\n",
      "2026-01-12 20:07:39,972 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:40,873 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:40,874 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:40,875 - INFO - Processing document 2510.25205v1.txt\n",
      "2026-01-12 20:07:41,200 - INFO - Finished converting document 2510.25205v1.txt in 1.24 sec.\n",
      "2026-01-12 20:07:41,345 - INFO - detected formats: [<InputFormat.HTML: 'html'>]\n",
      "2026-01-12 20:07:41,454 - INFO - Going to convert document batch...\n",
      "2026-01-12 20:07:41,455 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2026-01-12 20:07:41,456 - INFO - Processing document 2510.25223v1.txt\n",
      "2026-01-12 20:07:41,664 - INFO - Finished converting document 2510.25223v1.txt in 0.32 sec.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "papers = []\n",
    "abstracts = []\n",
    "\n",
    "for document in DOCUMENT_ID:\n",
    "    # paper = DocumentSchema.from_url(url=ARXIV_URL + document)\n",
    "    #document = DocumentSchema.from_url(url=os.path.join(HTML_PATH, document + '.txt'))\n",
    "    document = DocumentSchema.from_url(url=document)\n",
    "    documents.append(document)\n",
    "    papers.append(document.content)\n",
    "    abstracts.append(abstract_from_(papers[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d119cb",
   "metadata": {},
   "source": [
    "#### Tokenize & Lemmatize\n",
    "\n",
    "Method to pre-process papers for BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ee7a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_(paper: str) -> Tuple[List[List[str]], List[str]]:\n",
    "    \"\"\"Process paper into plain and lemmatized sentences.\"\"\"\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    sentence_split = nltk.sent_tokenize(paper)\n",
    "    word_split = [\n",
    "        nltk.word_tokenize(sentence) for\n",
    "        sentence in sentence_split]\n",
    "\n",
    "    plain = []\n",
    "    lemmatized = []\n",
    "    for i, sentence in enumerate(word_split):\n",
    "        lemmatized.append([])\n",
    "        plain.append(sentence_split[i].replace('\\n', ''))\n",
    "\n",
    "        for word in sentence:\n",
    "            token = word.lower()\n",
    "            if token.isalpha() and token not in stop_words:\n",
    "                lemmatized[-1].append(lemmatizer.lemmatize(token))\n",
    "\n",
    "        # Discard sentences where\n",
    "        # lemmatization returns nothing\n",
    "        if not lemmatized[-1]:\n",
    "            lemmatized.pop()\n",
    "            plain.pop()\n",
    "\n",
    "    return lemmatized, plain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777bf31",
   "metadata": {},
   "source": [
    "Pre-process each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34f944fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_papers = []\n",
    "plain_sentences = []\n",
    "\n",
    "for paper in papers:\n",
    "    lemmatized, plain = pre_process_(paper)\n",
    "    lemmatized_papers.append(lemmatized)\n",
    "    plain_sentences.append(plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1886e",
   "metadata": {},
   "source": [
    "## Select Relevant Sentences\n",
    "\n",
    "Method to rank sentences using BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7492e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rank_(lemmatized: str) -> List[int]:\n",
    "    \"\"\"Order index of sentences by BM25 similarity to whole document.\"\"\"\n",
    "    sentences = BM25Okapi(lemmatized)\n",
    "    scores = sentences.get_scores(sum(lemmatized, []))\n",
    "\n",
    "    indexes = sorted(\n",
    "        range(len(scores)),\n",
    "        key=lambda i: scores[i],\n",
    "        reverse=True)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492dca9",
   "metadata": {},
   "source": [
    "Calculate sentence ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "597c301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_rankings = []\n",
    "\n",
    "for sentences in lemmatized_papers:\n",
    "    bm25_rankings.append(bm25_rank_(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b583bcc-5cad-4e61-8d42-18530d44cf09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Given a feature implementation produced by the idea agent, along with high-quality examples from previous attempts and the data schema as concrete guidance, the code agent generates highly executable and arbitrarily sophisticated code Œ∏ t \\\\theta\\\\_{t} to transform raw features:|    | Œ∏ t = ùíú code  ( { { d i , j } j = 0 M i } i = 0 k , ‚Ñã , Œ∏ j &lt; t ) , \\\\theta\\\\_{t}=\\\\mathcal{A}\\\\_{\\\\text{code}}(\\\\{\\\\{d\\\\_{i,j}\\\\}\\\\_{j=0}^{M\\\\_{i}}\\\\}\\\\_{i=0}^{k},\\\\mathcal{H},\\\\theta\\\\_{j&lt;t}),   |    | (6)   ||----|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|-------|where { { d i , j } j = 0 M i } i = 0 k \\\\{\\\\{d\\\\_{i,j}\\\\}\\\\_{j=0}^{M\\\\_{i}}\\\\}\\\\_{i=0}^{k} denotes selected feature implementations from k k ideas, and t t denotes the iteration step in the FELA system.',\n",
       " 'In this paper, we propose FELA (Feature Engineering LLM Agents), a multi-agent evolutionary system that autonomously extracts meaningful and high-performing features from complex industrial event log data.',\n",
       " '<!-- image -->## VI ConclusionIn this work, we introduced FELA (Feature Engineering LLM Agents) , a comprehensive multi-agent collaboration system designed to automate and enhance feature engineering for complex industrial event log data.',\n",
       " 'To address the aforementioned challenges, we propose a comprehensive multi-agent collaborative system for automatic feature extraction from industrial event log data, termed Feature Engineering LLM Agents (FELA) .',\n",
       " 'Since LLMs are prone to hallucination [ [13](https://arxiv.org/html/2510.25223v1#bib.bib13) , [14](https://arxiv.org/html/2510.25223v1#bib.bib14) ] , particularly in high-dimensional and complex domains, FELA introduces critic agents to evaluate and verify outputs from both idea and code agents.',\n",
       " '### II-B LLMs on data centric tasksRecent advances in large language models (LLMs) have demonstrated their ability to leverage pre-trained knowledge for novel tasks through techniques such as prompt engineering and in-context learning, often without additional task-specific training [ [31](https://arxiv.org/html/2510.25223v1#bib.bib31) , [32](https://arxiv.org/html/2510.25223v1#bib.bib32) ] .',\n",
       " 'The idea agents propose new feature concepts or experimental ideas, which are then translated into executable feature engineering code by the code agents .',\n",
       " '## II BACKGROUND &amp; RELATED WORK### II-A Feature Engineering on Industrial DataFeature engineering-also referred to as feature transformation or feature generation-denotes the process of constructing, transforming, and selecting informative variables from raw data to enhance the performance and generalization of machine learning models [ [17](https://arxiv.org/html/2510.25223v1#bib.bib17) ] .',\n",
       " 'Through these designs, FELA makes the following key academic contributions:- ‚Ä¢ To the best of our knowledge, FELA is the first agentic system capable of performing automated feature engineering on industrial-scale event log data .',\n",
       " 'Leveraging the reasoning capabilities and domain priors of large language models, idea agents autonomously generate novel feature hypotheses grounded in dataset semantics.',\n",
       " 'Additionally, the critic agent can enhance the correctness and consistency of the generated idea and code, which improves the robustness in feature generation and evaluation in FELA.',\n",
       " '# FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log DataKun OuyangHaoyu WangDong Fang###### AbstractEvent log data, recording fine-grained user actions and system events, represent one of the most valuable assets for modern digital services.',\n",
       " 'Formally, the two-layer hierarchical knowledge base ùí¶ \\\\mathcal{K} can be expressed as|    | ùí¶ = { ( I i , { d i , j } j = 1 M i ) } i = 1 N I , \\\\mathcal{K}=\\\\{(I\\\\_{i},\\\\{d\\\\_{i,j}\\\\}\\\\_{j=1}^{M\\\\_{i}})\\\\}\\\\_{i=1}^{N\\\\_{I}},   |    ||----|------------------------------------------------------------------------------------------------------------------------------|----|where M i M\\\\_{i} denotes the number of features in i i th idea.',\n",
       " '### V-C Feature Engineering Performance (RQ1)Table [II](https://arxiv.org/html/2510.25223v1#S5.T2) presents the feature engineering performance of FELA and baseline methods on the tabular Dia dataset.',\n",
       " 'At each iteration step t t , an idea agent takes as input the current knowledge database ùí¶ t \\\\mathcal{K}\\\\_{t} and selects one of the following actions: (1) generate a new feature from an existing insight, (2) synthesize a new insight from existing insights, or (3) create an entirely new idea.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for s, r in zip(plain_sentences, bm25_rankings):\n",
    "#     print(s, r)\n",
    "#s, r\n",
    "\n",
    "get_plain_(sentences=s, indexes=r, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cac554",
   "metadata": {},
   "source": [
    "Method to gather plain sentences selected by BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff89f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plain_(sentences: List[str], indexes: List[int], n: int=15) -> List[str]:\n",
    "    \"\"\"Get top n sentences according to indexes.\"\"\"\n",
    "    result = []\n",
    "    for i in indexes[:n]:\n",
    "        result.append(sentences[i])\n",
    "    return result\n",
    "\n",
    "# print(get_plain_(plain_sentences[2], bm25_rankings[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91c858",
   "metadata": {},
   "source": [
    "## Shape Summary from Sentences\n",
    "\n",
    "Query used to format selected sentences into a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3324526",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = \"\"\"\n",
    "These are 15 sentences selected from a scientific paper:\n",
    "\n",
    "{sentences}\n",
    "\n",
    "Please format a concise summary of this paper by rewriting these sentences.\n",
    "The summary can re-order the sentences.\n",
    "The summary can discard least relevant sentences.\n",
    "The summary has to be 5 to 8 sentences long.\n",
    "Write from the perspective of the reader using phrases like:\n",
    "  - \"the paper claims to ...\"\n",
    "  - \"the authors state that ...\".\n",
    "  - \"the article asserts ...\".\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "OPENAI_MODEL = 'o4-mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd7869",
   "metadata": {},
   "source": [
    "Method to query LLM model to shape summary.\n",
    "\n",
    "**Note:** Requires OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "647aaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert key\n",
    "\n",
    "\n",
    "def build_summary_(sentences: List[str]) -> str:\n",
    "    client = OpenAI(api_key=key)\n",
    "    response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        instructions=\"Only reply with the rewritten paragraph.\",\n",
    "        input=QUERY_PROMPT.format(sentences=sentences)\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6ea89",
   "metadata": {},
   "source": [
    "Generate all summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1896f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:08:01,839 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:08:14,988 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:08:24,933 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:08:36,449 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:08:48,398 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:08:56,744 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:09:07,523 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:09:19,224 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:09:28,227 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2026-01-12 20:09:43,667 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "for s, r in zip(plain_sentences, bm25_rankings):\n",
    "    summaries.append(build_summary_(get_plain_(s, r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1faff93",
   "metadata": {},
   "source": [
    "Save temporary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a23120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output.json', 'w') as f:\n",
    "#     json.dump(summaries, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b4472-4e2b-41ef-9df2-ad6eecc0a9cf",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97334280-cf2e-48e7-9e42-f91c1a029920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1250, 1049, 1187, 1168, 903, 1330, 978, 1179, 1369, 1483]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)\n",
    "[len(s) for s in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "56f82206-dc85-4519-a338-69737ed7580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:34:03,230 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:05,241 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:06,313 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:07,493 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:08,631 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:09,716 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:11,208 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:12,491 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:13,553 - INFO - Using default tokenizer.\n",
      "2026-01-12 21:34:14,744 - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>method</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bert_score_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.24832v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.486911</td>\n",
       "      <td>0.225131</td>\n",
       "      <td>0.647632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25005v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.497903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25007v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>0.154613</td>\n",
       "      <td>0.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25014v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.391421</td>\n",
       "      <td>0.166220</td>\n",
       "      <td>0.573384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25065v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25091v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.328228</td>\n",
       "      <td>0.148796</td>\n",
       "      <td>0.553313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25101v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.606928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25179v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.492582</td>\n",
       "      <td>0.272997</td>\n",
       "      <td>0.693892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25205v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.384937</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.603851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>..\\data\\raw\\htmls\\2510.25223v1.txt</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.512249</td>\n",
       "      <td>0.240535</td>\n",
       "      <td>0.667206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             paper_id method    rouge1    rougeL  \\\n",
       "0  ..\\data\\raw\\htmls\\2510.24832v1.txt   BM25  0.486911  0.225131   \n",
       "1  ..\\data\\raw\\htmls\\2510.25005v1.txt   BM25  0.194915  0.127119   \n",
       "2  ..\\data\\raw\\htmls\\2510.25007v1.txt   BM25  0.339152  0.154613   \n",
       "3  ..\\data\\raw\\htmls\\2510.25014v1.txt   BM25  0.391421  0.166220   \n",
       "4  ..\\data\\raw\\htmls\\2510.25065v1.txt   BM25  0.387324  0.176056   \n",
       "5  ..\\data\\raw\\htmls\\2510.25091v1.txt   BM25  0.328228  0.148796   \n",
       "6  ..\\data\\raw\\htmls\\2510.25101v1.txt   BM25  0.364130  0.163043   \n",
       "7  ..\\data\\raw\\htmls\\2510.25179v1.txt   BM25  0.492582  0.272997   \n",
       "8  ..\\data\\raw\\htmls\\2510.25205v1.txt   BM25  0.384937  0.184100   \n",
       "9  ..\\data\\raw\\htmls\\2510.25223v1.txt   BM25  0.512249  0.240535   \n",
       "\n",
       "   bert_score_f1  \n",
       "0       0.647632  \n",
       "1       0.497903  \n",
       "2       0.575800  \n",
       "3       0.573384  \n",
       "4       0.596000  \n",
       "5       0.553313  \n",
       "6       0.606928  \n",
       "7       0.693892  \n",
       "8       0.603851  \n",
       "9       0.667206  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_results = []\n",
    "\n",
    "for k, curr_abstract, summary in zip(DOCUMENT_ID, abstracts, summaries):\n",
    "    rouge_scores_llm = calculate_rouge_score(curr_abstract,\n",
    "                                             summary)\n",
    "    bert_scores_llm = calculate_bert_score(curr_abstract, \n",
    "                                           summary)\n",
    "    \n",
    "    scoring_results.append(\n",
    "        {\n",
    "            \"paper_id\": k, \n",
    "            \"method\": \"BM25\",\n",
    "            \"rouge1\": rouge_scores_llm[\"rouge1_fmeasure\"], \n",
    "            \"rougeL\": rouge_scores_llm[\"rougeL_fmeasure\"],\n",
    "            \"bert_score_f1\": bert_scores_llm[\"bertscore_f1\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_results = pd.DataFrame(scoring_results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08dfd357-45da-4ec0-90ce-cc1916da7965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper claims to model the space of solution paths for open‚Äêended queries as a ‚ÄúReasoning Tree,‚Äù where each node represents an intermediate reasoning step and each path a potential solution trajectory. The authors argue that existing curriculum learning strategies rely on final solution accuracy and thus overlook richer query‚Äêlevel characteristics like the structural complexity of these trees. To address this, the article asserts a novel metric called the Reasoning Score (r-score), defined as the maximum sum of node evaluations under a fixed budget of selected nodes. The authors state that their Reasoning Tree Schedule (Re-Schedule) leverages this metric to construct a curriculum that prioritizes queries based on their structural richness rather than just difficulty. They integrate this scheduling strategy into reinforcement learning with verifiable rewards (RLVR), employing policy optimization methods such as GRPO. The paper claims that this approach consistently outperforms seven competitive baselines, including accuracy-based curricula and standard RL algorithms. Finally, the authors report evaluations across six mathematical reasoning benchmarks, demonstrating significant gains in both data efficiency and final performance.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c92e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Append Supporting Citation\n",
    "\n",
    "Method to find supporting evidence for summary using `verbatim_rag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05dad21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evidence_(summary: str, document: DocumentSchema, n: int) -> str:\n",
    "\n",
    "    chunker = MarkdownChunkerProvider(\n",
    "        min_chunk_size=500,\n",
    "        max_chunk_size=5000)\n",
    "    dense_provider = SentenceTransformersProvider(\n",
    "        model_name=\"ibm-granite/granite-embedding-english-r2\",\n",
    "        device='cpu')\n",
    "    vector_store = LocalMilvusStore(\n",
    "        db_path=f\"./rag_test_{n}.db\",\n",
    "        collection_name=f'rag_test_{n}',\n",
    "        dense_dim=dense_provider.get_dimension(),\n",
    "        enable_dense=True,\n",
    "        enable_sparse=False,\n",
    "        nlist=16384)\n",
    "    index = VerbatimIndex(\n",
    "        vector_store=vector_store,\n",
    "        dense_provider=dense_provider,\n",
    "        chunker_provider=chunker)\n",
    "    index.add_documents([document])\n",
    "\n",
    "    llm_client = LLMClient(model=OPENAI_MODEL, temperature=1.0)\n",
    "    rag = VerbatimRAG(index, llm_client=llm_client)\n",
    "    query_string = f\"Find supporting evidence for this summary:\\n {summary}\"\n",
    "\n",
    "    response = rag.query(query_string)\n",
    "    return response.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf77a1d",
   "metadata": {},
   "source": [
    "Find all citations for the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04e316d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 14:27:09,325 - INFO - Load pretrained SentenceTransformer: ibm-granite/granite-embedding-english-r2\n",
      "2026-01-06 14:27:13,036 - INFO - Loaded SentenceTransformers model: ibm-granite/granite-embedding-english-r2\n",
      "2026-01-06 14:27:14,180 - INFO - Created indexes for collection: rag_test_9\n",
      "2026-01-06 14:27:14,204 - INFO - Created documents collection: rag_test_9_documents\n",
      "2026-01-06 14:27:14,204 - INFO - Connected to Milvus Lite: ./rag_test_9.db\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:22<00:00, 22.19s/it]]\n",
      "2026-01-06 14:27:36,485 - INFO - Added 25 vectors to Milvus\n",
      "2026-01-06 14:27:36,512 - INFO - Added 1 documents to Milvus\n",
      "Adding documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:22<00:00, 22.31s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relevant spans...\n",
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 14:28:38,926 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Span not found verbatim in document: 'Finally, the solver generates the answer by conditioning on both the original inputs and the retriev...'\n",
      "Warning: Span not found verbatim in document: 'Problem Formulation. Consider a sequence of multimodal inputs (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô), where each input x·µ¢...'\n",
      "Warning: Span not found verbatim in document: 'As illustrated in Figure 2 (a), the cycle operates as follows: given problem x·µ¢, the system performs...'\n",
      "Processing spans...\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 14:28:44,593 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "n = 9\n",
    "citation = find_evidence_(summaries[n], documents[n], n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf5772",
   "metadata": {},
   "source": [
    "Preview summary and evidence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d76d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "MLLMs exhibit strong reasoning on isolated queries, yet they operate *de novo* -solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated , preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem , a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge-preserving stable, generalizable strategies while avoiding catastrophic forgetting.\n",
       "\n",
       "Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction-hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at\n",
       "\n",
       "[https://weihao-bo.github.io/ViLoMeo-page/](https://weihao-bo.github.io/ViLoMeo-page/) ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The paper claims to introduce a dual-stream memory architecture for multimodal LLMs that separately stores textual reasoning guidelines and visual priors. The authors state that each memory bank is queried via cosine similarity thresholds to retrieve top-k relevant entries, followed by a two-stage filtering process that first ranks visual candidates by perceptual embedding and then by textual relevance. The article asserts that an LLM-based error analysis module classifies reasoning failures and generates abstracted logic and visual guidelines whenever the model‚Äôs prediction diverges from the ground truth. The authors state that these new guidelines are merged into the respective memory banks through replace-or-merge operations, ensuring that both streams evolve with fresh, context-specific knowledge. The paper claims that final answers are generated by conditioning on the original image and question along with the retrieved dual-stream memories, integrating perception, question understanding, and guided reasoning. The article asserts that this cyclical process of retrieval, error analysis, guideline synthesis, and memory update systematically addresses reasoning errors and common visual pitfalls. The authors conclude that their approach yields more accurate, explainable solutions by continually refining the model‚Äôs perceptual and logical priors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here is the supporting evidence for the summary: \n",
       "- **Memory Retrieval**: [1] (c) Memory Retrieval : Specialized dual-stream retrieval mechanism. Visual memories undergo a two-stage process involving image-embedding retrieval followed by question-specific retrieval, since visual information must be conditioned on both image content and the textual query. Logical memories are retrieved through problem analysis and text-embedding similarity. \n",
       "- **Memory Generation**: [2] (b) Memory Generation : An error-attribution framework that employs an LLM for logical analysis and an MLLM for visual analysis, producing structured memory schemas through similarity-based merge and create operations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_markdown_(abstracts[n])\n",
    "print_markdown_(summaries[n])\n",
    "print_markdown_(citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cbf47",
   "metadata": {},
   "source": [
    "### Conclusions on Base Approach\n",
    "\n",
    "In general, the summaries capture more details on what the paper attempts to accomplish.\n",
    "There is often a enumeration of the steps followed in the research to a deeper level than discussed in the abstract.\n",
    "The level of detail can sometimes render statements that are missing context, but the context can often be found in the evidence section.\n",
    "Evidence generation from the whole summary often results in clear citations for the first half of the summary while ignoring the second.\n",
    "\n",
    "| Paper Index | Accurate Summary | Corroborating Evidence | Observation |\n",
    "|-------------|------------------|------------------------|-------------|\n",
    "| 0 | Yes. | No. | Consistently returns only 1 piece of evidence, often from the conclusion. |\n",
    "| 1 | Yes. | Yes. | Evidence is relevant and with high coverage. |\n",
    "| 2 | Yes. | Yes. |  |\n",
    "| 3 | Yes. | Yes. | Evidence focused only on first half of summary. |\n",
    "| 4 | - | - | Somehow VSCodium freezes everytime this runs. |\n",
    "| 5 | No, minor omission mixing different statistics. | Yes. | Evidence offers insight into missing statements in summary. |\n",
    "| 6 | Yes. | Yes. | Accurate summary and complete evidence. |\n",
    "| 7 | No. | Yes. | The summary looks into details of the steps in the research missing the overall point stated in the abstract. |\n",
    "| 8 | Yes. | No. | Summary goes deep is disjointed and goes into details while failing to describe the general idea. No evidence was found. |\n",
    "| 9 | Yes | Yes, but incomplete. |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab70717",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
