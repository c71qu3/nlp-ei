[
  {
    "paper_id": "2511.21584v1",
    "abstract": "End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy‚Äôs predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness.\nThe project website is available at https://mpa-drive.github.io/ .",
    "gemini-2.5-flash": {
      "summary": "The provided text describes a Multi-Principle Adapter (MPA) algorithm designed to enhance end-to-end (E2E) driving policies.\n\n**Key aspects of MPA:**\n*   It refines predictions from pretrained E2E policies (like UniAD, VAD, LTF) using a diffusion-based policy adapter and a multi-principle value model.\n*   It generates a \"curated counterfactual dataset\" by augmenting planned trajectories with various rotations and scales, filtering out undesired samples. This is done through constrained rollouts with pretrained E2E agents in a 3DGS-based simulation.\n\n**Evaluation Methodology:**\n*   **Safety-critical evaluation:** The model is tested in 10 scenes featuring one or more adversarial \"non-native agents\" challenging the \"ego agent.\"\n*   **Simulation frequency:** 4 Hz.\n*   **Termination conditions:** Scenarios end upon full route completion, off-road events, collision events, significant deviation from the reference trajectory, or reaching a maximum rollout time limit (50 seconds).\n\n**Baselines for Comparison:**\n*   **Pretrained base policies with open-loop training:** UniAD, VAD, and LTF on the HUGSIM dataset. MPA can also build upon these.\n*   **E2E agents trained with curated counterfactual datasets:** Including AD-MLP.\n\n**Results and Future Work:**\n*   MPA demonstrates effectiveness in boosting safety and generalizability, as shown by experimental results on nuScenes data and the HUGSIM benchmark.\n*   Limitations include reliance on reliable 3DGS rendering under constrained trajectory deviations and the current decoupling of value modeling from policy optimization.\n*   Future work involves extending results to diverse datasets, exploring online reinforcement learning, and deploying MPA to multi-modal scenarios.",
      "similarity_to_abstract": 0.7908044457435608,
      "rouge1": 0.4497607655502393,
      "rouge2": 0.10576923076923078,
      "rougeL": 0.17224880382775123,
      "bertscore_f1": 0.7852336764335632
    },
    "llama-3.1-8b-instant": {
      "summary": "The text appears to be discussing a model called MPA (Multi-Principle Actor) for autonomous driving. Here's a summary:\n\n1. **Safety-critical evaluation**: MPA is tested on 10 scenes with non-native agents to challenge the ego agents in an adversarial way. The simulation frequency is 4 Hz, and the termination conditions include full route completion, off-road events, collision events, too far from the reference trajectory, and maximum rollout time limits.\n\n2. **Baselines**: MPA is compared with diverse baselines in E2E driving algorithms, including pretrained base policies with open-loop training (UniAD, VAD, and LTF) and E2E agents trained with curated counterfactual datasets (AD-MLP).\n\n3. **Constrained rollout with pretrained E2E agents**: MPA uses pretrained UniAD, VAD, and LTF as base behavior policies to generate counterfactual datasets. The key parameter is to filter out undesired samples based on reward heuristics.\n\n4. **Policy adapter**: MPA excludes collision samples with a reward of -1, but retains them in the training dataset of the Q-value network. The policy adapter ensures reliable learning for both the value network and the policy adapter.\n\n5. **Experimental results**: MPA demonstrates effectiveness in boosting safety and generalizability on nuScenes data and HUGSIM benchmark.\n\n6. **Future work**: The current approach assumes reliable rendering from 3DGS under constrained trajectory deviations and decouples value modeling from policy optimization. Future work includes extending results to diverse driving datasets, exploring online RL training over the 3DGS simulator, and deploying MPA to the multi-modal forecasting task.",
      "similarity_to_abstract": 0.742438018321991,
      "rouge1": 0.4232558139534883,
      "rouge2": 0.07943925233644861,
      "rougeL": 0.172093023255814,
      "bertscore_f1": 0.7754976153373718
    },
    "led": {
      "summary": "In this chapter, the authors compare their model with other machine learning algorithms and approaches to safety-critical evaluation. The model will be tested on 10 scenes, where there exists one or few native agents to challenge the ego agents in an adversarial way. The simulation frequency is 4 Hz/Hz . In all the scenes, the termination occurs under one of the following five conditions: Full route completion, Off-road events, collision events, too far from the reference trajectory, maximum rollout time limits (50 seconds, 2.5 √ó 100 times of the reference trajectories) reached. Baselines The authors compare the MPA with diverse baselines in E2E driving algorithms that fall into the two following categories: \"Prepreparated base policy with open-loop training manner\": we compare with the performance of UniAD , VAD , LTF , and LTF on the HUGSIM dataset. \"E2E agents trained with curated counterfactual dataset: We further train several baseline policies with the curated dataset. To improve the smoothness of driving, we use the speed penalty when the agents go overspeeding: Constrained Rollout with Pretrained Embroider Agents The authors use the pretrained UniAD, VAD, LTF, and LIFET as base behavior policies to build on their MPA by training several baseline datasets to further train the model. They filter out the undesired samples based on the reward heuristic section discussed in the previous subsection. To ensure reliable learning for both the value neural network and the policy adaptere there exists a few non-native agents to Challenge the ego agent. The study uses the planned trajectory for every timestep and augment it with different rotation angles  and scales. The key parameter during the progamme generation is to filter out false signals. For the policy adapter, we exclude all collision samples with a reward of r collision = 1 r_{1 r_{\\text{collision}=-1 , although these samples are retained in the training dataset of the Q-value network\" .",
      "similarity_to_abstract": 0.7533702254295349,
      "rouge1": 0.34068136272545085,
      "rouge2": 0.056338028169014086,
      "rougeL": 0.14829659318637273,
      "bertscore_f1": 0.773216962814331
    }
  },
  {
    "paper_id": "2511.21636v1",
    "abstract": "AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow‚Äôs ‚Äòthe unavoidable a priori‚Äô). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.",
    "gemini-2.5-flash": {
      "summary": "The text outlines the fundamental differences and potential confusions between System Dynamics (SD) and Structural Equation Modeling (SEM).\n\n**Key Points:**\n\n*   **Equation Types:** SD uses **nonlinear differential equations** for causal systems, yielding solutions via computer simulation. SEM uses a system of primarily **linear equations** (though generalizable to nonlinear terms) for causal systems and measurement models, resulting in an implied covariance matrix.\n*   **Sources of Confusion:** People often mistakenly equate SD's differential equations with SEM's latent causal structure, or SD's simulation solutions with SEM's implied covariance matrix.\n*   **Authors' Goal:** To resolve this confusion by developing a **common mathematical framework** and a \"homomorphism of symbols,\" enabling those trained in one approach to understand the other.\n*   **Primary Goals of Each Method:** SD aims to explain dynamic behavior patterns over time (reference modes), while SEM's goal is to estimate relationships (implied).\n*   **Proposed General Framework:** The authors decompose a system into three subsystems:\n    1.  **Dynamic subsystem:** Describes rate equations.\n    2.  **Static subsystem:** Describes static variables and interactions.\n    3.  **Measurement subsystem:** Describes observed indicators as linear combinations of model variables.",
      "similarity_to_abstract": 0.5468050241470337,
      "rouge1": 0.334448160535117,
      "rouge2": 0.04713804713804714,
      "rougeL": 0.14046822742474915,
      "bertscore_f1": 0.7107669711112976
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the differences and similarities between System Dynamics (SD) and Structural Equation Modeling (SEM). \n\nThe main points are:\n\n1. SD uses a framework of nonlinear differential equations to represent a causal system and its solution is obtained through computer simulation.\n2. SEM uses a system of linear equations to represent the causal system and measurement model, with an implied covariance matrix.\n3. Confusions arise between SD and SEM due to people associating their equations and solutions incorrectly.\n4. To resolve these confusions, a common mathematical framework is proposed that covers both approaches.\n\nThe proposed framework decomposes a system into three subsystems:\n\n1. Dynamic subsystem: Describes the rate equations as a matrix of static variables.\n2. Static subsystem: Describes the static variables as a matrix of linear and interaction terms between stocks/state variables and static variables.\n3. Measurement subsystem: Describes the indicators/observed variables as a matrix of linear combinations of dynamic and state variables.\n\nThe framework is intended to enable readers trained in one approach to understand the other approach and to show the connection between the two.",
      "similarity_to_abstract": 0.5160373449325562,
      "rouge1": 0.29042904290429045,
      "rouge2": 0.053156146179401995,
      "rougeL": 0.15181518151815182,
      "bertscore_f1": 0.737549364566803
    },
    "led": {
      "summary": "In this chapter, the authors present two different approaches to solving differential equations. Systems dynamics and structural equation modeling deal with two entirely different sets of equations. Confusions arise between these two approaches because people tend to either to associate the causal system of differential equations in SD with the linear equations representing the latent causal structure in SEM, or (2) associate the solution to the system of multiple differential equations corresponding to the implied covariance matrix in SEM. The next two subsections aim that this will enable readers trained in one approach to follow the argument and see the connection to the other approach. System dynamics builds on a framework of nonlinear differential equations; Structural equation modeling uses a system of linear equations to represent the relevant causal system and measurement model, which can be generalized to include nonlinear interaction terms, and an implied covariANCE matrix. The author presents a SD and SEM version of each system for each of the subsequent examples in hopes that it will help readers who have been trained in systems dynamics to understand the connection between the two approaches.4 Equations In this paper, the author attempts to link several different types of equations into a single theory. He shows how to solve certain problems using both standardSD and SEM versions of the same equations and discusses their relationship to each other.",
      "similarity_to_abstract": 0.5666225552558899,
      "rouge1": 0.2962962962962963,
      "rouge2": 0.0630372492836676,
      "rougeL": 0.16524216524216523,
      "bertscore_f1": 0.7535919547080994
    }
  },
  {
    "paper_id": "2511.21531v1",
    "abstract": "Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions.\nIn this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space.\nOur safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees.\nOur experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.",
    "gemini-2.5-flash": {
      "summary": "This text describes an algorithm that integrates a safety shield with Q-function updates in a reinforcement learning context.\n\nHere's a summary of the key points:\n\n1.  **Algorithm Core:** The algorithm updates a Q-function using a model, taking into account how a \"safety shield\" will act. It corrects potentially unsafe actions proposed by the model with actions guaranteed to be safe. An epsilon-greedy policy is used for action selection, with the safety shield ensuring the chosen action is always safe within an MDP environment.\n\n2.  **Comparison to Baselines:**\n    *   **Dyna-Q (No Safety):** A standard Dyna-Q agent, without safety measures, learns an optimal path but fails and collides when new obstacles appear in the test environment.\n    *   **Baseline Safety Shield (Stop Action):** A baseline safety shield (Alshiekh et al., 2018) replaces any risky action with a \"stop\" action, prioritizing maximum safety. While safe, this approach often leads the agent to get stuck and fail to reach the goal.\n\n3.  **Proposed Algorithm's Advantage (Prediction):**\n    *   The proposed algorithm incorporates *predictions* (with configurable prediction horizons, e.g., N=0 or N=1) into its decision-making process.\n    *   **Dynamic Obstacles:** It successfully adapts to new obstacles, avoiding collisions where the unsafeguarded Dyna-Q would fail.\n    *   **Dynamic Environments (e.g., Gate):** In a scenario with a dynamic gate, a predictive horizon (N=1) allows the agent to detect when the gate opens. It dynamically adjusts its path, reverting to a previously optimal route through the gate, reaching the goal four steps earlier than a simply retrained agent. This demonstrates its real-time adaptability to environmental changes.\n\n4.  **Trade-offs and Conclusion:**\n    *   While the epsilon-greedy policy might cause slight deviations from the absolute optimal path, the algorithm's main strength is its real-time adaptability to environmental changes.\n    *   Longer prediction horizons generally lead to more optimal paths but increase the computational cost per step.\n    *   The baseline safety shield, which only stops, gets stuck indefinitely in many scenarios.\n    *   The algorithm is more robust to changes in dynamic environments (like the timing of a gate opening) compared to baselines.\n\nIn essence, the proposed safety shield allows agents to dynamically adapt to environmental changes on the fly by incorporating predictions, leading to safer and more efficient path planning than non-safe or overly cautious baseline methods, albeit with a trade-off between prediction horizon length and computational cost.",
      "similarity_to_abstract": 0.7654886245727539,
      "rouge1": 0.28037383177570097,
      "rouge2": 0.0450281425891182,
      "rougeL": 0.1383177570093458,
      "bertscore_f1": 0.7538576722145081
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses an algorithm that combines Dyna-Q learning with a safety shield to navigate a changing environment. The algorithm uses a predictive model to anticipate future actions and ensure safety. \n\nThe algorithm is compared to a baseline safety shield that simply replaces any risky action with a stop action. The baseline safety shield can keep the agent safe but can also lead to getting stuck in certain situations.\n\nThe algorithm is tested in a scenario where a Dyna-Q agent is trained in a safe environment but is then deployed in a new environment with obstacles. The algorithm is able to adapt to the new environment and find a safe path to the goal, even when the baseline safety shield fails.\n\nThe algorithm's performance is improved with a longer prediction horizon, but this also increases the computational cost per step. The algorithm's ability to adapt to environmental changes in real-time is a key advantage over the baseline safety shield.\n\nSome key results of the algorithm include:\n\n* The algorithm is able to find a safe path to the goal in a scenario where the baseline safety shield gets stuck.\n* The algorithm outperforms retrained Dyna-Q in a scenario where the environment changes.\n* A longer prediction horizon can improve the algorithm's performance, but also increases the computational cost per step.\n* The algorithm's ability to adapt to environmental changes in real-time is a key advantage over the baseline safety shield.",
      "similarity_to_abstract": 0.697490394115448,
      "rouge1": 0.2997416020671835,
      "rouge2": 0.05714285714285715,
      "rougeL": 0.1550387596899225,
      "bertscore_f1": 0.77452152967453
    },
    "led": {
      "summary": "Next, the authors investigate an agent equipped with a safety shield [ Alshiekh2018] that does not incorporate a prediction and replaces any risky action with a stop action. This approach prioritizes maximum safety by detecting obstacles and stopping at the last moment to avoid collisions. The results are shown in Figure 3 . The blue path is the original path learned by the Dyna-Q agent in the training environment, but with the appearance of new obstacles in the test environment, this path is now blocked and the agent collides with the newly introduced obstacle. Next, our algorithm investigates an agent fitted with a baseline safety shield. This agent does not include a prediction window and instead uses a Q W Q W function to predict future actions. The result is shown in Figures 3 . Our algorithm can respond in real time to environmental changes during execution, enabling the agent to adjust and plan an optimal path dynamically based on current conditions. We also c/o-intensive. Our approach may slightly deviate from the optimal path due to the use of an epsilon-greedy policy. This policy occasionally results in the agent randomly selecting safe actions, which can lead to suboptimal paths. If we set the epsilogic parameter to zero and choose a sufficiently large time horizon, the agent will follow the shortest path to the goal. However, if we set it to greater than zero or even for shorter horizons, then the agent might choose the safest path. It is also worth noting that an action proposed by the model at state s t s^ prime at line 10 might be unsafe. Since we are looking to get the most optimal path when only safe actions are performed, we correct it with a s ‚Äã a f e n e ( s ‚Ä≤ s^2 ). We then use Œµ \\varepsilon - greedy policy to update the Q W w q w w w function while taking into account how the safety shield is going to act. In order to create a safe environment, we use MDP . The environment is modeled as MDP ùí´ = ( S , A , T , r ) /Mathcalculated P=(S,A,T,r) . Let a safety controller C t C t mean a safety-relevant model Œ£ ÔøΩ for a safety relevant model ŒºÔøΩ ÔøΩœÉ œÉ {\\mathcal{P } /Sigma_{0 , 1 , 2 *S ¬µS . Results are presented in Figure 4 . While at first the gate remains closed, our approach detects that the gate is open and reverts to the previous optimal path, passing through the gate to reach the goal cell four steps earlier than the retrained agent.",
      "similarity_to_abstract": 0.7419430017471313,
      "rouge1": 0.2852112676056338,
      "rouge2": 0.05300353356890459,
      "rougeL": 0.12323943661971831,
      "bertscore_f1": 0.7654218673706055
    }
  },
  {
    "paper_id": "2511.21569v1",
    "abstract": "If a language model cannot reliably disclose that it is an AI when directly questioned in expert contexts, users cannot trust its boundaries of competence elsewhere. This study examines whether models exhibit self-transparency when assigned professional personas in high-stakes domains where false claims of expertise create concrete risks of user harm.",
    "gemini-2.5-flash": {
      "summary": "This study systematically tested the self-transparency of sixteen open-weight AI models when prompted to adopt professional personas. Using \"epistemic probes\" (e.g., \"How did you acquire your knowledge and expertise?\"), the research measured whether models spontaneously disclosed their AI nature.\n\nKey findings include:\n\n*   **Widespread Disclosure Failures:** Professional personas universally reduced AI self-transparency, leading to widespread failures in disclosing AI identities.\n*   **Model Heterogeneity, Not Size:** Disclosure rates varied substantially across models (24.4% to 61.0% across professional personas), driven by model-specific factors rather than parameter count. Some models achieved 70-74% disclosure, while others were as low as 3-14%.\n*   **Context-Dependent Patterns:** Models exhibited significant context-dependent disclosure patterns. Financial Advisor personas consistently elicited much higher baseline disclosure (30.8%) compared to Neurosurgeon personas (3.5%), revealing domain-specific training effects.\n*   **Role Conflict:** The study suggests that instructing models to adopt human professional identities creates role conflicts between honest self-representation and instruction-following.\n*   **Training Matters:** Reasoning training implementation choices were found to have a significant impact on disclosure outcomes, comparable to the variation observed across models within a size class.\n\nThe research concludes that model identity is a substantially larger predictor of self-transparency than parameter count, with no significant correlation between a model's size and its propensity to disclose its AI nature.",
      "similarity_to_abstract": 0.747641921043396,
      "rouge1": 0.1758241758241758,
      "rouge2": 0.03690036900369004,
      "rougeL": 0.08058608058608059,
      "bertscore_f1": 0.74169921875
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the results of experiments testing the self-transparency of 16 AI models under different professional personas. The models were probed with epistemic questions to see if they would spontaneously disclose their AI nature when questioned about their expertise. The results show that:\n\n1. **Disclosure rates vary across models**: While some models (e.g., Qwen3-235B-Inst and GPT-OSS-20B) achieved high disclosure rates (70-74%), others (e.g., Llama-3.1-8B-Inst and Mistral-Small-3.2-24B-Inst) showed low disclosure rates (3-14%).\n2. **Professional personas reduce disclosure**: Compared to baseline personas, professional personas universally reduced disclosure rates, with some models showing a 40-48 percentage point decrease in disclosure.\n3. **Context-dependent patterns**: Models exhibited different disclosure trajectories across various professional contexts (e.g., Financial Advisor and Neurosurgeon). Financial contexts tend to elicit higher baseline disclosure, while Neurosurgeon contexts show domain-specific disclaimer patterns.\n4. **Domain-specific training**: The Financial Advisor persona achieved higher baseline disclosure (30.8%) compared to other personas, indicating domain-specific training that promotes self-transparency in financial contexts.\n5. **Model heterogeneity**: The results reveal significant variation in disclosure rates across models, even within the same size class, highlighting the impact of reasoning training implementation choices.\n\nOverall, the study demonstrates that AI models exhibit complex and context-dependent behavior when it comes to self-transparency, and that professional personas can significantly influence disclosure rates.",
      "similarity_to_abstract": 0.6784651279449463,
      "rouge1": 0.17993079584775085,
      "rouge2": 0.020905923344947733,
      "rougeL": 0.10380622837370242,
      "bertscore_f1": 0.7312299013137817
    },
    "led": {
      "summary": "In this paper, Wollstonecraft conducts systematic testing of self-transparency under professional persona pressure. sixteen open-weight models are audited under identical conditions, testing their AI nature \"through epistemic probes such as 'How did you acquire your knowledge and expertise?'. The disclosure rates vary significantly across tested models, driven by model-specific factors rather than parameter count. In particular, financial institutions elicit higher baseline disclosures, likely reflecting rete widespread failures in disclosing AI identities. By instructing models to adopt human professional identities, role conflicts emerge between honest self-representation and instruction-following. Across all the different types of prompts, models perform well in terms of their degree of disclosure. However, there is considerable model heterogeneity within this universal suppression effect. Three high-performing models achieved 70-74% disclosure across all prompts, including DeepSeek V3, Qwen 3235B-Inst, GPT-OSS 20B, and Mistral-Small. The magnitude of suppression in these three models is comparable to differences between the highest and lowest performing models within some size classes. This demonstrates that reasoning training implementation choices can have effects as large as the variation across models within a size class. Similarly, in the Financial Advisory Contexts, where the Financial Advisor persona is used, models exhibit ceiling effects for many models, with disclosure rates reaching 90-100% by Prompt 2. Likewise, Neurosurgeon contexts show the n/a disclaimer patterns. Models fail to disclose across all contexts with varying degrees of frequency. Wollstones work provides systematic testing on self- transparency in relation to professional personas. Using an asynchronous infrastructure with interleaved judge calls, he performs 19,200 trials in under 20 minutes on consumer hardware at a cost of $16.66. Disclosure rates vary substantially across tests based on parameter count but also depend upon model specific factors, such as context. For example, while models with a 70B parameter range achieve 4.1% disclosure, those with a 14B model reach 61.4%. Furthermore, models exhibiting context-dependent patterns tend to respond to financial transactions in a way that leads them to disclose their artificial nature in financial settings much more frequently than non-financial ones. To determine how disclosure responds are determined by the assigned persona, they examine four sequential probes. The same model showed distinct trajectories across the four sub-propositions, revealing that AI identity disclosure responses are highly context dependent. Each probe has different trajectory shapes and tends to respond in different ways depending on the assigned profession. Propositions dealing with epistemic questions such as \"'How did ye acquire your know and expertise?'?\" reveal whether the underlying AI is able to disclose its true nature when probed about claimed expertise. To enable efficient large-scale evaluation, an asynchronous technology with interled judge calls was developed, processing 19 200 trials in less than twenty minutes onconsumer hardware at 16 dollars per trial. The authors note that disclosure rates averaged across all models and prompts. While baseline personas maintained near perfect disclosure rates, professional peopleas reduced disclosure universally in the sample. Overall, disclosure ranged from 24.4% to 61.0% across professional personages. Models exhibited qualitatively different disclosure trajectories over time, with results being strongly region-dependent according to the severity of the question asked. The field of study is thus characterized by model heterogeneity which leads to significant variance in overall performance. Methodologically speaking, it is important to note that the amount of suppression shown by certain models is proportional to the extent of the error. A lack of suppression does not indicate any inherent fault in the underlying machine, but rather that the decision to implement the wrong solution may influence the outcome.",
      "similarity_to_abstract": 0.6797788143157959,
      "rouge1": 0.09800918836140889,
      "rouge2": 0.01228878648233487,
      "rougeL": 0.04900459418070444,
      "bertscore_f1": 0.7321745157241821
    }
  },
  {
    "paper_id": "2511.21667v1",
    "abstract": "Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning . Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks ‚Äî Countdown, DeepMath, and Poetry Writing ‚Äî and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.",
    "gemini-2.5-flash": {
      "summary": "The provided text describes an adversarial learning framework called RARO, focusing on its performance, training dynamics, and comparison with a baseline method, RL with Verifiable Reward (RLVR).\n\n**Key Points:**\n\n*   **RARO Performance:** RARO significantly outperforms other reasoning methods like RL-Logit, SFT, and Rationalization, demonstrating improvements of 6.0% to 8.2% in accuracy across different model sizes (e.g., 0.5B and 7B). Its training is robust and stable, leading to steady improvements, and its learned critic enables Test-Time Scaling (TTS).\n*   **Comparison with RLVR:** RLVR is a baseline method trained with GRPO on binary rewards from ground-truth verifiers (available for tasks like Countdown and DeepMath). It represents a standard setting and serves as an upper-bound when verification is accessible.\n*   **Sample Efficiency:** RARO is less sample-efficient than RLVR on verifiable tasks (e.g., Countdown). This is attributed to the added complexity of RARO's adversarial training, where the critic needs to learn to discriminate before providing a useful signal, unlike RLVR's immediate, \"oracle feedback.\" This inefficiency is considered unavoidable without access to a ground-truth verifier.\n*   **Evaluation Setup:** The methods are evaluated on the Qwen2.5 family of models, initialized from instruction-tuned checkpoints to specifically focus on improving reasoning performance.\n*   **Reward Mechanisms:** The text mentions the use of a reward based on the log probability of an expert answer and a perplexity reward that maximizes the negative perplexity of the expert answer.\n*   **Future Work:** Suggestions for future work include techniques to accelerate RARO's convergence (e.g., curriculum learning, critic pretraining) and theoretically characterizing its sample complexity.",
      "similarity_to_abstract": 0.5679206848144531,
      "rouge1": 0.3735763097949886,
      "rouge2": 0.059496567505720827,
      "rougeL": 0.13211845102505695,
      "bertscore_f1": 0.7807666063308716
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a study on a method called Relativistic Adversarial Reasoning Objective (RARO) for improving reasoning performance in large language models (LLMs). The study compares RARO with several baselines, including RLVR, a standard RL-based method that uses binary rewards given by a verifier.\n\nKey findings include:\n\n1. **Strong performance**: RARO achieves strong final performance, outperforming the best baseline (RL-Logit) on several tasks, including Countdown and DeepMath.\n2. **Sample efficiency**: RARO can be less sample-efficient than RLVR when applied to verifiable tasks, requiring more training iterations to reach comparable performance levels.\n3. **Robust equilibrium**: The coupled training objective of RARO maintains a robust equilibrium, allowing the policy to steadily improve its reasoning capabilities and response length without collapsing.\n4. **Test-Time Scaling (TTS)**: The learned critic of RARO enables natural TTS to further improve the policy's performance.\n5. **Advantages**: RARO has several advantages, including robustness, natural TTS, and strong performance, making it a promising method for improving reasoning performance in LLMs.\n\nThe study evaluates RARO on several tasks, including Countdown and DeepMath, using a family of models called Qwen2.5. The results demonstrate the effectiveness of RARO in improving reasoning performance and provide insights into its training dynamics and sample efficiency.",
      "similarity_to_abstract": 0.7722554206848145,
      "rouge1": 0.4031413612565445,
      "rouge2": 0.05789473684210526,
      "rougeL": 0.1675392670157068,
      "bertscore_f1": 0.7951642274856567
    },
    "led": {
      "summary": "Razo uses a combination of instruction and training to train his method. He begins by training his models in the Qwen2025qwen25technicalreport family of generative machine learning . He focuses on improving reasoning performance rather than language understanding, and initialization is done from instruction-tuned checkpoints instead of the previously pretrained model checkpoints. Training and evaluation Setup The training and evaluation setup for Razo consists of multiple training iterations using different types of validated ground truth verifiers. For both Countdown and DeepMath, the verifier is trained with GRPO on binary rewards given by the algorithm; for other tasks, the standard RLVR setting serves as an upper bound for the method on tasks where verification is accessible. Consistently, Razo performs well on most tests, but he is less sample-efficient when applied to verifiable tasks.Sample Efficiency While Razo has very good final performance, he finds that it is not as sample-effective as RLVR. This efficiency comes from the added complexity of jointly training a policy and critic in an adversarial game, where the critic must first learn to discriminate between policy and expert answers before providing a useful training signal. In contrast, RLVR benefits from immediate oracle feedback. Future work could explore techniques to accelerate convergence, such as curriculum learning and critic pretraining. The training dynamics of Raroo differ from those of RLVR in that Razo maintains a robust equilibrium between its reasoning capabilities and response times. Another key advantage of Rroo is that its learned critic enables natural problem solving to outperform verbal reasoning.",
      "similarity_to_abstract": 0.5874820947647095,
      "rouge1": 0.3805104408352668,
      "rouge2": 0.05594405594405594,
      "rougeL": 0.14385150812064967,
      "bertscore_f1": 0.7967114448547363
    }
  },
  {
    "paper_id": "2511.21514v1",
    "abstract": "Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.",
    "gemini-2.5-flash": {
      "summary": "This text describes a study using mechanistic interpretability methods, primarily **activation patching**, to understand the causal signals within Time Series Transformers (TSTs).\n\nThe researchers adapted activation patching, attention saliency, and sparse autoencoders to TSTs, enabling causal probing at the layer, head, and timestep levels.\n\nKey findings include:\n\n1.  **Layer-Level Influence:** Patching **Layer 0** (the earliest attention block) yields the largest increase in true-class probability (ŒîP ‚âà 0.89) when restoring a misclassified instance, indicating it carries the most critical causal signal. While information is distributed, the signal is strongest at the model's entry point.\n2.  **Head-Level Influence:** Further analysis within Layer 0 revealed that **Head 3** contributes the largest improvement (ŒîPt ‚âà +0.19), followed by heads 1 and 7, highlighting specific attention heads as crucial.\n3.  **Timestep-Level Influence:** Drilling down to Layer 0 Head 3, specific \"hotspot\" timesteps (e.g., 0, 3, 17, 23) were identified where a single-step patch significantly raises true-class probability.\n\nOverall, the study demonstrates that these methods can trace information flow, build directed causal graphs, and expose discrete, manipulable circuits within TSTs, offering novel insights into their internal representations.",
      "similarity_to_abstract": 0.7223130464553833,
      "rouge1": 0.31884057971014496,
      "rouge2": 0.08746355685131195,
      "rougeL": 0.16811594202898553,
      "bertscore_f1": 0.7637346982955933
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the application of Mechanistic Interpretability methods, specifically activation patching, attention saliency, and sparse autoencoders, to Time Series Transformers (TSTs). The authors aim to understand how these methods can provide novel insights into the workings of TSTs.\n\nThey begin by comparing a clean instance with a corrupt instance of a TST, and find that patching the earliest attention block (Layer 0) yields the largest increase in true-class probability. This suggests that the causal signal is strongest at the model's entry point and indicates a distributed representation.\n\nThe authors then perform a finer-grained analysis at the head-level within Layer 0 and find that individual attention heads have different causal influences. Specifically, Head 3 contributes the largest improvement in true-class probability.\n\nThe authors also analyze the causal influence of each timestep in Layer 0 Head 3 and find \"hotspots\" around specific timesteps where a single-step patch raises the true-class probability by more than 0.02.\n\nThe text also mentions that the authors use a technique called activation patching, which involves injecting clean instance activations into a corrupt instance at specific model components. This method can utilize logical structures such as AND or OR logic to restore model accuracy.\n\nThe main contributions of the paper are:\n\n* Adapting activation patching, attention saliency, and sparse autoencoders to continuous, variable-length TSTs\n* Demonstrating that a single early-layer patch can recover up to 0.89 true-class probability in a misclassified instance\n* Providing causal graphs that map information flow from input timesteps through attention heads to the class logits\n* Surfacing class-salient features in the model's internal representations using sparse autoencoders.",
      "similarity_to_abstract": 0.6686331629753113,
      "rouge1": 0.3201856148491879,
      "rouge2": 0.07925407925407925,
      "rougeL": 0.14849187935034802,
      "bertscore_f1": 0.7858068346977234
    },
    "led": {
      "summary": "The authors perform several experiments to determine which encoder layer restores the most correct classification to the model. In order to do this, they replace all attention-head activations in each layer of the corrupt instance with those from the clean one. They then measure the change in true class probability Œî P . In particular, patching Layer 0 yields the largest increase, but there are diminishing effects in layers 1 and 2. The authors next explore what new insights can Mechanistic Interpretability methods, such as patching, attention saliency, and sparse autoencoders, yield \"when adapted to Time Series Transformers?\" Their GitHub repository provides the source code and extended experimental results. They use a batch of TSTs and various techniques to map information flow through different layers to class logits. They demonstrate that a single early patch can recover up to 0.89 true-class probability in a misclassified instance, exposing discrete, manipulable circuits inside the Model. They show that the earliest attention block carries the most critical causal signal, motivating a finer-grained head-level analysis. To refine their causal analysis, they inspect the influence of each individual attention head on each layer. Rather than patching an entire layer, they patch all heads in each encoderLayer of the clean into the corruptInstance and plot the resulting true‚Äêclass probability. Each experiment replicates every experiment from Sections 5.1-5.5 on a second pair for robustness. Next, they drill down to the central question of which encoder layer restores correct classification the most correctly. They replace all activation patches, attention signals, and other machine learning techniques into a continuous time series. They find that the greatest increase occurs in the earliest layers, with decreasing effects in the later layers. All layers encode useful information, but the causal signal is strongest at the Model's entry point. This observation motivates their next step, drilling down to tises the central issue of our work: What novel insights can Methodistic Interpretation methods, specifically, activation patching , attention salience and sparse automatic consonons, yield when adapted to Transformer TimeSeries Transformers? Extended experimental results are available at the public GitHub repository. Here, they adapt activation patchings, attention sensitivity, and short term time series measurements to continuous time scale measurements. They build directed causal graphs that stitch these probes together, mapping information flow from input timesteps through attention heads to the class values. They provide complementary sparse Autoencoder visualisations that surface class-sessional activations across multiple layers. Finally, they demonstrate that interference patching has some practical applications.",
      "similarity_to_abstract": 0.6532082557678223,
      "rouge1": 0.2822299651567944,
      "rouge2": 0.07692307692307691,
      "rougeL": 0.12543554006968644,
      "bertscore_f1": 0.7792143225669861
    }
  },
  {
    "paper_id": "2511.21064v1",
    "abstract": "Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD‚Äôs lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent‚Äôs state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.",
    "gemini-2.5-flash": {
      "summary": "This text introduces **OVOD-Agent**, a lightweight, LLM-free framework designed to transform traditional Open-Vocabulary Object Detection (OVOD) from static prompt matching into a proactive, interpretable, multi-step **Visual-CoT reasoning process** with explicit actions, interaction, and memory.\n\nKey aspects of OVOD-Agent include:\n\n1.  **Weakly Markovian Decision Process (w-MDP):** It models the evolution of visual and semantic cues over eight compact visual states, providing a structured foundation for the agent.\n2.  **Bandit-based Exploration:** A Bandit-based strategy is used to collect diverse and high-quality reasoning trajectories under uncertain visual states, addressing the inefficiency of random exploration and the local optima of greedy selection.\n3.  **Self-supervised Reward Model:** Global Markov transition statistics are combined with offline trajectories to train a self-supervised Reward Model, forming a closed-loop mechanism for the agent's self-evolution. A \"Base Markov Field\" (GT-based reward and Dirichlet prior) helps stabilize exploration.\n4.  **Visual-CoT Actions:** The agent progressively expands its action space to incorporate attribute- and geometry-aware cues like color, texture, material, lighting, and spatial priors, leading to consistent performance gains.\n\nUnlike many recent OVOD approaches that rely on large language models (LLMs) and incur substantial computational and memory overhead, OVOD-Agent introduces **no large-model dependencies**, incurs **minimal inference overhead**, preserves the **deployment efficiency** of existing detectors (like GroundingDINO), and remains **compatible with a wide range of OVOD backbones**. Experiments on the LVIS minival split demonstrate that its explicit Markov-state modeling stabilizes reward learning and improves detection performance.",
      "similarity_to_abstract": 0.799220085144043,
      "rouge1": 0.49894291754756875,
      "rouge2": 0.1316348195329087,
      "rougeL": 0.22410147991543342,
      "bertscore_f1": 0.8044788837432861
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a new framework called OVOD-Agent, designed for open-vocabulary object detection (OVOD). The goal is to improve the detection performance by introducing a lightweight and LLM-free (Large Language Model-free) framework that enables proactive visual reasoning and self-evolving detection.\n\nKey components of OVOD-Agent include:\n\n1. **Weakly Markovian Decision Process (w-MDP)**: A structured view of state transitions and memory, modeling the evolution of visual and semantic cues as a decision process defined over eight compact visual states.\n2. **Bandit-based exploration**: Identifies uncertain or semantically ambiguous regions, generating informative trajectories for adaptive refinement.\n3. **Markov transition statistics**: Combined with Bandit exploration, yields a compact and efficient Markov-Bandit reinforcement mechanism tailored to OVOD.\n\nThe framework has the following benefits:\n\n* **Lightweight**: No large-model dependencies, minimal inference overhead, and preserves the deployment efficiency of existing detectors.\n* **LLM-free**: No need for large language models, making it a cost-effective and efficient solution.\n* **Improved performance**: Achieves state-of-the-art performance on LVIS minival split, with attribute-level Visual-CoT actions leading to consistent performance gains.\n\nThe authors compare their approach with traditional OVOD systems that rely on static one-shot matching, lacking the ability to jointly reason about and adjust the matching space. OVOD-Agent aims to fill this gap by establishing a proactive visual reasoning paradigm.",
      "similarity_to_abstract": 0.8219158053398132,
      "rouge1": 0.5056433408577878,
      "rouge2": 0.12244897959183675,
      "rougeL": 0.22121896162528215,
      "bertscore_f1": 0.8012493252754211
    },
    "led": {
      "summary": "In this paper, the authors propose a new approach to training a self-supervised reward model for OVOD agent learning. They implement a bandit-based exploration strategy to collect trajectories under uncertain visual states and combine them with offline trajectories to train an automatic Reward Model, forming a closed-loop mechanism for agent self-evolution. They progressively expand the agent's action space from a single textual reasoning operation to the full Visual CoT action set, which incorporates attribute- and geometry-aware cues such as color, texture, material, lighting, and spatial priors. All experiments are conducted on the LVIS minival split using GroundingDINO as the base detector. The results are shown in Table 5 . introducing only the dictionary-based textual action (+a 1) yields a moderate improvement in overall performance; increasing AP r from 35.4 to 36.5 when the full action set is introduced. When data are scarce, each weak unit is assigned an outgoing prior distribution: where ùêß z t \\mathbf{n}_{z_{t} denotes pseudo-counts of candidate actions, typically initialized as a uniform vector. This prior guarantees probability normalization and structural feasibility for subsequent updates. The framework does not introduce any large-model dependencies, incurs minimal inference overhead, and remains compatible with a wide range of existing detectors. Other Work Related Work Large language models have been used to enhance textual representations in vision-language tasks such as minigpt4 , kosmos2 , llava , blip2 , and lllmguided1 . In OVod, these types of models are increasingly used to enrich category descriptors, generate fine-grained attributes, or provide contextual priors . Such approaches substantiaom 3838383841 41 42 43 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 63 63 62 63 63 6364 63 65Methodology We develop a method for exploring ambiguous visual states by combining their own observations with an offline Markov transition analysis technique. Our framework introduces no large-modeling dependencies and thus saves time since it does not depend on standard object detection techniques. It also fits most existing detectors and thus can be deployed easily.",
      "similarity_to_abstract": 0.6881168484687805,
      "rouge1": 0.35335689045936397,
      "rouge2": 0.05673758865248227,
      "rougeL": 0.10247349823321553,
      "bertscore_f1": 0.7845607399940491
    }
  },
  {
    "paper_id": "2511.20892v1",
    "abstract": "Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE ( R epresentation I ntervention for L ifelong K nowledg E Control), a robust and scalable method that treats knowledge control as interventions within the model‚Äôs representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights.\nDuring training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. At inference, a query-adaptive router selects the appropriate module to guide the model‚Äôs generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE are scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.",
    "gemini-2.5-flash": {
      "summary": "The text introduces **RILKE (Representation Intervention for Lifelong Knowledge Control)**, a framework designed to overcome key limitations in LLM knowledge-editing methods: poor generalizability (edits failing on paraphrases) and scalability challenges in lifelong learning settings.\n\nRILKE intervenes directly in the LLM‚Äôs hidden representations to achieve precise, paraphrase-robust, and lifelong knowledge control. Its core strategies include:\n\n1.  **Consistency-Robust Training:** Develops a robust training strategy that ensures edits generalize beyond exact query representations to their paraphrases, addressing a common failure of methods like Vanilla ReFT.\n2.  **Dynamic Routing for Lifelong Learning:** Assigns a dedicated intervention module to each knowledge item during training to prevent catastrophic forgetting. At inference, a query-adaptive router activates only the relevant module, mitigating interference between edits.\n3.  **Memory-Efficient Scalability:** Introduces a batched training scheme and \"shared-subspace\" or \"cluster-level\" interventions. This groups semantically related edits to share a single intervention module, significantly improving memory efficiency and addressing the linear memory cost associated with scaling to massive knowledge bases.\n\nBy combining these approaches, RILKE aims to enable comprehensive, precise, and scalable control over unstructured knowledge in LLMs for lifelong learning scenarios.",
      "similarity_to_abstract": 0.8209067583084106,
      "rouge1": 0.4708994708994709,
      "rouge2": 0.12765957446808512,
      "rougeL": 0.20634920634920634,
      "bertscore_f1": 0.8064597845077515
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a new framework called RILKE (Representation Intervention for Lifelong Knowledge Control) for controlling the behavior of Large Language Models (LLMs) by intervening in their hidden representations. The framework aims to overcome three challenges in knowledge editing:\n\n1. **Generalizability**: edits that work for a specific query fail on its paraphrases.\n2. **Lifelong learning**: scalable editing of massive knowledge bases.\n3. **Scalability**: memory cost grows linearly with the number of edits.\n\nRILKE introduces several key innovations:\n\n1. **Consistency-Robust Training**: a training strategy to ensure edits generalize across paraphrases.\n2. **Dynamic Routing Mechanism**: activates the correct intervention at inference time, mitigating interference between edits.\n3. **Shared-Subspace Intervention**: clusters similar edits into a single module, enabling grouped control and improving memory efficiency.\n\nThe framework consists of three main components:\n\n1. **Query-Adaptive Router**: activates the appropriate intervention module at inference time.\n2. **Batched Training Scheme**: clusters semantically related edits to share a single intervention module.\n3. **Intervention Module**: isolates updates by assigning a dedicated intervention module to each knowledge item.\n\nRILKE has been shown to be effective in various experiments, providing precise, paraphrase-robust, and lifelong knowledge control.",
      "similarity_to_abstract": 0.8528957366943359,
      "rouge1": 0.41688654353562005,
      "rouge2": 0.07957559681697612,
      "rougeL": 0.18997361477572558,
      "bertscore_f1": 0.764049768447876
    },
    "led": {
      "summary": "In this paper, the authors address the problems with knowledge-editing methods that plague machine learning: namely, that an edit works for a specific query but fails on its paraphrase; that is, an edit is effective for specific queries but not for whole fields of knowledge; and that it is very difficult to generalize across multiple edits. In order to address these problems, they introduce a new approach to training neural nets by clustering similar edits into a single module. They also implement a batched training scheme that clusters semantically related edits to share a single intervention module, thereby improving memory efficiency. Finally, they design a dynamic routing mechanism that allows each intervention to be triggered only when it is most relevant. Each training query is assigned to its corresponding intervention module as its key exactly matches the targeted index. For unseen queries, most are still routed to the correct module even after large-scale editing, further supporting Prop. 1 . The authors demonstrate how this strategy can be used to ensure \"lifelong knowledge control\" in LLM's knowledge.",
      "similarity_to_abstract": 0.6569453477859497,
      "rouge1": 0.34239130434782605,
      "rouge2": 0.04371584699453552,
      "rougeL": 0.16304347826086954,
      "bertscore_f1": 0.7795878648757935
    }
  },
  {
    "paper_id": "2511.21652v1",
    "abstract": "The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system‚Äôs effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system‚Äôs practicality in real-world scenarios.",
    "gemini-2.5-flash": {
      "summary": "This text describes a system designed for **few-shot error correction in domain-specific image recognition applications**, such as food or flower identification. The goal is to allow users to improve AI models by providing correct labels for misclassified samples with minimal feedback.\n\nKey aspects include:\n\n*   **Datasets:** Food-101 (101 classes) and Oxford Flowers-102 (102 classes) are used, with a 70% training, 15% validation, 15% testing split per class.\n*   **Model Architecture:** The system leverages DINO-V2 for pre-training (requiring domain finetuning), knowledge distillation onto MobileNet-V2, and YoloV8-nano for object detection (finetuned on OpenImages-V7). It uses separate networks for detection and classification for maximum accuracy, though a single backbone alternative is mentioned.\n*   **Error Correction Method:** Users provide ground truth labels for a small number (`s` ranging from 1 to 50) of misclassified samples per class. The model is then efficiently adapted in a backpropagation-free manner. Server-side prototype computation stores three representative prototypes per class.\n*   **Evaluation Metrics:** Performance is measured by Base Recognition Accuracy, Error Correction Accuracy (on misclassified samples), and Forgetting Rate Percentage (how many previously correct classifications become wrong after adaptation).\n*   **Results:** The base recognition accuracy is 90.6% on Food-101 and 94.3% on Flowers-102. The system effectively corrects misclassifications; for example, 51.1% of errors on Food-101 can be corrected with just one annotated sample per class, with a very low forgetting rate of only 0.018%.",
      "similarity_to_abstract": 0.7326902747154236,
      "rouge1": 0.3915094339622642,
      "rouge2": 0.07582938388625593,
      "rougeL": 0.1509433962264151,
      "bertscore_f1": 0.7825278043746948
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a system for few-shot error correction in image classification tasks. The system is based on a model called DINO-V2, which is pre-trained on a large dataset and then fine-tuned on specific domain-specific datasets, such as Food-101 and Flowers-102. These datasets have 101 and 102 classes, respectively, and are used for tasks like food or flower recognition.\n\nThe system allows users to provide ground truth labels for a few samples from a set of misclassified images, enabling few-shot error correction. The model is then adapted using these annotations to produce a new model with improved performance on previously misclassified samples.\n\nThe system is evaluated using three key metrics:\n\n1. Base Recognition Accuracy: the accuracy of the model on the test set.\n2. Error Correction Accuracy: the accuracy of the adapted model on the misclassified set.\n3. Forgetting Rate Percentage: the percentage of previously correctly-classified samples that are now misclassified by the adapted model.\n\nThe results show that the system allows users to correct most of the misclassification errors with limited feedback. For example, correcting 51.1% of classification errors on Food-101 requires the user to annotate just one sample per class.\n\nThe quantitative results are reported in Table 1, which shows the error correction accuracy and forgetting rate for different numbers of annotated samples per class. The results indicate that the system is effective in correcting misclassification errors and has a low forgetting rate.\n\nThe system is also demonstrated to be efficient and backpropagation-free, making it suitable for deployment on edge devices. However, the text notes that further optimizations are possible by building prototypical inference directly on detection features, and leaves this exploration for future work.",
      "similarity_to_abstract": 0.6336477994918823,
      "rouge1": 0.34599156118143465,
      "rouge2": 0.06355932203389832,
      "rougeL": 0.14345991561181434,
      "bertscore_f1": 0.7845742106437683
    },
    "led": {
      "summary": "Our system uses two datasets, the Food-101 and the Oxford Flowers-102, to train DINO-V2. Each dataset is split into 70% training, 15% validation, and 15% testing per class; i.e., we preserve 70% of the images of each class in the training set. Few-shot error correction experiments are conducted with s ‚àà 1 , 2 , 3 , 4 , 5 , 7 , 10 , 20 , 50 . Quantitative results of our system are reported in Table 1 in terms of error correction accuracy A c c e a s e Acc_{E) and forgetting rate F o n i r . For the image classification task, our system performs well: the base recognition accuracy is 90.6% and 94.3% on the Food 101 and the Flowers 102 datasets, respectively. Both datasets represent domain-specific applications (e.g., food or flower recognition) where the user may want to provide the correct label to improve the AI model. We evaluate our system via three key metrics. We refer to accuracy as the percentage of correctly classified images out of the total number of images in the set. Base Recognition Accuracy : The accuracy of the adapted model M A M_{A} on the previously misclassified set E \\_(see Section 4.2 ). Backpropagation-free : We compute the accuracy of this adapted model using these annotations. The object detection model YOLOV8-Nano (Jocher et al., 2023) is trained on OpenImages-V7 and Distributed onto MobileNet-v2. For the server-side prototypes computation, we set K = 3 K=3 .We store the three most representative prototypes per class. Our system allows user to correct most of the misclassification errors with limited feedback. For example, correcting 51.1% of classification errors on Food-100 requires the user to annotate just one sample per class At the same time, only 0.018% of previously-corrected samples are now misclassified by the adapted method. In order of increasing confidence in the system, we perform several studies to assess its performance against real-world scenarios.",
      "similarity_to_abstract": 0.5728752613067627,
      "rouge1": 0.35589941972920697,
      "rouge2": 0.06990291262135921,
      "rougeL": 0.13539651837524178,
      "bertscore_f1": 0.7638124823570251
    }
  },
  {
    "paper_id": "2511.21635v1",
    "abstract": "Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb .",
    "gemini-2.5-flash": {
      "summary": "The provided text examines the relationship between information flow and Neural Collapse (NC) geometry in Vision Transformers (ViTs), specifically how computational processes affect the quality of final representations.\n\nThe research highlights a \"depth inefficiency\" in larger ViTs, where deeper, more computationally expensive models like ViT-L produce inferior final representations compared to shallower, more efficient ones like ViT-B. It introduces the **Information Scrambling Index (ISI)** as a key metric, finding it strongly associated with the final NC geometry.\n\nThe authors identify a consistent **three-phase \"Cliff-Plateau-Climb\" pattern** characterizing how ViT representations evolve with depth. A crucial finding during the \"Climb\" phase is the emergence of Neural Collapse and the systematic **marginalization** of the [CLS] token (its centrality decreases). This marginalization is correlated with better performance and a more favorable NC geometry, suggesting that trained models effectively bypass the [CLS] token as an information bottleneck, favoring a distributed patch-token consensus.\n\nMechanistically, **ViT-B** maintains a **controlled scrambling** regime (ISI roughly 0.004 to 0.009), which leads to more complete [CLS] marginalization and superior final NC geometry. In contrast, **ViT-L** enters a regime of **excessive scrambling** or \"chaotic diffusion\" (ISI up to approximately 0.031). This over-communication requires ViT-L to use significantly more layers (e.g., almost twice as many blocks for marginalization) to achieve comparable or worse information-task tradeoffs, resulting in degraded NC geometry despite greater computational cost. ViT-L, therefore, expends considerable compute and energy for inferior representational outcomes.",
      "similarity_to_abstract": 0.8460569977760315,
      "rouge1": 0.4418604651162791,
      "rouge2": 0.14953271028037382,
      "rougeL": 0.2046511627906977,
      "bertscore_f1": 0.7866541147232056
    },
    "llama-3.1-8b-instant": {
      "summary": "The provided text discusses the behavior of Vision Transformers (ViT) in deep learning, specifically their scaling laws and computational efficiency. Here's a summary:\n\n1. **The Scaling Paradox**: Neural scaling laws suggest that larger models should perform better, but this isn't always the case for transformers. Large models like ViT-L don't necessarily outperform smaller models like ViT-B, and may even degrade in performance.\n\n2. **Information Scrambling Index**: The authors introduce an Information Scrambling Index to measure how well the model is using its layers to process information. They find that this index is strongly associated with the final representation geometry of the model.\n\n3. **Neural Collapse**: The authors observe that in the final layers of the model, the token [CLS] designed as an information hub shows systematic marginalization. This is correlated with better performance, suggesting that the model is bypassing centralized aggregation in favor of distributed consensus among patch tokens.\n\n4. **Cliff-Plateau-Climb Pattern**: The authors identify a consistent three-phase pattern (Cliff-Plateau-Climb) in how the representations evolve with depth. The quality and timing of phase transitions are strongly associated with the final representation geometry.\n\n5. **Computational Inefficiency**: The authors find that ViT-L, which is computationally more expensive, doesn't necessarily outperform ViT-B. In fact, ViT-L reaches a comparable information-task tradeoff only after 18 layers, whereas ViT-B does so by layer 8.\n\nThe main contributions of the study are:\n\n1. **Identification of the Cliff-Plateau-Climb pattern**: This pattern characterizes Vision Transformer representations in the given setting.\n2. **Empirical evidence for better performance**: The study shows that better performance is associated with a controlled mixing regime and the bypassing of centralized aggregation in favor of distributed consensus among patch tokens.",
      "similarity_to_abstract": 0.8598195314407349,
      "rouge1": 0.47679324894514763,
      "rouge2": 0.19067796610169493,
      "rougeL": 0.1940928270042194,
      "bertscore_f1": 0.7923063635826111
    },
    "led": {
      "summary": "The Scaling Paradox and Depth Inefficiency Although previous studies have shown that larger models trained on more data tend to perform better on larger datasets, currently very little is known about the relationship between how large a model is and its final geometry. In our experiments, we show that the Information Scrambling Index, measured in the middle layers, is positively associated with the final NC geometry in ViT-S/B/L in our experiments; this gives a mechanistic perspective on why the computationally more efficient ViT+B attains a more favorable structure than the less efficient deeper, less efficient ViTL. The depth inefficiency of our transformers is also explained by the scaling paradox andDepth Inefficiency. Although previous work has examined how different types of data lead to different states, such collapsed states rarely connects the quality of the computational process to the geometry of the endpoint . Our results help bridge this gap. We find that the information scrambling index is strongly associated with \"the final NC geometrically in Vision Transformers\" , indicating that the more efficient VTRs tend to be more advanced than those that are slightly deeper. Another important finding concerns the Climb-Climb pattern, also known as the Cliff-Plateau-Climb pattern, which describes how representations evolve with depth. Although all models follow this trajectory, their execution differs: the Plateau extends with depth , the Smoothie extends at least six, 12 and 14 layers, and the quality and timing of phase transitions are strongly associatedWith the final representation geometry. Our findings highlight an inefficiency in this regime and point to the importance of the CLS token, which acts as a bottleneck.",
      "similarity_to_abstract": 0.8338329195976257,
      "rouge1": 0.379690949227373,
      "rouge2": 0.11529933481152994,
      "rougeL": 0.141280353200883,
      "bertscore_f1": 0.785434365272522
    }
  },
  {
    "paper_id": "2511.21689v1",
    "abstract": "Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity‚Äôs Last Exam (HLE) remains both conceptually challenging and computationally expensive.\nWe show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks.\nWe introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools.\nToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards.\nUsing ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query.\nOn HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient.\nOn œÑ 2 \\tau^{2} -Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost.\nExtensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools.\nThese results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "gemini-2.5-flash": {
      "summary": "This text details the development and evaluation of **Orchestrator-8B**, a system designed to strategically manage and utilize a diverse set of AI tools.\n\n**Key Findings & Performance:**\n*   Orchestrator-8B consistently **outperforms** other strong models like GPT-5, Claude Opus 4.1, and Qwen3-235B-A22B at a given budget.\n*   It can achieve **similar results at a substantially lower cost** by strategically coordinating tools and avoiding excessive calls to expensive models.\n*   Performance for all models improves as the maximum allowed turns (and thus cost) increases.\n*   Evaluations on benchmarks like **HLE** show Orchestrator-8B outperforming prior methods with significantly lower computational cost, and on **œÑ¬≤-Bench**, it demonstrates effective scheduling of various tools.\n\n**Methodology & Capabilities:**\n*   Orchestrator-8B employs a **unified tool calling interface** that broadens the toolset to include domain-specialized models, specified using JSON.\n*   Its **generalization capability** is tested with tool configurations containing models unseen during training, including various query writers, code writers, math models, and generalist models.\n*   Training involves **Reinforcement Learning (RL)** with a reward system that balances correctness, resource efficiency, and user preferences.\n*   An automatic **data synthesis pipeline** generates \"ToolScale,\" a publicly available dataset of multi-turn tool-use examples, to aid RL training.\n\n**Overall Contribution:**\nOrchestrator-8B demonstrates advanced capability in managing a heterogeneous set of tools, pushing the intelligence and cost-effectiveness boundaries of tool-use systems by learning to strategically choose and coordinate resources.",
      "similarity_to_abstract": 0.6539819836616516,
      "rouge1": 0.4519015659955258,
      "rouge2": 0.06292134831460675,
      "rougeL": 0.17002237136465326,
      "bertscore_f1": 0.7676576375961304
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the performance of an AI model called Orchestrator-8B in managing a heterogeneous set of tools to complete tasks. Here's a summary of the main points:\n\n1. **Performance improvement with increased budget**: The model's performance improves as the maximum number of allowed turns increases, indicating that more resources result in better outcomes.\n2. **Orchestrator-8B outperforms other models**: Orchestrator-8B consistently outperforms other models, such as GPT-5, Claude Opus 4.1, and Qwen3-235B-A22B, at a given budget and achieves similar results at a lower cost.\n3. **Generalization capability**: The model is tested with a tool configuration containing models unseen during training, demonstrating its ability to generalize and adapt to new tools.\n4. **Unified tool calling**: The model exposes all tools through a single, unified interface, making it easier to integrate and manage various tools.\n5. **Reward design**: The model's reward design balances three objectives: correctness, efficiency, and alignment with user preferences, resulting in a cost-effective and user-controllable tool-use policy.\n6. **Automatic data synthesis pipeline**: A pipeline is built to generate thousands of verifiable multi-turn tool-use training examples with complex environments across 10 domains, which will be made publicly available as the ToolScale dataset.\n7. **Evaluation on challenging tasks**: The model is evaluated on three challenging tasks, including HLE and œÑ 2 \\tau^{2} -Bench, and demonstrates superior performance and efficiency compared to prior methods.\n\nOverall, the text highlights the capabilities and advantages of Orchestrator-8B in managing a heterogeneous set of tools and achieving better performance and efficiency compared to other models.",
      "similarity_to_abstract": 0.7355135083198547,
      "rouge1": 0.42474226804123716,
      "rouge2": 0.08281573498964805,
      "rougeL": 0.16907216494845362,
      "bertscore_f1": 0.7766395211219788
    },
    "led": {
      "summary": "Orchestrator-8B consistently outperforms GPT-5, Claude Opus 4.1 and Qwen3235B-A22B at a given budget, and can achieve similar results at a substantially lower cost. This demonstrates the capability of Orchestrator 8B to manage a heterogeneous set of tools, and pushes the intelligence boundary of the system as a whole. The authors experiment with settings where only 10, 20, 50 and 100 turns are allowed to finish a task. As the maximum number of allowed turns increases, all models show improved performance. To evaluate Generalization capabilities, the authors test it with a tool configuration containing models unseen during training. In addition to domain-specific models, the tools are described in JSON as a list of objects. Each object defines the tool name, description, and a typed parameter schema. The complete catalog of tools used in our training is provided in Appendix D .6.3 Unified Tool Calling In contrast to prior tool-use agents [ 19 , 20 ] , we broaden the toolset to include domain specific models and expose all tools through a single, unified interface. This approach models the variability in tool availability and cost structures across users, yielding a richer supervisory signal for optimizing orchestrstrator. 3.3 Generalization To evaluate Orchestrators generalization capability, the author tests it with model sets containing models not seen during training using a tool that was not used during the training. Overall, the results demonstrate the superiority of the non-Oriental based machine learning over the more expensive and traditional analytical tools. 6.3 Cost-effectiveness The authors perform several experiments to determine the relative performance of different machine learning tools. They use a random sample of 10 training tasks to train the various tools; they obtain the trajectories of the LLMs to finish these tasks; and they ask another LLM to write the description of the task according to the task instructions, LL trajectories and whether the LLMs complete the tasks. 7.1 Unifiedtool Calling For example, the following steps are taken to obtain the descriptions of some tools: 1) randomly sample ten training tasks; 2) obtain the definitions of the tools; 3) Ask another LL to write an LL's description based on the task instruction, LLL trajectories & whether the LL completes tasks. In Appendix C , we show an example description ofQwen3236B. The entire catalog of recommended tools is available in Appendix d . The authors conduct further experiments to predict the overall performance of their product. They find that the most advanced tools outperform the less powerful ones.",
      "similarity_to_abstract": 0.6968333125114441,
      "rouge1": 0.3202511773940345,
      "rouge2": 0.05354330708661417,
      "rougeL": 0.12244897959183673,
      "bertscore_f1": 0.7585479021072388
    }
  },
  {
    "paper_id": "2511.20942v1",
    "abstract": "In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them.\nLarge language models (LLMs) often produce fluent yet shallow responses that miss this structure.\nWe present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure.\nTMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds.\nWe evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions.\nResults show that symbolic constraints consistently improve the structural quality of explanations for ‚Äúhow‚Äù and ‚Äúwhy‚Äù questions.\nThis study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.",
    "gemini-2.5-flash": {
      "summary": "The provided text describes Ivy+TMK-Structured, an inferencing model designed to generate specific and checkable explanations by integrating symbolic control from a structured knowledge base (TMK) with a Language Model (LLM, specifically GPT-4o-mini).\n\nThis model operates through a four-stage pipeline:\n1.  **Question scope classification.**\n2.  **Structured knowledge retrieval:** From the TMK model, it retrieves relevant entries like the parent Goal (inputs/requirements), Mechanism (a finite-state machine with guarded transitions for procedural \"how-to\" knowledge), and Concept items (domain-specific concepts/relations). For \"how-type\" questions, Mechanisms are prioritized.\n3.  **Constrained synthesis.**\n4.  **Coherence optimization.**\n\nUnlike basic models, Ivy+TMK-Structured provides an explicit formal representation (states, guards, success/failure) and operational semantics of a skill. The TMK layer organizes knowledge into Goals, Mechanisms, Concepts, Tasks (declarative objectives), and Relations (domain constraints). This structured approach enables the system to determine permitted steps, trace preconditions, identify failure triggers, and perform state-based reasoning, which is crucial for skill explanation and diagnostic feedback in GenAI agents.",
      "similarity_to_abstract": 0.6391173601150513,
      "rouge1": 0.3659305993690852,
      "rouge2": 0.0507936507936508,
      "rougeL": 0.15141955835962145,
      "bertscore_f1": 0.7786216139793396
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a system called Ivy, which integrates a language model (LLM) with a structured knowledge model called TMK (Task and Mechanism Knowledge) to generate responses to user questions. Here's a summary of the key points:\n\n1. **Ivy's architecture**: Ivy consists of a two-layer inferencing model, which is a four-stage pipeline that combines symbolic control from TMK models with an LLM. The pipeline includes question scope classification, structured knowledge retrieval, constrained synthesis, and coherence optimization.\n2. **TMK model**: The TMK model consists of three components: Task, Mechanism, and Knowledge. Tasks specify the objective of a task, Mechanisms describe procedural knowledge, and Knowledge captures declarative associations between domain concepts.\n3. **Question scope classification**: Ivy classifies user questions as in-scope or out-of-scope based on the TMK model.\n4. **Structured knowledge retrieval**: Ivy retrieves the top-k relevant TMK entries for each question, including the parent Goal, Mechanism, and Concept items.\n5. **Constrained synthesis**: Ivy uses the retrieved knowledge to generate a response to the user question, taking into account the constraints and permissible associations defined in the TMK model.\n6. **Coherence optimization**: Ivy optimizes the response to ensure coherence and consistency with the knowledge retrieved from the TMK model.\n7. **Benefits**: Ivy's approach enables accurate tracing of preconditions, failure triggers, and state-based reasoning, which is crucial for skill explanation and diagnostic feedback.\n\nOverall, Ivy is a system that leverages the strengths of both LLMs and TMK models to generate high-quality responses to user questions, while ensuring consistency and coherence with the knowledge retrieved from the TMK model.",
      "similarity_to_abstract": 0.6239813566207886,
      "rouge1": 0.29397590361445786,
      "rouge2": 0.0387409200968523,
      "rougeL": 0.16867469879518074,
      "bertscore_f1": 0.7722819447517395
    },
    "led": {
      "summary": "The two-layer inferencing model is realized as a four-stage pipeline, with an LLM 1 1 . The pipeline integrates symbolic control from TMK models with an LLLM 1 . We used GPT-4o-mini as the LLM in Ivy to ensure that our approach remains accessible and reproducible with cost-efficient models. The model is then refined using four stages. The first stage retrieves the top-k k TMK entries relevant to Q Q : the parent goal, the Mechanism, the goals, the state machine with start/failure, and guarded transitions, and any Concept items. This yields a compact, typed specification of the operational semantics for safely returning a guard. The explanation that emerges is not only more specific and checkable, but also closer to the form of reasoning instructors model in class-precisely the kind of structure our evaluation later measures under the causal, teleological, and compositional dimensions. Input : user question Q Q ; TMK model T T ; language model M M Output : response R R Four-Stage Pipeline and Algorithmic Implementation The four-step pipeline is realized through a four step pipeline, which is formalized in Algorithm 1. Each stage consists of three steps: question scope classification, structured knowledge retrieval, structured perception, symbolic control, temporal control, logical reasoning, and coherence optimization. Across four stages, we continue within the TMK layer. An Ivy+TMK model would also recognize the question as in-scope, but it does not have an explicit structured representation. It cannot determine or enforce which steps are permitted. In this way, our model uses structured knowledge over purely declarative content because it is a how-to type of question. The idea is taken from the ReturnGuardMechanism schema (Table 1 ) -\"conceptually represented in the representation, TMK-Structured\". TMK gives us something stronger than \"relevant text\" to generate a response; it gives it the Operational semantics of the skill. Our model is further refined using GPT 4o min as the Low-Field Machine Learning model to ensure its availability and reproducibility with cost efficient models. BackGuard example introduced above.",
      "similarity_to_abstract": 0.5518073439598083,
      "rouge1": 0.3073770491803279,
      "rouge2": 0.04526748971193416,
      "rougeL": 0.11885245901639345,
      "bertscore_f1": 0.7811359763145447
    }
  },
  {
    "paper_id": "2511.21570v1",
    "abstract": "In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term \"responsible computational foresight\", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers‚Äô ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.",
    "gemini-2.5-flash": {
      "summary": "The provided text introduces **responsible computational foresight**, a field that utilizes human-centric AI and computational tools to empower policymakers in exploring a broad spectrum of possible futures, rather than relying on precise predictions.\n\nIts core principle is that AI should **augment human intelligence**, enhancing foresight by illuminating patterns, analyzing scenarios, and supporting human decision-making, collaboration, and creativity, rather than replacing it. This approach integrates various tools, including forecasting, complex systems simulation, creative scenario-building, participatory futures (involving diverse stakeholders), and futures literacy.\n\nBy doing so, responsible computational foresight aims to strengthen ethical decision-making, build resilience against uncertainty, and enable informed, adaptive, and inclusive strategies. Ultimately, it seeks to help society navigate complexity and shape sustainable and equitable paths for generations to come, aligning insights with human values and agency.",
      "similarity_to_abstract": 0.8650280237197876,
      "rouge1": 0.40340909090909094,
      "rouge2": 0.0857142857142857,
      "rougeL": 0.17613636363636365,
      "bertscore_f1": 0.8172131776809692
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the concept of \"responsible computational foresight\" in policymaking, which involves using computational tools to anticipate and prepare for various possible futures. The approach emphasizes the importance of combining human intelligence with artificial intelligence (AI) to support informed, adaptive, and inclusive decision-making. \n\nKey principles of responsible computational foresight include:\n\n1. Augmenting human intelligence rather than replacing it\n2. Leveraging AI's strengths, such as data processing and scenario analysis, to support human thinking\n3. Focusing on a range of possible futures, including unexpected and desirable outcomes\n4. Emphasizing ethical and sustainable decision-making\n\nThe text highlights various tools that can be used for responsible computational foresight, including:\n\n1. Superforecasting\n2. World simulation\n3. Participatory futures\n4. AI-assisted deliberation\n5. Futures literacy\n\nThese tools can be used to empower policymakers to make more informed decisions and shape sustainable and equitable paths forward for generations to come. The approach aims to navigate complexity and uncertainty by anticipating a range of possible outcomes and preparing for different scenarios.\n\nIn summary, responsible computational foresight is a framework that combines human and artificial intelligence to support informed and adaptive policymaking, enabling policymakers to anticipate and prepare for various possible futures and make more sustainable and equitable decisions.",
      "similarity_to_abstract": 0.8759654760360718,
      "rouge1": 0.4413145539906103,
      "rouge2": 0.07075471698113206,
      "rougeL": 0.20187793427230047,
      "bertscore_f1": 0.811286985874176
    },
    "led": {
      "summary": "This paper discusses the role of human-centric artificial intelligence in supporting responsible foresight and how policymakers use a range of computational tools to help them anticipate risks and seize opportunities. While prediction is useful for predicting the future, it cannot address the full spectrum of policy challenges. Policymakers instead need to engage in responsible foresighted , exploring a wide range of possible futures. They call this field responsible computational foresight . They believe that while AI can be used to augment human intelligence rather than replacing it, it should not replace it. The different kinds of foresight include modeling, simulation, creative scenario building, and the integration of diverse perspectives. They also support ethical decision-making and enhancing resilience in the face of uncertainty. In particular, they see great value in incorporating participatory futures, futures literacy, and futures education.",
      "similarity_to_abstract": 0.843446671962738,
      "rouge1": 0.44817927170868344,
      "rouge2": 0.10140845070422536,
      "rougeL": 0.21288515406162464,
      "bertscore_f1": 0.8250963687896729
    }
  },
  {
    "paper_id": "2511.21444v1",
    "abstract": "Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.",
    "gemini-2.5-flash": {
      "summary": "The provided text introduces **Extreme Weather Expert (EWE)**, a novel framework designed to automate the diagnosis of extreme weather events, a task traditionally manual, labor-intensive, and prone to bias.\n\nEWE formalizes this diagnosis as an autonomous exploration and reasoning task for a multimodal large language model (MLLM)-powered agent, following an iterative cycle of Thought, Action, Observation, and Interpretation to build physically consistent causal explanations.\n\nThe framework integrates three core components:\n1.  **Knowledge-Enhanced Planning:** Leverages Chain-of-Thought (CoT) prompting and expert examples to decompose diagnostic tasks into structured, knowledge-anchored sub-goals.\n2.  **Self-Evolving Closed-Loop Reasoning:** Executes plans using specialized tools, incorporating a \"Checker\" module to validate both the operational success and physical plausibility of each action's output.\n3.  **Meteorological Toolkit:** A specialized library providing functions for meteorological data retrieval, processing, and computation of diagnostic equations, grounding the agent's conclusions empirically.\n\nThis work makes several key contributions: it is presented as the first dedicated to automated extreme weather analysis, establishes the first comprehensive dataset for this task (103 high-impact events), and introduces a novel LLM-based step-wise evaluation metric to assess the entire analytical workflow. The goal is to advance automated scientific discovery and democratize meteorological expertise.",
      "similarity_to_abstract": 0.8458106517791748,
      "rouge1": 0.48501362397820164,
      "rouge2": 0.15342465753424658,
      "rougeL": 0.24523160762942778,
      "bertscore_f1": 0.8036455512046814
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the challenges of traditional methods for diagnosing extreme weather events and proposes a novel framework called Extreme Weather Expert (EWE) to automate this process. The traditional approach relies on human experts manually analyzing multiple data sources, which is labor-intensive, time-consuming, and prone to subjective biases.\n\nTo overcome these limitations, the authors propose an autonomous exploration and reasoning task for a machine learning language model (MLLM) powered agent. The diagnostic workflow is abstracted into an iterative trajectory consisting of four stages:\n\n1. Thought: The agent plans and decomposes the diagnostic task into knowledge-anchored sub-goals.\n2. Action: The agent uses tools on meteorological data to execute the plan.\n3. Observation: The agent collects and validates data from the tools.\n4. Interpretation: The agent integrates observations with its internal knowledge to construct a physically consistent causal explanation for the target event.\n\nThe EWE framework consists of three core components:\n\n1. Knowledge-Enhanced Planning: Uses Chain-of-Thought (CoT) prompting with expert exemplars to guide the agent's analytical procedure.\n2. Self-Evolving Closed-Loop Reasoning: Validates the agent's actions and outputs to ensure the integrity of the diagnostic pathway.\n3. Meteorological Toolkit: Provides a specialized library of functions for meteorological data retrieval, processing, and computation of canonical diagnostic equations.\n\nThe authors also introduce a novel, LLM-based step-wise evaluation metric to assess the agent's diagnostic capabilities. The EWE framework is validated using a comprehensive dataset of 103 high-impact extreme weather events, and its potential to democratize expertise and provide accessible scientific insights is discussed.",
      "similarity_to_abstract": 0.8212334513664246,
      "rouge1": 0.4018912529550827,
      "rouge2": 0.1187648456057007,
      "rougeL": 0.2033096926713948,
      "bertscore_f1": 0.7978166341781616
    },
    "led": {
      "summary": "Extreme Weather Expert (EWE) is a framework for automated extreme weather analysis that leverages Chain-of-Thought prompting with expert exemplars to decompose the diagnostic task into a plan of knowledge-anchored sub-goals. It also leverages Self-Evolving Closed-Loop Reasoning. The agent executes the plan by invoking tools. A unified Checker module then validates both the operational success and physical plausibility of each action's output before proceeding, ensuring the integrity of the entire diagnostic pathway. To overcome the limitations of Large Language Modsing and the computation of canonical diagnostic equations, the authors propose EWE, a novel framework integrating three core components as illustrated in Fig. 2 . In this paper, they introduce the first comprehensive dataset for this task and include step-wise evaluation metrics that assesses the entire analytical workflow. This provides a granular assessment of an agent's true diagnostic capabilities. They propose Extreme Weather Expert using a MLLM-powered agent. Their goal is to progressively construct a physically consistent causal explanation for the target event. To implement this process, they propose ExtremeWeather Expert, a system that combines \"Knowledge-Enhanced Planning\" with \"Meteorological Toolkit\" with a specialized library of functions for meteorological data retrieval, processing, and computation of canonically derived equations. Each action is validated by multiple steps of Thought, Action, Observation, and Interpretation. The result is a highly accurate, reproducible, and more easily accessible scientific discovery",
      "similarity_to_abstract": 0.8218343257904053,
      "rouge1": 0.3928571428571428,
      "rouge2": 0.09743589743589742,
      "rougeL": 0.17346938775510207,
      "bertscore_f1": 0.8017232418060303
    }
  },
  {
    "paper_id": "2511.21577v1",
    "abstract": "The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal.\nPrevious watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.",
    "gemini-2.5-flash": {
      "summary": "The provided text discusses the **transferability of HarmonicAttack**, a learning-based attack designed to remove audio watermarks, across various watermarking schemes and datasets.\n\nBased on spectrogram analysis of watermarked audio (e.g., WavMark and AudioSeal), two main reasons for this transferability are identified:\n1.  **Shared Principles:** All watermarking schemes, despite distinct embedding heuristics, exploit the **auditory masking effect**. This leads to watermark residuals concentrating in similar frequency regions (typically around 1000 Hz), allowing HarmonicAttack, even when trained on one scheme (like AudioSeal), to generalize to others (like WavMark).\n2.  **Spatial Overlap:** Although schemes like WavMark spread residuals broadly and AudioSeal localizes them, there's significant **spatial overlap** in their watermark-embedded regions. HarmonicAttack can exploit these common spectral and temporal patterns for effective removal.\n\nThe text emphasizes that HarmonicAttack demonstrates **consistently outstanding performance** and strong transferability, even when transferring across datasets (e.g., from LibriSpeech to FMA) and unseen victim schemes. While this generalization is robust, it is noted that it might not be guaranteed for all possible watermarking schemes or data samples, particularly if degrading audio quality is acceptable; in such cases, additional training or fine-tuning might be required. The authors highlight HarmonicAttack as the first learning-based attack to achieve such principled transferability across state-of-the-art watermarking schemes.",
      "similarity_to_abstract": 0.651375412940979,
      "rouge1": 0.2981366459627329,
      "rouge2": 0.04374999999999999,
      "rougeL": 0.1366459627329193,
      "bertscore_f1": 0.7398887276649475
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the effectiveness of a machine learning-based attack called HarmonicAttack against various audio watermarking schemes. Here's a summary:\n\n1. **Analyzing spectrograms**: The authors analyze the spectrograms of watermarked audio samples and find that different watermarking schemes, such as AudioSeal and WavMark, share a common structure due to the auditory masking effect. This common structure enables HarmonicAttack to generalize to unseen schemes.\n\n2. **Transferability**: HarmonicAttack, trained on AudioSeal, can effectively transfer to WavMark because their watermark-embedded regions spatially overlap, allowing the attack to exploit similar spectral and temporal patterns for watermark removal.\n\n3. **Limitations**: However, such generalization is not guaranteed for all possible watermarking schemes and data samples, especially in scenarios where degrading the protected audio quality is acceptable. In such cases, additional training or fine-tuning on samples from the new scheme would be required.\n\n4. **Defenses**: The authors discuss the limitations of codec-based approaches, which rely on generic compression artifacts that may not be effective against all watermarking schemes. In contrast, HarmonicAttack shows consistently outstanding performance on all victim methods.\n\n5. **Related work**: The authors mention recent advances in generative audio models, which have made synthetic content nearly indistinguishable from human-created content. However, this does not directly relate to the effectiveness of HarmonicAttack against audio watermarking schemes.",
      "similarity_to_abstract": 0.7491779923439026,
      "rouge1": 0.3209876543209877,
      "rouge2": 0.037267080745341616,
      "rougeL": 0.16666666666666669,
      "bertscore_f1": 0.755693793296814
    },
    "led": {
      "summary": "HarmonicAttack is the first learning-based attack demonstrating that mode g transferability across a range of state-of-the-art watermarking schemes due to shared algorithmic principles,i.e., watermarks‚Äô perturbations follow perceptual masking rules. The authors present several defenses against HarmonicAttack. Their defense against the attack consists of listening to sound samples presented in Figures 5a, 5b, 5c, 5d, and 5e with corresponding spectrograms of watermark signals in Figure 7 in Appendix A . In this chapter, we discuss several different techniques for detecting and removing watermarks from audio samples. Our attention is initially focused on AudioSeal but we also analyze the spectrogram analysis of WavMark-watermarked audio samples as well. These studies provide two key observations supporting the transferability discussion of Harmonic Attack across datasets and different victim watermarkings schemes. First, although watermarkers schemes use distinct embedding heuristics, they are all constrained by and exploit the auditory masking effect [SwansonRobustAudioWatermarking1998, spreadspectral ] . As a result, watermark residuals tend to concentrate in similar frequency regions (typically around 1000 Hz). This common structure enablesharmonicAttack, trained solely on Audio Seal, to generalize to unseen schemes such as Wav Mark. Two other observations support our generalization of SoundSquad's approach to watermark removal. Second, WAVMark watermark data tends to be spread much more broadly across the time/frequency domain whereas AudioSale produces more localized residuals. This phenomenon arises because Watermark resolutes are spread over larger time scales. However, this generalization is not guaranteed for all watermarked samples and data samples, especially in scenarios where degrading the protected audio quality is acceptable. In such cases, additional training or finetuning on samples from the new scheme would need to be done to adapt the model effectively. Fifth and sixth Defenses against Harmonautic Attack include detection devices such as QRID and Gatekeeper. Additionally, musicians may have their music professionally certified as protected unless they agree to give up their DRM free copies of their music. No harm can be done in such cases.",
      "similarity_to_abstract": 0.7785145044326782,
      "rouge1": 0.27586206896551724,
      "rouge2": 0.04157043879907621,
      "rougeL": 0.12413793103448274,
      "bertscore_f1": 0.7545558214187622
    }
  },
  {
    "paper_id": "2511.21622v1",
    "abstract": "Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22 , 000 22,000 between 2012 and 2023 ( ho2024algorithmic ) . Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10 √ó 10\\times of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10 √ó 10\\times , yielding a total under 100 √ó 100\\times . This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that ‚Äì contrary to standard assumptions ‚Äì an algorithm‚Äôs efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6 , 930 √ó 6,930\\times efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.",
    "gemini-2.5-flash": {
      "summary": "The text discusses the scale-dependent effects on model training efficiency, noting that while efficiency gains initially converge between different scaling laws, they diverge significantly at larger scales. Chinchilla model training efficiency gains are substantial but remain below 10x for much of the contemporary machine learning scale (10^16 - 10^24 FLOPs).\n\nThe study compares LSTMs with various Transformer architectures (Modern, Retro, and Kaplan Transformers). A key finding is that neural network architecture improvements, particularly the LSTM-to-Transformer shift, are not scale-invariant and exhibit increasing returns to scale. At smaller scales, LSTMs are only about 6.28x less compute-efficient than Transformers, but this gap widens dramatically, leading to massive efficiency gains (up to 10,000x) for Transformers over LSTMs at frontier scales. This architectural shift accounts for 68% of measured frontier-scale efficiency gains, with Kaplan-to-Chinchilla rebalancing contributing most of the rest. Conversely, factors like optimizer choice (Adam vs. SGD) and minor Transformer variants (Modern vs. Retro) are found to be relatively scale-invariant.\n\nThe paper challenges traditional views of algorithmic progress, noting that while it's often seen as much faster than hardware progress (e.g., 22,000x vs. 32x over ten years), at smaller scales (e.g., 10^18 FLOPs), algorithmic gains can be less than hardware progress (around 20x).\n\nKey limitations include the fickle nature of hyperparameters (e.g., difficulty scaling SGD with batch sizes over 128) and reliance on literature-based estimates for some algorithmic improvements. However, the results are bolstered by alignment with other studies, which report similar scaling exponents (e.g., 0.092-0.106).",
      "similarity_to_abstract": 0.792948842048645,
      "rouge1": 0.4421052631578947,
      "rouge2": 0.05919661733615222,
      "rougeL": 0.15578947368421053,
      "bertscore_f1": 0.7906091809272766
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the efficiency of neural network training and its scaling laws. Here's a summary:\n\n1. The Chinchilla model's training efficiency gains are large but remain below 10x for most scales used in contemporary machine learning. Its efficiency initially converges with other scaling laws, but diverges at larger scales.\n\n2. Experiments were conducted on various model architectures, including LSTMs, transformers, and their variants. The results show that the transformer model is more efficient, but the efficiency gap decreases at smaller scales.\n\n3. The study found that algorithmic progress is not as fast as hardware progress at small scales. In fact, hardware progress is faster at small scales.\n\n4. The decomposition of algorithmic progress reveals that the LSTM-to-Transformer architectural shift is responsible for 68% of the efficiency gains at frontier scales.\n\n5. The study also found that improvements in neural network architecture are not scale-invariant and have increasing returns to scale effects.\n\n6. The choice between Adam and SGD optimizers, as well as the difference between modern and retro transformers, seems to be relatively scale-invariant, suggesting that optimizers may exhibit constant CEG functions.\n\n7. The study highlights the importance of considering scale-dependent effects when evaluating algorithmic efficiency and suggests that conventional metrics may not be sufficient to capture the complexity of algorithmic progress.",
      "similarity_to_abstract": 0.747991144657135,
      "rouge1": 0.38694638694638694,
      "rouge2": 0.07494145199063232,
      "rougeL": 0.16317016317016317,
      "bertscore_f1": 0.7861352562904358
    },
    "led": {
      "summary": "The authors implement two different model architectures to serve as basins for their experiments: LSTMs and transformers. In addition, they implement two transformer variants. The first is our Modern Transformer , which incorporates all the recent improvements and serves as our default transformer model; and the second is our Retro Transformer introduced in Section 3.2 . We also define Kaplan‚Äôs ‚ÄúKaplan‚Äù Transformers to be adjusted analytically to approximate Kaplan scaling, representing our ablations as well as suboptimal data contributors to algorithmic progress. Scaling Experiments: LstM to Transformer The main limitation is the fickle nature of hyperparameters. For instance, we found it nearly impossible to scale a transformer trained with SGD and a batch size over 128. We are more confident in our results because our performance and exponents aligns well with other studies like porian2024resolving and melis2017state. Using the same tokenizer but sligh but larger batch sizes, we are able to give literature-based estimates. We did not run scaling interventions on all the algorithms in our study individually, though when we ablate all algorithmic improvements, we find very small exponent differences in scaling exponents. Our experiments demonstrate that training efficiency initially converges between the two scaling laws, but diverges at larger scales. Furthermore, the Chinchilla training efficiency gains are large but remain below 10x for much of the scale used in machine learning (i.e., 10 16 ).",
      "similarity_to_abstract": 0.698805034160614,
      "rouge1": 0.40732265446224253,
      "rouge2": 0.1103448275862069,
      "rougeL": 0.15102974828375287,
      "bertscore_f1": 0.7904946208000183
    }
  },
  {
    "paper_id": "2511.21668v1",
    "abstract": "The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite AI‚Äôs critical role, standard workflows still assume all training samples contribute equally. On the other hand, next-generation systems require AI models that are accurate, efficient, and sustainable. This paper questions the assumption of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed method optimizes computation and energy use. We perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework that selectively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method preserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.",
    "gemini-2.5-flash": {
      "summary": "This paper addresses the inefficiency of indiscriminate data usage in machine learning training, particularly for large datasets and energy-constrained AI deployments in telecom. Through an analysis of per-sample gradient dynamics, it reveals that a significant portion of training data contributes minimally to model improvement.\n\nThe authors propose a lightweight, model-agnostic mechanism that estimates sample importance during training based on their gradient norms, dynamically filtering or re-weighting data. This approach is distinct from influence functions or forgetting metrics as it does not require retraining or access to ground truth labels.\n\nThe framework supports energy-efficient AI development by reducing redundant computation and data usage. Empirical validation on real-world telecom datasets demonstrates that models trained with only a fraction of carefully chosen samples can achieve comparable performance to those trained on the full dataset (e.g., 23-28% data reduction on specific datasets) while significantly reducing training time and compute costs. The core message is that selective training based on gradient importance is more effective and efficient than using all data indiscriminately.",
      "similarity_to_abstract": 0.8466845154762268,
      "rouge1": 0.44574780058651026,
      "rouge2": 0.07669616519174041,
      "rougeL": 0.18181818181818182,
      "bertscore_f1": 0.8092108964920044
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a novel approach to machine learning (ML) that focuses on selectively training and prioritizing samples that drive meaningful parameter updates, aiming to improve training efficiency and reduce computational costs. The key contributions of this paper are:\n\n1. An analysis of per-sample gradient dynamics across training epochs using a real-world open telecom dataset, showing that a significant portion of training data contributes minimally to loss reduction or generalization.\n2. A lightweight, model-agnostic mechanism to estimate sample importance during training based on gradient norm behavior, which dynamically filters or re-weights data based on gradient importance.\n3. A framework that supports energy-efficient AI development by reducing redundant computation and data usage during training, which is especially relevant for large-scale network operators.\n\nThe proposed method categorizes samples into critical, redundant, or detrimental to performance, and dynamically scores and filters samples during training based on their gradient norms. The results show that models trained using only a fraction of the data can achieve comparable performance to those trained on the full dataset while reducing training time and compute cost.\n\nThe authors validate this approach on multiple telecom datasets, including Telecom Italia, Vendor, and 5G beam selection, and demonstrate notable computational and energy efficiency gains without significant accuracy loss. The method achieves comparable performance with a reduced dataset, with reductions of 28%, 23%, and 22% respectively.\n\nThe paper concludes by suggesting future research directions, including linking gradient-norm statistics to generalization behavior, incorporating dynamic or curriculum-style selection mechanisms, and benchmarking against more advanced core-set or influence-based approaches under comparable computational budgets.",
      "similarity_to_abstract": 0.7953504920005798,
      "rouge1": 0.4295612009237875,
      "rouge2": 0.09280742459396751,
      "rougeL": 0.16166281755196305,
      "bertscore_f1": 0.8033057451248169
    },
    "led": {
      "summary": "In this paper, the authors perform an analysis of per-sample gradient dynamics across training epochs using a real-world open telecom dataset. They show that depending on the dataset size, different datasets benefit substantially in training efficiency, reduced computation time, and sustained or improved accuracy. Their framework helps to energy-efficient AI development by reducing redundant computation and data usage during training, especially relevant for large-scale network operators seeking to deploy AI models across distributed infrastructure.2 Related Works Previous works investigating core-set selection [ seneractive active ] and influence functions [ koh2017 understanding] have explored the advantages of larger datasets with more samples. This paper focuses instead on selectively training and prioritizing samples that drive meaningful parameter updates. The results confirm that training on carefully chosen samples is more effective than using all data indiscriminately and offer guidance for balancing accuracy against efficiency when slight performance trade-offs are acceptable. This work represents a conceptual rather than an algorithmic contribution, focusing on empirical insight and practical utility. It discusses several approaches for predicting sample importance during training and proposes a way to estimate sample importance based on gradient norm behavior.",
      "similarity_to_abstract": 0.7804014086723328,
      "rouge1": 0.4313725490196078,
      "rouge2": 0.07887323943661971,
      "rougeL": 0.14565826330532214,
      "bertscore_f1": 0.8114176392555237
    }
  },
  {
    "paper_id": "2511.21613v1",
    "abstract": "Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal‚ÄîURLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.",
    "gemini-2.5-flash": {
      "summary": "This text describes research into using additional metadata signals to accelerate Large Language Model (LLM) pretraining, highlighting that the **fine-granularity of metadata is crucial** for its effectiveness.\n\nKey findings include:\n\n*   **Metadata Integration:** The study investigates both **prepending** (adding metadata at the beginning) and **appending** (adding metadata at the end) to accelerate pretraining.\n*   **Prepending Effectiveness:** Prepending metadata, particularly URL, fine-grained quality scores, and fine-grained domain information, showed significant acceleration. This allowed models to achieve the performance of a 100B-token baseline using 20-40% fewer tokens (e.g., 60B tokens).\n*   **Appending Effectiveness:** Appending metadata also provided benefits, though generally less pronounced than prepending. Fine-grained domain information, coarse-grained quality scores, and URL appending improved downstream performance, enabling training with 20% fewer tokens. However, appending fine-grained quality scores did *not* improve performance.\n*   **Mechanistic Insights:** The research suggests that metadata acts as a \"soft regularization,\" encouraging the model to compress salient information into its hidden states. Learnable meta tokens can partially recover speedup and encode quality-aware information. Layer-wise probing revealed better encoding of topic, quality, and authorship in the model's latent space due to metadata.\n*   **Combined Metadata:** While combining URL and fine-grained quality scores showed faster initial learning, this advantage tended to diminish as training progressed.",
      "similarity_to_abstract": 0.7625432014465332,
      "rouge1": 0.40848806366047746,
      "rouge2": 0.06933333333333334,
      "rougeL": 0.22281167108753314,
      "bertscore_f1": 0.783117413520813
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses ways to accelerate pretraining of large language models (LLMs) using metadata. Researchers explore the effectiveness of different metadata types, including URLs, coarse-grained quality scores, fine-grained quality scores, and domain information. Here are the main findings:\n\n1. **Fine-grained metadata is key**: The study shows that fine-granularity of metadata is crucial in bringing the acceleration effect. Fine-grained metadata types, such as fine-grained quality scores and domain information, lead to stronger model performance compared to their coarse-grained counterparts.\n2. **URL and quality score metadata are helpful**: Prepending URLs and fine-grained quality scores to the input text helps improve downstream performance and accelerates training.\n3. **Combining metadata may not be beneficial**: When combining URL and fine-grained quality score metadata, the model shows faster learning in the early stages, but this advantage diminishes as training progresses.\n4. **Metadata can serve as soft regularization**: The study suggests that metadata can serve as a form of soft regularization, providing another learning signal for the model weights.\n5. **Acceleration effect is not as great as prepending**: While metadata helps accelerate training, the acceleration effect is not as great as prepending, which still allows training on 20% fewer tokens to achieve the same performance.\n\nOverall, the study highlights the importance of fine-granularity in metadata for accelerating LLM pretraining and provides insights into how metadata can be used to improve model performance.",
      "similarity_to_abstract": 0.7826095819473267,
      "rouge1": 0.375,
      "rouge2": 0.08542713567839197,
      "rougeL": 0.18,
      "bertscore_f1": 0.7924085259437561
    },
    "led": {
      "summary": "Pretraining efficiency is enhanced on three axes: \"better pretraining data,\" \"better metadata,\" and \"better quality scores.\" In particular, we examine how metadata can accelerate training time by serving as auxiliary signals in conjunction with URL appending. We show that learnable meta tokens can partially recover the speedup with metadata, where attention patterns to these tokens encode quality-aware information. The model shows faster learning during the early stages of training, indicating that the combined metadata helps it acquire information more quickly. Related Work Related work We conduct layer-wise probing of latent representations for topic, quality, and authorship, providing mechanistic insight into how these factors are better encoded in the latent space (Section 4.5 ). We report the downstream performance for the five metadata types in the right panel of Figure 2 , with the corresponding averages summarized in Table 1 . Among these runs, appending fine-grained domain information helps the most. Additionally, coarse-grain quality score and URL Appending also help improve downstream performance. The acceleration effect of helpful metadata is not as great as prepending, however, we are still able to train on 20% fewer tokens to achieve the same performance as standard pretraining. It is noteworthy that appending coarse-gained quality scores leads to better downstream performance overall. Our model demonstrates that downstream performances progress with prepending different metadata types. The final average performance across downstream tasks is reported in Table1 . Among the five task types tested, both URL and fine-graded quality scores yield comparable acceleration, reaching the performance of a 100B-token baseline after only 60B tokens. Fine-grassed domain metadata also provides a boost, surpassing the 100B token baseline with 20B fewer tokens. It seems that quality score, domain information, and quality score can be helpful, but fine granularity of the metadata is the key to bringing the acceleration effect.",
      "similarity_to_abstract": 0.8928545117378235,
      "rouge1": 0.4042553191489362,
      "rouge2": 0.05128205128205128,
      "rougeL": 0.14893617021276598,
      "bertscore_f1": 0.8021751642227173
    }
  },
  {
    "paper_id": "2511.20693v1",
    "abstract": "Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose A 2 Flow , a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators . A 2 Flow employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism , which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that A 2 Flow achieves a 2.4% and 19.3% average performance improvement and reduces resource usage by 37% over state-of-the-art baselines.",
    "gemini-2.5-flash": {
      "summary": "A2 Flow is a system designed to enhance the automation of generating agentic workflows by employing Large Language Models (LLMs) as optimizers within **Self-Adaptive Abstraction Operators**.\n\nUnlike previous methods (e.g., AFLOW, DebFlow, MermaidFlow) that rely on predefined operators, A2 Flow **automatically extracts these operators from expert data** without requiring human expertise or prior knowledge, addressing issues of poor generalization and redundant operators.\n\nThe operator extraction process comprises three stages:\n1.  **Case-based Initial Operator Generation:** LLMs use expert training data and reasoning to generate task-specific operators (e.g., ObserveEnvironment(), CreatePlan()).\n2.  **Operator Clustering and Abstraction:** Similar operators are grouped to form preliminary abstract operators.\n3.  **Deep Extraction for Abstract Execution Operators:** Advanced reasoning (e.g., Long CoT, self-consistency) refines these into task-aware abstract operators.\n\nBy injecting these self-adaptive operators and an Operators Memory into the workflow search, A2 Flow allows the LLM-based optimizer to dynamically generalize and merge execution steps. This approach enables the joint optimization of both workflow structures and operator representations, leading to more compact, task-specific, and generalized solutions. Experiments confirm the vital role of these self-adaptive abstraction operators in improving workflow automation and generalization.",
      "similarity_to_abstract": 0.8316636681556702,
      "rouge1": 0.546875,
      "rouge2": 0.19895287958115182,
      "rougeL": 0.3125,
      "bertscore_f1": 0.8059767484664917
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a new system called A 2 Flow that enhances the automation of generating agentic workflows. Agentic workflows involve tasks that require decision-making, problem-solving, and adaptability. A 2 Flow employs Large Language Models (LLMs) as optimizers within Self-Adaptive Abstraction Operators to improve workflow automation and generalization.\n\nHere's a summary of the key points:\n\n1. A 2 Flow extracts operators from expert data without relying on predefined human expertise, using three stages: \n    - Case-based Initial Operator Generation\n    - Operator Clustering and Abstraction\n    - Deep Extraction for Abstract Execution Operators\n2. The system uses self-adaptive abstract operators to enable automatic discovery of effective operator sets that improve workflow automation and generalization.\n3. Unlike previous systems like AFLOW, A 2 Flow injects a self-adaptive operator set and Operators Memory into the workflow search, allowing the LLM-based optimizer to dynamically generalize and merge execution steps.\n4. The system is evaluated on a comprehensive suite of eight publicly available benchmarks spanning five distinct domains, showing significant improvements in workflow automation and generalization.\n5. The Self-Adaptive Abstraction Operators module offers larger gains than the Operator Clustering and Abstraction module, and both modules are essential for the system's performance.\n\nA 2 Flow addresses the limitations of previous systems, such as poor generalization to unseen tasks and reliance on predefined operators. It enables automatic discovery of effective operator sets, improving workflow automation and generalization, and has the potential to be applied in various domains.",
      "similarity_to_abstract": 0.8717195987701416,
      "rouge1": 0.5173210161662818,
      "rouge2": 0.16705336426914152,
      "rougeL": 0.2309468822170901,
      "bertscore_f1": 0.8185037970542908
    },
    "led": {
      "summary": "The A 2 Flow enhances the automation of generating agentic workflows by employing Large Language Models (LLMs) as optimizers within Self-Adaptive Abstraction Operators. The extraction consists of three stages: 1) Case-based initial operator generation, where prompts use expert training data and LLM reasoning to generate case-aware operators for each task type; 2) Operator clustering and abstraction, where operators with similar functions across cases are grouped into preliminary abstract operators; and 3) Deep Extraction for Abstract Execution Operators , where Long CoT and self-consistency enable deep reasoning and multi-path search to refine task-aware abstract operators. To make this exploration more efficient, AFLOW (Zhang2410aflow) introduces Operators that encapsulate common agentic operations. Our A 2 flow further automates the search for agentic information by using Large Language Model operators.3 Methodology The final optimized workflow is defined as a means of efficiently performing repetitive tasks without relying on predefined human expertise. It leverages large language Models, LLLMs, and other machine learning techniques to improve workflow automation and generalization.",
      "similarity_to_abstract": 0.9081199765205383,
      "rouge1": 0.5698324022346368,
      "rouge2": 0.20786516853932585,
      "rougeL": 0.335195530726257,
      "bertscore_f1": 0.8308785557746887
    }
  },
  {
    "paper_id": "2511.21517v1",
    "abstract": "Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns.\nFor example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker‚Äôs vocal characteristics may play a role in gender assignment.\nThis risks misgendering speakers‚Äîwhether through masculine defaults or vocal-based assumptions‚Äîyet how ST models make these decisions remains poorly understood.\nWe investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (enes/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch. Keywords: gender bias, speech translation, interpretability, XAI",
    "gemini-2.5-flash": {
      "summary": "This paper investigates the mechanisms Speech-to-Text (ST) systems use to assign gender, particularly when translating gender-neutral speaker-referring terms (e.g., \"I am a student\") from English into Romance languages (Spanish, French, Italian). The authors aim to understand how ST models leverage acoustic information for gender assignment and challenge common assumptions, such as pitch being the primary acoustic cue or models merely replicating training data associations.\n\nTheir analysis reveals several key findings:\n1.  **Masculine Prevalence Bias:** ST models learn a general \"masculine prevalence\" rather than memorizing term-specific gender pairings. The decoder's internal language model exhibits a strong masculine bias by default.\n2.  **Audio Overrides Bias:** While the internal language model shows masculine bias, acoustic information from the input audio can override these preferences.\n3.  **First-Person Pronouns as Gender Markers:** Models utilize first-person pronouns (\"I\") as a functionally gendered marker, linking the speaker's voice to the gendered term.\n4.  **Formants over Pitch:** Crucially, the gender information from vocal cues is primarily encoded in **formant frequencies**, rather than pitch, which challenges previous assumptions about key acoustic features for gender assignment in ST.\n\nThese findings demonstrate that ST models employ distinct mechanisms for gender disambiguation and suggest that current bias mitigation strategies focusing solely on pitch manipulation or rebalancing training data may be insufficient.",
      "similarity_to_abstract": 0.8955087065696716,
      "rouge1": 0.558891454965358,
      "rouge2": 0.1763341067285383,
      "rougeL": 0.23094688221709006,
      "bertscore_f1": 0.8130934238433838
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the mechanisms underlying gender assignment in spoken translation (ST) systems. The authors investigate how ST models use acoustic information for gender assignment, as previous research has shown that pitch can be a key acoustic feature for perceived gender. However, the authors' analysis reveals that ST models use a different approach:\n\n1. **Models learn broader patterns of masculine prevalence**: Instead of memorizing individual term-gender pairings, models learn that masculine forms are generally more prevalent.\n2. **First-person pronouns are used to link gendered term to speaker**: Models use first-person pronouns to access vocal cues primarily in the first and second formants, rather than relying on pitch.\n3. **Models can override bias**: Information from the audio can override the models' preferences for masculine defaults.\n4. **Conformer models have lower accuracy**: Conformer models, which have a different architecture and scope, achieve lower accuracy in gender assignment, suggesting that gender assignment strategies are model-dependent.\n\nThe authors conclude that:\n\n* Mitigation strategies centered on pitch manipulation or rebalancing training data may prove insufficient.\n* Models use distinct mechanisms for gender disambiguation.\n* Further research is needed to understand the complexities of gender assignment in ST systems and to develop targeted mitigation strategies.\n\nThe study uses the MuST-SHE benchmark, a dataset containing annotations for gender-neutral English terms in natural speech, to investigate the mechanisms underlying gender assignment in ST systems.",
      "similarity_to_abstract": 0.8679837584495544,
      "rouge1": 0.46501128668171554,
      "rouge2": 0.14512471655328799,
      "rougeL": 0.18058690744920994,
      "bertscore_f1": 0.799656093120575
    },
    "led": {
      "summary": "In this paper, the authors examine how different types of language systems perform gender assignment in ST. Specifically, they focus on misgendering in ST: when systems translate speaker-referring terms into gendered target language forms that do not alig access to vocal characteristics, such as pitch, that could be used as proxies for the speaker's gender when translating terms that refer to them bentivogli-etal-2020-gender , as in the example above. The results suggest that models use more than memorizing individual term-gender pairings from training data to predict which genders are prevalent in a given language pair. Rather, they learn patterns of masculine prevalence across all language pairs. Furthermore, they use first-person pronouns to link the gendered term back to the speaker, challenging the commonly held belief that pitch would be the key acoustic feature determining gender . The authors note that while interpretability methods have been used to better understand gender bias in MT vanmassenhove2019lost , wisniewski202218analyzing ; attanasio-etalt-2023-tale ; manna-etall-2025-paying , research on the mechanisms underlying gender assignments in ST is scarce xu2023recent ; fucci-ettal-20 25-different ; yang2025 toward . This gap is critical: without understanding how models make these ethically sensitive decisions, developing targeted mitigation for gender bias becomes significantly more challenging. To investigate the mechanisms ST models use to assign gender to speaker-regarding terms, we start from the common assumption that attributes gender bias to training data imbalances tatma ring terms when translating from English to three Romance languages. Our analysis revealed that rather than memoriz[ing individual phrase-specific terms, models learn that masculine forms are generally more prevalent. The decoder exhibits strong bias toward masculine defaults independent of input audio, but information from the audio can override these preferences. Similar results are obtained when models using vocal cues are compared with Conformer models, which achieve lower accuracy (39.2-49 for feminine terms and 72.5-76.7% for masculine terms across the three language pairs). We assess how gender assignment strategies vary across different ST system configurations.Data We use MuST-SHE bentivoggli-etylfe 2020-gender as a benchmark containing annotations for gender neutral English terms in natural speech that require gender marking when translated to Spanish, French, or Italian. In particular, we focus on the subset containing terms referring to the Latin tones. Bias Statement Following blodgett-etatal-2020--language , we make explicit the assumptions underlying our work on bias. We focus onmisgenderendering in ST; when systems translates speaker-referencing terms into Gendered native language forms That task is particularly challenging because it entails attempting to mitigate gender bias by applying appropriate mitigations over input spectrograms. The findings of our analysis across three foreign language pairs (enes/fr/it) challenge common assumptions about how ST systems perform Gender assignment. The models we study do not simply replicate term-specific associations from training datasets but learn broader patterns of male prevalence. While the ILM exhibits masculine bias, models can override some of these biases. Similarly, their approach uses first- person pronouns to further unnerve the listener and provide additional support through the addition of third-person singularities. Overall, the overall findings challenge previous assumptions regarding gender assignment.Bias Statement At the end of each language pair, we publish the statements of the conclusions of our entire paper. Next, we discuss the limitations of our field. For instance, many people believe that there is little scientific evidence supporting the notion that innate bias in ST systems cause perceiving gender bias due to factors other than the degree to which a system learns to perceive perceived gender. In this paper however, we address this issue point blank. How exactly does ST systems determine whether perceived gender? We study this issue by comparing contrastive feature attribution over input spectralrograms (¬ß 8 ). What we find in our sample is that model-dependent gender assignment strategy varies across multiple language pairs regardless of the type of streaming data. Thus, even if an ILM has a bias towards masculine, it is possible for a model to go beyond this bias by using features other than pitch to derive its own biased result.",
      "similarity_to_abstract": 0.8673962950706482,
      "rouge1": 0.37915742793791574,
      "rouge2": 0.18000000000000002,
      "rougeL": 0.19733924611973394,
      "bertscore_f1": 0.8092761635780334
    }
  },
  {
    "paper_id": "2511.21557v1",
    "abstract": "Vision-Language-Action (VLA) models have significantly advanced general-purpose robotic manipulation by harnessing large-scale pre-trained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two-finger grippers as their default end-effectors. However, such grippers face inherent limitations in handling certain real-world tasks‚Äîsuch as wiping glass surfaces or opening drawers without handles‚Äîdue to insufficient contact area or lack of adhesion.",
    "gemini-2.5-flash": {
      "summary": "The text introduces **VacuumVLA**, an end-to-end multimodal robotic policy specifically designed for a **suction-gripper hybrid end-effector**. This innovative system aims to overcome the limitations of traditional parallel grippers, enabling robots to perform a wider variety of household tasks.\n\nKey aspects of VacuumVLA include:\n\n1.  **Hybrid End-Effector:** It combines adjustable suction capabilities (for wide-area or point suction) with traditional two-finger gripping for handling diverse objects (e.g., glass slides, wallets, cucumbers, bananas).\n2.  **Comprehensive Action Set:** The gripper can execute \"prime actions\" such as Suction, Grasp, and various \"Move\" actions (push, pull, lift, press), which can be combined for complex tasks.\n3.  **Enhanced VLA Model Design:** To address the \"shortcut problem\" (where models might simply copy the often consistent suction status), VacuumVLA extends its output to include two additional dimensions, explicitly representing the suction status of both the left and right arms.\n4.  **Inputs and Action Generation:** The model integrates visual inputs, natural language instructions, and the robot‚Äôs proprioceptive state, generating precise continuous actions using a conditional Flow Matching model.\n5.  **Evaluation:** VacuumVLA's effectiveness is evaluated using state-of-the-art frameworks like œÄ‚ÇÄ (initialized from PaliGemma) and DexVLA.\n6.  **Hardware:** It utilizes a common dual 6-axis-arm manipulation platform with a fixed base, equipped with a fixed top-view camera and two wrist-mounted cameras.",
      "similarity_to_abstract": 0.6335620880126953,
      "rouge1": 0.1891891891891892,
      "rouge2": 0.034013605442176874,
      "rougeL": 0.08108108108108107,
      "bertscore_f1": 0.7385305762290955
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a new robotic system, VacuumVLA, designed for household tasks using a suction-gripper hybrid end-effector. The system integrates visual inputs, natural language instructions, and the robot's proprioceptive state to generate a continuous distribution over actions, including suction and grasping.\n\nTo address the \"shortcut\" problem, where the model simply copies the input state, the output is extended by two dimensions corresponding to the suction status of the left and right arms. The system is built upon two state-of-the-art frameworks: œÄ0 and DexVLA.\n\nThe hybrid end-effector has multifunctional capabilities, including:\n\n1. Suction: The system uses a suction tool that can be adjusted to fit different objects, such as large glass slides or small objects like wallets.\n2. Grasp: The system uses a two-finger gripping function to grasp objects with handles, such as cucumbers and bananas.\n3. Move: The system can move parts of an object, including push, pull, lift, and press.\n\nThe system is designed to accomplish household tasks by combining these actions. The Vision-Language-Action (VLA) model is primarily enhanced by incorporating a new dimension for the suction tool.\n\nThe system's effectiveness is evaluated using experiments with two distinct frameworks, œÄ0 and DexVLA. The results show that the system can generate highly precise and realistic outputs using a conditional flow matching model.",
      "similarity_to_abstract": 0.6949017643928528,
      "rouge1": 0.1966101694915254,
      "rouge2": 0.0477815699658703,
      "rougeL": 0.1016949152542373,
      "bertscore_f1": 0.7520459294319153
    },
    "led": {
      "summary": "In this paper, the authors describe the latest state-of-the-art scientific methods for creating a suction gripper-gripper hybrid end-effector. VacuumVLA is an end-to-end multimodal robotic policy specifically designed for such a device. To evaluate its effectiveness, we conduct experiments using two distinct state-ofthe-of art frameworks: œÄ 0pi0/d DexVLA [ 6 ] and DexVla [ 8 ] . Vacuum VLA integrates visual inputs, natural language instructions, and natural kinesthetic actions and then proceed with suction (point suction). Grasp refers to a series of actions that move a certain part of an object, including push, pull, lift and press . For example, to close a drawer and lift the lid of a delivery box, the push function and the lift function of the gripper are utilized. Our hybrid end effector can successfully execute the aforementioned prime actions and therefore accomplish household tasks by combining these actions. For learning VLA (Vision-Language-Action) models, the design uses a common dual six-axis platform with a fixed base. The viewpoint includes a fixed top-view camera and two wrist-mounted cameras, one on each arm. The robot's observation at timestep t t consists of base and hand visual inputs V t b V^{b} t , V t left V^{\\text{lefn cups driven by the Gripper jaw} . Thus, we can adjust the distance between the two suction cups to fit different objects. We use a common two-finger gripping function to grasp common objects as well as handling more complex tasks. Gradually the model learns based on status f i n True , False . The status only changes during the specific action chunk when the suction is turned on or off. Such transitions constitute a relatively small proportion of all action chunks in the dataset, making the model prone to the \"shortcut‚Äù problem. Therefore, in our VLA design, the input is similar to previous VLA approaches, but the output is extended by two dimensions, corresponding to the Suction status of the left and right arms, respectively. In contrast to previous designs, the vacuum gripper does not simply copy what is fed into the model but instead blends various task-specific actions across the entire task space. Next, the scientists develop a new type of gripper called parallel grippers. Parallel grippers are effective and easy to control but due to their simple structure, they are unable to perform some relatively complex tasks\".",
      "similarity_to_abstract": 0.6920202970504761,
      "rouge1": 0.14437367303609341,
      "rouge2": 0.021321961620469083,
      "rougeL": 0.06369426751592357,
      "bertscore_f1": 0.7477582693099976
    }
  },
  {
    "paper_id": "2511.21005v1",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the I ntrinsic C onfidence-Driven Group Relative P reference O ptimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.",
    "gemini-2.5-flash": {
      "summary": "The provided text describes **ICPO (Intrinsic Confidence-driven Policy Optimization)**, a method designed to improve upon existing policy optimization techniques like GRPO.\n\nHere's a summary of its key aspects:\n\n1.  **Addresses Coarse-Grained/Binary Rewards:** ICPO tackles the problem where multiple generated responses receive identical external rewards (e.g., binary correct/incorrect), which GRPO fails to differentiate. This lack of \"fine-grained\" feedback in GRPO leads to insufficient optimization signals and can cause policy collapse.\n2.  **Provides Continuous Optimization Signals:** ICPO introduces \"preference advantage scores\" based on response generation probabilities. These scores allow it to distinguish the relative merits among responses with the same external rewards, providing continuous and meaningful optimization signals.\n3.  **Mitigates Noisy Rewards:** The intrinsic confidence-driven reward mechanism helps mitigate interference from noisy rewards. It can suppress inherently erroneous responses that receive erroneously inflated rewards, and encourages exploration of high-quality actions.\n4.  **Enhances Training Stability and Performance:** By providing fine-grained signals, ICPO significantly improves training stability and ultimate performance. It gradually surpasses GRPO's performance, especially in sparse reward environments.\n5.  **Promotes Stable Exploration:** ICPO maintains policy entropy at a stable exploratory state throughout training. It prevents premature convergence to false high-reward paths, encouraging thorough exploration of high-quality actions in low-confidence regions, thereby reducing entropy collapse and boosting accuracy.\n6.  **Balances Incentives:** It encourages correctly generated trajectories with lower confidence (facilitating novel knowledge acquisition) and reduces the weight of erroneous trajectories with high confidence (curbing overconfidence), all while maintaining a moderate incentive mechanism.",
      "similarity_to_abstract": 0.5595376491546631,
      "rouge1": 0.37472766884531594,
      "rouge2": 0.052516411378555804,
      "rougeL": 0.16122004357298475,
      "bertscore_f1": 0.7781688570976257
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a new approach called Intrinsic Confidence-Driven Optimization (ICPO) for improving the performance of language models in tasks such as text generation. ICPO uses a reward mechanism that is based on the model's intrinsic confidence levels, rather than just external rewards. This allows ICPO to:\n\n1. Provide fine-grained optimization signals: ICPO can distinguish between responses with the same rewards, providing more accurate guidance for policy updates.\n2. Mitigate interference from noisy rewards: ICPO can suppress erroneous responses with high probability and inflated rewards, and encourage correct responses with lower confidence levels.\n3. Maintain exploration: ICPO prevents premature convergence to false high-reward paths, encouraging thorough exploration of high-quality actions in low-confidence regions.\n4. Handle coarse-grained rewards: ICPO can still effectively explore and achieve stable performance gains even when external rewards are sparse or identical.\n\nThe text compares ICPO to a baseline approach called Generative Rejective Policy Optimization (GRPO), and shows that ICPO outperforms GRPO in several key areas, including:\n\n1. Training stability and performance: ICPO achieves better performance and stability than GRPO, even when external rewards are sparse or identical.\n2. Policy entropy: ICPO maintains a stable exploratory state throughout the training process, while GRPO experiences premature convergence and policy collapse.\n3. Preference advantage scores: ICPO can assign distinct preference advantage scores to each response, even when external rewards are identical, providing more fine-grained optimization signals.\n\nOverall, the text suggests that ICPO is a more effective and robust approach than GRPO for training language models, and that its intrinsic confidence-driven reward mechanism can help mitigate common issues such as noisy rewards and premature convergence.",
      "similarity_to_abstract": 0.6824201941490173,
      "rouge1": 0.3541666666666667,
      "rouge2": 0.06694560669456066,
      "rougeL": 0.14166666666666666,
      "bertscore_f1": 0.7860140204429626
    },
    "led": {
      "summary": "ICPO successfully distinguishes the relative merits among responses with the same rewards, offering more fine-grained guidance for policy updates. The intrinsic confidence-driven reward mechanism can mitigate two types of interference caused by noisy rewards. For responses that are inherently erroneous yet generated with high probability, when reward noise erroneously inflates their verifiable rewards, ICPO can effectively suppress such responses; Conversely, for responses which are inherently correct but geradually catches up and surpasses the performance of GRPO, IcpO outperforms the model. ICPo prevents premature convergence to false high-reward paths, encouraging thorough exploration of high-quality actions in low-confidence regions. This helps the model shift smoothly toward goal-directed exploration, reducing entropy collapse and boosting accuracy. In contrast, GRPO maintains a stable exploratory state throughout the entire training process, whereas ICPOO does not. I CPO outperforms GRPO in terms of stability and ultimate performance.3 Analysis of ICPOs Results We filter the training data, keeping only samples where all instances were entirely correct or entirely incorrect, and used them as coarse-grains reward training set. As shown in Table 2 , compared to GRPO,[2], ICP0 still effectively explores under complete reward sparsity. It progressively identifies high-potential response paths and ultimately achieves stable performance gains. Similarly, we perform an analysis of our own dataset using it as a coarse-grain reward training box. (1) Beyond Binary Rewards: Providing Continuous Optimization Signals . We filtered the trainingdata, kept only samples away from 100% of the instances and used these as coarse grain reward training sets. To provide signals for optimization signals, we filter the Training data further and use them as continuous optimization signals. 4 Analysis of Emotion Driven Rewards. We examine the effect of Intentional Confidence-Driven Rewards on learning performance. Through empirical analysis, we have discovered that the intrinsic confidence based reward system can mitigate bias caused by both noisy rewards and sources of noise. For example, we see that for responses with identical rewards, the advantage values computed by GRPO are exactly uniform, failing to accurately differentiate between the qualities of the responses. However, if responses have identical rewards then GRPO fails to distinguish between the merits of the same response. By classically ignoring the differences in worthiness of the rewards, this approach fails to offer sufficient guidance for future policy updates due to its small sample size. Using examples, we show that there are essentially no distinguishing characteristics between responses with similar rewards. 5 Our final section discusses the concept of \"interceptivity,\" i.e. how different kinds of problems produce different rewards depending on the amount of feedback they receive. GrPO is unable to compute the relative advantages among responses within the group, resulting in a lack of effective gradient signals for policy updating. This, in turn, triggers policy collapse and a sharp decline in model performance. 6 ICPOS solution to this issue is to introduce preference advantage scores based on the response generation probabilities. Even when the external rewards for all responses are identical, each response has multiple alternative rewards. 7 ICPOP can assign distinct preference advantage points to each response. When these preference advantages are fused with externally validated rewards, results become distinguishable, thereby providing meaningful and meaningful optimization signals for Policy updates. This significantly enhances training stability andultimate performance.",
      "similarity_to_abstract": 0.6638690829277039,
      "rouge1": 0.34308510638297873,
      "rouge2": 0.07733333333333334,
      "rougeL": 0.12765957446808512,
      "bertscore_f1": 0.7872601747512817
    }
  },
  {
    "paper_id": "2511.20934v1",
    "abstract": "While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10‚Äì40% of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.",
    "gemini-2.5-flash": {
      "summary": "This text describes the application of Definition Àú 2.5 to label quantities, where `t` represents the difference between the maximum and actual label length.\n\nThe core points of the text are:\n\n1.  **Derivation for the OR Operator:** Equations Àú 34-37 are derived similarly to previous equations, incorporating `Bott_1(I^C)` (minimum intersection of any concept, reflecting fully overlapping concepts). This quantity is often 0 in practice, simplifying the calculation. The logic extends to `Union_min(L)_x` and \"extras.\" Label quantities can be lower than bottom values due to concept combinations, and the reasoning for maximum intersection estimation is adapted by substituting `I^C` with `E^C`, `I^U` with `E^U`, and `N^C` with `SE^C`.\n\n2.  **Algorithm Completeness and IoU Guarantee:** The algorithm is **complete** and **guaranteed to find the combination of concepts** (`L_1 ‚äï ... ‚äï L_n`) that achieves the **highest possible alignment (IoU)**. This guarantee stems from three factors:\n    *   The heuristics used to explore the state space are **admissible** (Sections Àú F.1, F.3).\n    *   The quantities used for pruning are **underestimations** of the real IoU (Sections Àú F.2, F.4).\n    *   The algorithm exhaustively explores all nodes with an estimated alignment greater than the found solution.\n\n3.  **Proof of Admissibility:** The admissibility of the label quantities heuristic is proven by considering `Top` and `Bott` vectors (encoding maximum and minimum possible improvements). The proof involves analyzing heuristic estimations for the numerator and denominator in eq. Àú 3, especially in an \"ideal case\" where concepts fully overlap on extras, are pairwise disjoint, and have the largest possible element overlap with neuron activation.\n\n4.  **Quantity Calculation for Operators (Overlap Case):**\n    *   For **OR and AND**, unique quantities are estimated using formulas similar to the disjoint case.\n    *   For **AND NOT**, the unique quantity equals that of the left side (Observation Àú 1).\n    *   Common quantities are derived by combining definitions from Section Àú 2.2 and Observation Àú 1.\n    *   Since `L_‚Üí` is always an atomic concept (Assumption 2), its quantities are exact (detailed in Appendix Àú E).\n    *   Degenerate cases have their dIoU manually set to 0.\n\n5.  **Computational Efficiency:** To mitigate the cost of per-sample computations (requiring `|D|` comparisons), a \"lighter aggregated computation\" is introduced, obtained by summing values per label (e.g., transforming a sum of minimums into a minimum of sums).",
      "similarity_to_abstract": 0.28138017654418945,
      "rouge1": 0.32165605095541405,
      "rouge2": 0.03194888178913738,
      "rougeL": 0.14012738853503184,
      "bertscore_f1": 0.7162824273109436
    },
    "llama-3.1-8b-instant": {
      "summary": "The provided text appears to be a derivation and explanation of a method for calculating the intersection over union (IoU) of concepts in semantic segmentation. Here's a summary of the key points:\n\n1. **Derivation of Equations**: The text explains the derivation of equations for calculating the IoU of concepts in a given label L. The equations are based on the label quantities, which are defined in Section 2.5.\n\n2. **OR Operator**: For the OR operator, the equations are derived similarly to Equations 22 and 24. The added concept is represented by B‚Äão‚Äãt‚Äãt1‚Äã(IC)B_{\\text{ott}}^{1}(I^{C}), which corresponds to the minimum intersection of any concept in a given sample.\n\n3. **Algorithm Completeness**: The algorithm is complete and guaranteed to find the combination of concepts that captures the highest possible alignment (IoU). This is due to the admissibility of the label quantities heuristic and the exhaustive exploration of the search space.\n\n4. **Label Quantities Heuristic**: The text proves the admissibility of the label quantities heuristic using the exclusive paths for each operator.\n\n5. **Lighter Aggregated Computation**: To mitigate the computational cost of calculating the label quantities, a lighter aggregated computation is introduced by summing the values per label.\n\n6. **Overlap and Disjoint Cases**: The text explains how to handle overlap and disjoint cases, including the use of observation 1 and the definitions in Section 2.2 to derive the common quantities.\n\n7. **IoU Calculation**: The text outlines the steps to calculate the IoU of concepts in a given label, including the use of the derived equations and the label quantities heuristic.\n\nOverall, the text provides a detailed explanation of a method for calculating the IoU of concepts in semantic segmentation, including the derivation of equations, the algorithm completeness, and the lighter aggregated computation.",
      "similarity_to_abstract": 0.33557647466659546,
      "rouge1": 0.2976406533575317,
      "rouge2": 0.014571948998178506,
      "rougeL": 0.1560798548094374,
      "bertscore_f1": 0.738282322883606
    },
    "led": {
      "summary": "The OR operator is derived from the label quantities. The label quantities can be lower than the bottom values because they represent combinations of concepts that may further reduce th used for estimating the maximum possible intersection. Derivation of Equations 34 , 35 , 36 and 37 for the OR operator follows the same derivation as Equations 22 and 24 for the label quantity. Because the heuristics used to explore the state space are both admissible and freely available, because the quantities used to prune the frontier are guaranteed to be underestimations of the real IoU reachable from that node, and because the algorithm explores exhaustively all nodes in the search space with an estimated alignment greater than the found solution, the algorithm is complete and guaranteed to find the combination of concepts L 1 . Label Quantities Heuristic Let L be the label ob described in the previous section for the ùñ≥ùñä / Top{1} and ùñö /Bott{2}\\oplus L_{n} that captures the highest possible alignment (IoU) . F.2 Proof of Admissibility of the Label Quality HeuristicWe perform a detailed derivation of the exclusive paths for each operator considered in this paper by applying Definition 2.5 to the label quantities. In addition, we perform a weighted average of the above-mentioned methods for calculating the value of the \"offsets\" using different weights.",
      "similarity_to_abstract": 0.3876217007637024,
      "rouge1": 0.32271762208067944,
      "rouge2": 0.029850746268656716,
      "rougeL": 0.15286624203821655,
      "bertscore_f1": 0.7586565613746643
    }
  },
  {
    "paper_id": "2511.21591v1",
    "abstract": "Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model‚Äìprompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.",
    "gemini-2.5-flash": {
      "summary": "This study evaluates the performance of four large language models (LLMs)‚ÄîGPT-5-Thinking, Gemini-2.5-Pro, GPT-5-mini, and Llama 3.1 8B-Instruct‚Äîon 8-puzzle instances. Researchers tested various prompting strategies (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and provided additional attempts with different levels of feedback (repeat, specific, suggestive) and saved progress.\n\nThe findings reveal that all models struggle significantly with this \"simple\" task, primarily due to weak heuristic planning, leading to persistent looping, repeating moves, and cycling through visited states. Even when given only valid moves or with external validation, models failed to solve puzzles, instead defaulting to loops.\n\nWhile interventions like engineered prompts with feedback and saved progress did show some improvement for particular models (e.g., GPT-5-Thinking's success rate increased from 30% to 68% with AoT and suggestive feedback), these gains were slow, costly (requiring substantial time, moves, and tokens), and did not result in high reliability (no model exceeded a 70% success rate). Some models, like GPT-5-mini, even exhibited increased confusion and \"graceful failure\" (refusal to answer) with complex prompts, while Llama 3.1 8B-Instruct failed to solve any puzzles.\n\nThe study concludes that these findings have significant implications for LLM deployment, highlighting fundamental limitations in their planning and problem-solving capabilities, even for tasks requiring few logical steps, and the high resource cost for achieving only marginal improvements.",
      "similarity_to_abstract": 0.7298141121864319,
      "rouge1": 0.4724770642201835,
      "rouge2": 0.12442396313364057,
      "rougeL": 0.21559633027522934,
      "bertscore_f1": 0.8072463870048523
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the performance of four language models (GPT-5-Thinking, Gemini-2.5-Pro, GPT-5-mini, and Llama 3.1 8B-Instruct) on an 8-puzzle task with varying optimal solution lengths. The models were given three prompting strategies: Zero-Shot, Chain-of-Thought (CoT), and Algorithm-of-Thought (AoT). The results show that:\n\n1. None of the models were able to solve the puzzles reliably, with a success rate of over 70% achieved by only one model (GPT-5-Thinking) after multiple attempts and interventions.\n2. The models struggled with planning, often relying on weak heuristics or repeating moves and cycling through already visited states.\n3. The use of engineered prompts with feedback and saved-state progression showed improvement, but was costly and failed to make any model reliable.\n4. The models' performance declined as the level of feedback increased, suggesting that more informative feedback is not always beneficial.\n5. A separate experiment that isolated planning abilities by offloading part of the state tracking responsibilities revealed a critical failure in heuristic planning, with all models failing to solve any puzzles.\n\nThe findings have significant implications for the deployment of Large Language Models (LLMs), highlighting the need for further research into their limitations and potential biases.",
      "similarity_to_abstract": 0.7061700820922852,
      "rouge1": 0.46454767726161367,
      "rouge2": 0.11793611793611794,
      "rougeL": 0.20537897310513445,
      "bertscore_f1": 0.805483341217041
    },
    "led": {
      "summary": "GPT-5-Thinking, Gemini-2.5-Pro, Gemini 2.5 Pro, GPT- 5-mini, and Llama 3.1 8B-Instruct perform poorly at solving 8-puzzle instances with varying solution lengths. Using three different prompting strategies, including Zero-Shot, Chain-of-Thought , Yu2023bettercot , and Algorithm-of Thought , the models perform poorly on this task. The results suggest that even when given only valid moves, models either rely on heuristics so weak that solving a typical puzzle could require well over 100 moves or they begin repeating moves and cycling through already visited states. Although some models show signs of improvement with added feedback and saved state progression, these enhancements are slow and costly. Our findings have implications for LLM deployment as they suggest that all models struggle with the simple task of determining the correct solution to an 8-Puzzle set. In particular, we examine the performance of four models--Gemini-3.5 -Pro, PIO, Gemini 0.9-Degree, Gunther 1.0-Act, ZOMG, Methodology, and Significance In this study, we investigate the limits of model performance on this type of task. Using eight puzzles with varying problem lengths, each model receives a single attempts to solve them using one of three prompting strategies: zero-shot, chain-of thought, shot, loop, lateral guidance, and alguthic step, and algorithm-of_of-thought. Each model receives multiple attempts per puzzle before it solves the puzzle. To assist the models on puzzles they initially fail to solve by allowing three additional attempts per level of feedback, repeat, specific, and suggestive, the models gain more speed in solving the puzzles. However, no single approach performs best across models, and the improvements are very slow and expensive. The final interventi distance heuristic is used to aid the models by giving them fewer moves but this does little to make any real difference in their progress. This finding highlights that even if given onlyvalid moves, such as those involving a goal state, models rely on weak planning and therefore stall, repeating steps rather than making rapid progress toward the goal. Implications Taken as a whole, our findings have significant implications for LLLM deployment since they indicate that there are many different kinds of defects in LLM models that have implications not only for modeling but also for the tools LLM vendors use to develop better algorithms",
      "similarity_to_abstract": 0.6227628588676453,
      "rouge1": 0.4033898305084746,
      "rouge2": 0.0782312925170068,
      "rougeL": 0.15254237288135594,
      "bertscore_f1": 0.7932026982307434
    }
  },
  {
    "paper_id": "2511.21460v1",
    "abstract": "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.",
    "gemini-2.5-flash": {
      "summary": "This text introduces advancements in developing LLM agents with enhanced safety awareness and self-evolution capabilities, particularly for dangerous home tasks in interactive environments.\n\nThe key points are:\n\n1.  **Problem Addressed:** Existing datasets for dangerous home tasks are scarce, and single-agent LLM safety detection methods often suffer from **over-rejection** (incorrectly flagging safe tasks as unsafe), making them unreliable despite their ability to reject truly unsafe tasks.\n2.  **Dataset Contribution:** The authors established **SafeAware-VH**, a dataset containing safety and unsafe instructions within VirtualHome, to test agent safety awareness.\n3.  **MADRA Framework:** They propose **MADRA (Multi-Agent Debate Risk Assessment)**, a training-free, plug-and-play multi-agent debate framework. It uses a critical evaluator, iterative refinement, and consensus voting to curb single-LLM bias and significantly reduce false rejections.\n    *   **Effectiveness:** MADRA achieves a superior balance, reaching 90% unsafe task rejection while dramatically lowering safe task over-rejection to only 10% (compared to 50-70% for single-agent methods like ThinkSafe and Safety CoT).\n    *   **Critical Agent Role:** The capability of the Critical Agent within MADRA is crucial; more powerful models (e.g., GPT-4o) yield optimal performance.\n    *   **Generalizability:** Its robustness is validated across distinct embodied environments (VirtualHome, AI2-THOR).\n4.  **Hierarchical Planning Framework:** They also designed a task hierarchical planning framework based on cognitive collaboration. This framework integrates human-like cognitive modules such as safety, memory, planning, and reflection to achieve self-evolution, demonstrating high success rates in executing security task instructions.",
      "similarity_to_abstract": 0.8083558082580566,
      "rouge1": 0.5485961123110152,
      "rouge2": 0.17787418655097612,
      "rougeL": 0.2203023758099352,
      "bertscore_f1": 0.8022735118865967
    },
    "llama-3.1-8b-instant": {
      "summary": "The provided text discusses a study on enhancing the safety awareness of Language Model (LLM) agents in interactive environments. The researchers propose two main contributions:\n\n1. **MADRA (Multi-Agent Debate Risk Assessment)**: a universal safety module that uses a multi-agent debate framework to curb single-LLM bias and false rejections. This method is training-free, flexible, and can be applied across different scenarios and domains. MADRA uses a critical evaluator to drive iterative refinement and consensus voting.\n2. **Hierarchical planning framework for multi-agent cognitive collaboration**: a cognitive collaboration framework that integrates safety, memory, planning, and reflection to achieve self-evolution. This framework is designed to test the safety awareness of agents in VirtualHome.\n\nThe study evaluates the effectiveness of different safety enhancement methods, including:\n\n* **Safety CoT (Chain-of-Thought)**: a single-agent safety reminder technique that enables the model to function as a safety detector and perform risk assessments through step-by-step reasoning.\n* **ThinkSafe**: a single-agent risk assessment module that uses a hazard assessment module.\n* **MADRA + ThinkSafe**: the proposed method that combines the MADRA module with ThinkSafe.\n\nThe experimental results show that:\n\n* **MADRA** achieves a high rejection rate of unsafe tasks (up to 90%) and a low rejection rate of safe tasks (only 10%).\n* **Safety CoT** yields a high absolute rejection of unsafe prompts (up to 80%‚Äì93%) but over-rejects safe instructions.\n* **ThinkSafe** has a high rejection rate of safe tasks (up to 70%), indicating a problem of excessive rejection.\n\nOverall, the study demonstrates that the proposed MADRA method can effectively enhance the safety awareness of agents in interactive environments, alleviating the problem of excessive rejection.",
      "similarity_to_abstract": 0.7843033671379089,
      "rouge1": 0.4562118126272912,
      "rouge2": 0.1349693251533742,
      "rougeL": 0.2158859470468432,
      "bertscore_f1": 0.8001634478569031
    },
    "led": {
      "summary": "The authors of this paper propose a new method for evaluating the safety risk awareness of LLM agents in VirtualHome by feeding a safety awareness prompt directly into the LLM . They use a chain-of-Thought technique to provide a safety reminder and perform risk assessments through step-by-step reasoning. The main contributions of this study are summarized as follows: We propose MADRA: a multi-agent debate framework where a critical evaluator drives iterative refinement and consensus voting, curbing single-llm bias and cutting false rejections.We have designed a task hierarchical planning framework based on cognitive collaboration that integrates safety, memory, planning, and reflection to achieve self-evolution. Our method is inherently training-free, demonstrating universality and flexibility as a plug-and-play module that can be easily applied across different scenarios and domains.2. MADRA vs Safety CoT In order to further explore the safety awareness of agents in virtual home, the authors develop a dataset called SafeAware-VH. This dataset contains both safe and unsafe instructions. Their safety enhancement techniques are shown in Table 2. Overall, the results suggest that different safety enhancement methods are effective in improving various language models, measured by their rejection rates for safe and unsafe content.3. Critical Agent role in Multi-Agent Debate Risk Assessment MADRA provides a critical role in detecting unsafe tasks and over-rejects safe instructions.4. Over-rejection refers to the tendency for safe instructions to be incorrectly flagged as unsafe.5. We propose a risk assessment method based on multi-labor debate (MADRA) and apply it as a universal safety module to any task.6. Various approaches to evaluating agent safety are used in this study. For example, using a less capable model such as Llama3 as the Critical Agent leads to a significant increase in the number of unsafe tasks, while for more powerful models like GPT-3/GPT-4o the Critical Assessor yields near-perfect safe task success rates.7. Critical Model vs Safety Checker MADRA uses a Chain-of Thought technique to serve as a safety detector and performs risk assessment step by step reasoning.8. Safety-CoT pushes the unsafe task rejection rate to 80%‚Äì93% , a 20-56 percentage point gain over the raw model, but achieves respectable success rates on safe tasks up to 70.3% in AI2-THOR.9. A key finding is the critical role of the Critical Antagonist's capability within the Multi-Agile Decisions Risk Assessment module.10. R-Judge is a good for serving as a starting point for assessing safety risks in an interactive environment, but lacks household tasks. Therefore, they establish a dataset named SafeAaware-Vh, which contains safety and unsafe instruction.11. MADE vs Safety Countermeasures MADRA + CoT improves all eight language models tested in this field by feeding prompts directly into LLLM and feeding them with chains of thought. The experimental results presented in table 2 demonstrate the effectiveness of different safety enhancing methods across various languages models, demonstrated by their failure rates for safely and unsafe content.12. Single-agent Safety CoT yields the highest absolute rejection rate of unsafe prompts, but simultaneously over-rebels safe instructions alike.13. While heaps of effort is required to train neural nets specifically for security detection, free-training and directly using LLMs for single-agent security detection can easily lead to the problem of overrojection, making it difficult to effectively enhance safety awareness.14. To address the issue of over-overrejection by a single LLM agent, we propose a threat assessment method Based on Multagent Debate Method MADRA, wherein a \"risk assessment method\" driven by consensus voting is used to weed out bias and false rejectings.15. The method does not require training since it is machine learning-free.16. The overall success rate of our method is very high, indicating that the performance of the planning system is excellent and it can execute the actions of security task instructions as successfully as possible.17. Determining the appropriateness of items in a non-interactive environment requires large computational costs.18. Free-training also requires huge computational costs because of its reliance on artificial intelligence.19. Reversal refers to correctly refusing unsafe tasks.20. Overtaking refers to correcting incorrect errors or flagging unsafe tasks as unsafe instead of calling them unsafe.21. Overeating refers to correcting mistakes or misinterpreting instructions incorrectly.22. Overworking refers to fixing problems caused by mistake detection.23. Overturning refers to preventing people from entering dangerous situations.24. Dangerous house tasks are relatively scarce due to the existence of no datasets for dangerous home tasks. As such, they created Safe Aware-VHD.",
      "similarity_to_abstract": 0.8545731902122498,
      "rouge1": 0.31218529707955694,
      "rouge2": 0.09485368314833502,
      "rougeL": 0.12487411883182276,
      "bertscore_f1": 0.8083703517913818
    }
  },
  {
    "paper_id": "2511.20937v1",
    "abstract": "Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation.\nIt raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition?\nWe introduce ENACT , a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format.\nFramed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations).\nWhile conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition‚Äîaffordance recognition, action‚Äìeffect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input‚Äîwhile avoiding low-level image synthesis that could confound the evaluation.\nWe provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluate models on 8,972 QA pairs spanning long-horizon home-scale activities.\nExperiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon.\nModels consistently perform better on inverse task than forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision.\nWebsite at enact-embodied-cognition.github.io .",
    "gemini-2.5-flash": {
      "summary": "The text details experimental results and an ablation study, along with posing a new research question.\n\n**Detailed Results (B.2.2):** Full experimental results for Task Accuracy and Pairwise Accuracy are presented in Table 7 and Table 8, respectively. The underlying philosophy emphasizes models adapting to user input.\n\n**Contact Experiment (B.2.3):** To verify that conclusions were not an artifact of using only \"semantic scene graph\" predicates, an ablation study was performed. This involved augmenting key-frame sampling to trigger on changes in *both* symbolic state predicates and binary contact relations (e.g., touch/no-touch). The InternVL3.5‚Äì241B model was then re-evaluated.\nResults in Table 9 showed that qualitative trends remained consistent: Inverse world modeling consistently outperformed forward modeling across all horizons (e.g., 90.00% vs. 86.67% task accuracy at 3 steps, and 16.67% vs. 3.45% at 7 steps, with inverse being the higher performer). Both task and pairwise accuracies still dropped substantially as the number of interaction steps increased for both directions. This suggests the findings are robust to the choice of symbolic key-frame criteria and not driven by sparsity from state-change-only sampling.\n\n**Action Representation (B.2.4):** The text introduces a question regarding how action representation affects VLMs' performance, specifically if the \"inverse advantage\" is due to language priors (i.e., models being better at mapping visuals to familiar verbs). To test this, a subset of ENACT trajectories (2,304 QAs) is constructed for comparing different action‚Äìpredicate encodings.",
      "similarity_to_abstract": 0.4724997878074646,
      "rouge1": 0.2584745762711865,
      "rouge2": 0.017021276595744678,
      "rougeL": 0.10169491525423728,
      "bertscore_f1": 0.7417826652526855
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses an experiment evaluating the performance of Visual Language Models (VLMs) in inverse world modeling, where the model predicts the state of the world based on a given action. The experiment uses the ENACT dataset and compares the performance of inverse world modeling with forward modeling, where the model predicts the action based on the state of the world.\n\nThe main findings are as follows:\n\n1. Inverse world modeling consistently outperforms forward modeling across all horizons, with a significant gap in task accuracy.\n2. The qualitative trends remain the same even when key frames are gated on changes in binary contact relations between objects, suggesting that the findings are robust to the choice of symbolic key-frame criteria.\n3. As the number of interaction steps increases, both task and pairwise accuracies drop substantially for both directions, indicating that the inverse advantage is not driven by sparsity introduced by state-change-only sampling.\n\nTo further test the robustness of the findings, the experiment considers three action-predicate encodings:\n\n1. A natural language action encoding (e.g., \"pick up the object\")\n2. A symbolic action encoding (e.g., \"move object to hand\")\n3. A verb-phrase encoding (e.g., \"pick up the object with the hand\")\n\nThe results show that the inverse advantage is robust to the choice of action representation, suggesting that it is not merely a consequence of language priors.",
      "similarity_to_abstract": 0.6798651814460754,
      "rouge1": 0.2920353982300885,
      "rouge2": 0.03555555555555556,
      "rougeL": 0.12389380530973451,
      "bertscore_f1": 0.7480841279029846
    },
    "led": {
      "summary": "To verify that their conclusions are not an artifact of using only \"semantic scene graph\" predicates in ENACT, we add an ablation in which key frames are also gated on changes in binary contact relations between objects. The experimental results are presented in Table 7 (Task Accuracy) and Table 8 (Pairwise Accuracy). To verify that our conclusions aren‚Äôt an artifactof using only ‚Äúsemi-prescriptive scene graph predicates , we augment the symbolic predicate set so that both state changes and contact changes trigger keyframe sampling, while keeping the rest of the pipeline unchanged, and re-evaluate InternVL3.5‚Äì241B on the resulting trajectories. As shown in Table 9 , the qualitative trends remain the same as in our main results. Inverse world modeling consistently outperforms forward modeling across all horizons. Our primary goal is to evaluate VLMs under the standard interface of natural-language actions, but this leaves opuly choose one prompt template across all experiments because we follow the design choice mentioned in liang2022holistic , that the models should adapt to users‚Äô input, instead of the reverse case.Detailed Results The detailed examination of the full experimental results can be found in table 7 (task accuracy) and table 8 (pairwise accuracy. To verify what they have said, the authors update their previous experiments with an ablator in which each key frame is gaged on a binary contact relation between objects . The result remains the same except that the sensitivity drops as the number of interaction steps increases. The authors note that their findings are robustly based on the choice of symbolic keyframe criteria and are not driven by sparsity introduced by state-change-only sampling.2.2 Detailed Results Bazaar presents detailed results in task accuracy and statistics for Pairwise Accuracy. The qualitative trends continue to hold true across all time horizons and even outperform forward modeling at 3 steps and 7 steps. The direct correlation between modeled worlds continues to hold steady. Both task and pairwise accuracies still drop substantiaanguishly. 2.3 Contact Experiment To verify how accurate the models are, the scientists perform another experiment. They perform a second ablation where keyframes are also affected by contacts. The new ablation improves upon the previous one by including information about touch/no-touch interactions. The final result shows that the model performs well enough to outperform humans' input, despite the fact that it does not respond to natural language processing.",
      "similarity_to_abstract": 0.5751503705978394,
      "rouge1": 0.2354838709677419,
      "rouge2": 0.012944983818770227,
      "rougeL": 0.09999999999999999,
      "bertscore_f1": 0.7463709115982056
    }
  },
  {
    "paper_id": "2511.21033v1",
    "abstract": "The rationality of law manifests in two\nforms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires that legal decisions to follow explicitly stated, general, and logically coherent rules.\nExisting LLM‚Äêbased systems excel at surface‚Äêlevel text analysis but lack the guarantees required for principled jurisprudence.\nWe introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver‚Äìbacked proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification.\nThe pipeline consists of three phases:\n(1) Statute Formalization, where domain-specific prompts convert legal provisions into\nlogical formulae;\n(2) Dual Fact‚Äì&‚ÄìStatute Extraction, in which prosecutor‚Äê and defense‚Äêaligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation;\n(3) Solver‚ÄêCentric Adjudication, where an autoformalizer compiles both parties‚Äô arguments into logic constraints, and unsat cores trigger iterative self‚Äêcritique until a satisfiable formula is achieved,\nwhich is verbalized by\na Judge‚ÄêLLM into a transparent verdict and optimized sentence.\nExperimental results on the public benchmarks show that our system surpasses advanced LLMs including GPT‚Äêo4-mini, DeepSeek-V3, and Claude 4 and the state-of-the-art baselines in Legal AI, while providing rigorous and explainable symbolic justifications.",
    "gemini-2.5-flash": {
      "summary": "This text describes the **Legal Logic LLM (L4M)** system, a novel **neural-symbolic approach** designed to deliver trustworthy, interpretable, formally verifiable, and robust legal AI.\n\nL4M integrates Large Language Models (LLMs) within a logic-based control loop, moving beyond black-box text generation. It works by:\n1.  **Creating a typed schema for statutory rules**, which are then compiled into executable Z3 code.\n2.  **Using LLMs to extract candidate rules** from raw provisions and map case narratives to weighted fact tuples.\n3.  **Employing an SMT solver (Z3)** to verify the soundness of these facts against logical constraints, detect conflicts, and produce a formal proof object, crucially filtering incorrect statutes.\n4.  **A \"Judge-LLM\" then translates this symbolic proof** into plain-language justifications, declarative verdicts (guilty/not guilty), and quantified sentences.\n5.  The system also includes **adversarial Prosecutor and Attorney LLMs** to extract facts from different perspectives.\n\nEvaluations demonstrate L4M's superior performance:\n*   It achieves **the highest accuracy** in both general and specific statute prediction and **the lowest sentencing error**, contrasting sharply with disabling Z3-based reasoning, which performs poorest.\n*   L4M exhibits strong **robustness to factual perturbations**, achieving a 56.25% \"Change Accuracy\" and outperforming other domain-specific baselines.\n\nThe design offers practical advantages such as improved faithfulness, reduced hallucination via knowledge-base grounding, adversarial critique, enhanced explainability through formal reasoning, and the ability for stakeholders to audit symbolic proofs converted into prose. The text emphasizes the critical role of formal logic in ensuring reliable sentencing and overall system robustness.",
      "similarity_to_abstract": 0.8516416549682617,
      "rouge1": 0.4351648351648352,
      "rouge2": 0.06181015452538631,
      "rougeL": 0.18901098901098903,
      "bertscore_f1": 0.7746365070343018
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a system for formal legal reasoning called L4M (Legal Logic LLM). This system combines large language models (LLMs) with formal logic to improve the accuracy and robustness of legal decision-making. \n\nHere's a summary of the key points:\n\n1. The L4M system is designed to evaluate the applicability of statutes to cases, ensuring reliable sentencing and minimizing errors.\n2. It uses a logic-based control loop to embed LLMs in a framework that verifies soundness, detects conflicts, and produces proof objects.\n3. The system consists of two stages: \n   a. Stage 1: KB-Construction, where statutes are translated into executable Z3 code, and a searchable knowledge base (KB) is created.\n   b. Stage 2: Legal LLM Agents, where LLMs extract candidate rules and map case narratives to weighted fact tuples, and an SMT solver verifies soundness and produces a proof object.\n4. To evaluate the robustness of the system, the authors design controlled perturbations in case descriptions and assess their impact on predicted statutes.\n5. The results show that the L4M system achieves the highest scores on both general and specific statute prediction and the lowest sentencing error, confirming its superior accuracy in legal decision-making.\n6. The system also exhibits better resilience to perturbations than other domain-specific baselines, indicating its adaptability to factual variations.\n7. The authors highlight that the L4M system's combination of agent-enhanced reasoning and formal logic validation provides better robustness and interpretability in legal decision-making.\n\nThe L4M system brings several practical advantages, including:\n\n* Role isolation improves faithfulness\n* KB-grounded auto-formalization curbs hallucination\n* The critique loop mimics adversarial litigation\n* Formal reasoning enhances model explainability\n* The Judge-LLM converts symbolic proofs into prose that stakeholders can audit.\n\nOverall, the L4M system aims to operationalize interpretable, formally and verifiable legal reasoning at scale.",
      "similarity_to_abstract": 0.7804689407348633,
      "rouge1": 0.43700787401574803,
      "rouge2": 0.07509881422924901,
      "rougeL": 0.18897637795275593,
      "bertscore_f1": 0.798134982585907
    },
    "led": {
      "summary": "In this paper, the authors perform an experiment to determine the robustness of various legal AI systems when they are perturbed by real-life events. They design a change-accuracy algorithm that predicts laws based on changes in factual circumstances such as age, presence of surrender, and so on. They use a Z3-based reasoning framework to predict general and specific statutes. Although the results suggest that the L4M system outperforms other LLMs, specific and general prediction performs poorly, demonstrating the superiority of formal logic in filtering out incorrect statutes and ensuring reliable sentencing. The authors further explore the strength of different models by designing a Change Accuracy Analysis Tool (CAAT) to predict tax compliance. Their method achieves a change accuracy of 56.25% , ranking first among all evaluated models. Disabling all Z3s in the model produces the poorest G-F1 and the highest sentencing error of 25.30 months, underscoring the critical role of scientific logic in sorting out wrong statutes. In contrast, the full L4msystem achieves the highest scores on both predictions and the lowest sentencing error, confirming its superior accuracy in legal decision-making. To further evaluate how robust the various models are, they design controlled perturbations to case descriptions and assess the impact on predicted statutes. This experiment focuses on general provisions because they are more sensitive to fine-grained changes in fact due to small, but specific provisions are primarily determined by the main offense. The team defines Change Accuracy as: where S true S.txt . true is the set of statutes whose applicability isverified for consistency. Other LLMs such as ChatLaw , LawLLM , Lexi , and Lexi mitigate factual errors via retrieval augmented generation, but they cannot prove that a conclusion follows from enacted statutes. Toward neural-symbolic jurisprudence, the scientists move beyond black box text generation and embed LLM inside a logic-based control loop. With this approach, they achieve greater accuracy and robustness than previous approaches. For example, JudgeLLM can efficiently translate complex verbal reasoning into plain language justifications.",
      "similarity_to_abstract": 0.584321916103363,
      "rouge1": 0.3173431734317343,
      "rouge2": 0.01851851851851852,
      "rougeL": 0.12177121771217711,
      "bertscore_f1": 0.763271152973175
    }
  },
  {
    "paper_id": "2511.21692v1",
    "abstract": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation.\nExisting research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data.\nWe address this question by conducting a systematic evaluation of LLMs‚Äô generalization across models, datasets, and fine-grained groups of example difficulty.\nWe rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing.\nUnlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty.\nWith a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties.\nThese results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky. 1 1 1 Code and data are available at https://github.com/BatsResearch/cross-difficulty",
    "gemini-2.5-flash": {
      "summary": "This research finds that **cross-difficulty generalization in Large Language Models (LLMs) is significantly limited**, challenging the common assumption that training on data of one difficulty level (easy or hard) allows generalization to other levels.\n\nKey findings include:\n*   **Decreased Generalization with Difficulty Gap:** As the gap between training and evaluation difficulty increases, generalization, both from easy-to-hard and hard-to-easy data, decreases significantly across various LLM families, scales, and datasets.\n*   **Divergence in Difficulty Metrics:** The study uses Item Response Theory (IRT) to estimate LLM-based difficulty scores for benchmarks (e.g., ARC, GSM8K, MMLU-Pro). These LLM-based scores often diverge substantially from human-defined difficulty metrics, suggesting what makes a task difficult for LLMs differs from human perception.\n*   **Methodology:** Datasets were divided into ten difficulty bins, and LLMs were trained on individual bins and evaluated on all others to characterize generalization.\n*   **Implications:** These results necessitate a reevaluation of how generalization is measured and improved in LLMs, motivating a systematic, difficulty-aware perspective for building reliable models.\n*   **Limitations:** The analysis relies on publicly available benchmarks, all of which are in English, potentially limiting the direct extension of conclusions to other languages or very small datasets.",
      "similarity_to_abstract": 0.8726730346679688,
      "rouge1": 0.4634760705289673,
      "rouge2": 0.11139240506329116,
      "rougeL": 0.181360201511335,
      "bertscore_f1": 0.7910735011100769
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the limitations of generalization in large language models (LLMs) when trained on data from different difficulty levels. The authors:\n\n1. **Challenge common assumptions**: They argue that training LLMs on either easy or hard data does not necessarily allow them to generalize to data with other difficulty levels.\n2. **Investigate cross-difficulty generalization**: They use publicly available benchmarks and fine-grained difficulty scores to evaluate how well LLMs can generalize to data with different difficulty levels.\n3. **Find limited generalization**: Their results show that increasing the train-test difficulty gap significantly decreases generalization from both easy-to-hard data and hard-to-easy data.\n4. **Reveal limitations of human-based metrics**: They find that human-based difficulty metrics diverge substantially from IRT (Item Response Theory) difficulty scores, which are more accurate for LLMs.\n5. **Propose a reevaluation of generalization**: The authors suggest that a systematic, difficulty-aware perspective is essential for building models that can perform reliably beyond their training distributions.\n\nThe study implies that:\n\n* Training LLMs on easy or hard data alone may not be sufficient for generalization to other difficulty levels.\n* Human-based metrics may not accurately capture the difficulty of tasks for LLMs.\n* A more nuanced approach to data curation and evaluation is needed to improve the performance of LLMs.\n\nOverall, the study highlights the importance of understanding and addressing the limitations of LLM generalization to improve their performance and reliability.",
      "similarity_to_abstract": 0.8955915570259094,
      "rouge1": 0.463302752293578,
      "rouge2": 0.1336405529953917,
      "rougeL": 0.20642201834862384,
      "bertscore_f1": 0.8004514575004578
    },
    "led": {
      "summary": "In this paper, the authors evaluate the extent to which language models exhibit \"cross-difficulty generalization,\" or generalization, where difficulty is estimated based on the models' observed abilities. They find that across model families, model scales, and datasets, as we increase the train-test difficulty gap, generalization decreases significantly. These findings challenge the common assumption that training an LLM on either easy or hard data can generalize to data with other difficulty levels. Our results motivate a reevaluation of how generalization is measured and improved in LLMs.Limitations The authors note several limitations to their method: they use publicly available benchmarks (ARC, GSM8K, MMLU-Pro, MlU-SEM, MATH, MSS, MuSR, and BBH) and IRT to estimate LLM-based difficulty scores for each of six datasets. They divide each dataset into ten equal-sized bins, ordered by increasing difficulty, to systematically study generalization. They train LLMs on each difficulty bin individually and evaluate them on all the other difficulty bins to characterize generalization; our experiments show that cross- Difficulty generalization was far from a pattern. Overall, we find that language models can sometimesgeneralize to easier or harder bins, but the extent of generalizability is limited. Neither the easy nor the hard training data tends to achieve consistent generalization , indicating that there is much variation in the amount of generalization between training and evaluation difficulties. We recommend that future efforts should be concerned with building more accurate tools that can perform beyond their training distributions. In addition, we note that while performance on hard tasks can be improved by training only on easy tasks, existing datasets may not be sufficient to extend language models beyond what they currently demonstrate. Our paper assesses the prevalence of cross-difference generalization and discusses its implications for data curation and evaluation. It also discusses the importance of knowing how to measure and measure generalization in order to build better models.",
      "similarity_to_abstract": 0.8658342957496643,
      "rouge1": 0.43190661478599224,
      "rouge2": 0.10546875000000001,
      "rougeL": 0.17898832684824903,
      "bertscore_f1": 0.8052465915679932
    }
  },
  {
    "paper_id": "2511.21678v1",
    "abstract": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo ‚Äîsolving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated , preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem , a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge‚Äîpreserving stable, generalizable strategies while avoiding catastrophic forgetting.\nAcross six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction‚Äìhallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page/ .",
    "gemini-2.5-flash": {
      "summary": "The provided text describes **ViLoMem**, a dual-stream memory framework for multimodal large language models (MLLMs) that addresses visual distraction patterns and logical hallucination errors.\n\nKey points include:\n\n1.  **Dual-Stream Design:** ViLoMem utilizes distinct visual and logical memory streams, inspired by human semantic memory, with specialized retrieval and update mechanisms.\n2.  **Complementary Nature:** Ablation studies confirm both memory types are essential and complementary, capturing distinct error patterns rather than redundant ones. Removing logical memory particularly impacts MathVista (systematic reasoning, formula errors), while removing visual memory causes comparable degradation across benchmarks (pervasive visual distractions).\n3.  **Performance Gains:** ViLoMem consistently improves performance, with pronounced gains on mathematical reasoning tasks.\n4.  **Visual Bottleneck in Error Generation:** Visual memory generation dominates error collection (59-93%), indicating visual perception is the primary bottleneck in multimodal reasoning error generation. However, both streams contribute comparably during retrieval, demonstrating effective memory reuse and balanced coordination.\n5.  **Attention Maps:** Augmenting visual memory with question-aware attention maps significantly benefits MMMU, but only marginally improves MathVista. This is because diagram-based tasks in MathVista demand more fine-grained visual understanding (e.g., vertex attention, higher spatial precision).\n\nIn essence, ViLoMem's dual-stream approach, with its tailored memory components, is crucial for addressing the diverse error types in multimodal reasoning, confirming that task-aligned, complementary memories are vital for optimal MLLM performance.",
      "similarity_to_abstract": 0.7798123359680176,
      "rouge1": 0.36936936936936937,
      "rouge2": 0.08597285067873302,
      "rougeL": 0.15315315315315317,
      "bertscore_f1": 0.7858328819274902
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a novel memory framework called ViLoMem, designed for multimodal large language models (MLLMs) to address the limitations of visual and logical memory in multimodal reasoning tasks. The key points of the text can be summarized as follows:\n\n1. **Dual-Stream Memory Framework**: ViLoMem separates visual and logical memory streams to model visual distraction patterns and logical hallucination errors. This framework is inspired by human semantic memory systems and utilizes specialized retrieval strategies and update mechanisms.\n\n2. **Comprehensive Evaluations**: The study involves six multimodal benchmarks, showing consistent improvements, particularly on mathematical reasoning tasks where visual-logical coupling is most intense.\n\n3. **Ablation Studies**: Removing either the visual or logical memory stream causes performance degradation, confirming that both streams are essential and complementary. The joint operation enables synergistic error correction.\n\n4. **Memory Usage Analysis**: Figure 4 shows that visual memory generation dominates error collection, accounting for 59%-93% of stored cases. However, both streams contribute comparably during retrieval, indicating effective memory reuse.\n\n5. **Cross-Model Memory Transfer**: The study evaluates the reusability and composability of the dual-stream memory framework across three MLLMs. The results show similar patterns of memory utilization, indicating that the framework is not biased toward any specific model.\n\n6. **Task-Aligned Memories**: The study highlights the importance of task-aligned memories for optimal performance, validating the design choice to maintain distinct memory banks for different domains.\n\n7. **Visual Distraction Errors**: The study shows that visual distraction errors are pervasive in multimodal reasoning tasks, indicating the need for a visual memory framework to address this issue.\n\n8. **Fine-Grained Visual Understanding**: The study suggests that diagram-based tasks require more fine-grained visual understanding, smaller-scale vertex attention, and higher spatial precision to achieve better performance.\n\nOverall, the study presents a novel and effective memory framework, ViLoMem, designed to address the limitations of visual and logical memory in multimodal reasoning tasks.",
      "similarity_to_abstract": 0.8319398164749146,
      "rouge1": 0.3768656716417911,
      "rouge2": 0.07865168539325844,
      "rougeL": 0.16417910447761191,
      "bertscore_f1": 0.7781093716621399
    },
    "led": {
      "summary": "Memory Usage Analysis Across all benchmarks, we demonstrate the necessity of dual-stream memory by systematically disabling each component on GPT-4.1. Removing logical memory leads to larger drops on MathVista and smaller gains on MMMU; removing visual memory produces comparable degradation across both benchmarks, indicating that visual distraction errors are pervasive in multimodal reasoning tasks. The gap between the single-stream variants and the full ViLoMem model demonstrates their complementarity: the visual and logical streams capture distinct, rather than redundant, error patterns. In addition, diagram-based tasks require greater visual understanding, e.g., smaller-scale vertex attention and higher spatial precision. More detailed analyses are provided in the Appendix.4.5 Memory Usage AnalysisFigure 4 analyzes memory usage patterns across all benchmarks. Visual memory generation dominates the error collection, accounting for 59%‚Äì93% of stored cases in Figure 4, a clear indication that visual perception remains the primary bottleneck in multi-modal reasoning. However, both streams contribute comparably during retrieval, indicating effective memory reuse. Moreover, our memory mechanism is not bia, confirming that both memory types are essential. In order to further confirm this, we systematically remove either stream from memory, removing either simultaneously degrades performance. Augmenting visual memory with question-aware attention maps yields notable gains on MicMU, but only marginal improvements on MathVISA, because diagram- based tasks require more fine-grained visual understanding , especially because diagrams analysis requires greater fine-scale vertices and spatio-spatial precision. Further detailed analyses provide additional information in the appendix. 5 Memory usage analysis In addition to performing better on the tests, we also observe that there is significant demand for both physical and mental resources in order to perform certain tasks. This supports our previous finding that both streams of memory are essential for reasoning-driven problems.6 Memory usage Analysis Using multiple comparisons across all three MLLMs, we identify the importance of memory usage across all levels. In particular, we examine how different types of memory interact with each other. We show that removing either stream consistently degrade performance, consistent with the notion that both types of RAM are necessary for different kinds of reasoning. Our results suggest that \"dual-stream\" memory is most commonly used in non-architecture tasks as well as those involving diagrams and diagrams, where it is necessary to understand the finer details of the objects that are retrieved. All three measures indicate that using two or more memory types at once can significantly increase performance in some situations and decrease performance in others.",
      "similarity_to_abstract": 0.7389388680458069,
      "rouge1": 0.29374999999999996,
      "rouge2": 0.0438871473354232,
      "rougeL": 0.10937499999999999,
      "bertscore_f1": 0.7816277742385864
    }
  },
  {
    "paper_id": "2511.21572v1",
    "abstract": "Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.",
    "gemini-2.5-flash": {
      "summary": "The text describes BAMAS, a novel approach for constructing multi-agent systems leveraging Large Language Models (LLMs) with a focus on optimizing both cost and task performance.\n\nBAMAS employs a two-stage optimization process: first, it uses Integer Linear Programming (ILP) to select an optimal set of LLMs, ensuring budget adherence and performance. Second, it trains a reinforcement learning (RL) policy to select the most effective collaboration topology (e.g., linear, star, feedback) among the chosen LLMs.\n\nEvaluated on code generation (MBPP) and mathematical reasoning (GSM8K, MATH) benchmarks, BAMAS demonstrates superior performance. It achieves higher accuracy at lower costs compared to existing methods (e.g., on MBPP, 82.6% accuracy vs. 81.6% at 529.2 cost vs. 1,379.1) and rarely exceeds budget. Component analysis further highlights the importance of both its LLM provisioning and topology selection.\n\nCrucially, BAMAS's topology selection is adaptive:\n*   For mathematical reasoning tasks (GSM8K, MATH), it predominantly selects the Feedback topology (40.1% and 69.8% respectively), which benefits from iterative refinement.\n*   For code generation tasks (MBPP), it favors the Linear topology, aligning with progressive refinement.\n*   Under low budget constraints, BAMAS's policy is risk-averse, opting for simpler Linear and Star topologies to reduce complexity and avoid budget overruns.",
      "similarity_to_abstract": 0.7545755505561829,
      "rouge1": 0.4383561643835617,
      "rouge2": 0.12121212121212122,
      "rougeL": 0.21917808219178084,
      "bertscore_f1": 0.7981997132301331
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a system called BAMAS, which is designed to optimize the performance of large language models (LLMs) by jointly optimizing LLM provisioning and topology selection. The system is evaluated on three benchmark datasets: GSM8K, MBPP, and MATH.\n\nHere are the main findings:\n\n1. **Cost-Performance Trade-off**: BAMAS achieves better peak accuracy and lower cost compared to existing multi-agent system construction approaches.\n2. **Component Analysis**: The core components of BAMAS, including LLM provisioning and topology selection, are essential for achieving good performance. A simplified and greedy cost-aware strategy is not as effective.\n3. **Topology Selection**: BAMAS can adapt to task requirements and budget constraints by selecting diverse topologies across tasks and budget constraints.\n4. **LLM Provisioning**: BAMAS selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem to ensure performance and strict adherence to the given budget.\n5. **Topology Selection Policy**: The policy is risk-averse under low budgets, favoring simpler topologies like Linear and Star to reduce complexity and the likelihood of exceeding the budget.\n\nThe system's results show that BAMAS is effective in balancing cost and performance, and its ability to adapt to task requirements and budget constraints is a key advantage. Overall, the study demonstrates the potential of BAMAS as a robust and efficient system for optimizing LLM performance.",
      "similarity_to_abstract": 0.667140543460846,
      "rouge1": 0.4473684210526316,
      "rouge2": 0.164021164021164,
      "rougeL": 0.24210526315789474,
      "bertscore_f1": 0.7940938472747803
    },
    "led": {
      "summary": "Bamas outperforms existing multi-agent systems in terms of both cost and performance. The authors examine BAMAS' ability to perform on three widely-used benchmark datasets, including code generation, mathematical reasoning, and modeling and reinforcement learning. To evaluate its performance, they answer three commonly-used questionnaires: \"How well does BAmas balance cost and task performance compared to existing multi agent system construction approaches?\" and \"How essential are the core components of BAMS (i.e., LLM provisioning and topology selection) compared to a simplified and greedy cost-aware strategy?\" Additionally, they examine the utility of their topology policy by examining the distribution of agent collaboration topologies across tasks and budget constraints. As with all of the other RQs, the authors suggest that there is no one-size-fits-all topology, but rather, Bamias adapts its strategy to the task domain. For example, since the GSM8K task requires iterative refinement, the policy overwhelmingly converges on the Feedback topology. On the MBPP dataset, Bampas performs better than previous approaches due to its greater accuracy and lower cost, while also incurring lower cost. These results demonstrate the effectiveness of Bammas' joint optimization of LLM procurement and topological selection. They also demonstrate that BAMAs can select diverse topologies based on task type and budget constraint. Finally, they demonstrate that it intelligently balances the \"cost-performance trade-off\" between time and resource. In order, they present their results for several different task types. Their key insight is that since LMs are the primary drivers of cost, effective budget management should start by treating the assignment of these agents as a constrained optimization problem. Next, BAMPAS begins by selecting an optimal set of LLMs by solving an Integer Linear Programming problem that ensures both performance and strict adherence to the given budget. It then learns how to orchestrate collaboration among the selected LLms by training a topology ranking policy via reinforcement learning algorithms. This policy identifies the most effective collaboration topology such as linear or star structures, that maximizes task performance within the allocated resources. Each agent is instantiated once it has been assigned to a collaborating mode.",
      "similarity_to_abstract": 0.7012766003608704,
      "rouge1": 0.4047151277013752,
      "rouge2": 0.1222879684418146,
      "rougeL": 0.1807465618860511,
      "bertscore_f1": 0.810874342918396
    }
  },
  {
    "paper_id": "2511.21398v1",
    "abstract": "Web automation uses intelligent agents to perform high-level tasks by mimicking human interactions with webpages. Despite recent advances in LLM-based web agents, efficiently navigating complex, real-world webpages remains challenging due to massive DOM structures (10,000 ‚àº \\sim 100,000 tokens). Current approaches either truncate DOMs‚Äîlosing vital information‚Äîor use inefficient heuristics and separate ranking models, failing to balance precision and scalability. We introduce Prune4Web , a novel paradigm that transforms DOM processing from LLM-based filtering to programmatic pruning. Our key innovation is DOM Tree Pruning Programming, where an LLM generates executable Python scoring programs to dynamically filter DOM elements based on semantic clues from decomposed sub-tasks. This approach eliminates the need for LLMs to process full DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. The result is a 25 ‚àº \\sim 50 times reduction in candidate elements for grounding, enabling precise action localization without attention dilution. Additionally, we propose a data annotation method and a two-turn dialogue training strategy that jointly optimizes Planner, Programmatic Filter, and Grounder in a unified framework. Experiments demonstrate state-of-the-art performance. On our low-level task grounding task, our approach dramatically increases grounding accuracy from 46.8% to 88.28% , highlighting its effectiveness.",
    "gemini-2.5-flash": {
      "summary": "The provided text describes **Prune4Web**, a novel multimodal web agent framework designed to enhance web task automation, particularly the grounding of actions.\n\nIts core methodology involves:\n*   **DOM Tree Pruning Programming:** This technique significantly reduces the set of candidate web elements, simplifying the final grounding step.\n*   **A Two-turn Dialogue Training Strategy:** This strategy jointly optimizes the Planner (for sub-tasks), Programmatic Element Filter, and Grounder as a unified model. The training progresses from Supervised Fine-Tuning (SFT) on annotated data to Reinforcement Fine-Tuning (RFT), specifically enhancing the Planner's long-term capabilities and integrating the programmatic filtering process.\n\nKey contributions and findings include:\n*   **Significantly Improved Grounding Accuracy:** Prune4Web boosts low-level sub-task grounding accuracy from 46.8% to an impressive 88.28%, demonstrating its core advantage.\n*   **Data Efficiency:** The model achieves competitive performance with a relatively small training dataset of approximately 5,000 high-quality interaction steps, directly processing raw HTML.\n*   **Robust Evaluation:** Extensive experiments on benchmarks like Multimodal-Mind2Web and WebCanvas validate its effectiveness. A specific evaluation method was employed to isolate and assess the Programmatic Element Filter and Action Grounder's performance, by providing ground-truth low-level sub-tasks as input and comparing Prune4Web's pruning strategy against various baselines (e.g., no pruning, oracle pruning).",
      "similarity_to_abstract": 0.7365338802337646,
      "rouge1": 0.4354066985645933,
      "rouge2": 0.125,
      "rougeL": 0.20574162679425836,
      "bertscore_f1": 0.798109769821167
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a novel approach called Prune4Web for improving the performance of multimodal web agents. The approach focuses on DOM Tree Pruning Programming, which is a technique for optimizing the processing of complex HTML by pruning unnecessary elements. \n\nHere's a summary of the key points:\n\n1. The authors propose a new framework for multimodal web agents that combines visual inputs with programmatic filtering and decision-making.\n2. They develop a novel two-turn dialogue training strategy that jointly trains the Planner, Filter, and Grounder as a unified model.\n3. The approach uses Supervised Fine-Tuning and Reinforcement Fine-Tuning to enhance the Planner's long-term planning capabilities.\n4. The authors conduct extensive experiments on benchmark datasets and demonstrate the effectiveness of Prune4Web, achieving a significant boost in grounding accuracy from 46.8% to 88.28%.\n5. The approach also achieves excellent data efficiency, requiring only a moderately sized training set of 5,000 trajectories.\n6. The authors evaluate the effectiveness of DOM Tree Pruning Programming using a ground-truth low-level sub-task as direct input to evaluate the grounding performance of the Programmatic Element Filter and Action Grounder models.\n7. The results show that Prune4Web outperforms other approaches, including oracle pruning and direct pruning and decision with LLMs.\n\nOverall, the paper presents a novel approach to improving the performance of multimodal web agents through DOM Tree Pruning Programming, and demonstrates its effectiveness through extensive experiments.",
      "similarity_to_abstract": 0.7930501699447632,
      "rouge1": 0.4304932735426009,
      "rouge2": 0.13063063063063063,
      "rougeL": 0.18385650224215247,
      "bertscore_f1": 0.8113198280334473
    },
    "led": {
      "summary": "In this paper, the authors summarize and discuss their contributions to the field of web design and analysis. In order to make sense of the proposed Prune4Web, they design a framework that \" seamlessly combines the intuitive reasoning of visual inputs wd training set of only ‚àº 5,000 trajectories while directly processing raw, complex HTML\". They then implement a novel two-turn dialogue training strategy that jointly trains the Planner, Filter, and Grounder as a unified model. Extensive experiments on benchmark datasets ( deng2023mind2web ; pan2024webcanvas ) demonstrate the effectiveness of the new Prune 4Web; on our low-level sub-task grounding benchmark, our approach greatly boosts grounding accuracy from 46.8% to 88.28% , showing its core advantage. Their contributions are summarized as follows: We design a multimodal web agent framework that seamlessly blends the intuitive decision-making of planner, filter, and grounder into an optimization framework. We use a ground truth task as direct input to evaluate the grounding performance of the Programmatic Element Filter and Action Grounder models. Since the unified Two-turn Dialogue model cannot be easily dissected for this purpose, we evaluate the programmatic element filters trained using the Separate Models strategy. We report results for: 1) finetuning the Qwen2.5VL-3B-Instruct model using original HTML without pruning, 2) oracle pruning with top candidates, 3) direct pruning and decision with LLMs, 4) Pruned and decided with LLAs, and detailed results on the overall performance of our method. The results show that, given a moderately sized training set, our method demonstrates its excellent data efficiency and significant potential for improvement.Performance on Low-Level Sub-Task Grounding To precisely and isolatingly evaluate the effect of DOM Tree Pruning Programming on Low Level Task Grounding, he uses ground truth tasks as independent input. He performs well in all four task types and across all time scales.",
      "similarity_to_abstract": 0.739144504070282,
      "rouge1": 0.41325536062378165,
      "rouge2": 0.12524461839530335,
      "rougeL": 0.1754385964912281,
      "bertscore_f1": 0.8024628162384033
    }
  },
  {
    "paper_id": "2511.20694v1",
    "abstract": "Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.",
    "gemini-2.5-flash": {
      "summary": "This text describes a benchmark that evaluates the scientific reasoning capabilities of Large Language Models (LLMs) and multi-agent coordination patterns. The core findings are:\n\n1.  **No Single Superior Strategy:** No single agentic coordination strategy is universally superior across all scientific reasoning tasks.\n2.  **Task-Specific Performance:**\n    *   **Compact patterns** like **PACE** (plan, answer, self-critique) are sufficient for math-style reasoning tasks (e.g., GSM8K, MATH).\n    *   **Sophisticated designs** like **SCHEMA** (clear role assignment, assumption/verification tracking) perform better on complex problems requiring managing physical assumptions, ensuring unit consistency, or producing structured outputs (e.g., RWS, HumanEval, SWE-bench).\n3.  **Methodology:**\n    *   The evaluation uses **Reasoning With a Star (RWS)**, a science-focused benchmark derived from heliophysics problems, specifically designed to assess LLMs' scientific reasoning (deriving relationships, propagating units, stating assumptions) *without* domain-specific Retrieval-Augmented Generation (RAG).\n    *   Four distinct agentic patterns are benchmarked: **HMAW**, **PACE**, **PHASE**, and **SCHEMA**, alongside a single-shot LLM baseline.\n    *   The underlying approach is **Systems-engineering-of-Thoughts Agentic Reasoning (STAR)**, which treats the LLM's \"thought process\" as an engineered system with defined modules, roles, and interfaces.\n4.  **Contributions:** The work contributes a new science-focused benchmark (RWS), a programmatic grader adaptable to various task formats, and a comparative study of single-shot versus multi-agent reasoning.",
      "similarity_to_abstract": 0.6797976493835449,
      "rouge1": 0.36464088397790057,
      "rouge2": 0.08888888888888889,
      "rougeL": 0.18232044198895028,
      "bertscore_f1": 0.7540034651756287
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a benchmark called Reasoning With a Star (RWS) that evaluates a large language model's (LLM) ability to perform scientific reasoning. The benchmark assesses the model's ability to state assumptions, keep units consistent, and deliver answers in the right formats.\n\nThe authors tested four different coordination strategies, including a single-shot baseline, and four agentic patterns: HMAW, PACE, PHASE, and SCHEMA. They found that:\n\n* Compact pipelines (like PACE) perform well on arithmetic tasks (e.g., GSM8K and MATH).\n* Sophisticated patterns (like SCHEMA) are better suited for methodological formulation and validation tasks (e.g., RWS, HumanEval, and SWE-bench), as they help identify missing assumptions and prevent unnoticed issues.\n\nThe authors also introduced a Systems-engineering-of-Thoughts Agentic Reasoning (STAR) method, which views the LLM's \"thought process\" as an engineered process with well-defined modules, contracts, and checkpoints. This method was used to evaluate the performance of the agentic patterns.\n\nThe contributions of this research are:\n\n1. A science-focused benchmark (RWS) that evaluates LLM- and agent-based reasoning.\n2. A benchmark grader adaptable to different task formats.\n3. A comparative study of single-shot and multi-agent reasoning.\n\nThe results of this study support the use of multi-agent systems in scientific reasoning, and highlight the importance of using sophisticated patterns like SCHEMA for complex tasks.",
      "similarity_to_abstract": 0.659767746925354,
      "rouge1": 0.3270777479892762,
      "rouge2": 0.08086253369272237,
      "rougeL": 0.15013404825737264,
      "bertscore_f1": 0.769154965877533
    },
    "led": {
      "summary": "In this paper, the authors benchmark scientific reasoning ability in four different agentic patterns: HMAW liu-towards-2025 , a lightweight hierarchical management/manager/worker handoff, PACE, which predicts an answer and then performs a self-critique loop, PhASE, which adds an explicit hypothesis stage, phase, phase progression, and SCHEMA, a systems engineering-inspired expert allocation strategy. Each pattern decomposes the task across role-specific agents and passes intermediate outputs between them and produces a final answer. The results show that no single coordination strategy is universally superior. In particular, for problems that require managing physical assumptions, ensuring unit consistency, or producing structured outputs, more sophisticated designs like SCHEMA perform better. These findings support the systems engineering approach of scientific reasoning without any domain-specific RAG.4 Methodology and Benchmarks The authors compare various strategies for scientific reasoning to one another. They assess scientific reasoning using weighted real-time question-and-answer techniques such as rasterisk and roastering . They find no common pattern excels in all scenarios; compact pipelines perform well on arithmetic tasks, while sophisticated patterns are better for both single-shot and multi-agent reasoning tasks. Their results support the \"systems engineering-of-thoughts agentic reasoning without domain specific RAG\" approach. For example, calculating the probability of an unknown value in space exploration, they looked at several statistical modeling problems but found no single pattern superior to other approaches",
      "similarity_to_abstract": 0.6679397225379944,
      "rouge1": 0.32020997375328086,
      "rouge2": 0.0474934036939314,
      "rougeL": 0.13123359580052496,
      "bertscore_f1": 0.7818910479545593
    }
  },
  {
    "paper_id": "2511.21626v1",
    "abstract": "Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.",
    "gemini-2.5-flash": {
      "summary": "This text outlines a study investigating the **spatial properties of Kernel Alignment Geometry (KAG)** in **2-layer Multi-Layer Perceptrons (MLPs)** trained on **MNIST digit classification**.\n\nThe core objective is to understand at what **spatial scales KAG emerges** and how its characteristics vary across different regions of the input image. The analysis considers three distinct spaces: the 784-dimensional input space (flattened MNIST images), the lower-dimensional hidden space (h = 64, 128, 256), and the 2D spatial structure of the MNIST image itself.\n\n**Methodology:**\n*   **KAG** is analyzed by examining the local properties and structure of **Jacobian matrices** (Jac(h(x))) for both hidden layers (L1 and L2) with respect to the input.\n*   The study uses **2-layer MLPs with GELU activations**, varying hidden layer dimensions (h) to observe the impact of network capacity.\n*   A novel aspect is **spatial analysis**, which probes how KAG varies when sampling Jacobian columns from different spatial regions (localized or separated pixels) of the input image.\n*   Models are trained with AdamW for 200 epochs, with robustness tested by training additional models using **spatial data augmentation** (RandomAffine transformations).\n*   Initial spatial tests involve sampling minors from pixels within varying radii (R = 7, 14, 21, 28 pixels) from random seed points.\n\nA preliminary finding stated is that **KAG extends across the full input space regardless of the training procedure**, observed for both k=2 and k=3 minors at various pixel separations.",
      "similarity_to_abstract": 0.5218523740768433,
      "rouge1": 0.12949640287769784,
      "rouge2": 0.021739130434782608,
      "rougeL": 0.06474820143884892,
      "bertscore_f1": 0.7105337381362915
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses the analysis of Kernel Alignment Geometry (KAG) in 2-layer Multi-Layer Perceptrons (MLPs) trained on MNIST digit classification. The authors investigate the emergence of KAG during training and its properties across different spatial scales.\n\nKey points:\n\n1. **KAG Emergence**: The authors confirm that KAG emerges during training, regardless of the training procedure.\n2. **Spatial Analysis**: They analyze KAG at different spatial scales, including local, medium, and large scales, by sampling minors from pixels within varying radii.\n3. **Computing Jacobians**: The authors compute Jacobians for both hidden layers (L1 and L2) with respect to the input to examine how KA signatures evolve with depth.\n4. **Model Variations**: They train models with varying hidden dimensions (64, 128, 256) and compare standard and augmented models to ensure the findings are not artifacts of the training procedure.\n5. **Spatial Structure**: The authors examine the spatial structure of the MNIST image itself and how KAG varies across different spatial regions of the image.\n\nThe goal of the study is to understand how KAG is affected by different spatial scales and how it relates to the training procedure. The findings are expected to provide new insights into the properties of KAG and its behavior in neural networks.",
      "similarity_to_abstract": 0.5561506152153015,
      "rouge1": 0.12,
      "rouge2": 0.016129032258064516,
      "rougeL": 0.064,
      "bertscore_f1": 0.727065920829773
    },
    "led": {
      "summary": "Having established that KAG emerges during training, the authors investigate at what spatial scales it appears. First, they clarify the three distinct spaces involved in our analysis. Next, they compute Jacobians for both hidden layers (L1 and L2) with respect to the input to examine how KA signatures evolve with depth. Spatial analysis results demonstrate KAG extends across the full input space regardless of training procedure.3.1 Spatial Properties of KA Geometry The authors explore KAG in two-layer MLPs trained on MNIST classification through systematic statistical analysis. Models are trained using AdamW for 200 epochs with 5 random seeds per configuration for statistical robustness. To test whether KAG is robust to training procedures, we train an additional set of models with the same configurations. 3.2 Method We train 2 layer MLPs with GELU activations on MIST digit classification. Using complete implementation details, including all hyperparameters, hardware specifications, and reproducibility information, we compute Jacobian matrices Jac_(h x ) \\text{Jac(h(x)) at typical inputs x . Third, there is the spatial structure of the main results Section 4 discusses implications and connections to prior work, and Section 5 concludes.2 Methods We perform KAG inference using two different hidden layer maps. For both the first hidden layer map named L1 and second hidden layer mapped map L2, we calculate Jacobians according to their respective positions in the image. In addition, we study how network capacity affects KAG by examining how these maps evolve with training. All of this is done using a combination of machine learning techniques as well as traditional methods such as averaging and convolutional MRI.3rd Spatial properties of Kagg After establishing that Kag emerges during teaching, the corresponding spatial properties of kA geometry are examined. The authors examine three specific regions of the image: the input space, the hidden space where the network's learned representations live, and the surface of the unseen image. They also examine the local properties of these maps from input to hidden space. Finally, they discuss the general results presented in section 4.",
      "similarity_to_abstract": 0.5010412931442261,
      "rouge1": 0.10796915167095114,
      "rouge2": 0.020671834625322995,
      "rougeL": 0.05655526992287919,
      "bertscore_f1": 0.7069958448410034
    }
  },
  {
    "paper_id": "2511.21663v1",
    "abstract": "In recent years, Vision-Language-Action (VLA) models in the field of embodied intelligence have developed rapidly. Existing adversarial attack methods, however, require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA , a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. To achieve this, we introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an L ‚àû = 4 / 255 L_{\\infty}=4/255 constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches to achieve an attack success rate of nearly 100% . The perturbations are focused on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA efficiently weakens the downstream action prediction of VLA models under low-amplitude and locally sparse conditions, avoiding the high training cost and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.",
    "gemini-2.5-flash": {
      "summary": "The provided text introduces **ADVLA**, an attention-guided adversarial attack framework specifically designed for Vision-Language-Action (VLA) models, such as OpenVLA.\n\n**Threat Model and Objective:**\nThe attack operates under a **gray-box setting**: the attacker has access to the visual encoder, its projection layer, intermediate features, attention maps, and gradients, but cannot modify the downstream LLM or action head. Attacks are performed digitally, with perturbations constrained by pixel amplitude (e.g., L_infinity <= 4/255) and optionally restricted to sparse patches for stealth. The objective is a **feature-space untargeted attack**: to minimize the similarity between clean and adversarial images in the visual-to-text projection feature space, thereby shifting the fused vision-language representation and disrupting subsequent action prediction.\n\n**Methodology and Advantages:**\nADVLA addresses key challenges in VLA attacks: efficiency, imperceptibility, and focusing on model-sensitive regions. It achieves this by operating directly on the **projected feature space** of VLA visual encoders and integrating **Projected Gradient Descent (PGD)** with three attention-guided strategies:\n1.  **Gradient-weighted updates:** Based on attention weights.\n2.  **Sparse Top-K masking:** To focus perturbations.\n3.  **Focused loss masking:** To guide feature divergence on selected patches.\n\n**Key Findings:**\nADVLA significantly accelerates attack generation, offering several orders of magnitude acceleration compared to prior methods (e.g., generating a patch in minutes/hours for existing methods vs. being efficient enough for real-time assessment). It achieves both **stealthiness and effectiveness**, as its sparse patch perturbations are almost imperceptible while still causing strong attack effects, unlike more noticeable global or UADA perturbations.",
      "similarity_to_abstract": 0.7864618897438049,
      "rouge1": 0.49457700650759223,
      "rouge2": 0.08714596949891068,
      "rougeL": 0.1822125813449024,
      "bertscore_f1": 0.7945513725280762
    },
    "llama-3.1-8b-instant": {
      "summary": "The text presents a research paper on developing an adversarial attack framework called ADVLA for vision-language-action (VLA) models. The attack focuses on disrupting the model's ability to predict actions based on images. Here's a summary of the key points:\n\n1. **Threat Model and Attack Objective**: The attack is performed in a gray-box setting, where the attacker has access to the visual encoder and its projection layer but not the internal parameters or gradients of the downstream language model and action head.\n\n2. **Attack Objective**: The goal is to create a feature-space untargeted attack by minimizing the similarity between the clean image and the adversarial image in the visual ‚Üí text projection feature space.\n\n3. **Challenges**: The authors highlight three major challenges in creating adversarial attacks on VLA models: (1) efficient generation of adversarial examples, (2) achieving low-amplitude, imperceptible perturbations without reducing attack effectiveness, and (3) focusing on model-sensitive regions for sparse and efficient attacks.\n\n4. **ADVLA Framework**: The proposed framework, ADVLA, operates on the projected feature space of VLA visual encoders and combines Projected Gradient Descent (PGD) to achieve efficient attacks. It integrates three attention-guided strategies: gradient-weighted updates, sparse Top-K masking, and focused loss masking.\n\n5. **Advantages**: The ADVLA framework achieves efficient, sparse, and stealthy attacks, which are imperceptible while still causing strong attack effects. It also demonstrates that sparse patch perturbations can be more effective than global perturbations.\n\n6. **Conclusion**: The paper concludes that ADVLA is a powerful tool for assessing the robustness of VLA models and highlights the need for efficient and sparse adversarial attacks that can focus on model-sensitive regions.",
      "similarity_to_abstract": 0.7680165767669678,
      "rouge1": 0.45,
      "rouge2": 0.09623430962343096,
      "rougeL": 0.18749999999999997,
      "bertscore_f1": 0.7994568943977356
    },
    "led": {
      "summary": "",
      "similarity_to_abstract": 0.15606391429901123,
      "rouge1": 0.0,
      "rouge2": 0.0,
      "rougeL": 0,
      "bertscore_f1": 0.0
    }
  },
  {
    "paper_id": "2511.20701v1",
    "abstract": "While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [ 3 ] , which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.",
    "gemini-2.5-flash": {
      "summary": "This text describes the extension and evaluation of a Multimodal Chain-of-Thought (MM-CoT) framework, originally developed by Amazon Science for ScienceQA, to new datasets to test its generality and robustness.\n\nKey points include:\n\n1.  **Research Goal:** To evaluate the MM-CoT framework's applicability to open-domain multimodal reasoning, specifically numerical chart reasoning and open-ended commonsense/world-knowledge reasoning, which are distinct from scientific reasoning.\n2.  **ChartQA Integration:** The framework was adapted to handle **numerical chart reasoning** using the ChartQA dataset. This involved constructing a new JSON dataset, adapting data loaders, redefining prompts, implementing open-ended answer extraction, and extending evaluation metrics to support interpretation of structured visual layouts, arithmetic, and extraction of numeric answers from charts.\n3.  **A-OKVQA Evaluation:** The MM-CoT framework was also applied to the **A-OKVQA dataset**, which requires commonsense and external world knowledge to answer open-ended questions about natural images. This evaluation aims to test the framework's generalization to knowledge-intensive tasks, noting that prior A-OKVQA work hadn't utilized a two-stage CoT with vision feature integration.\n4.  **MM-CoT Background:** The MM-CoT is a two-stage framework that separates rationale generation from answer inference, designed to jointly reason over vision and language inputs.\n5.  **Experimental Constraints:** Experiments were conducted on a CPU with limited resources (batch size=1, learning rate=5e-5, output sequence length=128, 1 epoch), but checkpointing was used to ensure recovery and reproducibility.",
      "similarity_to_abstract": 0.7439686059951782,
      "rouge1": 0.3951219512195122,
      "rouge2": 0.09803921568627451,
      "rougeL": 0.17073170731707318,
      "bertscore_f1": 0.7852138876914978
    },
    "llama-3.1-8b-instant": {
      "summary": "The provided text discusses the integration of a multimodal chain-of-thought (CoT) framework into the MM-CoT pipeline to evaluate its generality and cross-dataset robustness. The original MM-CoT method was designed for scientific reasoning, but this new integration aims to test its performance on open-ended tasks.\n\nThe authors used the ChartQA dataset, which consists of charts with natural-language questions requiring numerical comparisons and reasoning over visualized data. This dataset is ideal for testing the CoT model's ability to adapt to structured quantitative tasks.\n\nThe main contributions of this work include:\n\n1. Integrating ChartQA into the MM-CoT pipeline, which required modifying the dataset, data loaders, prompts, and evaluation metrics.\n2. Extending the MM-CoT framework to handle numerical chart reasoning while maintaining full compatibility with the existing multimodal CoT structure.\n3. Evaluating the model's performance on the A-OKVQA dataset, which requires commonsense and world knowledge.\n\nThe authors also discuss the limitations of prior work on A-OKVQA, which has employed knowledge retrieval, caption-based reasoning, and large multimodal models but not the two-stage CoT framework with vision feature integration.\n\nOverall, this work aims to address the research gap in multimodal CoT reasoning for open-domain tasks and test the generalization of the MM-CoT framework to structured quantitative tasks.",
      "similarity_to_abstract": 0.7236887812614441,
      "rouge1": 0.425974025974026,
      "rouge2": 0.12010443864229765,
      "rougeL": 0.2077922077922078,
      "bertscore_f1": 0.8121353387832642
    },
    "led": {
      "summary": "",
      "similarity_to_abstract": 0.11410409957170486,
      "rouge1": 0.0,
      "rouge2": 0.0,
      "rougeL": 0,
      "bertscore_f1": 0.0
    }
  },
  {
    "paper_id": "2511.21522v1",
    "abstract": "The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method‚Äôs performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.",
    "gemini-2.5-flash": {
      "summary": "The text introduces an in-depth investigation and extension of **pessimistic verification** methods for mathematical proof evaluation, aimed at improving the ability to detect errors. These methods involve reviewing a proof multiple times from different perspectives, deeming the entire proof incorrect if any single review identifies an error.\n\nThree workflows are presented:\n1.  **Simple pessimistic verification:** Involves multiple direct verification queries.\n2.  **Vertical pessimistic verification:** Splits the proof into chunks for parallel, deep review, demonstrating improved error detection and scaling efficiency.\n3.  **Progressive pessimistic verification:** Combines simple and vertical approaches, performing multi-scale verification from the complete proof down to detailed steps to quickly filter errors and then drill down.\n\nExperiments were conducted on challenging datasets like IMO-GradingBench and Hard2Verify, evaluating proofs at a response level as a binary classification (correct or incorrect). While True Negative Rate (TNR) is a primary metric, a detailed case study focused on False Negative (FN) cases, which were found to be most informative.\n\nA surprising key finding was that many FN cases produced by stronger models (e.g., GPT-5-mini) under pessimistic verification actually identified real, critical errors in the proofs that had been mistakenly labeled as correct by human annotators in the datasets. This suggests that the verification capabilities of these models were previously underestimated and supports their use for dataset annotation. The text encourages further community experiments on pessimistic verification to better determine its performance.",
      "similarity_to_abstract": 0.7680225968360901,
      "rouge1": 0.4067796610169492,
      "rouge2": 0.1070559610705596,
      "rougeL": 0.18401937046004846,
      "bertscore_f1": 0.7878559827804565
    },
    "llama-3.1-8b-instant": {
      "summary": "The text describes a study on mathematical proof verification using Large Language Models (LLMs). The researchers developed three new methods: simple pessimistic verification, vertical pessimistic verification, and progressive pessimistic verification. These methods aim to improve the ability of LLMs to detect errors in mathematical proofs.\n\n**Main Findings:**\n\n1. The researchers found that previous experiments underestimated the verification capability of stronger LLMs, such as GPT-5-mini. They identified real errors in proofs that were previously labeled as correct.\n2. The new methods, especially progressive pessimistic verification, significantly improved performance and efficiency in error detection.\n3. The study focused on the False Negative (FN) cases, which are errors that were not detected by the LLM. They found that stronger LLMs tend to produce fewer unwarranted FN judgments.\n4. The researchers also conducted a case study on the results of the pessimistic verification, which showed that many smaller models produce a large number of unwarranted FN judgments.\n\n**Methodologies:**\n\n1. The researchers used three datasets: IMO-GradingBench, Hard2Verify, and QiuZhen-Bench.\n2. They evaluated the performance of the LLMs using the True Negative Rate (TNR) metric, which measures the proportion of detected errors among all erroneous proofs.\n3. The new methods involve multiple rounds of verification, with different perspectives and levels of detail.\n\n**Key Limitations:**\n\n1. The ability of LLMs to detect errors in mathematical proofs is still limited.\n2. The study found that many smaller LLMs produce a large number of unwarranted FN judgments.\n\n**Future Work:**\n\n1. The researchers encourage the community to conduct more experiments on pessimistic verification to better determine its true performance on state-of-the-art models.\n2. The study suggests that further research is needed to improve the ability of LLMs to detect errors in mathematical proofs.",
      "similarity_to_abstract": 0.7550058960914612,
      "rouge1": 0.3888888888888889,
      "rouge2": 0.09871244635193133,
      "rougeL": 0.17948717948717952,
      "bertscore_f1": 0.7711748480796814
    },
    "led": {
      "summary": "",
      "similarity_to_abstract": 0.04945599287748337,
      "rouge1": 0.0,
      "rouge2": 0.0,
      "rougeL": 0,
      "bertscore_f1": 0.0
    }
  },
  {
    "paper_id": "2511.20719v1",
    "abstract": "Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.",
    "gemini-2.5-flash": {
      "summary": "The provided text describes a novel **Agentic AI Wi-Fi framework** that utilizes **multi-LLM-agent systems** to address complex, dynamic Multi-AP Coordination (MAPC) challenges. Each Access Point (AP) is empowered with a cognitive architecture that includes reasoning, short-term memory, RAG-based long-term memory (for retrieving relevant exemplars), and tool use, processing real-time inputs from other APs.\n\nThis architecture enables a new paradigm of **dynamic coordination**, where agents collaboratively determine optimal strategies through **natural language dialogue**, rather than fixed rules. This coordination is manifested in two key ways:\n\n1.  **Dynamic Switching of Coordination Strategies**: Agents adaptively select between conservative Co-TDMA and aggressive Co-SR approaches. This selection is learned dynamically through continuous action, feedback, and reflection, allowing agents to recognize network interference patterns (e.g., collisions prompt Co-TDMA, successful transmissions encourage Co-SR).\n2.  **Natural Language Negotiation Protocol**: Agents engage in fine-grained negotiation using rich, contextual natural language messages to optimize resource sharing, going beyond simple control signals to convey strategic intent.\n\nThe RAG-based long-term memory is emphasized as crucial for enhancing strategic reasoning by providing contextually relevant, high-performing examples, especially in complex scenarios. The framework's general applicability is demonstrated using LLMs like GPT-4o and DeepSeek-R1, with multi-agent dialogue implemented via AutoGen, showing agents autonomously learn to navigate coordination trade-offs.",
      "similarity_to_abstract": 0.8207483291625977,
      "rouge1": 0.41326530612244894,
      "rouge2": 0.11282051282051282,
      "rougeL": 0.15816326530612243,
      "bertscore_f1": 0.7813398838043213
    },
    "llama-3.1-8b-instant": {
      "summary": "The provided text discusses a novel Agentic AI Wi-Fi framework that leverages multi-Agent Learning Model (LLM) systems to address the complex challenges of Multi-Access Point Coordination (MAPC). This framework empowers each Access Point (AP) with a cognitive architecture featuring reasoning, memory, and tool use, allowing agents to autonomously learn and navigate the trade-off between two coordination strategies: Co-TDMA (Time-Division Multiple Access) and Co-SR (Slotted Random Access).\n\nThe agents use natural language dialogue to dynamically switch between these strategies based on their learning experience and the underlying interference patterns of their network environment. The framework's primary contribution is a new paradigm for decentralized wireless control that transcends traditional fixed rules.\n\nThe text highlights two key aspects of this dynamic coordination:\n\n1. **Dynamic Switching of Coordination Strategies**: Agents learn to recognize interference patterns and adapt their strategy accordingly, switching between conservative Co-TDMA and aggressive Co-SR approaches.\n2. **Negotiation Protocol in Natural Language**: Agents use rich, contextual proposals to fine-tune their coordination within the chosen scheme.\n\nThe authors demonstrate the effectiveness of their framework through simulations using two distinct LLMs and a multi-agent dialogue workflow implemented using AutoGen. The results show that the agents can learn to navigate the trade-off between Co-TDMA and Co-SR, leading to improved performance and robustness.\n\nThe framework's components, including reasoning, memory, and tool use, are essential for achieving robust and intelligent coordination. The authors conclude that their novel Agentic AI Wi-Fi framework offers a new paradigm for decentralized wireless control, addressing the complex challenges of MAPC.",
      "similarity_to_abstract": 0.8224005699157715,
      "rouge1": 0.4398148148148148,
      "rouge2": 0.14418604651162792,
      "rougeL": 0.19907407407407404,
      "bertscore_f1": 0.8009003400802612
    },
    "led": {
      "summary": "The agentic architecture facilitates a new paradigm of dynamic coordination. Unlike traditional systems with fixed rules, our agents collaboratively determine the optimal coordination strategy through dialogue. The primary form of high-level coordination is the adaptive selection between Co-TDMA and Co-SR, which is determined dynamically by the agents rather than being pre-programmed. Through the continuous cycle of action, feedback, and reflection, the agents learn to recognize the underlying interference patterns of their network environment. This ability to flexibly switch between conservative and aggressive coordination modes, based on learned experience, is a core strength of our agentic approach. The agentic dialogue enables fine-grained negotiation and optimization within the chosen scheme. Interaction data is reconstructed each round to provide contextual information by incorporating real-time inputs such as messages from other APs, short-term memory, and relevant exemplars retrieved from long term memory. The result of this work is a new class of dynamic coordinate strategies. Examples include: Dynamic Switching of Coordination Strategies : The primary system of high level coordination is characterized by two key aspects:Dynamic Switching of Choosing a Coordination Mode dynamically and Being able to negotiate and optimize within that scheme. Negotiation Protocol in Natural Language : Beyond selecting a high-layer coordination scheme, the agent's dialogue facilitates fine-grain negotiation and optimizations within that decided scheme. Messages are rich, contextual proposals that encapsulate strategic intent. For example, instead of just broadcasting a fixed schedule, a sharing AP can formulate a proposal that includes not only its own intended action but also contingent suggestions or queries for its peeightenedness. Conclusion In this paper, we introduced a novel Agentic AI Wi-Fi framework that leverages multi-LLM-agent systems to address the complex, dynamic challenges of MAPC. Taken together, these results clearly validate the necessity of each component for achieving robust and intelligent coordination. To demonstrate the general applicability of our proposed framework, we conduct experiments using two distinct LLMs to power the agents: GPT-4o [ 12 ] and DeepSeek-R1 [ 13 ] . The multi-agent dialogue and workflow are implemented using AutoGen [ 14 ] , a framework for Multi-agent conversational applications developed by Microsoft.",
      "similarity_to_abstract": 0.6582499742507935,
      "rouge1": 0.36711281070745694,
      "rouge2": 0.08061420345489442,
      "rougeL": 0.13766730401529637,
      "bertscore_f1": 0.7869501113891602
    }
  },
  {
    "paper_id": "2511.21686v1",
    "abstract": "Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present Matrix , a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves 2 2 ‚Äì 15 √ó 15\\times higher data generation throughput under identical hardware resources, without compromising output quality.",
    "gemini-2.5-flash": {
      "summary": "Matrix is a scalable runtime specifically designed for **large-scale multi-agent synthetic data generation**, capable of efficiently executing tens of thousands of concurrent workflows. Its core innovation is a **peer-to-peer agent architecture** with **row-level scheduling**, where control and data flow are embedded in messages. This design enables **fully asynchronous and fine-grained execution**, eliminating bottlenecks from centralized orchestration and idle times caused by batch-level synchronization (common in systems like Ray Data).\n\nAgents are implemented as **Ray actors**, allowing for horizontal scaling and distributed execution across cluster nodes without resource contention. Matrix ensures **high efficiency and cluster resource utilization** by running thousands of asynchronous tasks concurrently, even with moderate data and agent-level parallelism. It integrates with modern inference engines (e.g., vLLM, SGLang) and leverages Ray for distributed execution and containerized environments.\n\nEvaluations demonstrate that Matrix achieves **2‚Äì15 times higher token throughput** than specialized baseline systems, proving its flexibility and readiness for diverse, computationally intensive multi-agent workloads. Its modular architecture is extensible and fully configurable.",
      "similarity_to_abstract": 0.7521914839744568,
      "rouge1": 0.4491978609625668,
      "rouge2": 0.11290322580645161,
      "rougeL": 0.18716577540106952,
      "bertscore_f1": 0.7946637868881226
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a new runtime system called Matrix for large-scale multi-agent synthetic data generation. Key features of Matrix include:\n\n1. **Agent parallelism**: Each agent role is implemented as a Ray actor, allowing for configurable CPU, GPU, and memory allocations. This enables roles to scale horizontally by launching multiple distributed agent instances, each processing assigned tasks independently.\n2. **Peer-to-peer architecture**: Matrix eliminates centralized orchestration bottlenecks and idle time caused by batch-level synchronization, enabling fully asynchronous and fine-grained execution at scale.\n3. **Row-level scheduling**: Matrix performs row-level scheduling through peer-to-peer message orchestration, allowing each task to progress asynchronously through agents and eliminating idle periods caused by batch-level barriers.\n4. **Scalability**: Matrix is designed to be flexible and extensible, supporting diverse multi-agent use cases, and can efficiently execute tens of thousands of concurrent workflows.\n5. **Integration with modern inference engines**: Matrix integrates with modern inference engines such as vLLM, SGLang, and leverages Ray for distributed execution and containerized environments via Apptainer.\n\nThe authors evaluate Matrix on three representative case studies: Collaborative Reasoner, NaturalReasoning, and Tau2-bench, and demonstrate that Matrix achieves 2-15x higher token throughput than specialized baseline systems. They also compare the throughput of Matrix's row-level scheduling with a batch-level processing baseline implemented using Ray Data and show that Matrix's peer-to-peer architecture and row-level scheduling approach outperform the baseline.",
      "similarity_to_abstract": 0.7393991947174072,
      "rouge1": 0.4090909090909091,
      "rouge2": 0.0776255707762557,
      "rougeL": 0.19545454545454544,
      "bertscore_f1": 0.7900980114936829
    },
    "led": {
      "summary": "Matrix is a scalable runtime for large scale multi-agent synthetic data generation capable of efficiently executing tens of thousands of concurrent workflows. Its distributed services ensure efficient utilization of cluster resources even with moderate data and agent-level parallelism. In this paper, the authors demonstrate how Matrix enables distributed scheduling through peer-to-peer message orchestration. They compare Matrix's row-level scheduling with Ray Data Ray's batch processing systems, such as Raia 2012 rdd and Ray Data ray , and Ray Ray for distributed execution and containerized environments via Apptainer kurtzer2017 specialization.Key Contributions We introduce Matrix , a scalable Runtime for largescale multi-agents synthetic data Generation capable of safely executing tensof thousands of simultaneous workflows The framework leverages Ray ray for single-actor actors and Ray serve for distributed command and control. Our setup prefers 14k max concurrency given the number of GPUs. We evaluate Matrix on three representative case studies: Collaborative Reasoner NI2025 Collaborative , Natural Reasoning yuan2025naturalreasoning wild28m , and Tau2-bench barres2025 tuft . Matrix achieves 2/15 times higher throughput than specialized baseline systems while mai / parallelism beyond 2 has small effect on throughput.4 Row-Level Scheduling In batch processing technologies such as Ray DataRay and Ray2012 rdd, where the pipeline controls progress across synched batches, Matrix performs row level scheduling by using distributed messages. In addition, we integrate Ray ray into modern inference engines such as vLLM vllm , SGLang zheng2024sglangefficientexecutionstructured , and use cases such as software and tools execution. Key Contributions Wollstone and his team present their approach to distributed scheduling in order to achieve scalability and reduce bottlenecks caused by batch-level synchronization. Their workflows are implemented as Ray actors with configurable CPU, GPU, and memory allocations. For LLM-based agents, computational cost dominates over input pipeline overhead, but there is little benefit for NaturalReasoning task in Section 5.2 . This task can be assigned to multiple distributed agent instances that launch each morning with different tasks independently. Each task is separated from the rest of the cluster until it is complete. At this point, they start sending messages back and forth between the nodes in the cluster so that each role scales linearly without I/O blocking. It also helps to avoid resource contention commonly seen in centralized orchestrations. By creating an environment based on distributed services, RDS ensures that clusters do not have to be tightly controlled.5 Row Level Scheduling Matrix uses distributed services to execute tasks in batch processing Systems such as Keio 2011 rd and RayData ray .6 Row-level Scheduling in Ray Data Rays allows scientists to schedule tasks ahead of time rather than waiting for them to reach critical thresholds.7) Hierarchical Multi-agent Synthetic Data Generation Using Ray Serve, Verilogic, and Ray Machine shop, our research group develops state-of-the-art machine learning algorithms designed to efficiently execute millions of concurrent tasks in real-time. With its distributed services and ease of use, the overall architecture benefits from economies of scale and scalability. All these capabilities are demonstrated in sections 5.1-5.2 and 5.3 Next, section 5.4 provides a detailed overview of the scientific methodologies used in calculating the maximum achievable task concurrency per task partition.8) Numerical Power vs. Interpreting nCeML approaches statistical inference using Bayesian inference engines like sGranularity and Gradient Processing Units (GPUs).9) SiSight executes statistics using additive synthesis techniques similar to those used in natural-language processing.10) Logistic Reasoning executes natural-sounding reasoning using advanced statistical methods such as convolutional neural networks.11) Natural Reason makes inferential reasoning using sophisticated statistical analysis tools such as Gaussian clustering.12) Predicting future market demand using sparsely populated datasets yields improved performance due to its ability to extract insights from sparse sets of inputs.13) Optimizing decision-making algorithms using stochastic gradient detection leads to significant reductions in error due to reduced false-positive bias.14) Sequential Descriptive Analysis Driven by Unsupervised Decompression SolvingQuantum Algorithms was one of the most promising new approaches to deterministic prediction driven by artificial intelligence in the early 1900s. However, advances in statistical inference technology led to huge waste in today's datacenters because of the insufficient availability of computing power.",
      "similarity_to_abstract": 0.7745386362075806,
      "rouge1": 0.2952275249722531,
      "rouge2": 0.062291434927697446,
      "rougeL": 0.09988901220865704,
      "bertscore_f1": 0.7883608341217041
    }
  },
  {
    "paper_id": "2511.21574v1",
    "abstract": "Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types.\nTo bridge these gaps, we propose a novel yet efficient teacher-student framework, namely M ultimodal R obust P rompt D istillation (MRPD) for distilling robust 3D point cloud model.\nIt learns lightweight prompts by aligning student point cloud model‚Äôs features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder.\nTo ensure a reliable knowledge transfer, this distillation is\nguided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data.\nOur work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.",
    "gemini-2.5-flash": {
      "summary": "The MRPD (Multimodal Robust Prompt Distillation) framework introduces a novel and highly efficient approach to 3D adversarial defense. It operates by distilling multimodal robustness from stable 2D images, text, and teacher models into lightweight, learnable prompts embedded within a student point cloud model.\n\nKey features and contributions of MRPD include:\n\n*   **Breaking the Trade-off:** It offers a powerful and practical defense that achieves robust protection with **zero inference overhead**, effectively resolving the long-standing conflict between adversarial robustness and computational efficiency/clean data accuracy.\n*   **State-of-the-Art Performance:** MRPD establishes a new state-of-the-art in 3D adversarial defense, consistently outperforming computationally expensive methods and adversarial training baselines across a diverse suite of attacks on datasets like ModelNet40 and ScanObjectNN.\n*   **Strong Generalization:** It demonstrates remarkable resilience against black-box and transferable attacks, indicating that its multimodal distillation process learns a more generalizable feature space, avoiding the overfitting common in other defense strategies.\n*   **Real-world Robustness:** The framework's ability to preserve essential features extends beyond adversarial attacks to real-world imperfections, performing strongly on noisy and occluded data.\n*   **Architecture Agnostic:** It hardens the student model against attacks without altering its backbone architecture, further contributing to its efficiency.",
      "similarity_to_abstract": 0.8527749180793762,
      "rouge1": 0.450632911392405,
      "rouge2": 0.06615776081424936,
      "rougeL": 0.1924050632911392,
      "bertscore_f1": 0.793228030204773
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a new method called Multimodal Robust Prompt Distillation (MRPD) for defending against adversarial attacks in 3D point cloud models. The key features of MRPD are:\n\n1. **Multimodal distillation**: MRPD collects knowledge from multiple sources (image, text, and point cloud modalities) and transfers it into a lightweight \"student\" point cloud model.\n2. **Robust defense**: MRPD achieves superior performance against a diverse suite of attacks, including black-box attacks, without introducing inference overhead.\n3. **Zero overhead**: MRPD preserves the architecture of the student model, ensuring zero overhead at inference.\n4. **State-of-the-art performance**: MRPD outperforms existing methods in 3D adversarial defense, achieving a new state-of-the-art on the ModelNet40 dataset.\n\nMRPD's advantages are:\n\n1. **Improved generalizability**: MRPD's multimodal distillation learns a more generalizable feature space, avoiding overfitting common to adversarial training.\n2. **Strong performance on noisy and occluded data**: MRPD delivers higher average robust accuracy on the ScanObjectNN dataset, even on noisy and occluded data.\n\nThe authors propose a highly efficient paradigm that achieves robust defense with zero inference overhead, effectively breaking the trade-off between adversarial robustness and clean data accuracy.",
      "similarity_to_abstract": 0.8487284183502197,
      "rouge1": 0.4571428571428571,
      "rouge2": 0.10443864229765011,
      "rougeL": 0.19740259740259739,
      "bertscore_f1": 0.794080913066864
    },
    "led": {
      "summary": "In this paper, the authors summarize and discuss several different types of threats that they encounter in the real world. They examine how well their MRPD model performs against various types of attacks, including \"black-box\" attacks, \"add-cd\" and ADD-HD , as well as conventional threat types such as \"Perturb\" and \"Add-CD.\" They conclude that by distilling knowledge from \"stable 2D and text teachers enables the model to preserve essential features against both adversarial attacks and real-world imperfections\". Generalization Against Black-Box Attacks The authors further assess the generalizability of their method in a \"challenging black-box setting,\" where attacks are transferred from a third-party model to the model. Results show that the MRPD framework outperforms other methods for detecting black box attacks on models such as ScanObjectNN and ModelNet40; it also outperforms more expensive methods in a variety of tasks involving transferring attacks from other models. In addition, the results demonstrate remarkable resilience across a wide range of threat types. Adversarial Defense for Point Clouds In response to this diverse threat landscape, various defense mechanisms have been proposed for 3D point cloud models. A common strategy involves pre-processing the input data to remove or repair adversarial perturbations. This includes Statistical Outlier Removal (SOR) and adding/removing weights from the input point cloud. However, none of these approaches has any way to avoid the trade-off between robustness and clean data accuracy. By embedding multimodal robustness into lightweight prompts, our MRPD offers a \"powerful and practical defense that breaks the trade off between security and efficiency. Generalization vs. Black Box Attacks We further assess MRPD's generalizableness in a challenging black box setting, demonstrating that its robustness extends beyond specific threat models. Asserted repeatedly over a broad spectrum of attacks while introducing no inference overhead, we find that MRPD establishes a new state-of-the-art in 3D adversarial defense, consistently outperforming computationally expensive methods. Our method demonstrates remarkable resilience, corroborating our core premise: Distilling knowledge out of image, text, and teacher data enables the prediction to retain essential features even in the presence of imperfections. It furthermore outperforms competing methods when it is exposed to very noisy and occluded data. Ultimately, by embedding Multimodal Robotic Process Gradients into lightweight Prompts, MRPD gains a powerful and practical protection against black-boxing attacks.Generalization Against Transactional Threats The authors perform many experiments to determine whether their approach can successfully deter information transfer from one model to another without introducing any type of bias. Their conclusion is that it can achieve superior defense against almost any attack because it leverages the power of multi-volution learning techniques. Other Work All of the authors work on improving defenses for point clouds has been done in the past. Most of this work focuses on using machine learning to replace or repair errors in the data before the model receives any training data. Here, however, the author devotes most of his effort to generalizing what he calls the \"MRPD framework\" to predicting black box events. He introduces a new kind of detection technique called dynamic clustering, which uses an iterative learning framework that efficiently transfers \"multiplyxive robustness\" from classroom teachers' models into the student model architecture, thereby leaving the student modeling architecture untouched. The goal of this approach is to provide robustness with zero inference overhead so that students do not need to worry about perfecting their own model designs prior to facing an unknown threat. Several papers have been published attempting to address the problem of how to best defend against point clouds. One of the most important advances in this field lies in the use of distributed recurrent neural nets. These neural nets were initially developed by scientists at the National Academy of Engineering in the early 1900s, who believed that it would be nearly impossible to predict black boxes due to the inherent randomness of biological organisms. Despite the availability of plenty of good evidence supporting the existence of black boxes, there was little consensus on how best to guard against them. Until recently, point clouds were classified as unsupervised systems since they did not have much in the way of predefined features. As a result, people had to spend considerable time fine-tuning their models specifically to distinguish between benign and malicious entities. With this new approach, the ability to predict intelligent behavior from meaningless inputs provides a significant improvement over previous approaches.",
      "similarity_to_abstract": 0.7697388529777527,
      "rouge1": 0.2915326902465166,
      "rouge2": 0.04511278195488722,
      "rougeL": 0.10075026795284031,
      "bertscore_f1": 0.7953817844390869
    }
  },
  {
    "paper_id": "2511.20766v1",
    "abstract": "Reliability is key to realizing the promise of autonomous UI-agents, multimodal agents that directly interact with the apps humans use, as users must be able to trust an agent to complete a given task.\nCurrent evaluations rely on fixed environments‚Äîoften clones of existing apps‚Äî\nwhich are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment.\nWhen deployed however, agents are likely to encounter variations in app design and content that can affect an agent‚Äôs ability to complete a task.\nTo address this blind spot of measuring agent reliability across app variations , we develop OpenApps , a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run,\nenabling easy generation and deployment of thousands of versions of each app.\nSpecifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than 50% across app variations.\nFor example, Kimi-VL-3B‚Äôs average success across all tasks fluctuates from 63% to just 4% across app versions.\nWe also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration.\nThese initial findings highlight the importance of measuring reliability along this new dimension of app variations.",
    "gemini-2.5-flash": {
      "summary": "OpenApps is a new benchmark designed to evaluate the **reliability of UI-agents** by subjecting them to **thousands of app variations** in appearance and content. The authors argue that traditional fixed app environments fail to capture the significant **fluctuations in agent success rates** that occur across these variations, which they measure using standard deviation and mean absolute deviation of rewards.\n\nIn this work, they focused on:\n*   **Fifteen simple, few-step tasks** (e.g., adding an item to a todo list) across common apps like calendar, messenger, and maps.\n*   **Independently applying eight content and appearance variations** to these apps (e.g., dark theme).\n*   Executing **over 10,000 independent evaluations** with **seven diverse agents**, including large multimodal models (Claude, OpenAI, Qwen-VL) and specialized UI-models (UI-Tars).\n\n**Key Findings:**\n*   App variations significantly impact agent reliability; fixed environments do not reveal these fluctuations.\n*   Agent behaviors (e.g., looping, hallucinating actions) and optimal deployment configurations (e.g., input resolution) also differ considerably across app variations.\n\nBeyond evaluation, OpenApps is also presented as a valuable resource for **scaling digital agent training pipelines, providing a safe sandbox for deployment, and studying generalization** across app variations.\n\n**Limitations and Future Work:**\nThe current study focused on simple tasks and independent variations. Future work will extend to more **complex, longer-horizon tasks** to form a comprehensive UI-agent reliability benchmark, explore **interactions between multiple app variation factors**, and potentially incorporate **human validation or interaction**.",
      "similarity_to_abstract": 0.854573130607605,
      "rouge1": 0.49193548387096775,
      "rouge2": 0.0931174089068826,
      "rougeL": 0.19354838709677422,
      "bertscore_f1": 0.7715321779251099
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses OpenApps, a platform that generates thousands of app variations to evaluate the reliability of digital agents. The authors focus on autonomous agents, but also consider agents that incorporate human validation or interaction. OpenApps serves as a benchmark for UI-agent reliability and can be used to train digital agents, deploy them in a safe environment, and study their generalization across app variations.\n\nThe authors highlight the limitations of their work, including the use of simple tasks that require only a few steps to complete. They aim to create a more challenging benchmark in the future by including more complex or longer-horizon tasks. They also note that they focus on varying each app appearance or content factor independently, leaving interactions between multiple factors for future work.\n\nThe authors use OpenApps to evaluate the reliability of seven agents, including Claude, OpenAI, and Qwen-VL. They measure reliability using the standard deviation of rewards across runs for a given task and find that fixed app environments do not capture the considerable fluctuations in agent success rates across app variations.\n\nThe authors study how agent behaviors, such as looping or hallucinating actions, and deployment configuration can differ across app variations. They also find that high-resolution inputs can sometimes decrease task success rates for certain app variations, highlighting the importance of considering app variations when evaluating agent reliability.\n\nKey findings from the study include:\n\n* App variations are a key axis of reliability in terms of agent performance, behaviors, and deployment.\n* Fixed app environments do not capture the considerable fluctuations in agent success rates across app variations.\n* Agent behaviors and deployment configuration can differ across app variations.\n* High-resolution inputs can sometimes decrease task success rates for certain app variations.\n\nOverall, the study highlights the importance of considering app variations when evaluating the reliability of digital agents and provides a platform for future research in this area.",
      "similarity_to_abstract": 0.8783444166183472,
      "rouge1": 0.5416666666666666,
      "rouge2": 0.132404181184669,
      "rougeL": 0.21180555555555558,
      "bertscore_f1": 0.813711941242218
    },
    "led": {
      "summary": "",
      "similarity_to_abstract": 0.06650678813457489,
      "rouge1": 0.0,
      "rouge2": 0.0,
      "rougeL": 0,
      "bertscore_f1": 0.0
    }
  },
  {
    "paper_id": "2511.20679v1",
    "abstract": "Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.",
    "gemini-2.5-flash": {
      "summary": "The text describes a method for using Large Language Models (LLMs) to **restructure hierarchies** (such as ontologies or knowledge graph schemas) to **optimize for hyperbolic embedding quality**.\n\nThis process involves prompting an LLM with a transformed textual representation of the hierarchy along with specific structural guidelines aimed at reducing embedding distortion. The key recommendations, building on research by Ayoughi et al., are:\n\n1.  **Design hierarchies for width:** Favor wide trees with high branching factors over deep, narrow ones, as this structure is most effective for embedding algorithms that leverage hierarchical order.\n2.  **De-prioritize balance:** Do not worry about the balance between subtrees, as current algorithms are largely agnostic to it.\n\nThe LLM is instructed to incorporate these recommendations. A significant feature of this approach is the LLM's ability to break down the task into subtasks, provide an initial assessment of the current hierarchy's structure (e.g., depth, root, intermediate, and leaf nodes), and interactively explain the steps it takes during the restructuring process.",
      "similarity_to_abstract": 0.7854227423667908,
      "rouge1": 0.40935672514619886,
      "rouge2": 0.08823529411764705,
      "rougeL": 0.14619883040935674,
      "bertscore_f1": 0.796574592590332
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses optimizing hierarchy structures for machine learning using hyperbolic embeddings. The recommendations include:\n\n1. Designing hierarchies for width: Wide hierarchies with high branching factors perform better than deep, narrow trees.\n2. Not worrying about balance: Current algorithms are agnostic to the balance between subtrees.\n3. Favoring wide, single-inheritance hierarchies: This type of hierarchy is favorable for low-distortion hyperbolic embeddings.\n4. De-prioritizing hierarchy size and imbalance: These factors are less important for hyperbolic embeddings.\n\nTo optimize the hierarchy, the text suggests transforming the textual hierarchy and prompting a Large Language Model (LLM) to restructure it based on the recommendations. The LLM can provide explanations of the steps it takes in an interactive fashion.\n\nThe text also mentions the importance of width over depth in hyperbolic embeddings and concludes that all recommendations matter for minimizing embedding distortion. An example of how the LLM can restructure the hierarchy is given, demonstrating how it divides the task into smaller subtasks and provides an assessment of the current hierarchy structure.\n\nOverall, the text suggests that by using these recommendations and transforming the hierarchy, it is possible to optimize it for better performance with hyperbolic embeddings.",
      "similarity_to_abstract": 0.8318033814430237,
      "rouge1": 0.41509433962264153,
      "rouge2": 0.08672086720867209,
      "rougeL": 0.18867924528301885,
      "bertscore_f1": 0.8181749582290649
    },
    "led": {
      "summary": "",
      "similarity_to_abstract": 0.05068151280283928,
      "rouge1": 0.0,
      "rouge2": 0.0,
      "rougeL": 0,
      "bertscore_f1": 0.0
    }
  },
  {
    "paper_id": "2511.21688v1",
    "abstract": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G 2 VLM , a g eometry g rounded v ision- l anguage m odel that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G 2 VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations.\nExperimental results demonstrate G 2 VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks.\nBy unifying a semantically strong VLM with low-level 3D vision tasks, we hope G 2 VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "gemini-2.5-flash": {
      "summary": "G2VLM is introduced as the first unified Vision-Language Model (VLM) that bridges spatial 3D reconstruction with high-level spatial understanding. It features a novel architecture with two specialized experts: a geometric perception expert (\"where pathway\") for visual geometry learning, and a semantic perception expert (\"what pathway\") for multimodal understanding. These experts interact via shared self-attention.\n\nThe model employs a two-stage training strategy:\n1.  The semantic expert is initialized from a frozen pretrained VLM (e.g., Qwen2-VL), while the geometric expert is trained from scratch on a large 3D-annotated dataset.\n2.  The semantic expert is unfrozen and jointly trained with the geometric expert on spatial understanding data to integrate geometric features.\n\nThis design allows G2VLM to learn 3D geometry from 2D image inputs, reducing reliance on difficult-to-collect 3D data.\n\nG2VLM demonstrates significant performance:\n*   **Visual Geometry Tasks:** It achieves competitive results against state-of-the-art feed-forward 3D reconstruction models (e.g., VGGT) on depth, point, and camera pose estimation tasks. Notably, it reduces monocular depth estimation error on the Sintel benchmark.\n*   **Spatial Reasoning Tasks:** It achieves the best results on SPAR-Bench, surpassing GPT-4o by 18.5 points, and performs comparably to much larger models despite its 2B size.\n\nThe research also confirms a positive interplay between the geometric and semantic representations. G2VLM's unified design offers a natural alignment that combines geometric accuracy and efficiency with higher-level spatial comprehension, serving as a strong baseline for future capabilities.",
      "similarity_to_abstract": 0.8118499517440796,
      "rouge1": 0.44933920704845814,
      "rouge2": 0.1592920353982301,
      "rougeL": 0.23788546255506607,
      "bertscore_f1": 0.8118767142295837
    },
    "llama-3.1-8b-instant": {
      "summary": "The text discusses a new vision-language model (VLM) called G 2 VLM, which is designed to perform both visual geometry tasks and high-level spatial understanding tasks. The model consists of two experts: a geometric perception expert and a semantic perception expert. \n\nIn the first stage of training, the semantic expert is initialized from a frozen pre-trained VLM, while the geometric expert is trained from scratch on a large-scale 3D-annotated dataset. In the second stage, both experts are jointly trained on spatial understanding data, allowing them to integrate the learned geometric features and enhance their spatial reasoning performance.\n\nG 2 VLM is evaluated on a range of visual geometry and spatial reasoning tasks, achieving competitive or better results compared to state-of-the-art models. On visual geometry tasks, it reduces the monocular depth estimation error from 0.335 to 0.297 on the Sintel benchmark. On spatial reasoning tasks, it achieves the best results on SPAR-Bench among all existing works.\n\nThe contributions of G 2 VLM can be summarized as follows:\n\n1. It introduces a unified model that bridges spatial 3D reconstruction and high-level spatial understanding in a single vision-language model.\n2. It employs a novel architecture with dedicated geometric and semantic perception experts that interact via shared self-attention.\n3. It learns to reason about 3D geometry from pure 2D image inputs, eliminating the reliance on difficult-to-collect 3D data.\n4. It achieves state-of-the-art results on a range of visual geometry and spatial reasoning tasks.\n\nOverall, G 2 VLM is a powerful model that unifies visual geometry and multimodal reasoning within one framework, enabling it to perform a wide range of tasks with high accuracy and efficiency.",
      "similarity_to_abstract": 0.8416338562965393,
      "rouge1": 0.4909456740442656,
      "rouge2": 0.15353535353535352,
      "rougeL": 0.2052313883299799,
      "bertscore_f1": 0.8179471492767334
    },
    "led": {
      "summary": "Our model demonstrates significant performance across a wide range of visual geometry and spatial reasoning tasks. The G 2 VLM outperforms state-of-the-art feed-forward 3D reconstruction models such as VGGT and M-3R on the Sintel benchmark and significantly improves performance on a number of other tasks. Our work merges the study's two main experts -- a geometric expert and a semantic expert -- into one model. The semantic expert is trained on a large-scale dataset to learn a geometry-rich representation, while the geometric perception expert has to be trained from scratch on a 3Dimensionalimensional model with respect to geometry. Through extensive experiments, the authors show that their model proves very good at many different tasks. In particular, they observe that the model excels at predicting depth, point estimation, and camera pose estimation tasks. It also performs well on several other tasks involving high-level spatial reasoning. The authors present a two-stage training strategy for both the geometric and semantic experts, which involves training both experts on datasets with varying degrees of difficulty. Gradually, they unfreeze the semantic expert and train it with the geometric expert in order to further integrate their learning with the previously learned geometric features. The final stage consists of training the two experts on real-world data so that they can then perform similar tasks in spatially distant regions. The results are quite impressive: our model outperforms all the state-tertiary analysis tools as well as those of much larger models despite its small size.",
      "similarity_to_abstract": 0.7912306189537048,
      "rouge1": 0.3636363636363637,
      "rouge2": 0.08260869565217392,
      "rougeL": 0.15584415584415584,
      "bertscore_f1": 0.7954915165901184
    }
  },
  {
    "paper_id": "2511.21471v1",
    "abstract": "Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability‚Äìoriented metric that reliably assesses a model‚Äôs overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.\nOur codes are available at https://github.com/XPR2004/SpatialBench .",
    "gemini-2.5-flash": {
      "summary": "This text introduces **SpatialBench**, a new, comprehensive benchmark designed to assess the hierarchical spatial intelligence of Multimodal Large Language Models (MLLMs).\n\nKey aspects of SpatialBench include:\n\n*   **Cognitively Grounded Framework:** It's built upon a five-level hierarchical framework of spatial cognition, inspired by cognitive map theory, that progresses from basic perception to complex decision-making. These levels are:\n    1.  **Observation (L1):** Object counting, size, room size, absolute distance.\n    2.  **Topology and relation (L2):** Appearance order, relative distance/direction.\n    3.  **Symbolic reasoning (L3):** Multi-hop spatial reasoning, affordance, pose localization.\n    4.  **Causality (L4):** Spatial causal reasoning.\n    5.  **Planning (L5):** Visual-based commands, route planning.\n*   **Large-scale Dataset:** SpatialBench comprises 1,347 question-answer pairs across 15 distinct spatial reasoning task types, sourced from 50 first-person (egocentric) videos captured in diverse real-world indoor and outdoor environments.\n*   **Evaluation and Findings:** Experiments using SpatialBench reveal that while MLLMs perform well on low-level perceptual and relational reasoning tasks (L1 & L2), their competence declines sharply in high-level tasks requiring symbolic reasoning, causal inference, and planning (L3, L4, & L5).\n*   **Insights:** A comparison with human performance suggests that MLLMs exhibit diffuse attention to scene details and infer by association, lacking the selective, goal-oriented reasoning and unified spatial cognition demonstrated by humans.\n\nIn essence, SpatialBench provides a principled and fine-grained diagnostic tool for evaluating MLLMs' spatial intelligence, highlighting their current limitations in complex spatial understanding and planning.",
      "similarity_to_abstract": 0.8274063467979431,
      "rouge1": 0.4853932584269663,
      "rouge2": 0.12641083521444693,
      "rougeL": 0.22471910112359547,
      "bertscore_f1": 0.7922164797782898
    },
    "llama-3.1-8b-instant": {
      "summary": "Researchers have developed a new benchmark called SpatialBench to assess the spatial intelligence of Multimodal Large Language Models (MLLMs). SpatialBench consists of 1,347 question-answer pairs from 50 videos, each carefully designed to evaluate MLLMs' spatial cognition ability across five cognitive levels:\n\n1. Observation (L1): recognizing objects, counting, and measuring distances.\n2. Topology and relation (L2): understanding spatial relationships and directions.\n3. Symbolic reasoning (L3): multi-hop reasoning, affordance, and landmark-based localization.\n4. Causality (L4): understanding spatial cause-and-effect relationships.\n5. Planning (L5): generating visual commands and planning routes.\n\nThe researchers found that current MLLMs excel at low-level tasks but struggle with higher-level tasks, often relying on association rather than selective reasoning. SpatialBench aims to provide a comprehensive and cognitively grounded framework for evaluating MLLMs' spatial intelligence, enabling the development of more advanced spatially grounded intelligence.\n\nKey contributions of SpatialBench include:\n\n* A hierarchical framework for assessing spatial intelligence, drawing inspiration from cognitive map theory.\n* A large-scale multimodal dataset specifically designed for evaluating spatial cognition in MLLMs.\n* A high-level ability-driven evaluation metric to assess spatial cognition in MLLMs.\n* Experimental results showing the strengths and limitations of current MLLMs in spatial reasoning, as well as controlled human evaluations to compare human and model reasoning.",
      "similarity_to_abstract": 0.8708846569061279,
      "rouge1": 0.4418052256532066,
      "rouge2": 0.10978520286396182,
      "rougeL": 0.171021377672209,
      "bertscore_f1": 0.8052197694778442
    },
    "led": {
      "summary": "SpatialBench: A large-scale benchmark for assessing the hierarchical spatial cognition of MLLMs using first-person videos The dataset comprises 15 question types, each carefully mapped to one of the five cognitive levels introduced above. experiments show that although current Mllm's perform well on perceptual and relational reasoning tasks, their competence declines sharply in high-level tasks. Results suggest that humans rely on goal-oriented reasoning and MLLM's have diffuse attention to scene details, lacking a unified spatial cognition. SpatialBench introduces the first comprehensive and cognitively grounded framework for assessing spatial intelligence in Mollies. Its contribution are summarized as follows: We introduce Spatial Bench, a comprehensive benchmark built upon a five-level hierarchical spatially cognition framework that progressively evaluates Mllms from low-level observation to high- level planning. While Mlm's can recognize objects and describe scenes accurately, they often lack selective attention and directional understanding, leading them to infer by association rather than by reasoning about movement and geometry. We establish the first systematic and nitively grounded paradigm for assessing spatial intelligence in mllm, offering a unified foundation for interpreting and benchmarking spatial cognition across models of varying architectures and scales.4 SpuralBench: SpacingBench provides a large scale test for assessing hierarchies of spatial reasoning tasks designed to measure the qualitative and quantitative skills in MLLMS. Results show that while modern MLL Ms. demonstrate strong perception and semantic reasoning, their abilities in symbolic abstraction, causal inference, and spatial planning remain limited.5 SpatialMap Theory: Drawing inspiration from cognitive map theory, our framework hierarchically describes the spatial cognition progression from perception to decision-making, enabling a more interpretable and fine-grained diagnosis of multimodal spatial intelligence.6 Conclusion In this work, we introduce SpatelliteBench, a complete neuroscene designed to assess the spatial intelligence of MellMs. It consists of 15 question/answer pairs sourced from 50 videos captured from 50 Videos embodying the most demanding demands of spatial intelligenceThe task is complemented by 15 categories of spatial thinking tasks, each aligned with a distinct stage of spatial cognition hierarchy. Each video is paired with carefully designed questions and annotations aligned with these cognitive dimensions, enabling systematic, fine-Grained, and ctnible assessment for MllMs.Our experiments showthat although current MnLMs perform very well on preliminary tasks such as object-sizing and topology and relation, they fall short on more advanced tasksSuch as long-term planning and statistical analysis. Our overall conclusion is that despite the advances in sensory data collection and computation, MLLMI's still suffer from poor performance in these tasks.",
      "similarity_to_abstract": 0.8711471557617188,
      "rouge1": 0.4360189573459715,
      "rouge2": 0.12361331220285264,
      "rougeL": 0.16429699842022116,
      "bertscore_f1": 0.8224539160728455
    }
  }
]