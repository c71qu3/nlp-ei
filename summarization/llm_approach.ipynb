{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ddcc1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "import sys, os, glob, json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from pathlib import Path\n",
    "from utils.metrics import evaluate_summary,calculate_bert_score,calculate_rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "290a6080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read ../data/raw/abstracts/2510.25732v1.txt\n",
      "Read ../data/raw/abstracts/2510.25612v1.txt\n",
      "Read ../data/raw/abstracts/2510.25729v1.txt\n",
      "Read ../data/raw/abstracts/2510.25634v1.txt\n",
      "Read ../data/raw/abstracts/2510.25065v1.txt\n",
      "Read ../data/raw/abstracts/2510.25668v1.txt\n",
      "Read ../data/raw/abstracts/2510.25205v1.txt\n",
      "Read ../data/raw/abstracts/2510.25590v1.txt\n",
      "Read ../data/raw/abstracts/2510.25223v1.txt\n",
      "Read ../data/raw/abstracts/2510.25517v1.txt\n",
      "Read ../data/raw/abstracts/2510.25616v1.txt\n",
      "Read ../data/raw/abstracts/2510.25005v1.txt\n",
      "Read ../data/raw/abstracts/2510.25731v1.txt\n",
      "Read ../data/raw/abstracts/2510.25595v1.txt\n",
      "Read ../data/raw/abstracts/2510.25101v1.txt\n",
      "Read ../data/raw/abstracts/2510.25694v1.txt\n",
      "Read ../data/raw/abstracts/2510.25510v1.txt\n",
      "Read ../data/raw/abstracts/2510.25179v1.txt\n",
      "Read ../data/raw/abstracts/2510.25320v1.txt\n",
      "Read ../data/raw/abstracts/2510.25557v1.txt\n",
      "Read ../data/raw/abstracts/2510.25007v1.txt\n",
      "Read ../data/raw/abstracts/2510.25577v1.txt\n",
      "Read ../data/raw/abstracts/2510.25770v1.txt\n",
      "Read ../data/raw/abstracts/2510.25588v1.txt\n",
      "Read ../data/raw/abstracts/2510.25758v1.txt\n",
      "Read ../data/raw/abstracts/2510.25091v1.txt\n",
      "Read ../data/raw/abstracts/2510.25518v1.txt\n",
      "Read ../data/raw/abstracts/2510.25014v1.txt\n",
      "Read ../data/raw/abstracts/2510.25626v1.txt\n",
      "Read ../data/raw/abstracts/2510.25602v1.txt\n",
      "Read ../data/raw/abstracts/2510.25683v1.txt\n",
      "Read ../data/raw/abstracts/2510.24832v1.txt\n",
      "Read ../data/raw/abstracts/2510.25724v1.txt\n",
      "Read ../data/raw/abstracts/2510.25662v1.txt\n",
      "Read ../data/raw/abstracts/2510.25563v1.txt\n",
      "Read ../data/raw/abstracts/2510.25679v1.txt\n",
      "Read ../data/raw/abstracts/2510.25388v1.txt\n",
      "Read ../data/raw/abstracts/2510.25445v1.txt\n",
      "Read ../data/raw/abstracts/2510.25504v1.txt\n",
      "Read ../data/raw/abstracts/2510.25232v1.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2510.25320v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Recent advances in large language model (LLM)-based autonomous agents have demonstrated remarkable capabilities in complex problem-solving tasks [ 1 , 2 , 3 , 4 , 5 , 6 ] , ranging from scientific research and code generation to interactive web navigation and data analysis. A key enabler of these capabilities is tool-augmented reasoning, where agents leverage external tools such as search engines, calculators, code interpreters, and APIs to extend their problem-solving capacity beyond the inherent limitations of parametric knowledge.\\nCurrent approaches to tool-augmented reasoning can be broadly categorized into two paradigms: multi-agent systems (MAS) and tool-integrated reasoning (TIR) models. Multi-agent frameworks orchestrate multiple specialized agents with distinct roles and tool sets to collaboratively solve complex tasks. These systems have shown impressive performance on benchmarks requiring sophisticated workflows, such as software development and scientific research. However, they suffer from critical limitations: (1) high computational overhead due to redundant inter-agent communication and complex orchestration mechanisms; (2) inability to learn from data, as the underlying LLMs are not specifically trained for multi-agent coordination; and (3) reliance on prompt engineering rather than native model capabilities to achieve multi-turn, multi-tool workflows.\\nIn contrast, Tool-Integrated Reasoning (TIR) models represent an emerging paradigm that explicitly trains LLMs to incorporate tool usage into their reasoning process. Recent work such as Search-R1 [ 7 ] and WebThinker [ 5 ] has demonstrated that end-to-end training of models to invoke tools (e.g., <search> functions) at appropriate reasoning steps significantly outperforms prompt-engineered approaches. The TIR framework naturally aligns with the ReAct paradigm [ 4 ] , enabling models to follow a ‚Äúthink-act-observe‚Äù pipeline in an end-to-end manner. However, existing TIR methods are fundamentally limited to sequential reasoning trajectories. They execute one action at a time and thus fail to exploit opportunities for parallel tool execution when sub-tasks are independent.\\nTo address these limitations, we introduce Graph-based Agent Planning Paradigm (GAP), a novel training paradigm that enables LLM-based agents to perform dependency-aware planning through explicit graph-based reasoning. Our key insight is that by training models to construct and reason over task dependency graphs, they acquire the capability to autonomously determine optimal execution strategies, thereby executing independent tools in parallel when possible and sequential ones when necessary. This approach combines the efficiency and learnability of TIR models with the expressive power of multi-agent coordination, without the overhead of actual multi-agent orchestration.\\nOur main contributions are:\\nWe introduce GAP, a novel training paradigm for agent foundation models that incorporates dependency-aware task planning, enabling dynamic parallel and serial tool execution. To our knowledge, this is the first work to explicitly train LLMs for graph-based reasoning over task dependencies in tool-augmented settings.\\nWe design and curate a high-quality dataset of 7,000 graph-based planning traces from the Multi-Hop Question Answering (MHQA) benchmark, using GPT-4o to synthesize dependency-aware reasoning trajectories. We apply a rigorous filter mechanism, ensuring that training data emphasize dependency modeling.\\nWe demonstrate through extensive experiments across seven question-answering benchmarks that GAP achieves a 0.9% average performance improvement on multi-hop reasoning tasks over state-of-the-art baselines. Moreover, our method significantly enhances efficiency by reducing interaction turns by up to 33.4%, while decreasing response length by 24.9% and maintaining robust generalization to out-of-domain datasets.\\nOur work establishes graph-based dependency modeling as a critical direction for developing more efficient autonomous agents, bridging the gap between sequential TIR models and complex multi-agent coordination. Through extensive experiments on MHQA, we demonstrate that GAP achieves significant improvements over traditional ReAct baselines in both accuracy and efficiency.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Background',\n",
       "    'paragraphs': 'Complex task reasoning often requires structured decomposition, specialized capabilities, and external tool integration. We review two prominent paradigms that used in single agent:\\nThe ReAct-style approach, exemplified by ReAct [ 4 ] , leveraged few-shot exemplars to guide an LLM to interleave reasoning traces and actions within a \"Thought-Action-Observation\" cycle. This framework augments LLMs with structured reasoning by interleaving thought steps œÑ t ‚àà ùíØ \\\\tau_{t}\\\\in\\\\mathcal{T} for planning, action steps a t ‚àà ùíú a_{t}\\\\in\\\\mathcal{A} for tool use, and observation steps o t ‚àà ùí™ o_{t}\\\\in\\\\mathcal{O} for outcome processing. The reasoning trajectory follows:\\nwhere each thought œÑ t \\\\tau_{t} conditions on the history h t = [ œÑ 1 : t ‚àí 1 , a 1 : t ‚àí 1 , o 1 : t ‚àí 1 ] h_{t}=[\\\\tau_{1:t-1},a_{1:t-1},o_{1:t-1}] to determine next action.\\nTool-Integrated Reasoning (TIR) enhances LLMs‚Äô code reasoning capabilities by tightly coupling natural language reasoning with external tool execution environments [ 8 , 9 , 10 ] . This approach enables a single agent to leverage external tools ùíØ = { t 1 , t 2 , ‚Ä¶ , t M } \\\\mathcal{T}=\\\\{t_{1},t_{2},...,t_{M}\\\\} by maintaining a global state S t S_{t} and selecting tools via policy œÄ \\u200b ( t k ‚à£ S t ) \\\\pi(t_{k}\\\\mid S_{t}) . After executing tool t k t_{k} , the agent observes outcome o t ‚àº ùí™ \\u200b ( t k , S t ) o_{t}\\\\sim\\\\mathcal{O}(t_{k},S_{t}) and updates its state:\\nwhere S t S_{t} denotes the reasoning state, t k t_{k} represents the selected tool, and o t o_{t} captures tool execution outcomes.',\n",
       "    'subsections': [{'title': 'ReAct-style Tool-Using',\n",
       "      'paragraphs': 'The ReAct-style approach, exemplified by ReAct [ 4 ] , leveraged few-shot exemplars to guide an LLM to interleave reasoning traces and actions within a \"Thought-Action-Observation\" cycle. This framework augments LLMs with structured reasoning by interleaving thought steps œÑ t ‚àà ùíØ \\\\tau_{t}\\\\in\\\\mathcal{T} for planning, action steps a t ‚àà ùíú a_{t}\\\\in\\\\mathcal{A} for tool use, and observation steps o t ‚àà ùí™ o_{t}\\\\in\\\\mathcal{O} for outcome processing. The reasoning trajectory follows:\\nwhere each thought œÑ t \\\\tau_{t} conditions on the history h t = [ œÑ 1 : t ‚àí 1 , a 1 : t ‚àí 1 , o 1 : t ‚àí 1 ] h_{t}=[\\\\tau_{1:t-1},a_{1:t-1},o_{1:t-1}] to determine next action.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Tool-Integrated Reasoning',\n",
       "      'paragraphs': 'Tool-Integrated Reasoning (TIR) enhances LLMs‚Äô code reasoning capabilities by tightly coupling natural language reasoning with external tool execution environments [ 8 , 9 , 10 ] . This approach enables a single agent to leverage external tools ùíØ = { t 1 , t 2 , ‚Ä¶ , t M } \\\\mathcal{T}=\\\\{t_{1},t_{2},...,t_{M}\\\\} by maintaining a global state S t S_{t} and selecting tools via policy œÄ \\u200b ( t k ‚à£ S t ) \\\\pi(t_{k}\\\\mid S_{t}) . After executing tool t k t_{k} , the agent observes outcome o t ‚àº ùí™ \\u200b ( t k , S t ) o_{t}\\\\sim\\\\mathcal{O}(t_{k},S_{t}) and updates its state:\\nwhere S t S_{t} denotes the reasoning state, t k t_{k} represents the selected tool, and o t o_{t} captures tool execution outcomes.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Graph-based Agent Planning Paradigm',\n",
       "    'paragraphs': 'In this section, we introduce the Graph-based Agent Planning (GAP) paradigm, a novel framework that enables LLM-based agents to perform dependency-aware reasoning and adaptive tool execution. We first formalize the problem setting (¬ß 3.1 ), then describe the core components of GAP including graph-based task decomposition (¬ß 3.2 ) and the dependency-aware execution strategies (¬ß 3.3 ). Figure Àú 1 presents the complete GAP reasoning workflow, integrating task decomposition, graph construction, and adaptive execution.\\nWe consider a task-solving scenario where an agent must answer a complex query q q by leveraging a set of external tools ùíØ = { t 1 , t 2 , ‚Ä¶ , t n } \\\\mathcal{T}=\\\\{t_{1},t_{2},\\\\ldots,t_{n}\\\\} . Each tool t i t_{i} represents a specific capability, such as information retrieval ( search ), numerical computation ( calculator ), or code execution ( python ).\\nGiven a complex query q q , the agent must decompose it into a sequence of sub-tasks S = { s 1 , s 2 , ‚Ä¶ , s m } S=\\\\{s_{1},s_{2},\\\\ldots,s_{m}\\\\} , where each sub-task s i s_{i} requires invoking one or more tools from ùíØ \\\\mathcal{T} . The goal is to determine both which tools to invoke and when to invoke them.\\nWe model task dependencies as a directed acyclic graph (DAG): G = ( V , E ) G=(V,E) , where each vertex v i ‚àà V v_{i}\\\\in V represents a sub-task s i s_{i} and each directed edge ( v i , v j ) ‚àà E (v_{i},v_{j})\\\\in E indicates that sub-task s j s_{j} depends on the output of sub-task s i s_{i} .\\nThe absence of an edge between two vertices indicates independence, meaning those sub-tasks can be executed in parallel. The agent‚Äôs objective is to construct this dependency graph and execute tools accordingly to maximize both efficiency and correctness.\\nUnlike traditional sequential reasoning approaches (e.g., ReAct) that generate one action at a time, GAP explicitly constructs a task dependency graph during the planning phase. This process consists of three steps:\\nThe model first analyzes the input query q q and identifies the atomic sub-tasks required to solve it. For example, given the query ‚ÄúWhat are the populations of the capitals of France and Germany?‚Äù, the model identifies four sub-tasks: s 1 s_{1} retrieves the capital of France, s 2 s_{2} retrieves the capital of Germany, s 3 s_{3} retrieves the population of s 1 s_{1} ‚Äôs result, and s 4 s_{4} retrieves the population of s 2 s_{2} ‚Äôs result.\\nThe model then reasons about dependencies between sub-tasks by analyzing their input-output relationships. A sub-task s j s_{j} depends on s i s_{i} if and only if s j s_{j} requires the output of s i s_{i} as input. In the example above, s 3 s_{3} depends on s 1 s_{1} as it needs to know Paris before querying its population, and similarly s 4 s_{4} depends on s 2 s_{2} as it needs to know Berlin. However, s 1 s_{1} and s 2 s_{2} are independent and can be executed in parallel, as are s 3 s_{3} and s 4 s_{4} given their respective dependencies are satisfied.\\nBased on the dependency analysis, the model constructs the dependency graph G G . We represent this graph using an adjacency structure that explicitly encodes:\\nThe model outputs this graph structure in a structured format that enables downstream execution planning. We train the model to generate this representation using a special token sequence:\\nGiven the constructed dependency graph G G , GAP determines an optimal execution strategy that balances parallelization opportunities with dependency constraints. We formalize this as a scheduling problem.\\nWe partition the graph G G into execution levels L 0 , L 1 , ‚Ä¶ , L k L_{0},L_{1},\\\\ldots,L_{k} using topological sorting, where:\\nLevel L 0 L_{0} contains all nodes with no incoming edges (independent initial tasks)\\nLevel L i L_{i} (for i > 0 i>0 ) contains nodes whose dependencies are all in levels L 0 , ‚Ä¶ , L i ‚àí 1 L_{0},\\\\ldots,L_{i-1}\\nAll sub-tasks within the same level L i L_{i} can be executed in parallel, as they have no dependencies on each other.\\nFor sub-tasks in the same execution level, the model generates a parallel tool call batch:\\nwhere t j t_{j} is the tool selected for sub-task s j s_{j} and args j \\\\text{args}_{j} are its arguments. All tools in Batch i \\\\text{Batch}_{i} are invoked simultaneously, and the model waits for all results before proceeding to the next level. In Algorithm Àú 1 , we demonstrate the reasoning process of our proposed method.',\n",
       "    'subsections': [{'title': '3.1 Problem Formulation',\n",
       "      'paragraphs': 'We consider a task-solving scenario where an agent must answer a complex query q q by leveraging a set of external tools ùíØ = { t 1 , t 2 , ‚Ä¶ , t n } \\\\mathcal{T}=\\\\{t_{1},t_{2},\\\\ldots,t_{n}\\\\} . Each tool t i t_{i} represents a specific capability, such as information retrieval ( search ), numerical computation ( calculator ), or code execution ( python ).\\nGiven a complex query q q , the agent must decompose it into a sequence of sub-tasks S = { s 1 , s 2 , ‚Ä¶ , s m } S=\\\\{s_{1},s_{2},\\\\ldots,s_{m}\\\\} , where each sub-task s i s_{i} requires invoking one or more tools from ùíØ \\\\mathcal{T} . The goal is to determine both which tools to invoke and when to invoke them.\\nWe model task dependencies as a directed acyclic graph (DAG): G = ( V , E ) G=(V,E) , where each vertex v i ‚àà V v_{i}\\\\in V represents a sub-task s i s_{i} and each directed edge ( v i , v j ) ‚àà E (v_{i},v_{j})\\\\in E indicates that sub-task s j s_{j} depends on the output of sub-task s i s_{i} .\\nThe absence of an edge between two vertices indicates independence, meaning those sub-tasks can be executed in parallel. The agent‚Äôs objective is to construct this dependency graph and execute tools accordingly to maximize both efficiency and correctness.',\n",
       "      'subsections': [{'title': 'Task Decomposition.',\n",
       "        'paragraphs': 'Given a complex query q q , the agent must decompose it into a sequence of sub-tasks S = { s 1 , s 2 , ‚Ä¶ , s m } S=\\\\{s_{1},s_{2},\\\\ldots,s_{m}\\\\} , where each sub-task s i s_{i} requires invoking one or more tools from ùíØ \\\\mathcal{T} . The goal is to determine both which tools to invoke and when to invoke them.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Dependency Graph.',\n",
       "        'paragraphs': 'We model task dependencies as a directed acyclic graph (DAG): G = ( V , E ) G=(V,E) , where each vertex v i ‚àà V v_{i}\\\\in V represents a sub-task s i s_{i} and each directed edge ( v i , v j ) ‚àà E (v_{i},v_{j})\\\\in E indicates that sub-task s j s_{j} depends on the output of sub-task s i s_{i} .\\nThe absence of an edge between two vertices indicates independence, meaning those sub-tasks can be executed in parallel. The agent‚Äôs objective is to construct this dependency graph and execute tools accordingly to maximize both efficiency and correctness.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.2 Graph-based Task Decomposition',\n",
       "      'paragraphs': 'Unlike traditional sequential reasoning approaches (e.g., ReAct) that generate one action at a time, GAP explicitly constructs a task dependency graph during the planning phase. This process consists of three steps:\\nThe model first analyzes the input query q q and identifies the atomic sub-tasks required to solve it. For example, given the query ‚ÄúWhat are the populations of the capitals of France and Germany?‚Äù, the model identifies four sub-tasks: s 1 s_{1} retrieves the capital of France, s 2 s_{2} retrieves the capital of Germany, s 3 s_{3} retrieves the population of s 1 s_{1} ‚Äôs result, and s 4 s_{4} retrieves the population of s 2 s_{2} ‚Äôs result.\\nThe model then reasons about dependencies between sub-tasks by analyzing their input-output relationships. A sub-task s j s_{j} depends on s i s_{i} if and only if s j s_{j} requires the output of s i s_{i} as input. In the example above, s 3 s_{3} depends on s 1 s_{1} as it needs to know Paris before querying its population, and similarly s 4 s_{4} depends on s 2 s_{2} as it needs to know Berlin. However, s 1 s_{1} and s 2 s_{2} are independent and can be executed in parallel, as are s 3 s_{3} and s 4 s_{4} given their respective dependencies are satisfied.\\nBased on the dependency analysis, the model constructs the dependency graph G G . We represent this graph using an adjacency structure that explicitly encodes:\\nThe model outputs this graph structure in a structured format that enables downstream execution planning. We train the model to generate this representation using a special token sequence:',\n",
       "      'subsections': [{'title': 'Sub-task Identification.',\n",
       "        'paragraphs': 'The model first analyzes the input query q q and identifies the atomic sub-tasks required to solve it. For example, given the query ‚ÄúWhat are the populations of the capitals of France and Germany?‚Äù, the model identifies four sub-tasks: s 1 s_{1} retrieves the capital of France, s 2 s_{2} retrieves the capital of Germany, s 3 s_{3} retrieves the population of s 1 s_{1} ‚Äôs result, and s 4 s_{4} retrieves the population of s 2 s_{2} ‚Äôs result.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Dependency Analysis.',\n",
       "        'paragraphs': 'The model then reasons about dependencies between sub-tasks by analyzing their input-output relationships. A sub-task s j s_{j} depends on s i s_{i} if and only if s j s_{j} requires the output of s i s_{i} as input. In the example above, s 3 s_{3} depends on s 1 s_{1} as it needs to know Paris before querying its population, and similarly s 4 s_{4} depends on s 2 s_{2} as it needs to know Berlin. However, s 1 s_{1} and s 2 s_{2} are independent and can be executed in parallel, as are s 3 s_{3} and s 4 s_{4} given their respective dependencies are satisfied.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Graph Construction.',\n",
       "        'paragraphs': 'Based on the dependency analysis, the model constructs the dependency graph G G . We represent this graph using an adjacency structure that explicitly encodes:\\nThe model outputs this graph structure in a structured format that enables downstream execution planning. We train the model to generate this representation using a special token sequence:',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.3 Dependency-Aware Execution Strategies',\n",
       "      'paragraphs': 'Given the constructed dependency graph G G , GAP determines an optimal execution strategy that balances parallelization opportunities with dependency constraints. We formalize this as a scheduling problem.\\nWe partition the graph G G into execution levels L 0 , L 1 , ‚Ä¶ , L k L_{0},L_{1},\\\\ldots,L_{k} using topological sorting, where:\\nLevel L 0 L_{0} contains all nodes with no incoming edges (independent initial tasks)\\nLevel L i L_{i} (for i > 0 i>0 ) contains nodes whose dependencies are all in levels L 0 , ‚Ä¶ , L i ‚àí 1 L_{0},\\\\ldots,L_{i-1}\\nAll sub-tasks within the same level L i L_{i} can be executed in parallel, as they have no dependencies on each other.\\nFor sub-tasks in the same execution level, the model generates a parallel tool call batch:\\nwhere t j t_{j} is the tool selected for sub-task s j s_{j} and args j \\\\text{args}_{j} are its arguments. All tools in Batch i \\\\text{Batch}_{i} are invoked simultaneously, and the model waits for all results before proceeding to the next level. In Algorithm Àú 1 , we demonstrate the reasoning process of our proposed method.',\n",
       "      'subsections': [{'title': 'Execution Levels.',\n",
       "        'paragraphs': 'We partition the graph G G into execution levels L 0 , L 1 , ‚Ä¶ , L k L_{0},L_{1},\\\\ldots,L_{k} using topological sorting, where:\\nLevel L 0 L_{0} contains all nodes with no incoming edges (independent initial tasks)\\nLevel L i L_{i} (for i > 0 i>0 ) contains nodes whose dependencies are all in levels L 0 , ‚Ä¶ , L i ‚àí 1 L_{0},\\\\ldots,L_{i-1}\\nAll sub-tasks within the same level L i L_{i} can be executed in parallel, as they have no dependencies on each other.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Parallel Execution.',\n",
       "        'paragraphs': 'For sub-tasks in the same execution level, the model generates a parallel tool call batch:\\nwhere t j t_{j} is the tool selected for sub-task s j s_{j} and args j \\\\text{args}_{j} are its arguments. All tools in Batch i \\\\text{Batch}_{i} are invoked simultaneously, and the model waits for all results before proceeding to the next level. In Algorithm Àú 1 , we demonstrate the reasoning process of our proposed method.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '4 Training Pipeline',\n",
       "    'paragraphs': 'During the Supervised Fine-Tuning (SFT) stage, we generate Graph-based Action Planning (GAP) trajectories using our proprietary multi-agent system. This approach is inspired by the multi-agent distillation framework proposed by Chain-of-Agents [ 11 ] . Starting with the Natural Questions (NQ) [ 12 ] and HotpotQA [ 13 ] datasets, we employ GPT-4o as the backend model to simulate the graph-based planning process. The prompt template refers to Appendix Àú B .\\nTo ensure the quality of the GAP training, we implemented a filtering process to select only high-quality, non-trivial trajectories from the varied data sources. We apply three key filtering criteria to curate the training data:\\n(1) Complexity threshold: We remove samples that can be completed with fewer than 3 search operations, as such trajectories are overly simplistic and do not benefit from parallel retrieval strategies.\\n(2) Task diversity: We maintain a 6:4 ratio between samples utilizing parallel retrieval and those using sequential retrieval, ensuring the model‚Äôs generalization capability across different retrieval patterns.\\n(3) Length constraint: We filter out excessively long samples, retaining only those within approximately 2000 tokens. Overlong samples typically indicate missing relevant content in the offline dataset rather than genuine retrieval difficulty, and such redundant samples are detrimental to training efficiency, particularly given our objective of minimizing redundancy and maximizing retrieval efficiency.\\nFollowing this pipeline, approximately 7,000 high-quality training trajectories were generated through trajectory synthesis and quality filtering.\\nWe fine-tuned the Qwen2.5-3B-Instruct model on our filtered dataset. The model learns to internalize graph-based planning strategies, enabling it to solve tasks by leveraging graph representations. The training objective minimizes:\\nwith observation masking ( ùí™ \\\\mathcal{O} ) to prevent environmental noise propagation. This establishes robust cold start for downstream RL.\\nWhile supervised training establishes a baseline understanding of parallel execution, it merely guides the model to imitate the provided demonstrations, and does not optimize computational efficiency or reasoning effectiveness. We further fine-tune the language model with fully end-to-end reinforcement learning. During RL-based finetuning, we iteratively sample reasoning traces from our current policy, assign them a reward according to the correctness of the proposed solution, and optimize policy parameters with DAPO [ 14 ] . In this stage, the model learns to strategically determine when, how, and how broadly to invoke child threads, maximizing performance by balancing the trade-offs between parallel exploration and the context window constraint. We use the VeRL framework [ 15 ] for DAPO training.\\nReward signals are critical for shaping RL dynamics in open-ended web agent tasks. Our framework adopts a graph-based design, built on two key considerations: Format consistency is inherently ensured through high-quality supervised fine-tuning and effective cold-start, obviating the need for explicit format validation rewards. For evaluating answer correctness, we use rule-based metrics to provide binary assessments. Our reward function is:\\nwhere s \\u200b c \\u200b o \\u200b r \\u200b e answer ‚àà { 0 , 1 } score_{\\\\text{answer}}\\\\in\\\\{0,1\\\\} is 1 if the final prediction is correct. Future work could productively explore multi-objective reward formulations that incorporate auxiliary signals.',\n",
       "    'subsections': [{'title': '4.1 Data Synthesis',\n",
       "      'paragraphs': 'During the Supervised Fine-Tuning (SFT) stage, we generate Graph-based Action Planning (GAP) trajectories using our proprietary multi-agent system. This approach is inspired by the multi-agent distillation framework proposed by Chain-of-Agents [ 11 ] . Starting with the Natural Questions (NQ) [ 12 ] and HotpotQA [ 13 ] datasets, we employ GPT-4o as the backend model to simulate the graph-based planning process. The prompt template refers to Appendix Àú B .\\nTo ensure the quality of the GAP training, we implemented a filtering process to select only high-quality, non-trivial trajectories from the varied data sources. We apply three key filtering criteria to curate the training data:\\n(1) Complexity threshold: We remove samples that can be completed with fewer than 3 search operations, as such trajectories are overly simplistic and do not benefit from parallel retrieval strategies.\\n(2) Task diversity: We maintain a 6:4 ratio between samples utilizing parallel retrieval and those using sequential retrieval, ensuring the model‚Äôs generalization capability across different retrieval patterns.\\n(3) Length constraint: We filter out excessively long samples, retaining only those within approximately 2000 tokens. Overlong samples typically indicate missing relevant content in the offline dataset rather than genuine retrieval difficulty, and such redundant samples are detrimental to training efficiency, particularly given our objective of minimizing redundancy and maximizing retrieval efficiency.\\nFollowing this pipeline, approximately 7,000 high-quality training trajectories were generated through trajectory synthesis and quality filtering.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Supervised Fine-tuning for Cold Start',\n",
       "      'paragraphs': 'We fine-tuned the Qwen2.5-3B-Instruct model on our filtered dataset. The model learns to internalize graph-based planning strategies, enabling it to solve tasks by leveraging graph representations. The training objective minimizes:\\nwith observation masking ( ùí™ \\\\mathcal{O} ) to prevent environmental noise propagation. This establishes robust cold start for downstream RL.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 End-to-End Agentic Reinforcement Learning',\n",
       "      'paragraphs': 'While supervised training establishes a baseline understanding of parallel execution, it merely guides the model to imitate the provided demonstrations, and does not optimize computational efficiency or reasoning effectiveness. We further fine-tune the language model with fully end-to-end reinforcement learning. During RL-based finetuning, we iteratively sample reasoning traces from our current policy, assign them a reward according to the correctness of the proposed solution, and optimize policy parameters with DAPO [ 14 ] . In this stage, the model learns to strategically determine when, how, and how broadly to invoke child threads, maximizing performance by balancing the trade-offs between parallel exploration and the context window constraint. We use the VeRL framework [ 15 ] for DAPO training.\\nReward signals are critical for shaping RL dynamics in open-ended web agent tasks. Our framework adopts a graph-based design, built on two key considerations: Format consistency is inherently ensured through high-quality supervised fine-tuning and effective cold-start, obviating the need for explicit format validation rewards. For evaluating answer correctness, we use rule-based metrics to provide binary assessments. Our reward function is:\\nwhere s \\u200b c \\u200b o \\u200b r \\u200b e answer ‚àà { 0 , 1 } score_{\\\\text{answer}}\\\\in\\\\{0,1\\\\} is 1 if the final prediction is correct. Future work could productively explore multi-objective reward formulations that incorporate auxiliary signals.',\n",
       "      'subsections': [{'title': 'Reward function',\n",
       "        'paragraphs': 'Reward signals are critical for shaping RL dynamics in open-ended web agent tasks. Our framework adopts a graph-based design, built on two key considerations: Format consistency is inherently ensured through high-quality supervised fine-tuning and effective cold-start, obviating the need for explicit format validation rewards. For evaluating answer correctness, we use rule-based metrics to provide binary assessments. Our reward function is:\\nwhere s \\u200b c \\u200b o \\u200b r \\u200b e answer ‚àà { 0 , 1 } score_{\\\\text{answer}}\\\\in\\\\{0,1\\\\} is 1 if the final prediction is correct. Future work could productively explore multi-objective reward formulations that incorporate auxiliary signals.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '5 Experiments',\n",
       "    'paragraphs': 'We select seven benchmark datasets that encompass a diverse range of search with reasoning challenges by following the setup of [ 7 ] . These datasets are categorized as follows: (1) General Question Answering: NQ [ 12 ] , TriviaQA [ 16 ] , and PopQA [ 17 ] . (2) Multi-Hop Question Answering: HotpotQA [ 13 ] , 2WikiMultiHopQA [ 18 ] , Musique [ 19 ] , and Bamboogle [ 20 ] . Following [ 7 ] , we merge the training sets of NQ and HotpotQA as the training data and conduct evaluations on the validation or test sets.\\nWe use Exact Match (EM) as the evaluation metric to assess both in-domain and out-of-domain performance. In Figure Àú 2 , we follow [ 21 ] and adopt the cost-of-pass metric to quantify model efficiency. The cost-of-pass metric, denoted as v \\u200b ( m , p ) v(m,p) , represents the expected monetary cost of using a model m m to generate a correct solution for a problem p p . It is computed as the ratio of the cost of a single inference attempt, C m \\u200b ( p ) C_{m}(p) , to the success rate, R m \\u200b ( p ) R_{m}(p) :\\nHere, the cost of a single inference attempt, C m \\u200b ( p ) C_{m}(p) , is defined as:\\nwhere n in \\u200b ( m , p ) n_{\\\\text{in}}(m,p) and n out \\u200b ( m , p ) n_{\\\\text{out}}(m,p) are the number of input and output tokens for model m m on problem p p , respectively. The success rate R m \\u200b ( p ) R_{m}(p) is estimated by the proportion of correct responses. This metric represents the expected cost of using a model to generate a correct solution for a problem.\\nWe conduct comprehensive comparisons against state-of-the-art methods to evaluate our approach across MHQA datasets. We systematically evaluate a suite of tool-augmented methods, including Search-R1 [ 7 ] , ZeroSearch [ 22 ] , StepSearch [ 23 ] and Chain of Agents [ 11 ] .\\nWe conduct experiments using Qwen2.5-3B models (Yang et al., 2024) as the backbone of the agent, E5 [ 24 ] as the embedding model, and 2018 Wikipedia dump [ 25 ] as the corpus. All experiments are conducted on 8 NVIDIA A100 GPUs.\\nMethods Single-Hop QA Multi-Hop QA NQ ‚Ä† TriviaQA * PopQA * HotpotQA ‚Ä† 2wiki * Musique * Bamboogle * Qwen2.5-3B-Instruct 10.5 13.2 18.8 9.9 20.2 4.7 1.2 Search-R1 38.3 59.3 43.5 37.6 31.7 15.1 37.1 ZeroSearch 43.3 61.6 41.4 27.4 30.0 9.8 11.1 StepSearch - - - 34.5 32.0 17.4 34.4 AFM-RL-3B 39.3 58.2 42.4 41.1 39.8 19.0 43.2 GAP-3B (Ours) 39.6 59.1 40.1 42.5 41.7 18.7 43.8\\nTable Àú 1 presents comprehensive results comparing GAP against baseline methods across seven benchmarks using four model configurations. Beyond accuracy improvements, GAP demonstrates significant efficiency gains on multi-hop reasoning tasks through parallel decomposition of independent sub-queries. As shown in Table Àú 2 and Figure Àú 3 , our method achieves superior performance across multiple efficiency metrics compared to sequential baselines. Figure Àú 2 further illustrates this advantage through a performance-cost trade-off analysis on HotpotQA. Our analysis reveals several key findings:\\nOur method demonstrates particular strength on multi-hop benchmarks, outperforming the best baseline by 0.9% on average across four multi-hop datasets (HotpotQA, 2Wiki, Musique, Bamboogle). This indicates that GAP successfully learns strategies for decomposing and parallelizing complex queries. On single-hop questions, GAP achieves comparable performance to ZeroSearch, which trains an LLM to simulate search engines and generate pseudo-context. Compared to Search-R1, our method shows a substantial 3.95% improvement.\\nCompared to Search-R1, which retrieves information via sequential query generation, GAP significantly reduces the number of LLM interaction turns. On HotpotQA, GAP requires only 1.78 turns compared to Search-R1‚Äôs 2.27 turns (21.6% reduction), while on 2Wiki, the reduction is even more pronounced (2.03 vs. 3.05 turns, 33.4% reduction). The cumulative distribution functions in Figure 3 further illustrate this advantage: our method efficiently responds to questions within 2 turns in most cases, whereas Search-R1 typically requires 3-6 turns. This reduction in interaction turns directly translates to faster execution times, with GAP achieving 32.3% and 21.4% time cost reductions on HotpotQA (168 vs. 248s) and 2Wiki (206s vs. 262s), respectively. Notably, the model autonomously determines parallelizability based on learned patterns during inference, demonstrating strong generalization ability.\\nGAP also significantly reduces response length compared to baselines. As shown in Figure Àú 3 , Search-R1 generates substantially more tokens to support reasoning over retrieved documents, while GAP learns efficient reasoning strategies that reduce response length by 24.9% on HotpotQA (416 vs. 554 tokens) and 20.3% on 2Wiki (452 vs. 567 tokens). This reduction in generated tokens directly decreases deployment costs and increases throughput, which are critical factors for real-world applications. Furthermore, these efficiency gains generalize across domains: while HotpotQA is an in-domain dataset, similar improvements are observed on out-of-domain benchmarks, demonstrating that the learned parallel decomposition patterns transfer effectively to new scenarios. These results validate that GAP not only improves accuracy but also makes multi-hop reasoning more practical and cost-effective for deployment.\\nHotpotQA ‚Ä† Acc‚Üë Length‚Üì Time Cost(s)‚Üì # Turns‚Üì Qwen2.5-3B-Instruct 9.9 256 114 1.11 Search-R1 25.3 584 221 2.69 AFM-RL-3B 35.7 554 248 2.27 GAP-3B (Ours) 36.7 416 168 1.78 2wiki * Acc‚Üë Length‚Üì Time Cost(s)‚Üì # Turns‚Üì Qwen2.5-3B-Instruct 10.5 277 121 1.12 Search-R1 31.7 651 254 3.05 AFM-RL-3B 39.8 567 262 2.64 GAP-3B (Ours) 41.7 452 206 2.03',\n",
       "    'subsections': [{'title': '5.1 Setup',\n",
       "      'paragraphs': 'We select seven benchmark datasets that encompass a diverse range of search with reasoning challenges by following the setup of [ 7 ] . These datasets are categorized as follows: (1) General Question Answering: NQ [ 12 ] , TriviaQA [ 16 ] , and PopQA [ 17 ] . (2) Multi-Hop Question Answering: HotpotQA [ 13 ] , 2WikiMultiHopQA [ 18 ] , Musique [ 19 ] , and Bamboogle [ 20 ] . Following [ 7 ] , we merge the training sets of NQ and HotpotQA as the training data and conduct evaluations on the validation or test sets.\\nWe use Exact Match (EM) as the evaluation metric to assess both in-domain and out-of-domain performance. In Figure Àú 2 , we follow [ 21 ] and adopt the cost-of-pass metric to quantify model efficiency. The cost-of-pass metric, denoted as v \\u200b ( m , p ) v(m,p) , represents the expected monetary cost of using a model m m to generate a correct solution for a problem p p . It is computed as the ratio of the cost of a single inference attempt, C m \\u200b ( p ) C_{m}(p) , to the success rate, R m \\u200b ( p ) R_{m}(p) :\\nHere, the cost of a single inference attempt, C m \\u200b ( p ) C_{m}(p) , is defined as:\\nwhere n in \\u200b ( m , p ) n_{\\\\text{in}}(m,p) and n out \\u200b ( m , p ) n_{\\\\text{out}}(m,p) are the number of input and output tokens for model m m on problem p p , respectively. The success rate R m \\u200b ( p ) R_{m}(p) is estimated by the proportion of correct responses. This metric represents the expected cost of using a model to generate a correct solution for a problem.\\nWe conduct comprehensive comparisons against state-of-the-art methods to evaluate our approach across MHQA datasets. We systematically evaluate a suite of tool-augmented methods, including Search-R1 [ 7 ] , ZeroSearch [ 22 ] , StepSearch [ 23 ] and Chain of Agents [ 11 ] .\\nWe conduct experiments using Qwen2.5-3B models (Yang et al., 2024) as the backbone of the agent, E5 [ 24 ] as the embedding model, and 2018 Wikipedia dump [ 25 ] as the corpus. All experiments are conducted on 8 NVIDIA A100 GPUs.\\nMethods Single-Hop QA Multi-Hop QA NQ ‚Ä† TriviaQA * PopQA * HotpotQA ‚Ä† 2wiki * Musique * Bamboogle * Qwen2.5-3B-Instruct 10.5 13.2 18.8 9.9 20.2 4.7 1.2 Search-R1 38.3 59.3 43.5 37.6 31.7 15.1 37.1 ZeroSearch 43.3 61.6 41.4 27.4 30.0 9.8 11.1 StepSearch - - - 34.5 32.0 17.4 34.4 AFM-RL-3B 39.3 58.2 42.4 41.1 39.8 19.0 43.2 GAP-3B (Ours) 39.6 59.1 40.1 42.5 41.7 18.7 43.8',\n",
       "      'subsections': [{'title': 'Datasets',\n",
       "        'paragraphs': 'We select seven benchmark datasets that encompass a diverse range of search with reasoning challenges by following the setup of [ 7 ] . These datasets are categorized as follows: (1) General Question Answering: NQ [ 12 ] , TriviaQA [ 16 ] , and PopQA [ 17 ] . (2) Multi-Hop Question Answering: HotpotQA [ 13 ] , 2WikiMultiHopQA [ 18 ] , Musique [ 19 ] , and Bamboogle [ 20 ] . Following [ 7 ] , we merge the training sets of NQ and HotpotQA as the training data and conduct evaluations on the validation or test sets.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Metrics',\n",
       "        'paragraphs': 'We use Exact Match (EM) as the evaluation metric to assess both in-domain and out-of-domain performance. In Figure Àú 2 , we follow [ 21 ] and adopt the cost-of-pass metric to quantify model efficiency. The cost-of-pass metric, denoted as v \\u200b ( m , p ) v(m,p) , represents the expected monetary cost of using a model m m to generate a correct solution for a problem p p . It is computed as the ratio of the cost of a single inference attempt, C m \\u200b ( p ) C_{m}(p) , to the success rate, R m \\u200b ( p ) R_{m}(p) :\\nHere, the cost of a single inference attempt, C m \\u200b ( p ) C_{m}(p) , is defined as:\\nwhere n in \\u200b ( m , p ) n_{\\\\text{in}}(m,p) and n out \\u200b ( m , p ) n_{\\\\text{out}}(m,p) are the number of input and output tokens for model m m on problem p p , respectively. The success rate R m \\u200b ( p ) R_{m}(p) is estimated by the proportion of correct responses. This metric represents the expected cost of using a model to generate a correct solution for a problem.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Baseline',\n",
       "        'paragraphs': 'We conduct comprehensive comparisons against state-of-the-art methods to evaluate our approach across MHQA datasets. We systematically evaluate a suite of tool-augmented methods, including Search-R1 [ 7 ] , ZeroSearch [ 22 ] , StepSearch [ 23 ] and Chain of Agents [ 11 ] .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Implementation Details',\n",
       "        'paragraphs': 'We conduct experiments using Qwen2.5-3B models (Yang et al., 2024) as the backbone of the agent, E5 [ 24 ] as the embedding model, and 2018 Wikipedia dump [ 25 ] as the corpus. All experiments are conducted on 8 NVIDIA A100 GPUs.\\nMethods Single-Hop QA Multi-Hop QA NQ ‚Ä† TriviaQA * PopQA * HotpotQA ‚Ä† 2wiki * Musique * Bamboogle * Qwen2.5-3B-Instruct 10.5 13.2 18.8 9.9 20.2 4.7 1.2 Search-R1 38.3 59.3 43.5 37.6 31.7 15.1 37.1 ZeroSearch 43.3 61.6 41.4 27.4 30.0 9.8 11.1 StepSearch - - - 34.5 32.0 17.4 34.4 AFM-RL-3B 39.3 58.2 42.4 41.1 39.8 19.0 43.2 GAP-3B (Ours) 39.6 59.1 40.1 42.5 41.7 18.7 43.8',\n",
       "        'subsections': []}]},\n",
       "     {'title': '5.2 Results and Efficiency Analysis',\n",
       "      'paragraphs': 'Table Àú 1 presents comprehensive results comparing GAP against baseline methods across seven benchmarks using four model configurations. Beyond accuracy improvements, GAP demonstrates significant efficiency gains on multi-hop reasoning tasks through parallel decomposition of independent sub-queries. As shown in Table Àú 2 and Figure Àú 3 , our method achieves superior performance across multiple efficiency metrics compared to sequential baselines. Figure Àú 2 further illustrates this advantage through a performance-cost trade-off analysis on HotpotQA. Our analysis reveals several key findings:\\nOur method demonstrates particular strength on multi-hop benchmarks, outperforming the best baseline by 0.9% on average across four multi-hop datasets (HotpotQA, 2Wiki, Musique, Bamboogle). This indicates that GAP successfully learns strategies for decomposing and parallelizing complex queries. On single-hop questions, GAP achieves comparable performance to ZeroSearch, which trains an LLM to simulate search engines and generate pseudo-context. Compared to Search-R1, our method shows a substantial 3.95% improvement.\\nCompared to Search-R1, which retrieves information via sequential query generation, GAP significantly reduces the number of LLM interaction turns. On HotpotQA, GAP requires only 1.78 turns compared to Search-R1‚Äôs 2.27 turns (21.6% reduction), while on 2Wiki, the reduction is even more pronounced (2.03 vs. 3.05 turns, 33.4% reduction). The cumulative distribution functions in Figure 3 further illustrate this advantage: our method efficiently responds to questions within 2 turns in most cases, whereas Search-R1 typically requires 3-6 turns. This reduction in interaction turns directly translates to faster execution times, with GAP achieving 32.3% and 21.4% time cost reductions on HotpotQA (168 vs. 248s) and 2Wiki (206s vs. 262s), respectively. Notably, the model autonomously determines parallelizability based on learned patterns during inference, demonstrating strong generalization ability.\\nGAP also significantly reduces response length compared to baselines. As shown in Figure Àú 3 , Search-R1 generates substantially more tokens to support reasoning over retrieved documents, while GAP learns efficient reasoning strategies that reduce response length by 24.9% on HotpotQA (416 vs. 554 tokens) and 20.3% on 2Wiki (452 vs. 567 tokens). This reduction in generated tokens directly decreases deployment costs and increases throughput, which are critical factors for real-world applications. Furthermore, these efficiency gains generalize across domains: while HotpotQA is an in-domain dataset, similar improvements are observed on out-of-domain benchmarks, demonstrating that the learned parallel decomposition patterns transfer effectively to new scenarios. These results validate that GAP not only improves accuracy but also makes multi-hop reasoning more practical and cost-effective for deployment.\\nHotpotQA ‚Ä† Acc‚Üë Length‚Üì Time Cost(s)‚Üì # Turns‚Üì Qwen2.5-3B-Instruct 9.9 256 114 1.11 Search-R1 25.3 584 221 2.69 AFM-RL-3B 35.7 554 248 2.27 GAP-3B (Ours) 36.7 416 168 1.78 2wiki * Acc‚Üë Length‚Üì Time Cost(s)‚Üì # Turns‚Üì Qwen2.5-3B-Instruct 10.5 277 121 1.12 Search-R1 31.7 651 254 3.05 AFM-RL-3B 39.8 567 262 2.64 GAP-3B (Ours) 41.7 452 206 2.03',\n",
       "      'subsections': [{'title': 'Superior performance on complex multi-hop reasoning.',\n",
       "        'paragraphs': 'Our method demonstrates particular strength on multi-hop benchmarks, outperforming the best baseline by 0.9% on average across four multi-hop datasets (HotpotQA, 2Wiki, Musique, Bamboogle). This indicates that GAP successfully learns strategies for decomposing and parallelizing complex queries. On single-hop questions, GAP achieves comparable performance to ZeroSearch, which trains an LLM to simulate search engines and generate pseudo-context. Compared to Search-R1, our method shows a substantial 3.95% improvement.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Reduced interaction turns and faster execution.',\n",
       "        'paragraphs': 'Compared to Search-R1, which retrieves information via sequential query generation, GAP significantly reduces the number of LLM interaction turns. On HotpotQA, GAP requires only 1.78 turns compared to Search-R1‚Äôs 2.27 turns (21.6% reduction), while on 2Wiki, the reduction is even more pronounced (2.03 vs. 3.05 turns, 33.4% reduction). The cumulative distribution functions in Figure 3 further illustrate this advantage: our method efficiently responds to questions within 2 turns in most cases, whereas Search-R1 typically requires 3-6 turns. This reduction in interaction turns directly translates to faster execution times, with GAP achieving 32.3% and 21.4% time cost reductions on HotpotQA (168 vs. 248s) and 2Wiki (206s vs. 262s), respectively. Notably, the model autonomously determines parallelizability based on learned patterns during inference, demonstrating strong generalization ability.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Shorter response length and lower deployment cost.',\n",
       "        'paragraphs': 'GAP also significantly reduces response length compared to baselines. As shown in Figure Àú 3 , Search-R1 generates substantially more tokens to support reasoning over retrieved documents, while GAP learns efficient reasoning strategies that reduce response length by 24.9% on HotpotQA (416 vs. 554 tokens) and 20.3% on 2Wiki (452 vs. 567 tokens). This reduction in generated tokens directly decreases deployment costs and increases throughput, which are critical factors for real-world applications. Furthermore, these efficiency gains generalize across domains: while HotpotQA is an in-domain dataset, similar improvements are observed on out-of-domain benchmarks, demonstrating that the learned parallel decomposition patterns transfer effectively to new scenarios. These results validate that GAP not only improves accuracy but also makes multi-hop reasoning more practical and cost-effective for deployment.\\nHotpotQA ‚Ä† Acc‚Üë Length‚Üì Time Cost(s)‚Üì # Turns‚Üì Qwen2.5-3B-Instruct 9.9 256 114 1.11 Search-R1 25.3 584 221 2.69 AFM-RL-3B 35.7 554 248 2.27 GAP-3B (Ours) 36.7 416 168 1.78 2wiki * Acc‚Üë Length‚Üì Time Cost(s)‚Üì # Turns‚Üì Qwen2.5-3B-Instruct 10.5 277 121 1.12 Search-R1 31.7 651 254 3.05 AFM-RL-3B 39.8 567 262 2.64 GAP-3B (Ours) 41.7 452 206 2.03',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '6 Conclusion',\n",
       "    'paragraphs': 'In this paper, we introduced GAP (Graph-based Agent Planning), a novel paradigm that enables LLM-based agents to perform dependency-aware reasoning and adaptive tool execution. By explicitly modeling task dependencies through graph-based planning, GAP addresses the fundamental limitation of sequential execution in existing approaches like ReAct, achieving significant improvements in both efficiency and accuracy. Our key contribution lies in training agent foundation models to decompose complex queries into dependency graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. Through a carefully designed two-stage training strategy, we demonstrate that GAP substantially outperforms traditional sequential baselines, particularly on multi-step retrieval tasks requiring sophisticated reasoning.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Related Work',\n",
       "    'paragraphs': 'Training Large Language Models for multi-turn Tool-Integrated Reasoning (TIR) represents a promising frontier in Reinforcement Learning.\\nRepresentative works such as ARPO [ 26 ] , SimpleTIR [ 9 ] , and ToRL [ 8 ] adopt similar strategies: models are post-trained with SFT or RL, and outputs are structured (e.g., <code>‚Ä¶</code>) to trigger tool execution, feeding results back into the reasoning loop. Some extend RL-based Tool-Integrated Reasoning by improving small LLMs‚Äô tool-use capability, stabilizing multi-turn reasoning, and rewarding tool-use sequences independent of final answers.\\nToday, such tool-integrated reasoning is no longer a niche capability but a baseline feature of advanced agentic models. Mature commercial and open-source systems, such as OpenAI‚Äôs DeepResearch and o3 [ 27 ] , Kimi K2 [ 28 ] , Microsoft rStar2-Agent [ 29 ] and Meituan LongCat [ 30 ] , routinely incorporate these RL-honed strategies, underscoring the centrality of outcomedriven optimization in tool-augmented intelligence.\\nRecent work theoretically proves that TIR fundamentally expands LLM capabilities beyond the ‚Äúinvisible leash‚Äù of pure-text RL by introducing deterministic tool-driven state transitions, establishes token-efficiency arguments for feasibility under finite budget.\\nThe development of Agent Foundation Models (AFMs) marks a pivotal shift towards building models with innate reasoning and tool-use capabilities. A significant insight driving this field is that exceptional agentic performance is not solely dependent on model scale. Recent pioneering works, notably Chain-of-Agents [ 11 ] and Cognitive Kernel-Pro [ 31 ] , have demonstrated that even models at smaller scales can achieve state-of-the-art agentic abilities when trained with rigorous, purpose-built paradigms.\\nThese approaches address the limitations of scale-dependent capabilities through two key innovations: sophisticated data synthesis and specialized agent-centric training. The Chain-of-Agents framework employs a process of multi-agent knowledge distillation and outcome-driven reinforcement learning. This teaches a single, smaller model to internally simulate the collaborative roles of a multi-agent team, enabling it to rival the performance of much larger models or complex systems on benchmarks like GAIA [ 32 ] and WebArena [ 33 ] , but with dramatically improved inference efficiency.\\nSimilarly, Cognitive Kernel-Pro demonstrates that a meticulously designed open-source framework, combined with a systematic methodology for generating high-quality, verifiable training data across various domains (web, file, code), can produce smaller models that compete with systems relying on massive proprietary APIs. Collectively, these works prove that the strategic focus on training quality and architectural innovation is a viable path to creating highly capable and practical agents, making advanced agentic intelligence more accessible and efficient.',\n",
       "    'subsections': [{'title': 'A.1 Tool-Integrated Reasoning Method',\n",
       "      'paragraphs': 'Training Large Language Models for multi-turn Tool-Integrated Reasoning (TIR) represents a promising frontier in Reinforcement Learning.\\nRepresentative works such as ARPO [ 26 ] , SimpleTIR [ 9 ] , and ToRL [ 8 ] adopt similar strategies: models are post-trained with SFT or RL, and outputs are structured (e.g., <code>‚Ä¶</code>) to trigger tool execution, feeding results back into the reasoning loop. Some extend RL-based Tool-Integrated Reasoning by improving small LLMs‚Äô tool-use capability, stabilizing multi-turn reasoning, and rewarding tool-use sequences independent of final answers.\\nToday, such tool-integrated reasoning is no longer a niche capability but a baseline feature of advanced agentic models. Mature commercial and open-source systems, such as OpenAI‚Äôs DeepResearch and o3 [ 27 ] , Kimi K2 [ 28 ] , Microsoft rStar2-Agent [ 29 ] and Meituan LongCat [ 30 ] , routinely incorporate these RL-honed strategies, underscoring the centrality of outcomedriven optimization in tool-augmented intelligence.\\nRecent work theoretically proves that TIR fundamentally expands LLM capabilities beyond the ‚Äúinvisible leash‚Äù of pure-text RL by introducing deterministic tool-driven state transitions, establishes token-efficiency arguments for feasibility under finite budget.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Agent Foundation Model',\n",
       "      'paragraphs': 'The development of Agent Foundation Models (AFMs) marks a pivotal shift towards building models with innate reasoning and tool-use capabilities. A significant insight driving this field is that exceptional agentic performance is not solely dependent on model scale. Recent pioneering works, notably Chain-of-Agents [ 11 ] and Cognitive Kernel-Pro [ 31 ] , have demonstrated that even models at smaller scales can achieve state-of-the-art agentic abilities when trained with rigorous, purpose-built paradigms.\\nThese approaches address the limitations of scale-dependent capabilities through two key innovations: sophisticated data synthesis and specialized agent-centric training. The Chain-of-Agents framework employs a process of multi-agent knowledge distillation and outcome-driven reinforcement learning. This teaches a single, smaller model to internally simulate the collaborative roles of a multi-agent team, enabling it to rival the performance of much larger models or complex systems on benchmarks like GAIA [ 32 ] and WebArena [ 33 ] , but with dramatically improved inference efficiency.\\nSimilarly, Cognitive Kernel-Pro demonstrates that a meticulously designed open-source framework, combined with a systematic methodology for generating high-quality, verifiable training data across various domains (web, file, code), can produce smaller models that compete with systems relying on massive proprietary APIs. Collectively, these works prove that the strategic focus on training quality and architectural innovation is a viable path to creating highly capable and practical agents, making advanced agentic intelligence more accessible and efficient.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Case Study',\n",
       "    'paragraphs': 'We conduct case studies to gain a deeper understanding of the behavior and capabilities of our GAP framework. The results are shown in the following tables.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.'},\n",
       " '2510.25232v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Psychiatric disorders account for over 125 million disability-adjusted life years globally (Collaborators et\\xa0al., 2022 ) . A major challenge lies in psychiatric comorbidity, i.e., the co-occurrence of multiple conditions, which significantly complicates diagnosis and treatment. For instance, in a Netherlands study of depression and anxiety, 67% of individuals with a primary depression diagnosis had current and 75% had lifetime comorbid anxiety disorders (Lamers et\\xa0al., 2011 ) . Yet, most existing datasets and models narrowly focus on single disorders (Aich et\\xa0al., 2024 ; Yao et\\xa0al., 2022 ) , while the few corpora covering multiple conditions lack fine-grained annotations and fail to capture symptom co-occurrence and progression within diagnostic processes (Sun et\\xa0al., 2021 ; Yin et\\xa0al., 2025 ; Cohan et\\xa0al., 2018 ) . As a result, large language models (LLMs) have not been systematically evaluated on multi-disorder diagnostic tasks, limiting the development of reliable screening systems that require step-by-step reasoning grounded in DSM-5 standards (Association et\\xa0al., 2013 ) . 1 1 1 DSM-5 is the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders by the American Psychiatric Association. It provides standardized criteria for diagnosing mental disorders and is used to ensure accuracy and consistency.\\nTo construct large-scale comorbidity diagnostic dialogues, diverse patient profiles reflecting real-world complexity are needed. Such dialogues are essential because they simulate step-by-step diagnostic reasoning and enable downstream applications such as multi-disorder screening, clinical decision support, and the training of dialogue agents for psychiatry. Yet profiles alone often lack structured detail to support accurate diagnosis. Electronic medical records (EMRs), by contrast, provide a standardized format that includes a wide range of clinical data, such as symptoms and medical history, which are crucial for later training. Still, EMRs can not handle diverse and complex clinical scenarios during diagnosis and treatment planning and do not capture the dynamic doctor‚Äìpatient interaction, making it necessary to design a structured clinical dialogue flow that guides the diagnostic process and ensures realism and clinical validity.\\nThis work aims to advance comorbidity diagnosis through the construction of PsyCoTalk, a large-scale dialogue dataset designed for both data-driven modeling and systematic evaluation of diagnostic reasoning. Our framework transforms self-reported social media posts into structured EMRs, which serve directly as patient agent profiles to drive a three-agent diagnostic system (doctor, patient, tool) for generating clinically grounded multi-turn dialogues. As illustrated in Figure\\xa01 , we introduce an innovative data construction and dialogue generation pipeline and make three key contributions:\\n‚Ä¢ We develop a nuanced pipeline for EMR-driven dialogue generation that constructs synthetic, clinically grounded EMRs for comorbidity patients, offering a set of 502 synthetic EMRs along with detailed personal experiences reflecting common comorbid conditions, including Depression, Anxiety, Bipolar and Attention-Deficits .\\n‚Ä¢ We propose a clinically grounded multi-agent framework combining a Hierarchical Diagnostic State Machine (HDSM) and Diagnosis Context Tree (DCT) for diagnosis, inspired by clinical interview manuals and covering 130+ diagnostic states while following psychiatric assessment protocols.\\n‚Ä¢ We introduce PsyCoTalk , a large-scale dataset of 3,000 multi-turn diagnostic dialogues grounded in our EMRs and validated by psychiatrists, which is the largest of its kind and features longer and more clinically deep dialogues compared to existing corpora.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Mental‚Äëhealth dialogue corpora Recent datasets fall into two groups. Single‚Äëdisorder resources dominate: D 4 (Yao et\\xa0al., 2022 ) contains 1\\u2009339 Chinese doctor‚Äìpatient dialogues on depression; PsyQA (Sun et\\xa0al., 2021 ) offers 22\\u2009000 question‚Äìanswer pairs, later enlarged to 55\\u2009000 supportive conversations in SMILECHAT (Qiu et\\xa0al., 2023 ) ; and EFAQA (Hai Liang\\xa0Wang, 2020 ) adds 20\\u2009000 real counselling sessions. Broader clinical sets remain rare. CED‚ÄëBS (Aich et\\xa0al., 2024 ) targets bipolar disorder and schizophrenia, while MDD‚Äë5k (Yin et\\xa0al., 2025 ) is the largest Chinese diagnostic corpus so far, with 5\\u2009000 multi‚Äëturn simulations covering more than 25 disorders. Although MDD‚Äë5k uses a one‚Äëto‚Äëmany case‚Äëto‚Äëdialogue strategy to boost diversity, it still treats each illness in isolation and offers limited control over combinatorial symptom paths.\\nLLM‚Äëdriven dialogue simulation LLMs now power many mental‚Äëhealth studies. Early work assessed general chatbots: LLM‚ÄëEmpowered Chatbots (Chen et\\xa0al., 2023 ) measured ChatGPT‚Äôs diagnostic empathy, and Patient‚Äë Œ® \\\\Psi (Wang et\\xa0al., 2024 ) combined cognitive‚Äëbehavioural‚Äëtherapy rules with LLMs to tutor counsellors. CPsyCoun (Zhang et\\xa0al., 2024 ) turned structured therapy notes into trainable conversations, improving topic coverage.\\nMulti‚Äëagent designs add stronger clinical grounding. The AMC framework (Lan et\\xa0al., 2024 ) links doctor, patient, and supervisor agents through a three‚Äëlevel memory, enabling domain adaptation without fine-tuning. Building on AMC, MDD‚Äë5k introduced a neuro‚Äësymbolic controller and dynamic diagnosis tree to steer topic flow (Yin et\\xa0al., 2025 ) . These ideas inform our HDSM‚ÄëAgents , which extends the agent paradigm with a hierarchical state machine and context tree explicitly aligned to authoritative interview standards and, for the first time, generates dialogues reflecting comorbid diagnostic reasoning. Yet no prior corpus or simulator provides a large-scale, clinically structured resource for psychiatric comorbidity. Our work addresses this critical gap for the first time.',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Patient EMR and Experience Generation Workflow',\n",
       "    'paragraphs': 'To create standardized EMRs for training purposes, we collaborated with psychiatrists to develop a reference standard based on real clinical cases. Each EMR includes seven key components: Demographic Information , Chief Complaint , Medical Condition , Medical History , Personal History , Family History , and Preliminary Diagnosis . Mental status examination and auxiliary tests are excluded due to the limitations of social media data.\\nDetailed descriptions of the content structure and language requirements for each section are provided in Appendix A.3 .\\nDataset Selection We selected the PsySym (Zhang et\\xa0al., 2022 ) dataset as our base due to its scale, depth of annotation, and suitability for modeling psychiatric comorbidity. PsySym originates from Reddit users with self-reported psychiatric disorders, offering fine-grained symptom-level annotations across 7 major mental disorders. The dataset contains 5,624 diagnosed users, with an average of 102.5 posts per user, making it ideal for our needs. User Filtering To ensure meaningful interactions, we filtered users based on symptom diversity and frequency. We retained users with at least 10 symptom-related posts and 20 distinct symptom types. Using a DSM-5-aligned disease‚Äìsymptom graph, we excluded users with inconsistent symptom‚Äìlabel pairs, ensuring clinical reliability. Generation Strategy We adopted a modular approach to EMR generation, which outperformed a single-step aggregation method in terms of information recall, classification accuracy, and reasoning coherence. This approach involves categorizing post content into the seven standard EMR sections, generating each section individually using either rule-based methods or LLM inference, and merging them into a complete record. Processing Strategies As illustrated in Figure\\xa02 , we designed four strategies for information extraction and generation:\\nChief Complaint and Medical Condition: Using dual classifiers for symptoms and life events, we generate binary symptom-event vectors for each post, which are then summarized and integrated into prompts for LLM-based generation.\\nMedical, Personal, and Family History: Posts are categorized using keyword-based classification, summarized, and grouped into segments for LLM inference.\\nEducation, Occupation, and Implicit Age: Relevant content is retrieved through keyword search, trimmed with local context, and formatted into structured prompts.\\nGender, Marital Status, and Explicit Age: Demographic information is extracted directly using predefined keyword rules and rule-based classification.\\nAfter two rounds of user filtering and using a modular strategy for content extraction and label injection, we generated 502 structured EMRs from retained texts. These cover six disease combinations of four core psychiatric conditions: Major Depressive Disorder\\xa0(MDD), Anxiety Disorder\\xa0(AD), Bipolar Disorder\\xa0(BD), and Attention-Deficit/Hyperactivity Disorder\\xa0(ADHD) . Table\\xa01 shows user counts before and after filtering, plus statistics on average posts, posts with symptoms, life events, and distinct symptoms. Results show retained users provide rich, dense, analyzable content.\\nTo validate the realism of the synthetic EMRs, we compared them with real-world data 2 2 2 For real-world comparison, we obtained approximately 1,000 de-identified clinical records from the National Mental Health Center, provided with institutional approval and used strictly for research purposes. across disease distribution, demographic factors (age and gender), and family history. The results are illustrated in Figure\\xa03 . Disease prevalence in synthetic EMRs broadly aligns with real-world data while exhibiting a more balanced distribution. Age shows a peak in the 20‚Äì24 group for synthetic data versus 30‚Äì34 for real records (Collaborators et\\xa0al., 2022 ) , likely reflecting the younger social media demographic (DataReportal, 2020 ) . Gender proportions are more balanced in synthetic records, consistent with reduced stigma in online contexts (Porteous & Armstrong, 2021 ) . Family history is slightly overrepresented but remains within clinically plausible bounds. Psychiatrist feedback on sampled records confirmed sufficient diversity and clinical plausibility, closely matching real-world documentation.\\nEMRs provide structured and factual information, but they lack the narrative richness and variability found in real patient-doctor interactions. By generating personalized fictitious experiences, we can create a more comprehensive and varied dataset that includes not only the factual data from EMRs but also the nuanced, context-specific details that arise in clinical conversations.\\nPsychiatric diagnoses primarily rely on language-based interactions, making it feasible to generate multiple coherent diagnostic dialogues from a single EMR, provided the symptom presentation remains consistent with the diagnosis (Yin et\\xa0al., 2025 ) .\\nWe therefore propose a personalized fictitious experience generation method grounded in structured EMRs to increase the diversity and realism of simulated dialogues. Unlike MDD-5k, which randomly samples templates, our approach aligns sampled content with EMR attributes to avoid semantic conflicts and incorporates clinically relevant lifestyle attributes. This ensures both semantic coherence and diagnostic validity in generated dialogues.\\nConcretely, we employ LLMs to process each structured EMR via prompt-based instruction tuning. For each case, the model is guided to generate a total of 5 personalized Personal Histories , including both existing entries from the EMR and newly generated ones when necessary. In addition, 10 semantically consistent Fictitious Experiences are produced, all aligned with the EMR content and free from logical conflicts. Here, Personal Histories refer to lifestyle or health background (e.g., ‚Äúprefers light food, smokes and drinks occasionally, and exercises three times per week‚Äù ), while Fictitious Experiences describe past events potentially affecting mental health (e.g., ‚Äúwas blamed as the main cause of a failed company project one year ago‚Äù ). As shown in Figure\\xa04 , the generation process outputs two structured dictionaries: ùíü his \\\\mathcal{D}_{\\\\text{his}} for personal histories and ùíü fic \\\\mathcal{D}_{\\\\text{fic}} for fictitious experiences. Given a structured EMR input ùê± EMR \\\\mathbf{x}_{\\\\text{EMR}} and e ~ \\\\tilde{e} represents the free-text narrative description corresponding to a selected fictitious experience e e . Formally, we define:\\nThis two-stage generation mechanism enables each EMR to yield up to 50 unique fictitious experiences, thereby enhancing the diversity, flexibility, and clinical realism of downstream diagnostic dialogues while preserving semantic consistency. For detailed description of prompt please refer to Appendix A.4 .',\n",
       "    'subsections': [{'title': '3.1 EMR Generation',\n",
       "      'paragraphs': 'Dataset Selection We selected the PsySym (Zhang et\\xa0al., 2022 ) dataset as our base due to its scale, depth of annotation, and suitability for modeling psychiatric comorbidity. PsySym originates from Reddit users with self-reported psychiatric disorders, offering fine-grained symptom-level annotations across 7 major mental disorders. The dataset contains 5,624 diagnosed users, with an average of 102.5 posts per user, making it ideal for our needs. User Filtering To ensure meaningful interactions, we filtered users based on symptom diversity and frequency. We retained users with at least 10 symptom-related posts and 20 distinct symptom types. Using a DSM-5-aligned disease‚Äìsymptom graph, we excluded users with inconsistent symptom‚Äìlabel pairs, ensuring clinical reliability. Generation Strategy We adopted a modular approach to EMR generation, which outperformed a single-step aggregation method in terms of information recall, classification accuracy, and reasoning coherence. This approach involves categorizing post content into the seven standard EMR sections, generating each section individually using either rule-based methods or LLM inference, and merging them into a complete record. Processing Strategies As illustrated in Figure\\xa02 , we designed four strategies for information extraction and generation:\\nChief Complaint and Medical Condition: Using dual classifiers for symptoms and life events, we generate binary symptom-event vectors for each post, which are then summarized and integrated into prompts for LLM-based generation.\\nMedical, Personal, and Family History: Posts are categorized using keyword-based classification, summarized, and grouped into segments for LLM inference.\\nEducation, Occupation, and Implicit Age: Relevant content is retrieved through keyword search, trimmed with local context, and formatted into structured prompts.\\nGender, Marital Status, and Explicit Age: Demographic information is extracted directly using predefined keyword rules and rule-based classification.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 PsyCoProfile Dataset description and statistics',\n",
       "      'paragraphs': 'After two rounds of user filtering and using a modular strategy for content extraction and label injection, we generated 502 structured EMRs from retained texts. These cover six disease combinations of four core psychiatric conditions: Major Depressive Disorder\\xa0(MDD), Anxiety Disorder\\xa0(AD), Bipolar Disorder\\xa0(BD), and Attention-Deficit/Hyperactivity Disorder\\xa0(ADHD) . Table\\xa01 shows user counts before and after filtering, plus statistics on average posts, posts with symptoms, life events, and distinct symptoms. Results show retained users provide rich, dense, analyzable content.\\nTo validate the realism of the synthetic EMRs, we compared them with real-world data 2 2 2 For real-world comparison, we obtained approximately 1,000 de-identified clinical records from the National Mental Health Center, provided with institutional approval and used strictly for research purposes. across disease distribution, demographic factors (age and gender), and family history. The results are illustrated in Figure\\xa03 . Disease prevalence in synthetic EMRs broadly aligns with real-world data while exhibiting a more balanced distribution. Age shows a peak in the 20‚Äì24 group for synthetic data versus 30‚Äì34 for real records (Collaborators et\\xa0al., 2022 ) , likely reflecting the younger social media demographic (DataReportal, 2020 ) . Gender proportions are more balanced in synthetic records, consistent with reduced stigma in online contexts (Porteous & Armstrong, 2021 ) . Family history is slightly overrepresented but remains within clinically plausible bounds. Psychiatrist feedback on sampled records confirmed sufficient diversity and clinical plausibility, closely matching real-world documentation.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 Fictitious Patient Experience Generation',\n",
       "      'paragraphs': 'EMRs provide structured and factual information, but they lack the narrative richness and variability found in real patient-doctor interactions. By generating personalized fictitious experiences, we can create a more comprehensive and varied dataset that includes not only the factual data from EMRs but also the nuanced, context-specific details that arise in clinical conversations.\\nPsychiatric diagnoses primarily rely on language-based interactions, making it feasible to generate multiple coherent diagnostic dialogues from a single EMR, provided the symptom presentation remains consistent with the diagnosis (Yin et\\xa0al., 2025 ) .\\nWe therefore propose a personalized fictitious experience generation method grounded in structured EMRs to increase the diversity and realism of simulated dialogues. Unlike MDD-5k, which randomly samples templates, our approach aligns sampled content with EMR attributes to avoid semantic conflicts and incorporates clinically relevant lifestyle attributes. This ensures both semantic coherence and diagnostic validity in generated dialogues.\\nConcretely, we employ LLMs to process each structured EMR via prompt-based instruction tuning. For each case, the model is guided to generate a total of 5 personalized Personal Histories , including both existing entries from the EMR and newly generated ones when necessary. In addition, 10 semantically consistent Fictitious Experiences are produced, all aligned with the EMR content and free from logical conflicts. Here, Personal Histories refer to lifestyle or health background (e.g., ‚Äúprefers light food, smokes and drinks occasionally, and exercises three times per week‚Äù ), while Fictitious Experiences describe past events potentially affecting mental health (e.g., ‚Äúwas blamed as the main cause of a failed company project one year ago‚Äù ). As shown in Figure\\xa04 , the generation process outputs two structured dictionaries: ùíü his \\\\mathcal{D}_{\\\\text{his}} for personal histories and ùíü fic \\\\mathcal{D}_{\\\\text{fic}} for fictitious experiences. Given a structured EMR input ùê± EMR \\\\mathbf{x}_{\\\\text{EMR}} and e ~ \\\\tilde{e} represents the free-text narrative description corresponding to a selected fictitious experience e e . Formally, we define:\\nThis two-stage generation mechanism enables each EMR to yield up to 50 unique fictitious experiences, thereby enhancing the diversity, flexibility, and clinical realism of downstream diagnostic dialogues while preserving semantic consistency. For detailed description of prompt please refer to Appendix A.4 .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Multi-agent Diagnosis Framework',\n",
       "    'paragraphs': 'To construct a clinically realistic and diverse dataset for psychiatric diagnostic dialogues, we developed a multi-agent framework, that integrates structured EMRs with dynamic dialogue generation. This framework comprises two main components: a clinical-grounded interview pipeline, and a multi-agent execution mechanism.\\nThe Structured Clinical Interview for DSM-5 (SCID-5-RV) is a standardized tool for assessing mental disorders, offering a systematic approach to symptom evaluation and diagnosis. Adhering to SCID-5-RV grounds our framework in established clinical practices. To guide LLM-based psychiatric interviews, we restructure SCID-5-RV into two parts: a Hierarchical Diagnostic State Machine (HDSM) and a Diagnostic Context Tree (DCT). This design ensures a structured, dynamic, and clinically coherent dialogue flow.\\nHierarchical Diagnostic State Machine (HDSM) The HDSM adheres to the SCID-5-RV protocol, assigning one sub-state machine to each target disorder (MDD, AD, BD, ADHD).\\nEach sub-state machine terminates in explicit diagnostic outcomes; for MDD, the terminal states depression1 ‚Äì depression5 in Figure\\xa05 represent five mutually exclusive diagnoses (see the complete architecture in Appendix A.7 ).\\nIn contrast to MDD-5k, which selected topics in random order, HDSM enables the agent to both ask questions and refine the diagnosis iteratively, reflecting real clinical reasoning. Following clinical design, the HDSM consists of a Graphical three-level hierarchy:\\nHigh-Level States (HLS) : Overarching modules (e.g., current-episode screening), shown as large hollow rectangles and represent overarching modules, such as current-episode screening (A.1) or episode history (A.15).\\nIntermediate-Level States (ILS) : Group related symptoms under an HLS, displayed as solid rectangles . They group related symptoms under an HLS and typically have no fixed time window. A special subset of ILS, referred to as sub-state groups , is represented by dashed rectangles , and aggregates closely related questions for joint inquiry.\\nBasic-Level States (BLS) : Terminal nodes corresponding to individual questions.drawn as solid circles . These are terminal nodes corresponding to individual questions, typically constrained by time and embedded within sub-state groups.\\nExcept for the dashed sub-state groups, each ILS and BLS node corresponds to a specific question derived from SCID-5-RV. Questions fall into one of four categories:\\n(i) affective or cognitive symptoms,\\n(ii) physiological or behavioral changes,\\n(iii) functional impairment or risk, and\\n(iv) comorbid or contributing factors (for a full list of nodes and their categories, see Appendix A.5 ).\\nNatural language cues. Within each sub-state group, only the first question adopts a precise temporal phrase such as ‚Äúin the past two weeks‚Äù; subsequent questions use looser expressions like ‚Äúrecently‚Äù to avoid unnatural repetition.\\nBinary symptom scale and flow control. The original four-point SCID-5-RV symptom scale (‚Äúunclear‚Äù, ‚Äúabsent‚Äù, ‚Äúsubthreshold‚Äù, ‚Äúthreshold‚Äù) is reduced to a binary form: present or absent .\\nColored arrows in Figure\\xa05 illustrate how different responses guide the interview trajectory.\\nIf a BLS node has no valid successor, the agent performs a localized random traversal within the sub-state group. The transition to the next diagnostic state occurs only after all terminal BLS nodes (i.e., those without successors) have been visited. If the number of ‚Äúpositive‚Äù (i.e., ‚Äúpresent‚Äù) responses exceeds a threshold, the group is deemed positive and follows the corresponding path; otherwise, it is absent and triggers an alternative transition.\\nDiagnostic Context Tree (DCT) To further enhance the semantic depth and contextual coherence of diagnostic dialogues, we introduce the DCT, a tree-structured semantic controller that operates alongside the HDSM. The DCT is designed to dynamically manage the flow of the dialogue, ensuring that it remains clinically relevant and logically coherent. The top-level branches of the DCT include three main categories: Family History , Personal History , and Experience Inquiry . These categories are structured to capture the comprehensive background information necessary for a thorough psychiatric evaluation.\\nThe Experience Inquiry node within the DCT is dynamically triggered at the end of each turn, based on the context of the conversation. This ensures that the dialogue remains responsive and adaptive to the specific needs of the interaction. After the completion of the HDSM, the remaining leaf nodes of the DCT are visited in a randomized order to maintain a natural flow of conversation. Personal history templates are conditionally selected based on patient gender, ensuring that the dialogue is tailored to the individual patient‚Äôs background (see Figure\\xa04 for male-specific examples).\\nBy integrating the DCT with the HDSM, we effectively augment the elicitation of relevant background information, leading to dialogues that are more logically coherent and clinically aligned with real-world psychiatric consultations. This combination ensures that the dialogue not only follows a structured diagnostic process but also incorporates the necessary contextual details to support a comprehensive and accurate diagnosis.\\nOur simulator employs three specialized agents to facilitate each dialogue generation:\\nPatient Agent The patient agent formulates responses based on (i) the structured EMR, (ii) its fictitious personal experience, and (iii) the current topic provided by the tool agent. Initial experiments revealed a bias towards affirming all symptoms, even those not documented in the EMR. To mitigate this, we developed a Disease‚ÄìSymptom Description Knowledge Graph (DSD-KG) derived from SCID-5-RV guidelines. For each question, the agent consults the DSD-KG; if the symptom is absent from the EMR or contradicts the provisional diagnosis, the agent responds with no . This mechanism filters out hallucinated agreements, enhancing the credibility of the dialogue.\\nDoctor Agent The doctor agent poses questions according to the topic queued by the tool agent. To avoid monotony, we defined five distinct doctor profiles varying in age, specialty, empathy style, verbosity, diagnostic speed, and explanation frequency. Profile cues and high-quality few-shot examples are injected at prompt time, enabling the LLM to adapt its responses (e.g., concise, gentle, or analytical). Reply length limits and a rotating pool of empathy phrases further reduce repetitive responses.\\nTool Agent The Tool Agent serves as the central controller, bridging natural-language dialogue with the symbolic diagnostic framework. Its responsibilities are divided into two main categories: tree management and dialogue coordination.\\nTree Management\\n‚Ä¢ Sub-State-Machine-OrderGen(conv) ‚Üí \\\\rightarrow list: Determines the execution order of the four disorder-specific sub-state machines after the first dialogue turn. It supports two strategies: random shuffle or symptom-informed mode, which prioritizes modules based on current symptoms.\\n‚Ä¢ ResponseClassifier(conv) ‚Üí \\\\rightarrow bool: Classifies the patient‚Äôs latest answer as present or absent and forwards this label to the HDSM to trigger state transitions.\\n‚Ä¢ NeedExpBranch(conv) ‚Üí \\\\rightarrow node: Decides whether to enter the ‚ÄúExperience Inquiry‚Äù branch of the context tree based on the previous turn‚Äôs content.\\nDialogue Interface\\n‚Ä¢ BuildPrompt(topic) ‚Üí text: Generates aligned prompts for the doctor and patient agents based on the current topic node in the diagnostic tree, ensuring semantic consistency.\\n‚Ä¢ IsDialEnd(tree) ‚Üí bool: Determines whether the consultation should terminate. The dialogue ends when all disorder-specific sub-state machines reach their terminal states and all required nodes in the context tree‚Äîexcluding the optional experience inquiry‚Äîhave been traversed.',\n",
       "    'subsections': [{'title': '4.1 Clinical-Grounded Interview Pipeline',\n",
       "      'paragraphs': 'The Structured Clinical Interview for DSM-5 (SCID-5-RV) is a standardized tool for assessing mental disorders, offering a systematic approach to symptom evaluation and diagnosis. Adhering to SCID-5-RV grounds our framework in established clinical practices. To guide LLM-based psychiatric interviews, we restructure SCID-5-RV into two parts: a Hierarchical Diagnostic State Machine (HDSM) and a Diagnostic Context Tree (DCT). This design ensures a structured, dynamic, and clinically coherent dialogue flow.\\nHierarchical Diagnostic State Machine (HDSM) The HDSM adheres to the SCID-5-RV protocol, assigning one sub-state machine to each target disorder (MDD, AD, BD, ADHD).\\nEach sub-state machine terminates in explicit diagnostic outcomes; for MDD, the terminal states depression1 ‚Äì depression5 in Figure\\xa05 represent five mutually exclusive diagnoses (see the complete architecture in Appendix A.7 ).\\nIn contrast to MDD-5k, which selected topics in random order, HDSM enables the agent to both ask questions and refine the diagnosis iteratively, reflecting real clinical reasoning. Following clinical design, the HDSM consists of a Graphical three-level hierarchy:\\nHigh-Level States (HLS) : Overarching modules (e.g., current-episode screening), shown as large hollow rectangles and represent overarching modules, such as current-episode screening (A.1) or episode history (A.15).\\nIntermediate-Level States (ILS) : Group related symptoms under an HLS, displayed as solid rectangles . They group related symptoms under an HLS and typically have no fixed time window. A special subset of ILS, referred to as sub-state groups , is represented by dashed rectangles , and aggregates closely related questions for joint inquiry.\\nBasic-Level States (BLS) : Terminal nodes corresponding to individual questions.drawn as solid circles . These are terminal nodes corresponding to individual questions, typically constrained by time and embedded within sub-state groups.\\nExcept for the dashed sub-state groups, each ILS and BLS node corresponds to a specific question derived from SCID-5-RV. Questions fall into one of four categories:\\n(i) affective or cognitive symptoms,\\n(ii) physiological or behavioral changes,\\n(iii) functional impairment or risk, and\\n(iv) comorbid or contributing factors (for a full list of nodes and their categories, see Appendix A.5 ).\\nNatural language cues. Within each sub-state group, only the first question adopts a precise temporal phrase such as ‚Äúin the past two weeks‚Äù; subsequent questions use looser expressions like ‚Äúrecently‚Äù to avoid unnatural repetition.\\nBinary symptom scale and flow control. The original four-point SCID-5-RV symptom scale (‚Äúunclear‚Äù, ‚Äúabsent‚Äù, ‚Äúsubthreshold‚Äù, ‚Äúthreshold‚Äù) is reduced to a binary form: present or absent .\\nColored arrows in Figure\\xa05 illustrate how different responses guide the interview trajectory.\\nIf a BLS node has no valid successor, the agent performs a localized random traversal within the sub-state group. The transition to the next diagnostic state occurs only after all terminal BLS nodes (i.e., those without successors) have been visited. If the number of ‚Äúpositive‚Äù (i.e., ‚Äúpresent‚Äù) responses exceeds a threshold, the group is deemed positive and follows the corresponding path; otherwise, it is absent and triggers an alternative transition.\\nDiagnostic Context Tree (DCT) To further enhance the semantic depth and contextual coherence of diagnostic dialogues, we introduce the DCT, a tree-structured semantic controller that operates alongside the HDSM. The DCT is designed to dynamically manage the flow of the dialogue, ensuring that it remains clinically relevant and logically coherent. The top-level branches of the DCT include three main categories: Family History , Personal History , and Experience Inquiry . These categories are structured to capture the comprehensive background information necessary for a thorough psychiatric evaluation.\\nThe Experience Inquiry node within the DCT is dynamically triggered at the end of each turn, based on the context of the conversation. This ensures that the dialogue remains responsive and adaptive to the specific needs of the interaction. After the completion of the HDSM, the remaining leaf nodes of the DCT are visited in a randomized order to maintain a natural flow of conversation. Personal history templates are conditionally selected based on patient gender, ensuring that the dialogue is tailored to the individual patient‚Äôs background (see Figure\\xa04 for male-specific examples).\\nBy integrating the DCT with the HDSM, we effectively augment the elicitation of relevant background information, leading to dialogues that are more logically coherent and clinically aligned with real-world psychiatric consultations. This combination ensures that the dialogue not only follows a structured diagnostic process but also incorporates the necessary contextual details to support a comprehensive and accurate diagnosis.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Multi-Agent Execution Mechanism',\n",
       "      'paragraphs': 'Our simulator employs three specialized agents to facilitate each dialogue generation:\\nPatient Agent The patient agent formulates responses based on (i) the structured EMR, (ii) its fictitious personal experience, and (iii) the current topic provided by the tool agent. Initial experiments revealed a bias towards affirming all symptoms, even those not documented in the EMR. To mitigate this, we developed a Disease‚ÄìSymptom Description Knowledge Graph (DSD-KG) derived from SCID-5-RV guidelines. For each question, the agent consults the DSD-KG; if the symptom is absent from the EMR or contradicts the provisional diagnosis, the agent responds with no . This mechanism filters out hallucinated agreements, enhancing the credibility of the dialogue.\\nDoctor Agent The doctor agent poses questions according to the topic queued by the tool agent. To avoid monotony, we defined five distinct doctor profiles varying in age, specialty, empathy style, verbosity, diagnostic speed, and explanation frequency. Profile cues and high-quality few-shot examples are injected at prompt time, enabling the LLM to adapt its responses (e.g., concise, gentle, or analytical). Reply length limits and a rotating pool of empathy phrases further reduce repetitive responses.\\nTool Agent The Tool Agent serves as the central controller, bridging natural-language dialogue with the symbolic diagnostic framework. Its responsibilities are divided into two main categories: tree management and dialogue coordination.\\nTree Management\\n‚Ä¢ Sub-State-Machine-OrderGen(conv) ‚Üí \\\\rightarrow list: Determines the execution order of the four disorder-specific sub-state machines after the first dialogue turn. It supports two strategies: random shuffle or symptom-informed mode, which prioritizes modules based on current symptoms.\\n‚Ä¢ ResponseClassifier(conv) ‚Üí \\\\rightarrow bool: Classifies the patient‚Äôs latest answer as present or absent and forwards this label to the HDSM to trigger state transitions.\\n‚Ä¢ NeedExpBranch(conv) ‚Üí \\\\rightarrow node: Decides whether to enter the ‚ÄúExperience Inquiry‚Äù branch of the context tree based on the previous turn‚Äôs content.\\nDialogue Interface\\n‚Ä¢ BuildPrompt(topic) ‚Üí text: Generates aligned prompts for the doctor and patient agents based on the current topic node in the diagnostic tree, ensuring semantic consistency.\\n‚Ä¢ IsDialEnd(tree) ‚Üí bool: Determines whether the consultation should terminate. The dialogue ends when all disorder-specific sub-state machines reach their terminal states and all required nodes in the context tree‚Äîexcluding the optional experience inquiry‚Äîhave been traversed.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Final Product: PsyCoTalk Dataset',\n",
       "    'paragraphs': 'Generation Process We begin with 502 structured EMRs from PsyCoProfile. For each EMR, we sample five distinct fictitious experiences from the generated ùíü fic \\\\mathcal{D}_{\\\\text{fic}} and pair them with five different personal histories from ùíü his \\\\mathcal{D}_{\\\\text{his}} , resulting in five Fictitious Experience Descriptions (FEDs) that capture patient-specific contextual narratives. Each FED is assigned a unique doctor profile that varies in empathy level, verbosity, and diagnostic pacing. All three agents‚Äîthe doctor, patient, and tool agent‚Äîare instantiated using the Qwen2.5-72B model (Bai et\\xa0al., 2023 ) , deployed locally on a server with four NVIDIA A100-SXM4-80GB GPUs using vLLM (Kwon et\\xa0al., 2023 ) . For each FED and doctor profile pair, we generate two dialogues using different sub-state machine scheduling strategies: one that is symptom-informed and another that uses a randomly determined order. This process yields approximately 5,000 dialogues. From these, we select 3,000 dialogues whose final diagnostic labels match one of the six predefined comorbidity combinations in PsyCoProfile to form the final PsyCoTalk dataset. As all human evaluators are native Chinese speakers, we release only the Chinese version to ensure annotation quality. For comparative analysis, we include two additional Chinese diagnostic datasets, D 4 and MDD-5k , along with a de-identified clinical dialogue set ( Real-World Dial ) obtained from a psychiatric hospital. Dataset statistics are reported in Table\\xa02 .\\nEvaluation We conduct two types of human evaluation. First, 50 dialogues from PsyCoTalk are rated in a double-blind fashion by five licensed psychiatrists with more than seven years of clinical experience. Each dialogue is evaluated across six dimensions grouped under four criteria: Professionalism , Communication , Fluency , and Realism . Professionalism assesses whether the doctor successfully elicits all symptoms necessary for diagnosis. Communication evaluates (i) the doctor‚Äôs proactivity in questioning and (ii) the patient‚Äôs responsiveness. Fluency measures (i) syntactic and topical coherence and (ii) avoidance of redundancy. Realism assesses how closely the dialogue resembles actual psychiatric consultations. Second, we perform an AB test using 10 two-turn excerpts sampled from each dataset (PsyCoTalk, D 4 , MDD-5k, and Real-World Dial). The same five psychiatrists are asked to judge whether each excerpt appears real or AI-generated. Each excerpt identified as ‚Äúreal‚Äù receives one point, and final realism scores are normalized to a 10-point scale.\\nObjective Comparison As shown in Table\\xa02 , PsyCoTalk excels in scale and structural fidelity. It is the only dataset for psychiatric comorbidity, with 3,000 dialogues-over twice D 4 and close to MDD-5k. Each conversation averages 45.9 turns, nearly double other corpora. Utterance lengths are 34.0 characters (doctors) and 43.5 (patients), closest to real clinical conversations (28.3 and 35.8). By contrast, MDD-5k deviates most, with overly long utterances that reduce realism.\\nTo verify the effectiveness of the fictitious personal experience module under EMR reuse and to validate the cross-lingual generalization of our pipeline, we assess dialogue diversity and conduct a small-scale English generation experiment compared with existing corpora; detailed metrics and results are reported in Appendix A.2 .\\nTo assess diagnostic accuracy, we use initial EMR labels as ground truth and evaluate with exact-match (all 5 disorders correct). A zero-shot Qwen2.5-72B baseline reaches 0.22 subset accuracy, while our HDSM-guided multi-agent system improves to 0.31 , a significant gain (McNemar‚Äôs test, p = 7 √ó 10 ‚àí 6 < 0.001 p=7\\\\times 10^{-6}<0.001 ). On 200 sampled cases, GPT-4o-mini and Deepseek-v3 achieve below 0.1, Qwen3-32B below 0.02, and Qwen3-8B below 0.04. Per-label F1: MDD ( 0.92 ), AD ( 0.81 ), ADHD ( 0.64 ), BD ( 0.40 ). These align with clinical trends where BD and ADHD are harder to diagnose than MDD and AD due to symptom overlap, subtler onset, and higher heterogeneity (Hui et\\xa0al., 2018 ; Barkley & Brown, 2008 ) .\\nSubjective Results Table\\xa04 summarizes expert evaluation results across six dimensions. PsyCoTalk ranks highest in communication (8.14 for doctor initiative, 8.24 for patient engagement), alongside strong scores in professionalism (7.72) and fluency (7.42 and 6.79), reflecting coherent and context-aware interactions. In the AB test for perceived realism, Table\\xa04 shows that PsyCoTalk achieves a score of 5, second only to real-world data (6), indicating that its dialogue style closely approximates clinical expectations. By contrast, MDD-5k scores the lowest (1) due to its templated and repetitive utterances, which reduce perceived authenticity. These findings confirm that our HDSM-based multi-agent framework produces dialogues that are not only diagnostically informative but also linguistically coherent and clinically credible across diverse expert judgments.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Limitations',\n",
       "    'paragraphs': 'PsyCoTalk focuses on four prevalent psychiatric disorders and their comorbidities, which reflects common real-world cases but limits coverage of rarer conditions. While we conducted a small-scale English generation experiment, the main dataset remains Chinese, restricting multilingual applicability. Nevertheless, our pipeline is extensible and can be scaled to broader disorder coverage and cross-lingual settings in future work.',\n",
       "    'subsections': []},\n",
       "   {'title': '6 Conclusion',\n",
       "    'paragraphs': 'In this paper, we introduce a two-stage pipeline that creates the first large-scale, clinically standard dataset for psychiatric comorbidity. First, we develop PsyCoProfile, converting social media posts into 502 structured electronic medical records that reflect real-world prevalence of six common disorder combinations. Second, we propose a multi-agent interview and diagnosis framework that transforms these records into PsyCoTalk, a corpus of 3,000 multi-turn dialogues mimicking clinical interviews. PsyCoTalk provides the necessary scale and detail to train models for multi-disorder screening.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Ethics Statement',\n",
       "    'paragraphs': 'The authors have read and adhere to the ICLR Code of Ethics. This work does not involve human subjects, identifiable private data, or harmful applications.\\nNo external sponsorship or conflict of interest influenced the design or conclusions of this work.\\nData provenance and privacy. The raw posts that seed PsyCoProfile are drawn from the publicly available PsySym corpus, whose collection was approved by an institutional review board (IRB I2022158P) and complies with the Personal Information Protection Law of the People‚Äôs Republic of China. All posts are anonymous, stripped of usernames, time stamps and locations, and stored under random identifiers. Our pipeline rewrites each post into a synthetic electronic medical record; no verbatim text that could re-identify an author is retained. The fictitious personal histories and life events are generated entirely by large language models. Five licensed psychiatrists reviewed random samples and confirmed that no record contains protected health information or disallowed content. The final dialogues in PsyCoTalk are synthetic and will be released only under a data-usage agreement that forbids attempts at re-identification or clinical deployment.\\nIntended use and risk mitigation. The dataset is designed for research on multi-disorder screening and dialogue modelling. It is not a diagnostic tool, nor does it provide treatment advice. Synthetic conversations may still reflect biases inherited from web data and large language models; users must apply rigorous evaluation before any downstream use. We urge practitioners to keep humans in the loop when analysing sensitive mental-health text, and to follow relevant professional, legal and ethical guidelines. Future work will extend coverage while continuing to consult ethics boards and domain experts.\\nSocietal impact. This work has the potential to positively contribute to early-stage screening research and the development of more inclusive diagnostic tools for psychiatric profiles, particularly in settings with limited clinical resources. However, the release of realistic synthetic dialogues and records also raises risks of misuse, such as inappropriate clinical adoption or amplification of model biases in downstream systems. To mitigate these risks, we emphasize that the dataset is strictly intended for research use and should be paired with responsible modeling and evaluation practices. Broader impacts will be monitored in future iterations in collaboration with ethics committees and clinical stakeholders.\\nAnnotator Compensation. We ensured fair compensation for all annotators. They were compensated at a rate of $25/hour. This rate is above the local minimum wage and aligns with guidelines for ethical crowdsourcing practices. Annotators were briefed about the nature of the task, estimated time per task, and payment prior to participation.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Appendix',\n",
       "    'paragraphs': 'LLMs were primarily employed for data generation, forming the basis of our multi-agent architecture. For the writing of this paper, we note that LLMs were used only to aid or polish the text, for example in grammar checking and improving readability. They were not involved in generating research ideas or analyzing results.\\nTo address concerns about EMR reuse, we enrich each dialogue with varied backgrounds while retaining symptom consistency.\\nWe measure intra-EMR diversity by 1 ‚àí J \\u200b ( E \\u200b M \\u200b R i ) 1-\\\\mathrm{J}(EMR_{i}) , where\\nwith K j , K k K_{j},K_{k} denoting keyword sets from dialogues generated under the same EMR i i .\\nThe average diversity score is 0.647 ¬± 0.030 0.647\\\\pm 0.030 , i.e., only 35.3% overlap, demonstrating high variability among dialogues derived from the same case.\\nWe further compare synthetic (PsyCoTalk) and real-world clinical dialogues (1,731 patients, with 40.7% comorbid cases) using three diversity metrics:\\nNormalized Entropy H \\u200b ( X ) / log 2 \\u2061 ( V ) H(X)/\\\\log_{2}(V) , where H \\u200b ( X ) H(X) is Shannon entropy and V V is vocabulary size.\\nHapax Proportion N hapax / V N_{\\\\text{hapax}}/V , where N hapax = | { w i ‚àà V ‚à£ f \\u200b ( w i ) = 1 } | N_{\\\\text{hapax}}=|\\\\{w_{i}\\\\in V\\\\mid f(w_{i})=1\\\\}| .\\nSemantic Diversity 1 ‚àí MeanCosSim 1-\\\\text{MeanCosSim} , with MeanCosSim = 2 n \\u200b ( n ‚àí 1 ) \\u200b ‚àë i < j cos \\u2061 ( ùêØ i , ùêØ j ) \\\\text{MeanCosSim}=\\\\tfrac{2}{n(n-1)}\\\\sum_{i<j}\\\\cos(\\\\mathbf{v}_{i},\\\\mathbf{v}_{j}) , where each session s i s_{i} is represented by an embedding vector ùêØ i \\\\mathbf{v}_{i} .\\nTable\\xa05 shows that PsyCoTalk (Chinese) closely matches the diversity of real clinical dialogues, while PsyCoTalk-eng exhibits comparable or even higher diversity than the AlexanderStreet dataset.\\nUsing the same pipeline, we construct a preliminary English dataset (PsyCoTalk-eng, 205 dialogues from 41 patients) and compare it with the AlexanderStreet dataset (Alex, 1,254 dialogues without diagnostic labels). As shown in Table\\xa05 , PsyCoTalk-eng achieves higher normalized entropy and semantic diversity than Alex, while Chinese PsyCoTalk exhibits a higher Hapax proportion but lower semantic diversity than its English counterpart. This discrepancy may be attributed to language structure: Chinese tends to use more unique or low-frequency words, while English dialogues often display greater semantic variation.\\nWe compare word frequency distributions in 200 sampled English and 200 Chinese dialogues, tokenized by NLTK (English) and jieba (Chinese). The top 100 high-frequency words (frequency > 100 >100 ) are categorized into four groups, and the results are summarized in Table\\xa06 . Here, High-freq Count (%) refers to the number and proportion of distinct word types from each category that appear among the top 100 (e.g., 28 Chinese words fall into ‚ÄúSymptoms/States,‚Äù accounting for 35%). Total Freq (%) denotes the cumulative frequency and proportion of all words in that category within the sampled dialogues (e.g., Chinese symptom/state words occur 27,342 times in total, 21.88% of all top-100 occurrences). This dual perspective captures both the lexical coverage (Count) and the usage weight (Total Freq) of different categories across languages.\\nAlthough distributional differences exist, both languages are dominated by descriptive and symptom-related words, with similar category rankings. Figure\\xa06 illustrates representative word clouds for the ‚Äúsymptom/state‚Äù category in both languages, showing numerous synonymous or semantically close pairs (e.g., anxious /ÁÉ¶Ë∫Å, mind /ÂøÉÈáå/ËÑëÂ≠êÈáå).\\nEnglish words exhibit a more even distribution, potentially due to linguistic structural differences.\\nThis appendix provides detailed specifications for each section in the standardized EMR template developed in this study. The content structure was developed based on a standard clinical case template obtained directly from practicing psychiatrists. This template reflects real-world documentation practices used in psychiatric settings and was adopted to ensure clinical realism and consistency.\\nDemographic Information should include the patient‚Äôs gender, age, educational background, marital status, and occupation. The language should be concise and factual.\\nChief Complaint describes the primary psychological symptoms reported by the patient, along with their duration. The text should be brief, focused, and clinically relevant.\\nMedical Condition covers the current psychiatric presentation in more detail. This includes the onset and course of symptoms, any identifiable triggers, and the impact on the patient‚Äôs daily functioning. General physical health indicators such as sleep patterns, appetite, and weight changes should also be summarized when available.\\nMedical History lists prior medical conditions including chronic diseases, previous hospitalizations, surgical procedures, known allergies, and history of infections. This section helps assess comorbid risks.\\nPersonal History records health-related lifestyle factors such as smoking, alcohol use, and drug consumption. In female patients, it may also include relevant information on menstruation.\\nFamily History identifies psychiatric or neurological disorders in close biological relatives (e.g., parents, siblings). Information should be structured to reflect genetic predisposition or family dynamics relevant to diagnosis.\\nPreliminary Diagnosis provides an initial clinical impression based on the information presented in the EMR. This diagnosis is tentative and may refer to DSM-based categories such as MDD, BD, or ADHD, depending on symptom patterns.\\nTo enrich patient narratives with personalized and semantically coherent context, we design a structured prompt-based mechanism for generating Fictitious Experience Descriptions . The generation is executed by the Tool Agent using an instruction-following large language model.\\nThe input to this process is a template sentence derived from each patient‚Äôs structured EMR, formatted as: ‚ÄúI am a { age } -year-old { gender } with { diagnosis } , currently working or studying as a { occupation } . Past experience: { experience } ‚Äù . The fields { age }, { gender }, { diagnosis }, and { occupation } are directly drawn from the EMR, while { experience } is sampled from the fictitious experience dictionary ùíü fic \\\\mathcal{D}_{\\\\text{fic}} .\\nThe prompt guides the model to craft a first-person narrative that elaborates on the given background, using the following system message:\\n‚ÄúYou are a powerful, imaginative text assistant skilled at writing stories. Given keywords describing a patient‚Äôs mental health background and experiences, imagine and write a detailed, first-person account of the patient‚Äôs past. Expand on the causes, progression, and consequences of a psychologically impactful event. Do not use metaphors, exaggerated language, or fictitious names. Avoid therapeutic expressions like ‚Äòunder the doctor‚Äôs guidance‚Äô or ‚Äòtrying to get better.‚Äô The output should be a coherent and realistic narrative.‚Äù\\nAn example output is shown below :\\nFour years ago, I was a 12-year-old middle school student. I was cheerful by nature, but often had trouble focusing. During class, I couldn‚Äôt help but look around, and my hands were constantly doodling on paper. The teacher was very displeased and reminded me multiple times to pay attention, but it barely worked.\\nOne day in math class, the teacher was explaining a complicated geometry problem. I had stayed up late the night before playing video games, and couldn‚Äôt follow the lesson at all. My attention drifted, and the teacher‚Äôs voice gradually became a blur. Suddenly, the teacher raised his voice, ‚ÄúYou! Stand up!‚Äù I was startled, jumped to my feet, heart pounding, and my face flushed with embarrassment.\\nHe stared at me sternly and said in front of the entire class, ‚ÄúYou‚Äôre not listening again, are you? With that kind of attitude, how do you expect to succeed? Do you think your parents would be proud of this?‚Äù The room fell silent. All eyes were on me. I felt utterly humiliated, and tears welled up in my eyes. The teacher went on: ‚ÄúIf you don‚Äôt take this seriously, you‚Äôll never amount to anything.‚Äù\\nHis words pierced through me like needles. I felt defeated and deeply ashamed. After class, students whispered among themselves‚Äîsome mocked me, others looked at me with pity. I locked myself in the bathroom and cried uncontrollably.\\nFor a while after that, I began to fear going to school. Just entering the classroom would make my palms sweat, and I couldn‚Äôt concentrate at all. My grades plummeted. My parents, anxious and disappointed, constantly scolded me for not studying properly.\\nA few months later, I was diagnosed with depression and ADHD. The doctor said that the public criticism had dealt a severe psychological blow, and combined with academic pressure and high family expectations, it ultimately triggered the onset of my condition.\\nThis prompt design ensures that generated Fictitious Experience Descriptions are aligned with EMR content, emotionally rich, and free from contradictions. It balances creative storytelling with medical plausibility, enabling the system to simulate realistic psychological histories without relying on deterministic templates.\\nThis appendix collates the full hierarchy of the HDSM, aligning each Basic‚Äë, Intermediate‚Äë, and High‚ÄëLevel State with its corresponding clinical topic. States are grouped according to the four SCID‚Äë5‚Äìderived categories:\\n(i) Affective & Cognitive Symptoms,\\n(ii) Physiological & Behavioral Changes,\\n(iii) Functional Impairment & Risk, and\\n(iv) Comorbid or Contributing Factors.\\nFor completeness, terminal nodes that represent each sub‚Äëstate machine‚Äôs final diagnostic outcomes are also listed.\\nThe lists that follow serve as a reference for all experiments, ensuring that every state‚Äôs semantic scope‚Äîand its precise role in the dialogue‚Äëgeneration pipeline‚Äîis clearly documented.\\nI. MDD Sub‚ÄëState Machine (including HBL\\u2005A.1, A.15)\\nA.1: Current Major‚ÄëDepressive‚ÄëEpisode Screening\\nA.15: History of Major‚ÄëDepressive Episodes\\nII. BD Sub‚ÄëState Machine (including HBL\\u2005A.23, A.43, D.1)\\nA.23: Current Manic/Hypomanic Episode\\nA.43: History of Manic/Hypomanic Episodes\\nD.1: Determinative Clauses for Bipolar Diagnosis\\nIII. AD State Machine (including HBL\\u2005F.25, F.31)\\nF.25: Current Generalized-Anxiety-Disorder Symptoms\\nF.31: History of Generalized-Anxiety Disorder\\nIV. ADHD State Machine (including HBL\\u2005K.1)\\nK.1: Comprehensive ADHD Symptom Assessment\\nTo aid interpretability, we provide an English translation of a structured EMR originally generated in Chinese. As shown in Figure\\xa07 , the record describes a 20‚Äì24-year-old unmarried female student with a history of major depressive disorder and generalized anxiety.\\nThe chief complaint includes emotional instability, fatigue, and occasional negative thoughts. Her medical condition outlines symptom progression over 28 months, including episodes of social withdrawal, panic attacks, sleep disturbances, and compulsive behavior, with gradual improvement following psychological intervention. Her past medical history includes SSRI treatment, hospitalization, and CBT, as well as group-based mindfulness therapy and weekly psychiatric supervision.\\nHer family history is positive for bipolar disorder (mother) and antisocial personality disorder (father). Personal history reports menstrual irregularities, no smoking, regular caffeine intake, and a preference for outdoor exercise. Demographics indicate a university education and no history of marriage. The preliminary diagnosis is depression and anxiety.\\nFigure\\xa05 presents the full structure of the Hierarchical Diagnostic State Machine (HDSM), covering all four targeted psychiatric disorders. Each disorder is assigned an independent sub-state machine, with corresponding high-level states (HLS) shown in the figure: A.1 and A.15 for MDD, F.25 and F.31 for AD, A.1, A.43, and D.1 for BD, and K.1 for ADHD.\\nEach HLS contains one or two sub-state groups (dashed rectangles), except D.1, which serves as a summary node in the BD sub-state machine. Sub-state groups are designed based on SCID-5-RV item clusters, preserving clinical logic and content alignment. Each sub-state group has a predefined threshold, typically 3 or 5, for determining whether the collected responses support a positive diagnostic transition. This design enables the system to emulate real-world psychiatric evaluations with high clinical fidelity.\\nTerminal states, such as depression1 through depression5 in the MDD branch, represent mutually exclusive diagnostic outcomes based on structured symptom elicitation. All state transitions, node categories, and thresholds strictly follow the SCID-5-RV guidelines, ensuring consistency between the simulated dialogue flow and professional diagnostic protocols.\\nTo illustrate the structure and quality of PsyCoTalk, we present an example doctor‚Äìpatient dialogue in both Chinese (original) and English (translated) versions. These dialogues are generated by multi-agent interaction under the Diagnosis-Agents framework, where the patient agent is instantiated with a personalized EMR and fictitious experience, and the doctor agent is guided by a structured diagnostic state machine.\\nThe dialogue includes a complete diagnostic process, from initial inquiry to the final diagnosis issued by the doctor agent. We present below an English translation of the original dialogues for illustrative purposes (see Figure\\xa09 ‚Äì Figure\\xa014 ).',\n",
       "    'subsections': [{'title': 'A.1 Use of LLMs',\n",
       "      'paragraphs': 'LLMs were primarily employed for data generation, forming the basis of our multi-agent architecture. For the writing of this paper, we note that LLMs were used only to aid or polish the text, for example in grammar checking and improving readability. They were not involved in generating research ideas or analyzing results.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Dialogue Diversity and Cross-Lingual Analysis',\n",
       "      'paragraphs': 'To address concerns about EMR reuse, we enrich each dialogue with varied backgrounds while retaining symptom consistency.\\nWe measure intra-EMR diversity by 1 ‚àí J \\u200b ( E \\u200b M \\u200b R i ) 1-\\\\mathrm{J}(EMR_{i}) , where\\nwith K j , K k K_{j},K_{k} denoting keyword sets from dialogues generated under the same EMR i i .\\nThe average diversity score is 0.647 ¬± 0.030 0.647\\\\pm 0.030 , i.e., only 35.3% overlap, demonstrating high variability among dialogues derived from the same case.\\nWe further compare synthetic (PsyCoTalk) and real-world clinical dialogues (1,731 patients, with 40.7% comorbid cases) using three diversity metrics:\\nNormalized Entropy H \\u200b ( X ) / log 2 \\u2061 ( V ) H(X)/\\\\log_{2}(V) , where H \\u200b ( X ) H(X) is Shannon entropy and V V is vocabulary size.\\nHapax Proportion N hapax / V N_{\\\\text{hapax}}/V , where N hapax = | { w i ‚àà V ‚à£ f \\u200b ( w i ) = 1 } | N_{\\\\text{hapax}}=|\\\\{w_{i}\\\\in V\\\\mid f(w_{i})=1\\\\}| .\\nSemantic Diversity 1 ‚àí MeanCosSim 1-\\\\text{MeanCosSim} , with MeanCosSim = 2 n \\u200b ( n ‚àí 1 ) \\u200b ‚àë i < j cos \\u2061 ( ùêØ i , ùêØ j ) \\\\text{MeanCosSim}=\\\\tfrac{2}{n(n-1)}\\\\sum_{i<j}\\\\cos(\\\\mathbf{v}_{i},\\\\mathbf{v}_{j}) , where each session s i s_{i} is represented by an embedding vector ùêØ i \\\\mathbf{v}_{i} .\\nTable\\xa05 shows that PsyCoTalk (Chinese) closely matches the diversity of real clinical dialogues, while PsyCoTalk-eng exhibits comparable or even higher diversity than the AlexanderStreet dataset.\\nUsing the same pipeline, we construct a preliminary English dataset (PsyCoTalk-eng, 205 dialogues from 41 patients) and compare it with the AlexanderStreet dataset (Alex, 1,254 dialogues without diagnostic labels). As shown in Table\\xa05 , PsyCoTalk-eng achieves higher normalized entropy and semantic diversity than Alex, while Chinese PsyCoTalk exhibits a higher Hapax proportion but lower semantic diversity than its English counterpart. This discrepancy may be attributed to language structure: Chinese tends to use more unique or low-frequency words, while English dialogues often display greater semantic variation.\\nWe compare word frequency distributions in 200 sampled English and 200 Chinese dialogues, tokenized by NLTK (English) and jieba (Chinese). The top 100 high-frequency words (frequency > 100 >100 ) are categorized into four groups, and the results are summarized in Table\\xa06 . Here, High-freq Count (%) refers to the number and proportion of distinct word types from each category that appear among the top 100 (e.g., 28 Chinese words fall into ‚ÄúSymptoms/States,‚Äù accounting for 35%). Total Freq (%) denotes the cumulative frequency and proportion of all words in that category within the sampled dialogues (e.g., Chinese symptom/state words occur 27,342 times in total, 21.88% of all top-100 occurrences). This dual perspective captures both the lexical coverage (Count) and the usage weight (Total Freq) of different categories across languages.\\nAlthough distributional differences exist, both languages are dominated by descriptive and symptom-related words, with similar category rankings. Figure\\xa06 illustrates representative word clouds for the ‚Äúsymptom/state‚Äù category in both languages, showing numerous synonymous or semantically close pairs (e.g., anxious /ÁÉ¶Ë∫Å, mind /ÂøÉÈáå/ËÑëÂ≠êÈáå).\\nEnglish words exhibit a more even distribution, potentially due to linguistic structural differences.',\n",
       "      'subsections': [{'title': 'A.2.1 Intra-EMR Diversity',\n",
       "        'paragraphs': 'To address concerns about EMR reuse, we enrich each dialogue with varied backgrounds while retaining symptom consistency.\\nWe measure intra-EMR diversity by 1 ‚àí J \\u200b ( E \\u200b M \\u200b R i ) 1-\\\\mathrm{J}(EMR_{i}) , where\\nwith K j , K k K_{j},K_{k} denoting keyword sets from dialogues generated under the same EMR i i .\\nThe average diversity score is 0.647 ¬± 0.030 0.647\\\\pm 0.030 , i.e., only 35.3% overlap, demonstrating high variability among dialogues derived from the same case.',\n",
       "        'subsections': []},\n",
       "       {'title': 'A.2.2 Comparison with Real Clinical Dialogues',\n",
       "        'paragraphs': 'We further compare synthetic (PsyCoTalk) and real-world clinical dialogues (1,731 patients, with 40.7% comorbid cases) using three diversity metrics:\\nNormalized Entropy H \\u200b ( X ) / log 2 \\u2061 ( V ) H(X)/\\\\log_{2}(V) , where H \\u200b ( X ) H(X) is Shannon entropy and V V is vocabulary size.\\nHapax Proportion N hapax / V N_{\\\\text{hapax}}/V , where N hapax = | { w i ‚àà V ‚à£ f \\u200b ( w i ) = 1 } | N_{\\\\text{hapax}}=|\\\\{w_{i}\\\\in V\\\\mid f(w_{i})=1\\\\}| .\\nSemantic Diversity 1 ‚àí MeanCosSim 1-\\\\text{MeanCosSim} , with MeanCosSim = 2 n \\u200b ( n ‚àí 1 ) \\u200b ‚àë i < j cos \\u2061 ( ùêØ i , ùêØ j ) \\\\text{MeanCosSim}=\\\\tfrac{2}{n(n-1)}\\\\sum_{i<j}\\\\cos(\\\\mathbf{v}_{i},\\\\mathbf{v}_{j}) , where each session s i s_{i} is represented by an embedding vector ùêØ i \\\\mathbf{v}_{i} .\\nTable\\xa05 shows that PsyCoTalk (Chinese) closely matches the diversity of real clinical dialogues, while PsyCoTalk-eng exhibits comparable or even higher diversity than the AlexanderStreet dataset.',\n",
       "        'subsections': []},\n",
       "       {'title': 'A.2.3 English Data and Cross-Lingual Comparison',\n",
       "        'paragraphs': 'Using the same pipeline, we construct a preliminary English dataset (PsyCoTalk-eng, 205 dialogues from 41 patients) and compare it with the AlexanderStreet dataset (Alex, 1,254 dialogues without diagnostic labels). As shown in Table\\xa05 , PsyCoTalk-eng achieves higher normalized entropy and semantic diversity than Alex, while Chinese PsyCoTalk exhibits a higher Hapax proportion but lower semantic diversity than its English counterpart. This discrepancy may be attributed to language structure: Chinese tends to use more unique or low-frequency words, while English dialogues often display greater semantic variation.',\n",
       "        'subsections': []},\n",
       "       {'title': 'A.2.4 Word Frequency and Part-of-Speech Comparison',\n",
       "        'paragraphs': 'We compare word frequency distributions in 200 sampled English and 200 Chinese dialogues, tokenized by NLTK (English) and jieba (Chinese). The top 100 high-frequency words (frequency > 100 >100 ) are categorized into four groups, and the results are summarized in Table\\xa06 . Here, High-freq Count (%) refers to the number and proportion of distinct word types from each category that appear among the top 100 (e.g., 28 Chinese words fall into ‚ÄúSymptoms/States,‚Äù accounting for 35%). Total Freq (%) denotes the cumulative frequency and proportion of all words in that category within the sampled dialogues (e.g., Chinese symptom/state words occur 27,342 times in total, 21.88% of all top-100 occurrences). This dual perspective captures both the lexical coverage (Count) and the usage weight (Total Freq) of different categories across languages.\\nAlthough distributional differences exist, both languages are dominated by descriptive and symptom-related words, with similar category rankings. Figure\\xa06 illustrates representative word clouds for the ‚Äúsymptom/state‚Äù category in both languages, showing numerous synonymous or semantically close pairs (e.g., anxious /ÁÉ¶Ë∫Å, mind /ÂøÉÈáå/ËÑëÂ≠êÈáå).\\nEnglish words exhibit a more even distribution, potentially due to linguistic structural differences.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'A.3 EMR Section Guidelines',\n",
       "      'paragraphs': 'This appendix provides detailed specifications for each section in the standardized EMR template developed in this study. The content structure was developed based on a standard clinical case template obtained directly from practicing psychiatrists. This template reflects real-world documentation practices used in psychiatric settings and was adopted to ensure clinical realism and consistency.\\nDemographic Information should include the patient‚Äôs gender, age, educational background, marital status, and occupation. The language should be concise and factual.\\nChief Complaint describes the primary psychological symptoms reported by the patient, along with their duration. The text should be brief, focused, and clinically relevant.\\nMedical Condition covers the current psychiatric presentation in more detail. This includes the onset and course of symptoms, any identifiable triggers, and the impact on the patient‚Äôs daily functioning. General physical health indicators such as sleep patterns, appetite, and weight changes should also be summarized when available.\\nMedical History lists prior medical conditions including chronic diseases, previous hospitalizations, surgical procedures, known allergies, and history of infections. This section helps assess comorbid risks.\\nPersonal History records health-related lifestyle factors such as smoking, alcohol use, and drug consumption. In female patients, it may also include relevant information on menstruation.\\nFamily History identifies psychiatric or neurological disorders in close biological relatives (e.g., parents, siblings). Information should be structured to reflect genetic predisposition or family dynamics relevant to diagnosis.\\nPreliminary Diagnosis provides an initial clinical impression based on the information presented in the EMR. This diagnosis is tentative and may refer to DSM-based categories such as MDD, BD, or ADHD, depending on symptom patterns.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.4 Prompt details for virtual experience generation',\n",
       "      'paragraphs': 'To enrich patient narratives with personalized and semantically coherent context, we design a structured prompt-based mechanism for generating Fictitious Experience Descriptions . The generation is executed by the Tool Agent using an instruction-following large language model.\\nThe input to this process is a template sentence derived from each patient‚Äôs structured EMR, formatted as: ‚ÄúI am a { age } -year-old { gender } with { diagnosis } , currently working or studying as a { occupation } . Past experience: { experience } ‚Äù . The fields { age }, { gender }, { diagnosis }, and { occupation } are directly drawn from the EMR, while { experience } is sampled from the fictitious experience dictionary ùíü fic \\\\mathcal{D}_{\\\\text{fic}} .\\nThe prompt guides the model to craft a first-person narrative that elaborates on the given background, using the following system message:\\n‚ÄúYou are a powerful, imaginative text assistant skilled at writing stories. Given keywords describing a patient‚Äôs mental health background and experiences, imagine and write a detailed, first-person account of the patient‚Äôs past. Expand on the causes, progression, and consequences of a psychologically impactful event. Do not use metaphors, exaggerated language, or fictitious names. Avoid therapeutic expressions like ‚Äòunder the doctor‚Äôs guidance‚Äô or ‚Äòtrying to get better.‚Äô The output should be a coherent and realistic narrative.‚Äù\\nAn example output is shown below :\\nFour years ago, I was a 12-year-old middle school student. I was cheerful by nature, but often had trouble focusing. During class, I couldn‚Äôt help but look around, and my hands were constantly doodling on paper. The teacher was very displeased and reminded me multiple times to pay attention, but it barely worked.\\nOne day in math class, the teacher was explaining a complicated geometry problem. I had stayed up late the night before playing video games, and couldn‚Äôt follow the lesson at all. My attention drifted, and the teacher‚Äôs voice gradually became a blur. Suddenly, the teacher raised his voice, ‚ÄúYou! Stand up!‚Äù I was startled, jumped to my feet, heart pounding, and my face flushed with embarrassment.\\nHe stared at me sternly and said in front of the entire class, ‚ÄúYou‚Äôre not listening again, are you? With that kind of attitude, how do you expect to succeed? Do you think your parents would be proud of this?‚Äù The room fell silent. All eyes were on me. I felt utterly humiliated, and tears welled up in my eyes. The teacher went on: ‚ÄúIf you don‚Äôt take this seriously, you‚Äôll never amount to anything.‚Äù\\nHis words pierced through me like needles. I felt defeated and deeply ashamed. After class, students whispered among themselves‚Äîsome mocked me, others looked at me with pity. I locked myself in the bathroom and cried uncontrollably.\\nFor a while after that, I began to fear going to school. Just entering the classroom would make my palms sweat, and I couldn‚Äôt concentrate at all. My grades plummeted. My parents, anxious and disappointed, constantly scolded me for not studying properly.\\nA few months later, I was diagnosed with depression and ADHD. The doctor said that the public criticism had dealt a severe psychological blow, and combined with academic pressure and high family expectations, it ultimately triggered the onset of my condition.\\nThis prompt design ensures that generated Fictitious Experience Descriptions are aligned with EMR content, emotionally rich, and free from contradictions. It balances creative storytelling with medical plausibility, enabling the system to simulate realistic psychological histories without relying on deterministic templates.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.5 Hierarchical Diagnostic State Taxonomy and Topic Mapping',\n",
       "      'paragraphs': 'This appendix collates the full hierarchy of the HDSM, aligning each Basic‚Äë, Intermediate‚Äë, and High‚ÄëLevel State with its corresponding clinical topic. States are grouped according to the four SCID‚Äë5‚Äìderived categories:\\n(i) Affective & Cognitive Symptoms,\\n(ii) Physiological & Behavioral Changes,\\n(iii) Functional Impairment & Risk, and\\n(iv) Comorbid or Contributing Factors.\\nFor completeness, terminal nodes that represent each sub‚Äëstate machine‚Äôs final diagnostic outcomes are also listed.\\nThe lists that follow serve as a reference for all experiments, ensuring that every state‚Äôs semantic scope‚Äîand its precise role in the dialogue‚Äëgeneration pipeline‚Äîis clearly documented.\\nI. MDD Sub‚ÄëState Machine (including HBL\\u2005A.1, A.15)\\nA.1: Current Major‚ÄëDepressive‚ÄëEpisode Screening\\nA.15: History of Major‚ÄëDepressive Episodes\\nII. BD Sub‚ÄëState Machine (including HBL\\u2005A.23, A.43, D.1)\\nA.23: Current Manic/Hypomanic Episode\\nA.43: History of Manic/Hypomanic Episodes\\nD.1: Determinative Clauses for Bipolar Diagnosis\\nIII. AD State Machine (including HBL\\u2005F.25, F.31)\\nF.25: Current Generalized-Anxiety-Disorder Symptoms\\nF.31: History of Generalized-Anxiety Disorder\\nIV. ADHD State Machine (including HBL\\u2005K.1)\\nK.1: Comprehensive ADHD Symptom Assessment',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.6 Case Study of PsyCoProfile',\n",
       "      'paragraphs': 'To aid interpretability, we provide an English translation of a structured EMR originally generated in Chinese. As shown in Figure\\xa07 , the record describes a 20‚Äì24-year-old unmarried female student with a history of major depressive disorder and generalized anxiety.\\nThe chief complaint includes emotional instability, fatigue, and occasional negative thoughts. Her medical condition outlines symptom progression over 28 months, including episodes of social withdrawal, panic attacks, sleep disturbances, and compulsive behavior, with gradual improvement following psychological intervention. Her past medical history includes SSRI treatment, hospitalization, and CBT, as well as group-based mindfulness therapy and weekly psychiatric supervision.\\nHer family history is positive for bipolar disorder (mother) and antisocial personality disorder (father). Personal history reports menstrual irregularities, no smoking, regular caffeine intake, and a preference for outdoor exercise. Demographics indicate a university education and no history of marriage. The preliminary diagnosis is depression and anxiety.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.7 HDSM Framework',\n",
       "      'paragraphs': 'Figure\\xa05 presents the full structure of the Hierarchical Diagnostic State Machine (HDSM), covering all four targeted psychiatric disorders. Each disorder is assigned an independent sub-state machine, with corresponding high-level states (HLS) shown in the figure: A.1 and A.15 for MDD, F.25 and F.31 for AD, A.1, A.43, and D.1 for BD, and K.1 for ADHD.\\nEach HLS contains one or two sub-state groups (dashed rectangles), except D.1, which serves as a summary node in the BD sub-state machine. Sub-state groups are designed based on SCID-5-RV item clusters, preserving clinical logic and content alignment. Each sub-state group has a predefined threshold, typically 3 or 5, for determining whether the collected responses support a positive diagnostic transition. This design enables the system to emulate real-world psychiatric evaluations with high clinical fidelity.\\nTerminal states, such as depression1 through depression5 in the MDD branch, represent mutually exclusive diagnostic outcomes based on structured symptom elicitation. All state transitions, node categories, and thresholds strictly follow the SCID-5-RV guidelines, ensuring consistency between the simulated dialogue flow and professional diagnostic protocols.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.8 Case Study of PsyCoTalk',\n",
       "      'paragraphs': 'To illustrate the structure and quality of PsyCoTalk, we present an example doctor‚Äìpatient dialogue in both Chinese (original) and English (translated) versions. These dialogues are generated by multi-agent interaction under the Diagnosis-Agents framework, where the patient agent is instantiated with a personalized EMR and fictitious experience, and the doctor agent is guided by a structured diagnostic state machine.\\nThe dialogue includes a complete diagnostic process, from initial inquiry to the final diagnosis issued by the doctor agent. We present below an English translation of the original dialogues for illustrative purposes (see Figure\\xa09 ‚Äì Figure\\xa014 ).',\n",
       "      'subsections': []}]}],\n",
       "  'abstract': 'Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research.\\n Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.'},\n",
       " '2510.25563v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'The ocean plays a crucial role in multiple aspects of the planet, from regulating the global climate [ 1 ] to the availability of strategic marine resources [ 2 ] . Understanding its processes and predicting changes is essential for decision-making in the fishing industry and maritime transport sectors [ 3 ] . Anticipating the behavior of oceanographic variables is important in the current scenario [ 4 ] , marked by climate change and the urgent need for sustainable policies [ 5 ] , including the development of new tools to assess the present and future states of the Earth, as current approaches are still limited [ 6 ] .\\nOcean analysis is critical since it influences global climate, storm formation, and heat exchange with the atmosphere through sea surface temperature [ 7 ] . Additionally, marine biodiversity depends on the dynamic balance of the ocean‚Äôs physical and chemical properties. Human activities also require information about ocean conditions to ensure safety, optimize maritime routes, manage energy resources, and implement ecological policies [ 8 ] .\\nTraditional ocean prediction methodologies rely on numerical models that solve complex fluid dynamics equations. However, these models struggle to capture data across multiple spatial and temporal scales and have high computational costs. These limitations, combined with the increasing volume of data, create new research opportunities. In this context, deep learning has emerged as a promising alternative to overcome these challenges, enabling the processing of large volumes of data across different scales. Furthermore, with the growing quantity and diversity of data from in situ sources, satellites, and reanalysis, we now have an opportunity to address these challenges [ 9 ] .\\nThese new models learn complex relationships from data without explicitly relying on physical equations. By removing the strict dependence on theoretical formulations, it becomes feasible to capture patterns that emerge from the interaction of multiple variables. This approach can provide more detailed solutions, reduce computational costs, and integrate heterogeneous data. Models based on deep learning could enable the development of more efficient methods in various scenarios [ 10 ] .\\nThe field of atmospheric prediction has witnessed a transformative shift with the advent of deep learning models. Recent breakthroughs include models like FourCastNet [ 11 ] , known for its speed and accuracy in predicting extreme weather events, and GraphCast [ 12 ] , which demonstrates remarkable skill in medium-range global weather forecasting based on Graph Neural Networks. Furthermore, models such as NeuralGCM [ 13 ] explore the integration of neural networks within general circulation models, aiming to enhance the representation of sub-grid scale processes. More recently, the Aurora [ 14 ] model has emerged, showcasing the potential of deep learning to not only predict atmospheric conditions but also to extend these capabilities to coupled systems by learning complex interactions across different Earth system components. These advancements signify a new era in weather and climate modeling, promising more efficient and accurate predictions for a range of applications.\\nThis work aims to adapt the Aurora foundational model, initially developed for global atmospheric applications, to subregional ocean prediction. The model is fine-tuned for predicting the evolution of the SST in the Canary Upwelling System. We are interested in adapting the model for predicting the ocean‚Äôs potential temperature ( Œ∏ 0 \\\\theta_{0} ) in a specific geographic area and period. Its three-dimensional architecture enables the efficient integration of complex and heterogeneous data. Additionally, its reduced training time, ability to leverage previously learned representations, and flexibility to incorporate data from various sources make it an ideal candidate for tackling current challenges in ocean prediction.\\nWe use data from the GLORYS12V1 [ 15 ] ocean prediction system of the Copernicus Marine Service [ 16 ] . This corresponds to the global ocean physical reanalysis, which provides three-dimensional daily fields with a horizontal resolution of 1/12¬∞ and 50 vertical levels. This dataset, based on the assimilation of satellite and in situ observations, provides information on potential temperature, salinity, currents, and two-dimensional variables, such as sea level and mixed-layer thickness.\\nThis model offers advantages in terms of computational cost reduction and prediction quality, although limitations exist due to the complexity of the domain and physical constraints. This should be seen as complementary to other methodologies rather than a complete replacement.\\nSection 2 summarizes state-of-the-art works. Section 3 details the dataset used in this work and the area of study. Section 4 explains the Aurora model and how we adapted it to our problem. The experimental configuration in Section 5 addresses various details about the training process, the loss function, and the metrics used to evaluate the model. Section 6 tackles the fine-tuning process and shows some preliminary results. Finally, the conclusions in Section 8 summarize the main ideas and contributions of the work and propose some ideas for future research.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Prediction in oceanography is essential for anticipating and understanding natural phenomena that affect global climate and various human activities [ 17 ] . Sea temperature plays a fundamental role in the distribution of water masses, global circulation, and the ocean-atmosphere energy balance [ 18 , 19 ] . Thus, estimating changes in the physical and chemical properties of the ocean, such as temperature, is crucial for multiple activities [ 20 ] , in addition to being key to anticipating natural risks.\\nOcean observation and prediction results from global efforts [ 10 ] , in which multiple national and international agencies contribute scientific data. Among them, the World Meteorological Organization (WMO) stands out as it establishes frameworks for international cooperation in data exchange, standardization of procedures, and guidelines that facilitate climate understanding on a planetary scale [ 21 ] .\\nThe Copernicus Program, promoted by the EU in collaboration with the ESA, provides services and information about the ocean [ 16 ] . The Copernicus Marine Environment Monitoring Service (CMEMS) integrates satellite observations, in situ measurements, and numerical models to offer various oceanographic products. This service supplies data on sea temperature, salinity, currents, and sea level variables. As a result, it provides a solid foundation for scientific studies, operational applications, and decision-making in marine environments [ 22 ] . On the other hand, the USA National Oceanic and Atmospheric Administration (NOAA) also collects and processes oceanic and atmospheric data. These data and products have become standard references for the international community [ 23 ] . Additionally, ECMWF provides reanalysis and numerical models, globally recognized for their quality and accuracy [ 24 ] .\\nSatellite observations have revolutionized ocean monitoring. Missions such as ESA and Copernicus‚Äô Sentinel-3 or NASA‚Äôs MODIS [ 25 ] allow for detailed information on sea surface temperature, sea level height, surface salinity, and chlorophyll concentration‚Äîamong other variables‚Äîcharacterized by high spatial and temporal resolution. However, satellite measurements can be affected by clouds or other atmospheric factors. To compensate for these shortcomings and obtain a more comprehensive view, it is necessary to combine these observations with other data sources [ 26 ] .\\nIn situ data come from direct measurements taken by buoys, research vessels, fixed platforms, and autonomous underwater vehicles [ 27 ] . These measurements provide precise data on key variables. Although these measurements have limited spatial and temporal coverage, their high quality and detail are indispensable for calibration, validation, and assimilation in numerical models. Reanalysis and numerical models integrate satellite and in situ data, offering coherent representations of the ocean.\\nNumerical Weather Prediction (NWP) models have been essential for predicting ocean dynamics for decades. These models are based on fluid dynamics and thermodynamics equations, solving the evolution of fundamental variables over three-dimensional grids that cover the globe [ 28 ] . The main advantage of NWP lies in its physical consistency, as it is based on fundamental laws governing ocean motion. However, it also has some limitations, such as the high computational cost required to make predictions, which makes high-resolution simulations expensive. Additionally, the complexity of ocean-atmosphere interactions leads to small errors that accumulate over time, reducing the accuracy of long-term predictions and limiting the ability to capture small-scale phenomena [ 29 ] .\\nThese challenges have led to methodologies aimed at increasing the capacity to process large volumes of data in a computationally scalable manner [ 30 ] . In this regard, deep learning has emerged as a promising approach to improving efficiency and accuracy when working with heterogeneous data.\\nInitially, CNNs [ 31 ] gained popularity in meteorology and oceanography due to their ability to capture spatial patterns in two-dimensional data such as temperature maps or ocean currents. Some use cases included precipitation prediction and identifying spatial patterns in the ocean [ 32 ] . However, these models were inefficient at capturing temporal dependencies or three-dimensional structures, limiting their effectiveness for more complex tasks.\\nLater, with the emergence of graph neural networks (GNN), GraphCast [ 12 ] was introduced in 2023, a model capable of tackling medium-range global weather prediction with high detail. This approach leverages graph structures to capture complex spatial relationships, showing promising results in forecasting meteorological and climate variables. Additionally, diffusion-based methods such as GenCast [ 33 ] were developed to integrate uncertainty into the models. This stochastic approach generates multiple future states, making it a valuable tool for scenario analysis and risk assessment, particularly in capturing ocean variability. The Artificial Intelligence/Integrated Forecasting System (AIFS) [ 34 ] is an evolution of GraphCast in which the processor is replaced with a shifted window attention Transformer.\\nGNNs have been adapted to ocean prediction. For instance, SeaCast [ 35 ] implements medium-range ocean forecasting in a limited area region defined by the Mediterranean Sea, and a more recent approach [ 36 , 37 ] predicts the SST in the region of the Canary Islands and the Northwest African coast. The importance of the underlying mesh in these architectures has been studied in [ 38 ] : Structured meshes produce artifacts in medium-range forecasting, which can be tackled using bathymetry-aware unstructured meshes, as shown in [ 39 ] .\\nOne of the most significant breakthroughs in this trajectory came with Transformers [ 40 ] . Their attention mechanism enabled a more efficient representation of spatial and temporal relationships. FourCastNet [ 11 ] integrated adaptive Fourier operators to perform fast and accurate global climate predictions while reducing computational complexity. The three-dimensional architecture of Pangu-Weather [ 41 ] improved vertical representation and the ability to capture complex atmospheric phenomena.\\nOver time, Transformers were further adapted for oceanography. For instance, XiHe [ 42 ] , a model designed for global ocean eddy-resolving forecasting, combined the GLORYS12 reanalysis with data from ERA5 [ 43 ] and satellite observations to anticipate the evolution of physical variables, providing high-resolution estimates. Another example is Orca [ 44 ] , which employed an encoder-based architecture, a fusion module, and a decoder with temporal attention mechanisms to simulate global ocean circulation.\\nMore recently, a three-dimensional foundational model, Aurora [ 14 ] , combined pretraining on vast amounts of heterogeneous data with a fine-tuning process tailored to different scales and variables. Inspired by successes in other fields, such as protein structure prediction and natural language processing [ 45 , 46 ] , Aurora is capable of generating high-resolution forecasts with lower computational costs and greater accuracy in extreme events. This model stands out for its ability to handle sparse and heterogeneous data while overcoming the limitations of other architectures by integrating with different types of climate data.\\nTable 1 illustrates the evolution of deep learning in climate and oceanographic prediction over time. Organized chronologically, this table helps us understand improvements in resolution, the type and quantity of variables that can be handled, and whether the approach is deterministic (D) or stochastic (E).',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Dataset and Study Area',\n",
       "    'paragraphs': 'This section presents the data used in this work, including the study area, period, and selected variables. It details the data source, the rationale for choosing the Northeastern Atlantic region, and the time frame. The complexity of patterns in the study area is also examined. Finally, it explains the temporal division of data for training, validation, and testing.\\nThe CMEMS [ 47 ] is a European service dedicated to monitoring the marine environment, providing a wide range of global and regional oceanic data for the scientific community. Its products are organized into different thematic categories: the Blue Ocean, which focuses on physical variables such as temperature, salinity, sea level, and currents; the Green Ocean, which covers biogeochemical variables like chlorophyll concentration and dissolved oxygen; and the White Ocean, which includes data on sea ice and polar regions. This classification enables users to filter information based on specific project needs, ensuring high-quality and relevant data [ 16 ] .\\nIn this work, we selected the Blue Ocean category, specifically the GLOBAL_MULTIYEAR_PHY_001_030 product, which is formulated as a global physical ocean reanalysis. This product spans from 1993 to 2021, offering data with a spatial resolution of 1 / 12 o 1/12^{o} and fifty vertical levels. The GLORYS12V1 ocean prediction system is part of the reanalysis operated by Mercator Ocean, based on the NEMO model. This scheme integrates techniques such as SEEK to assimilate satellite data (sea level altimetry, sea surface temperature) and in situ observations (ARGO profiles and oceanographic cruises), achieving a coherent and physically consistent representation of the global ocean environment [ 48 , 49 ] . The primary function of this service is to offer reliable and up-to-date information on ocean conditions.\\nThe product includes multiple data configurations: daily and monthly averages of three-dimensional variables such as potential temperature, salinity, and currents, as well as surface variables like sea level height, mixed layer thickness, and sea ice characteristics. This structure allows the analysis of mesoscale variability and seasonal patterns, providing a versatile dataset suitable for dynamic and climate studies.\\nThe data is available in NetCDF format, which follows the CF-1.6 convention, ensuring standardization of variables and metadata [ 50 , 51 ] .\\nThe selection of this product is driven by the need for a dataset that combines high spatial resolution with extensive temporal coverage and rigorous scientific quality [ 47 ] . This approach enables the capture of key dynamic phenomena, such as ocean eddies and fronts, while also allowing for the analysis of long-term trends and seasonal cycles.\\nOur study area is limited to the Northeastern Atlantic region, near the African coast and off the Canary Islands, an area of interest due to its intense coastal upwelling phenomena. This region is characterized by high biological productivity and a significant influence on the regional climate [ 52 ] .\\nFigure 1 shows a map of the selected area, highlighting key geographic features relevant to coastal upwellings, such as Capes Ghir and Juby. These capes are crucial in coastal currents and nutrient transport to the surface, promoting phytoplankton proliferation [ 19 ] . Tabla 2 shows the coordinates that define our area of interest.\\nThis region is characterized by a strong influence of the trade winds and the Canary Current, both considered main factors of coastal upwelling, affecting biodiversity and local dynamics [ 20 ] . Our data period extends from January 1, 2014, to January 1, 2021. This seven-year interval allows the exploration of seasonal variations and medium-term trends.\\nOur dataset contains a series of variables that provide a general overview of ocean dynamics. These variables range from potential temperature to salinity, sea level height, mixed layer depth, and zonal and meridional components of current velocity [ 7 ] .\\nFigure 2 shows a set of maps for each variable, representing the region of interest. Different patterns highlight the influence of the trade winds and coastal upwelling, showing these factors as generators of distinct thermal and saline gradients.\\nIn this work, we use the potential temperature, Œ∏ 0 \\\\theta_{0} . It is defined as the temperature of the water brought to atmospheric pressure. This concept eliminates the effects that pressure generates at different depths. Hence, potential temperature becomes a fundamental tool for comparing water masses and studying the thermal structure of the ocean [ 53 ] . The Œ∏ 0 \\\\theta_{0} map shows that the warmest waters are located in the south at the open ocean, while the coldest waters are found in the north and the African coast, in the upwelling region. This pattern is influenced by coastal upwelling and the interactions between the Canary Current and the trade winds [ 20 ] . The Œ∏ 0 \\\\theta_{0} map also shows thermal gradients along the coast, indicating active mixing processes. This behavior affects marine biodiversity, as colder waters transport nutrients from the depths to the surface [ 6 ] .\\nIn addition to its relevance in thermal structure, Œ∏ 0 \\\\theta_{0} is essential for understanding heat exchange processes between the ocean and the atmosphere [ 18 ] , thus, it becomes a fundamental tool for analyzing seasonal evolution and its impact on the ocean [ 19 ] .\\nOn the other hand, salinity shows a higher concentration in the open ocean, possibly due to thermal influence and ocean circulation [ 7 ] . Sea level height shows patterns of mesoscale dynamics, evidencing depressions and elevations characteristic of ocean eddies [ 30 ] . Mixed layer depth shows low values near the coast, reflecting the influence of surface waters and upwelling masses.\\nThe integration of these variables allows for a holistic view of ocean dynamics in this region. In particular, Œ∏ 0 \\\\theta_{0} help us understand the thermal structure and its relationship with physical, biological, and climatic processes. This approach is essential for modeling and predicting the evolution of marine systems in the Northeastern Atlantic [ 54 ] .\\nFigure 3 shows a time series of potential temperature during the study period, elucidating seasonal variations.',\n",
       "    'subsections': [{'title': '3.1 Global Ocean Physics Reanalysis Data',\n",
       "      'paragraphs': 'The CMEMS [ 47 ] is a European service dedicated to monitoring the marine environment, providing a wide range of global and regional oceanic data for the scientific community. Its products are organized into different thematic categories: the Blue Ocean, which focuses on physical variables such as temperature, salinity, sea level, and currents; the Green Ocean, which covers biogeochemical variables like chlorophyll concentration and dissolved oxygen; and the White Ocean, which includes data on sea ice and polar regions. This classification enables users to filter information based on specific project needs, ensuring high-quality and relevant data [ 16 ] .\\nIn this work, we selected the Blue Ocean category, specifically the GLOBAL_MULTIYEAR_PHY_001_030 product, which is formulated as a global physical ocean reanalysis. This product spans from 1993 to 2021, offering data with a spatial resolution of 1 / 12 o 1/12^{o} and fifty vertical levels. The GLORYS12V1 ocean prediction system is part of the reanalysis operated by Mercator Ocean, based on the NEMO model. This scheme integrates techniques such as SEEK to assimilate satellite data (sea level altimetry, sea surface temperature) and in situ observations (ARGO profiles and oceanographic cruises), achieving a coherent and physically consistent representation of the global ocean environment [ 48 , 49 ] . The primary function of this service is to offer reliable and up-to-date information on ocean conditions.\\nThe product includes multiple data configurations: daily and monthly averages of three-dimensional variables such as potential temperature, salinity, and currents, as well as surface variables like sea level height, mixed layer thickness, and sea ice characteristics. This structure allows the analysis of mesoscale variability and seasonal patterns, providing a versatile dataset suitable for dynamic and climate studies.\\nThe data is available in NetCDF format, which follows the CF-1.6 convention, ensuring standardization of variables and metadata [ 50 , 51 ] .\\nThe selection of this product is driven by the need for a dataset that combines high spatial resolution with extensive temporal coverage and rigorous scientific quality [ 47 ] . This approach enables the capture of key dynamic phenomena, such as ocean eddies and fronts, while also allowing for the analysis of long-term trends and seasonal cycles.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Area and Period of Study',\n",
       "      'paragraphs': 'Our study area is limited to the Northeastern Atlantic region, near the African coast and off the Canary Islands, an area of interest due to its intense coastal upwelling phenomena. This region is characterized by high biological productivity and a significant influence on the regional climate [ 52 ] .\\nFigure 1 shows a map of the selected area, highlighting key geographic features relevant to coastal upwellings, such as Capes Ghir and Juby. These capes are crucial in coastal currents and nutrient transport to the surface, promoting phytoplankton proliferation [ 19 ] . Tabla 2 shows the coordinates that define our area of interest.\\nThis region is characterized by a strong influence of the trade winds and the Canary Current, both considered main factors of coastal upwelling, affecting biodiversity and local dynamics [ 20 ] . Our data period extends from January 1, 2014, to January 1, 2021. This seven-year interval allows the exploration of seasonal variations and medium-term trends.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 The Potential Temperature',\n",
       "      'paragraphs': 'Our dataset contains a series of variables that provide a general overview of ocean dynamics. These variables range from potential temperature to salinity, sea level height, mixed layer depth, and zonal and meridional components of current velocity [ 7 ] .\\nFigure 2 shows a set of maps for each variable, representing the region of interest. Different patterns highlight the influence of the trade winds and coastal upwelling, showing these factors as generators of distinct thermal and saline gradients.\\nIn this work, we use the potential temperature, Œ∏ 0 \\\\theta_{0} . It is defined as the temperature of the water brought to atmospheric pressure. This concept eliminates the effects that pressure generates at different depths. Hence, potential temperature becomes a fundamental tool for comparing water masses and studying the thermal structure of the ocean [ 53 ] . The Œ∏ 0 \\\\theta_{0} map shows that the warmest waters are located in the south at the open ocean, while the coldest waters are found in the north and the African coast, in the upwelling region. This pattern is influenced by coastal upwelling and the interactions between the Canary Current and the trade winds [ 20 ] . The Œ∏ 0 \\\\theta_{0} map also shows thermal gradients along the coast, indicating active mixing processes. This behavior affects marine biodiversity, as colder waters transport nutrients from the depths to the surface [ 6 ] .\\nIn addition to its relevance in thermal structure, Œ∏ 0 \\\\theta_{0} is essential for understanding heat exchange processes between the ocean and the atmosphere [ 18 ] , thus, it becomes a fundamental tool for analyzing seasonal evolution and its impact on the ocean [ 19 ] .\\nOn the other hand, salinity shows a higher concentration in the open ocean, possibly due to thermal influence and ocean circulation [ 7 ] . Sea level height shows patterns of mesoscale dynamics, evidencing depressions and elevations characteristic of ocean eddies [ 30 ] . Mixed layer depth shows low values near the coast, reflecting the influence of surface waters and upwelling masses.\\nThe integration of these variables allows for a holistic view of ocean dynamics in this region. In particular, Œ∏ 0 \\\\theta_{0} help us understand the thermal structure and its relationship with physical, biological, and climatic processes. This approach is essential for modeling and predicting the evolution of marine systems in the Northeastern Atlantic [ 54 ] .\\nFigure 3 shows a time series of potential temperature during the study period, elucidating seasonal variations.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 A Foundational Model for Forecasting the SST',\n",
       "    'paragraphs': 'This section explains the Aurora model and how we adapted it to our purposes. It is a foundational model for atmospheric prediction, but its versatility permits it to be adapted to oceanography. It can process heterogeneous data with multiple variables, pressure levels, and spatial resolutions. These characteristics make Aurora a flexible and scalable system capable of forecasting the physical state of the ocean. This model can be adapted to more limited contexts with scarce data.\\nThe scalability of this model is largely due to the local window attention mechanism and the U-Net [ 55 ] architecture. Both allow for information extraction at multiple scales. This multiscale capability is essential for capturing large- and small-scale structures. The model generates long-term forecasts by iteratively applying its prediction function in an autoregressive way:\\nwhere X ^ t + 1 \\\\hat{X}^{t+1} is the prediction at time t + 1 t+1 and Œ¶ \\\\Phi is the model function. This scheme, analogous to recurrent networks, extends the temporal horizon, although it requires controlling the accumulation of errors. Aurora benefits from parallel training and mixed precision, reducing memory usage without degrading prediction quality. This facilitates its practical implementation in operational contexts, allowing for increased resolution without incurring prohibitive computational costs.\\nAurora presents multiple configurations, differentiated by the number of parameters, the size of the embeddings, and the number of layers in the encoder and decoder. These variations impact computational performance and the model‚Äôs ability to capture complex relationships.\\nTable 3 shows the size of the different configurations. We chose the medium-size model, looking for a balance between complexity and computational cost. Additionally, we adopted mixed precision to optimize memory.\\nThe model‚Äôs architecture consists of three components: the encoder , the processor , and the decoder . The encoder transforms raw inputs into a normalized internal representation, the processor temporally evolves this representation, capturing significant relationships between the input data, and the decoder restores the variables to their original resolution and levels. The architecture benefits from three-dimensional Transformers and attention mechanisms [ 40 , 56 ] , integrated with U-Net. These components allow feature extraction at various scales, processing data with different resolutions, levels, and static variables.\\nThe objective of the encoder is to unify the inputs into a coherent three-dimensional representation. It converts heterogeneous data into a uniform internal representation, preparing it for the temporal and multiscale processing of the next module. It employs tokenization and compression of climatic states through cross-attention blocks based on a Perceiver IO [ 57 ] model. Thus, each atmospheric and surface variable is represented in a three-dimensional volume, incorporating Fourier encodings that add spatial and temporal information [ 56 ] .\\nTo normalize the information, embeddings based on sinusoidal encodings have been used, assigning each pressure level p l p_{l} a representation of the type:\\nThese embeddings integrate vertical information, reducing pressure levels to a fixed set of latent levels C L C_{L} through Perceiver IO attention mechanisms. This allows the model to work with a manageable number of levels, maintaining the representativeness of vertical conditions while balancing computational cost. Dynamic and static variables are tokenized, incorporating positional and Fourier encoding, which unifies the differences between levels and resolutions.\\nThe backbone is the temporal core of the model, responsible for evolving the internal representation over time. To achieve this, it relies on a 3D Swin Transformer [ 58 , 59 ] , organized with a U-Net structure, which allows it to capture features at multiple scales.\\nAn internal encoder gradually reduces spatial resolution and increases semantic depth. This is followed by an internal decoder, which recovers the original resolution and integrates the information obtained at each level. This hierarchical arrangement combines global and local information, capturing large-scale patterns and more detailed structures.\\nAttention is implemented through local windows, allowing each layer to process spatial subsets. This reduces computational complexity by avoiding global attention that spans entire layers. This local attention is given by:\\nwhere ùêê \\\\mathbf{Q} , ùêä \\\\mathbf{K} , and ùêï \\\\mathbf{V} are the query, key, and value matrices, respectively, and d k d_{k} is the dimension of the keys. These local windows limit attention operations to specific regions. This strategy makes Aurora scalable, allowing it to handle higher resolutions without increasing computational cost.\\nThe result is an enriched representation in which spatiotemporal relationships are explicitly modeled at multiple levels. This representation is then configured to be translated back to the original variables in the decoder.\\nThe latent representation of the variables, generated by the processor, is restored to their original pressure levels and resolution in the decoder. This involves using inverse modules based on Perceiver IO models, which can reconstruct the desired variables from the internal representations, assigning each level and variable its corresponding spatial characteristics.\\nEach variable, v v , and level, l l , is obtained by applying a linear transformation and regrouping spatial patches until the original grid is recovered. Thus, the decoder ensures that the final predictions maintain coherence with the input dimensions and the information captured by the model is translated into physically interpretable forecasts.\\nAdapting the model to oceanographic variables requires fundamental differences from atmospheric conditions. While Aurora was pre-trained assuming distributions and ranges typical of atmospheric variables (normalized in Kelvin and with pressure levels), oceanic variables, such as potential temperature, present narrower ranges and different scales. For example, sea surface temperature usually varies between approximately -2 o C and 30 o C, a narrower range than in atmospheric data. This disparity initially led to unrealistic predictions, as the model expected different distributions.\\nIn the first step, we converted temperatures in Celsius to Kelvin and reduced depth levels. The data were normalized to zero mean and unit variance. Geographic coordinates were refined to maintain consistency in the latitude-longitude grid, and the land mask was carefully interpolated to ensure seamless integration of oceanic variables within the model‚Äôs infrastructure.\\nWe scaled the original atmospheric data to a lower resolution of 0.5 ‚àò 0.5\\\\circ degrees and carried out an inference with the pre-trained Aurora model. Figure 4 shows the global temperature map in Kelvin in two different time instants and compares to the HRES product. These results indicate that the resolution change was successful.\\nWe adapted the oceanographic data by interpolating, filling in missing values, and converting to Kelvin. The latitude and longitude coordinates are interpolated to match the model‚Äôs resolution, and missing values are replaced with the mean. The pre-trained model is loaded, and the predictions with our data are consistent with the expected global atmospheric patterns.\\nInitial attempts with oceanic data did not yield good performance, as the model still carried a bias inherited from its pre-training. To overcome this challenge, we fine-tuned the model in two phases. In the first phase, the entire network was frozen except for the decoder, so that it could learn the ocean variables without altering the already acquired internal representations. Once the decoder assimilated the new dynamics, all the model‚Äôs parameters were trained using a lower learning rate to avoid losing prior knowledge. We used the AdamW optimizer as in the original work [ 14 ] , which helps regularize the model and avoids overfitting.',\n",
       "    'subsections': [{'title': '4.1 The Encoder',\n",
       "      'paragraphs': 'The objective of the encoder is to unify the inputs into a coherent three-dimensional representation. It converts heterogeneous data into a uniform internal representation, preparing it for the temporal and multiscale processing of the next module. It employs tokenization and compression of climatic states through cross-attention blocks based on a Perceiver IO [ 57 ] model. Thus, each atmospheric and surface variable is represented in a three-dimensional volume, incorporating Fourier encodings that add spatial and temporal information [ 56 ] .\\nTo normalize the information, embeddings based on sinusoidal encodings have been used, assigning each pressure level p l p_{l} a representation of the type:\\nThese embeddings integrate vertical information, reducing pressure levels to a fixed set of latent levels C L C_{L} through Perceiver IO attention mechanisms. This allows the model to work with a manageable number of levels, maintaining the representativeness of vertical conditions while balancing computational cost. Dynamic and static variables are tokenized, incorporating positional and Fourier encoding, which unifies the differences between levels and resolutions.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 The Processor',\n",
       "      'paragraphs': 'The backbone is the temporal core of the model, responsible for evolving the internal representation over time. To achieve this, it relies on a 3D Swin Transformer [ 58 , 59 ] , organized with a U-Net structure, which allows it to capture features at multiple scales.\\nAn internal encoder gradually reduces spatial resolution and increases semantic depth. This is followed by an internal decoder, which recovers the original resolution and integrates the information obtained at each level. This hierarchical arrangement combines global and local information, capturing large-scale patterns and more detailed structures.\\nAttention is implemented through local windows, allowing each layer to process spatial subsets. This reduces computational complexity by avoiding global attention that spans entire layers. This local attention is given by:\\nwhere ùêê \\\\mathbf{Q} , ùêä \\\\mathbf{K} , and ùêï \\\\mathbf{V} are the query, key, and value matrices, respectively, and d k d_{k} is the dimension of the keys. These local windows limit attention operations to specific regions. This strategy makes Aurora scalable, allowing it to handle higher resolutions without increasing computational cost.\\nThe result is an enriched representation in which spatiotemporal relationships are explicitly modeled at multiple levels. This representation is then configured to be translated back to the original variables in the decoder.\\nThe latent representation of the variables, generated by the processor, is restored to their original pressure levels and resolution in the decoder. This involves using inverse modules based on Perceiver IO models, which can reconstruct the desired variables from the internal representations, assigning each level and variable its corresponding spatial characteristics.\\nEach variable, v v , and level, l l , is obtained by applying a linear transformation and regrouping spatial patches until the original grid is recovered. Thus, the decoder ensures that the final predictions maintain coherence with the input dimensions and the information captured by the model is translated into physically interpretable forecasts.',\n",
       "      'subsections': [{'title': '4.2.1 The Decoder',\n",
       "        'paragraphs': 'The latent representation of the variables, generated by the processor, is restored to their original pressure levels and resolution in the decoder. This involves using inverse modules based on Perceiver IO models, which can reconstruct the desired variables from the internal representations, assigning each level and variable its corresponding spatial characteristics.\\nEach variable, v v , and level, l l , is obtained by applying a linear transformation and regrouping spatial patches until the original grid is recovered. Thus, the decoder ensures that the final predictions maintain coherence with the input dimensions and the information captured by the model is translated into physically interpretable forecasts.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.3 Subregional Oceanographic Prediction',\n",
       "      'paragraphs': 'Adapting the model to oceanographic variables requires fundamental differences from atmospheric conditions. While Aurora was pre-trained assuming distributions and ranges typical of atmospheric variables (normalized in Kelvin and with pressure levels), oceanic variables, such as potential temperature, present narrower ranges and different scales. For example, sea surface temperature usually varies between approximately -2 o C and 30 o C, a narrower range than in atmospheric data. This disparity initially led to unrealistic predictions, as the model expected different distributions.\\nIn the first step, we converted temperatures in Celsius to Kelvin and reduced depth levels. The data were normalized to zero mean and unit variance. Geographic coordinates were refined to maintain consistency in the latitude-longitude grid, and the land mask was carefully interpolated to ensure seamless integration of oceanic variables within the model‚Äôs infrastructure.\\nWe scaled the original atmospheric data to a lower resolution of 0.5 ‚àò 0.5\\\\circ degrees and carried out an inference with the pre-trained Aurora model. Figure 4 shows the global temperature map in Kelvin in two different time instants and compares to the HRES product. These results indicate that the resolution change was successful.\\nWe adapted the oceanographic data by interpolating, filling in missing values, and converting to Kelvin. The latitude and longitude coordinates are interpolated to match the model‚Äôs resolution, and missing values are replaced with the mean. The pre-trained model is loaded, and the predictions with our data are consistent with the expected global atmospheric patterns.\\nInitial attempts with oceanic data did not yield good performance, as the model still carried a bias inherited from its pre-training. To overcome this challenge, we fine-tuned the model in two phases. In the first phase, the entire network was frozen except for the decoder, so that it could learn the ocean variables without altering the already acquired internal representations. Once the decoder assimilated the new dynamics, all the model‚Äôs parameters were trained using a lower learning rate to avoid losing prior knowledge. We used the AdamW optimizer as in the original work [ 14 ] , which helps regularize the model and avoids overfitting.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Experimental Configuration',\n",
       "    'paragraphs': 'The split of the dataset for training the neural network considered oceanic conditions, such as seasonal cycles or adverse phenomena. For this reason, the training set was defined annually between 2014 and 2018, the validation set comprised 2019, and data from 2020 was reserved for the test set. This roughly represents 70% for training, 15% for validation, and 15% for testing, maintaining the temporal order to prevent overlaps. This approach ensures that data used to validate and test the model are independent of those used in training, for assessing its ability to make predictions in unobserved scenarios.\\nSplitting data in this way also benefits the study of oceanic phenomena, as training with data from an extended period allows the model to process multiple conditions. Additionally, using recent data for validation and testing, evaluates the model‚Äôs adaptability to current conditions. This classification also ensures that each subset contains samples from all seasons.\\nFigure 5 illustrates how the data was divided. To attain the 70%-15%-15% rate, the training set spans from January 1, 2014, to November 25, 2018; the validation set covers November 26, 2018, to December 13, 2019; and the test set extends from December 14, 2019, to January 1, 2021.\\nBatches are organized with samples of three consecutive values in sliding windows over the temporal axis. The samples are shuffled, and a padding mechanism is applied to ensure that all batches contain the same number of samples. The process begins with the generation of sliding windows, a mechanism that divides the dataset into consecutive temporal partitions. The window size is adjusted depending on the number of days in each window.\\nWe used the mean absolute error (MAE) as the loss function for optimizing the model‚Äôs parameters. Its formula is given by:\\nwhere y i y_{i} is the ground truth value, y ^ i \\\\hat{y}_{i} is the model prediction, and N N the total number of predictions. In case of multiple variables, a loss function similar to MAE could be employed. The function below is a variant of MAE that takes into account the sum across variables with their respective weights and scales:\\nwhere X ^ t = ( S ^ t , A ^ t ) \\\\hat{X}^{t}=(\\\\hat{S}^{t},\\\\hat{A}^{t}) is the predicted state, and X t = ( S t , A t ) X^{t}=(S^{t},A^{t}) is the actual state, with S S and A A representing surface and atmospheric variables, respectively. Here, Œ± \\\\alpha , Œ≤ \\\\beta , and Œ≥ \\\\gamma are weighting factors; w k S w_{k}^{S} and w k , c A w_{k,c}^{A} are the weights for each variable; and H , W , C , V S , V A H,W,C,V_{S},V_{A} are the spatial dimensions and the variable set. The loss function adjusts the contribution from each variable, prioritizing the most relevant for the study‚Äôs objective.\\nThe metrics used to evaluate the model‚Äôs performance were the root mean squared error (RMSE), the weighted bias (BIAS), and the anomaly correlation coefficient (ACC) [ 60 ] . These metrics consider weights based on the cosine of latitude to account for the Earth‚Äôs spherical geometry.\\nFigure 6 illustrates the weights, defined as w = cos \\u2061 ( œï ) w=\\\\cos(\\\\phi) , which decrease toward the poles. This ensures that the weighted metrics remain globally representative.\\nThe weighted RMSE calculates the average magnitude of the error between the predictions y ^ i \\\\hat{y}_{i} and the ground truth values y i y_{i} and is computed as:\\nwhere w i norm w_{i}^{\\\\text{norm}} is defined as:\\nIn this formulation, w i w_{i} represents the original weight associated with each observation, and w ¬Ø \\\\overline{w} is the average of these weights. This approach ensures that the error is not biased toward regions with higher data density.\\nThe weighted Bias was used to evaluate the average difference between predictions and actual values. Its calculation helped identify whether the model systematically underestimates or overestimates observed values. The formula is given by:\\nThis measure provides an accurate metric for identifying the model‚Äôs systematic tendencies. The weighted ACC, on the other hand, measures the model‚Äôs ability to replicate variation patterns between predictions and actual values. It is calculated as\\nwhere Cov \\u200b ( A , B ) \\\\text{Cov}(A,B) is the covariance and Var \\u200b ( ‚ãÖ ) \\\\text{Var}(\\\\cdot) the variance, given by\\nwith A i = y i ‚àí y ¬Ø A_{i}=y_{i}-\\\\overline{y} and B i = y ^ i ‚àí y ^ ¬Ø B_{i}=\\\\hat{y}_{i}-\\\\overline{\\\\hat{y}} the anomalies, i.e., the deviations from the weighted averages, and\\nThis process ensures that the ACC reflects the model‚Äôs ability to capture global patterns over the prediction anomalies compared to the actual values.',\n",
       "    'subsections': [{'title': '5.1 Dataset Split',\n",
       "      'paragraphs': 'The split of the dataset for training the neural network considered oceanic conditions, such as seasonal cycles or adverse phenomena. For this reason, the training set was defined annually between 2014 and 2018, the validation set comprised 2019, and data from 2020 was reserved for the test set. This roughly represents 70% for training, 15% for validation, and 15% for testing, maintaining the temporal order to prevent overlaps. This approach ensures that data used to validate and test the model are independent of those used in training, for assessing its ability to make predictions in unobserved scenarios.\\nSplitting data in this way also benefits the study of oceanic phenomena, as training with data from an extended period allows the model to process multiple conditions. Additionally, using recent data for validation and testing, evaluates the model‚Äôs adaptability to current conditions. This classification also ensures that each subset contains samples from all seasons.\\nFigure 5 illustrates how the data was divided. To attain the 70%-15%-15% rate, the training set spans from January 1, 2014, to November 25, 2018; the validation set covers November 26, 2018, to December 13, 2019; and the test set extends from December 14, 2019, to January 1, 2021.\\nBatches are organized with samples of three consecutive values in sliding windows over the temporal axis. The samples are shuffled, and a padding mechanism is applied to ensure that all batches contain the same number of samples. The process begins with the generation of sliding windows, a mechanism that divides the dataset into consecutive temporal partitions. The window size is adjusted depending on the number of days in each window.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Loss Function',\n",
       "      'paragraphs': 'We used the mean absolute error (MAE) as the loss function for optimizing the model‚Äôs parameters. Its formula is given by:\\nwhere y i y_{i} is the ground truth value, y ^ i \\\\hat{y}_{i} is the model prediction, and N N the total number of predictions. In case of multiple variables, a loss function similar to MAE could be employed. The function below is a variant of MAE that takes into account the sum across variables with their respective weights and scales:\\nwhere X ^ t = ( S ^ t , A ^ t ) \\\\hat{X}^{t}=(\\\\hat{S}^{t},\\\\hat{A}^{t}) is the predicted state, and X t = ( S t , A t ) X^{t}=(S^{t},A^{t}) is the actual state, with S S and A A representing surface and atmospheric variables, respectively. Here, Œ± \\\\alpha , Œ≤ \\\\beta , and Œ≥ \\\\gamma are weighting factors; w k S w_{k}^{S} and w k , c A w_{k,c}^{A} are the weights for each variable; and H , W , C , V S , V A H,W,C,V_{S},V_{A} are the spatial dimensions and the variable set. The loss function adjusts the contribution from each variable, prioritizing the most relevant for the study‚Äôs objective.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Evaluation Metrics',\n",
       "      'paragraphs': 'The metrics used to evaluate the model‚Äôs performance were the root mean squared error (RMSE), the weighted bias (BIAS), and the anomaly correlation coefficient (ACC) [ 60 ] . These metrics consider weights based on the cosine of latitude to account for the Earth‚Äôs spherical geometry.\\nFigure 6 illustrates the weights, defined as w = cos \\u2061 ( œï ) w=\\\\cos(\\\\phi) , which decrease toward the poles. This ensures that the weighted metrics remain globally representative.\\nThe weighted RMSE calculates the average magnitude of the error between the predictions y ^ i \\\\hat{y}_{i} and the ground truth values y i y_{i} and is computed as:\\nwhere w i norm w_{i}^{\\\\text{norm}} is defined as:\\nIn this formulation, w i w_{i} represents the original weight associated with each observation, and w ¬Ø \\\\overline{w} is the average of these weights. This approach ensures that the error is not biased toward regions with higher data density.\\nThe weighted Bias was used to evaluate the average difference between predictions and actual values. Its calculation helped identify whether the model systematically underestimates or overestimates observed values. The formula is given by:\\nThis measure provides an accurate metric for identifying the model‚Äôs systematic tendencies. The weighted ACC, on the other hand, measures the model‚Äôs ability to replicate variation patterns between predictions and actual values. It is calculated as\\nwhere Cov \\u200b ( A , B ) \\\\text{Cov}(A,B) is the covariance and Var \\u200b ( ‚ãÖ ) \\\\text{Var}(\\\\cdot) the variance, given by\\nwith A i = y i ‚àí y ¬Ø A_{i}=y_{i}-\\\\overline{y} and B i = y ^ i ‚àí y ^ ¬Ø B_{i}=\\\\hat{y}_{i}-\\\\overline{\\\\hat{y}} the anomalies, i.e., the deviations from the weighted averages, and\\nThis process ensures that the ACC reflects the model‚Äôs ability to capture global patterns over the prediction anomalies compared to the actual values.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Results',\n",
       "    'paragraphs': 'We used the test set to evaluate the model. In the first experiment, we assess the performance of the fine-tuning strategy. We follow three strategies: in the first one, we unfroze all the parameters of the network and trained with a low learning rate of 1 √ó 10 ‚àí 5 \\\\times 10^{-5} ; in the second one, we only unfroze the decoder and trained with a learning rate of 1 √ó 10 ‚àí 4 \\\\times 10^{-4} , so that atmospheric knowledge was maintained in the backbone, and oceanic features were gradually learned by the decoder; in the last one, we maintained de parameters of the previous strategy and unfroze all the layers, training the model with a low learning rate of 1 √ó 10 ‚àí 5 \\\\times 10^{-5} and a few epochs.\\nWe used a batch size of 3 and 15 epochs, obtaining an RMSE of 0.131K for the first strategy and 0.140K for the second strategy. Finally, the network of the latter was fully fine-tuned with a learning rate of 1 √ó 10 ‚àí 5 \\\\times 10^{-5} , obtaining an RMSE of 0.124K.\\nThese experiments were repeated, increasing the batch size to 8 samples and using more computational resources. This contributed to an approximate time reduction of 15% per epoch. We also increased the number of epochs to 30 and obtained an RMSE of 0.130K for the first strategy and 0.135K for the second one. The last fine-tuning step reduced the RMSE to 0.134K. Table 4 shows the final results obtained in these experiments.\\nWhen the batch size was increased to 8, the results became somewhat more competitive. This finding shows that with better resources, using larger batches speeds up training and also improves the quality of the predictions. The high ACC (ACC ‚âà 0.997 \\\\approx 0.997 ) suggests that the model captures the relationships between actual and predicted anomalies. This agreement reveals that the model does not simply fit the mean of the data but also accounts for unique phenomena. As for the Bias, in most cases, values close to -0.060K were obtained, except for the last training, where we obtained a lower bias of -0.033K.\\nFigure 7 presents an example of an inference. This figure shows the target value, the model prediction, and the difference between the two. The values are represented in Kelvin with color scales, where greenish tones indicate intermediate values, and blue and yellow highlight more extreme values between the prediction and target values. The difference between them is shown with a scale centered on zero, where red and blue indicate overestimation and underestimation, respectively.\\nIt reveals that the potential temperature prediction model has more errors in certain regions, such as in coastal zones. This is due to the inherent complexity of these areas, characterized by more intense temperature variations compared to other zones, as noted in [ 52 ] .\\nThe difference manifests the variability in coastal zones through more saturated tones, indicating that the model struggles at the ocean-continent boundary. This difficulty is likely due to the interaction of various factors influencing potential temperature in these zones, such as ocean currents, coastal topography, and local winds. These factors can generate microclimates and significant temperature variations at small spatial scales.\\nIn contrast, conditions in the open ocean are more uniform and stable, and the difference between the model prediction and the observed data is reduced. The lower variability in these areas allows for capturing potential temperature patterns with greater accuracy.\\nThis suggests that specific training aimed at improving the results in coastal areas could improve the overall model‚Äôs performance. This training could incorporate high-resolution data of relevant variables in coastal zones or use modeling techniques that better capture the complexity of these zones.\\nAn additional experiment consisted of increasing the prediction to ten days. Each sample included ten temporal steps, and the data was analyzed at each lead time. As the time horizon increases, we arrive at an increasingly challenging scenario, as uncertainty accumulates and small initial inaccuracies can be amplified. Figure 8 shows the evolution of the difference between the prediction and the target value for ten-day forecasts. The difference is small in the first days, and errors accumulate with more autoregressive steps, which become accentuated in the last days. This figure is particularly interesting for identifying errors and analyzing problematic regions. This long-term visualization demonstrates that maintaining effectiveness with an increased time horizon presents a significant challenge.\\nFigure 9 shows the average RMSE for the four seasons over ten-day forecasts. The RMSE increases progressively as the time horizon expands, with greater increases in those seasons that present more difficult conditions. Indeed, summer (plotted in green) shows a more pronounced increase, reaching values close to 0.9K after 10 days, suggesting that seasonal cycles significantly influence the results.',\n",
       "    'subsections': []},\n",
       "   {'title': '7 Discussion',\n",
       "    'paragraphs': 'Oceanography has traditionally relied on physically based numerical models. Although these models are robust and grounded in well-established physical principles, they are often complex, computationally expensive, and limited in representing certain spatial and temporal scales. In contrast, deep learning models derive relationships directly from data, operating without explicit physical constraints. They can anticipate phenomena that are difficult to reproduce numerically and, with sufficient data availability, have the potential to accelerate forecasting processes [ 4 ] .\\nThe results presented in this study demonstrate that deep learning models can leverage prior knowledge from foundational models trained in different contexts. Oceanographic research can benefit from these advances through reduced computational costs, easier integration of diverse data sources, rapid generation of forecasts, and the ability to test numerous configurations efficiently. Given the strong dependence of oceanography on data quality and quantity, the capability of deep learning techniques to assimilate and exploit vast datasets paves the way for long-term innovation.\\nThis work expands the range of forecasting methodologies available to oceanography, introducing approaches not previously explored in the field [ 4 ] . The use of foundational models pretrained in other fields helps lower barriers to entry and enhances resource efficiency.\\nDespite these advantages, several limitations remain: Training three-dimensional models is computationally demanding; hardware constraints significantly affected this study, as each training session required considerable time, limiting both resolution and the number of variables; addressing this challenge would require more powerful resources as suggested in [ 61 ] .\\nAnother key challenge concerns data representativeness. Although the dataset employed here is based on reanalysis products, these are derived from numerical models and partial observations. Consequently, predictions may inherit biases from the underlying data. Regions with sparse observations or suboptimal conditions may yield less accurate results. Future work should incorporate additional information, such as in situ measurements and satellite data, to better evaluate and improve prediction quality.\\nThis study focuses solely on potential temperature; however, ocean dynamics depend on multiple interacting variables, including salinity, density, and currents. Extending the model to incorporate these factors will increase complexity, as the model must learn new interdependencies. Nevertheless, this added complexity could yield more realistic oceanic scenarios and improve predictive accuracy.\\nAnother limitation lies in model interpretability. Neural networks remain largely opaque. Future research should therefore investigate the internal representations and decision-making processes of these models. Enhancing explainability will foster a deeper understanding of how predictions are generated and allow for greater confidence and control [ 62 ] .\\nDespite these challenges, this work opens promising avenues for future exploration. Integrating larger and higher-quality datasets could extend prediction horizons and enhance model robustness. Including additional variables, such as salinity and current velocity, would enable the study of hydrodynamic processes and provide a more comprehensive understanding of ocean behavior. Increasing spatial resolution through high-resolution models is another promising direction, though it will demand greater computational power. This could be achieved through progressive training strategies or by dividing the domain into smaller subregions for subsequent integration.\\nFinally, incorporating physical knowledge directly into training represents another exciting research direction. Physics-informed neural networks (PINNs) introduce physical constraints by penalizing violations of conservation laws in the loss function. Embedding simplified fluid dynamics equations within the learning process can reduce the model‚Äôs dependence on data alone, guiding it toward physically consistent solutions [ 63 , 64 ] .\\nContinual learning also presents a valuable opportunity. Because the ocean is a dynamic and ever-changing system, models that can be regularly updated with new data will improve adaptability and responsiveness to evolving conditions. Such models could react more effectively to extreme events and provide continuously improving forecasts over time.',\n",
       "    'subsections': []},\n",
       "   {'title': '8 Conclusion',\n",
       "    'paragraphs': 'This study established the conceptual, methodological, and technical groundwork for adapting a foundational atmospheric model to oceanographic applications. Specifically, the Aurora model was successfully applied to forecast ocean potential temperature in a local region.\\nAlthough the atmosphere and ocean are tightly coupled components of the climate system, they differ in structure and dynamics. Atmospheric variables‚Äîon which Aurora was originally trained‚Äîare organized by pressure levels and evolve rapidly, whereas oceanic variables are distributed by depth, governed by thermo-haline properties, and change more slowly due to higher density [ 53 ] . Aurora‚Äôs capacity to assimilate oceanic data and produce coherent results demonstrates cross-domain adaptability, suggesting potential for future knowledge transfer between disciplines.\\nWhile this study focused on potential temperature, the experiments indicate that Aurora can be extended to more complex, multivariable scenarios given sufficient computational resources. The sensitivity observed across training configurations, such as partial layer freezing and learning rate adjustments, highlights the importance of hyperparameter tuning and normalization strategies in achieving optimal performance. Overall, Aurora‚Äôs pre-trained parameters can be effectively fine-tuned for new oceanographic tasks, reinforcing the model‚Äôs flexibility for broader environmental applications.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'The accurate prediction of oceanographic variables is crucial for understanding climate change, managing marine resources, and optimizing maritime activities. Traditional ocean forecasting relies on numerical models; however, these approaches face limitations in terms of computational cost and scalability. In this study, we adapt Aurora, a foundational deep learning model originally designed for atmospheric forecasting, to predict sea surface temperature (SST) in the Canary Upwelling System. By fine-tuning this model with high-resolution oceanographic reanalysis data, we demonstrate its ability to capture complex spatiotemporal patterns while reducing computational demands. Our methodology involves a staged fine-tuning process, incorporating latitude-weighted error metrics and optimizing hyperparameters for efficient learning. The experimental results show that the model achieves a low RMSE of 0.119K, maintaining high anomaly correlation coefficients (ACC ‚âà0.997\\\\approx 0.997). The model successfully reproduces large-scale SST structures but faces challenges in capturing finer details in coastal regions. This work contributes to the field of data-driven ocean forecasting by demonstrating the feasibility of using deep learning models pre-trained in different domains for oceanic applications. Future improvements include integrating additional oceanographic variables, increasing spatial resolution, and exploring physics-informed neural networks to enhance interpretability and understanding. These advancements can improve climate modeling and ocean prediction accuracy, supporting decision-making in environmental and economic sectors.'},\n",
       " '2510.25590v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'In recent years, diffusion models (Rombach et\\xa0al., 2022 ) have achieved rapid progress in generative tasks, particularly in visual generation, where state-of-the-art models can synthesize highly realistic images. Within this context, the task of editing existing images according to user requirements has gradually emerged as an important direction (Kawar et\\xa0al., 2023 ) . Recently, diffusion-based foundation models, such as FLUX.1 Kontext (Labs et\\xa0al., 2025 ) , Qwen-Image-Edit (Wu et\\xa0al., 2025 ) , and Step1X-Edit (Liu et\\xa0al., 2025d ) , have been developed. These models can perform precise image editing using only textual instructions, offering a novel solution for instruction-based image editing and providing more powerful tools for image post-processing (Choi et\\xa0al., 2024 ) .\\nAlthough diffusion-based IIE models can achieve impressive editing results, their high inference latency limits their use in real-time applications. Previous research on efficient diffusion inference has primarily focused on image generation. For instance, some studies reduce model parameters through pruning (Rombach et\\xa0al., 2022 ; Castells et\\xa0al., 2024 ) , others decrease model bit-width via quantization (Shang et\\xa0al., 2023 ; Zhao et\\xa0al., 2025 ) , and some employ distillation to reduce model size (Kim et\\xa0al., 2023 ) and the number of timesteps (Sauer et\\xa0al., 2024 ) . In the two-stage inversion-based editing paradigm (Pan et\\xa0al., 2023 ; Wang et\\xa0al., 2025 ) , redundancy in the inversion and denoising stages has been analyzed, leading to methods like EEdit (Yan et\\xa0al., 2025 ) that accelerate both stages simultaneously. However, for the emerging denoising-only paradigm of IIE, the redundancy and feasibility of efficient inference remain largely unexplored.\\nOur study reveals that current IIE models exhibit two significant types of redundancy: 1) Spatial Generation Redundancy. Unlike image generation tasks, which require reconstructing the entire image, IIE models often need to modify only local regions specified by the instructions, while the remaining areas remain essentially unchanged. For example, as shown in Figure 1 , the model edits only the region around the hat. Nevertheless, IIE models apply the same computational effort to both edited and unedited areas, resulting in significant redundancy in the latter.\\n2) Redundancy across diffusion timesteps. First, at neighboring timesteps, the key and value within the attention layers at the same network depth are highly similar. Second, in the middle stages of denoising, the velocity output by the diffusion transformer (DiT) at adjacent timesteps is also highly similar.\\nTo mitigate spatial and temporal redundancy in IIE models, this paper introduces RegionE, a training-free, adaptive, and region-aware generative framework that accelerates the current IIE models.\\nFirstly, we observed that the trajectories of edited regions are often more curved, making it difficult to accurately predict the final edited results at early timesteps, as shown in Figure 1 . In contrast, unedited regions follow nearly linear trajectories, allowing more reliable predictions from the same early steps.\\nBased on this observation, RegionE introduces an Adaptive Region Partition (ARP), which performs a one-step estimation for the final image in the early stage and compares its similarity with the reference (instruction) image. Regions with high similarity (minimal change after editing) are classified as unedited, whereas regions with low similarity are classified as edited.\\nThen, we perform region-aware generation on the two separated parts. Specifically,\\nWe replace multi-step denoising with one-step estimation for the unedited areas and apply region-iterative denoising for edited areas.\\nDuring edited region generation, RegionE discards unedited region tokens and instruction image tokens, and effectively reinjects global context into local generation through our proposed Region-Instruction KV Cache (RIKVCache), which leverages the similarity of key and value across timesteps. This process primarily addresses redundancy in spatial.\\nFinally, regarding temporal redundancy, we find that the velocity outputs of DiT at adjacent timesteps are highly consistent in direction but decay in magnitude over time, with the decay dependent on the timestep.\\nTo exploit this property, RegionE introduces an Adaptive Velocity Decay Cache (AVDCache), which accurately models this pattern and further accelerates the region generation process.\\nExperimental results demonstrate that RegionE achieves speedups of approximately 2.57√ó, 2.41√ó, and 2.06√ó on Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, respectively, while maintaining PSNR values of 30.520, 32.133, and 31.115 before and after acceleration. Evaluations using GPT-4o further indicate that the perceptual differences are negligible, confirming that RegionE effectively eliminates redundancy in IIE tasks without compromising image quality.\\nThe contributions of our paper are as follows:\\nWe observe that in IIE tasks, unedited regions exhibit nearly linear generation trajectories, allowing early-stage velocities to provide reliable estimates for multi-step denoised images, including the final image. In contrast, edited regions follow more curved trajectories, making the final image harder to predict. Nevertheless, the velocity remains consistent across consecutive timesteps.\\nWe propose RegionE, a training-free, efficient IIE method with adaptive, region-aware generation. It reduces spatial redundancy by performing early adaptive predictions for edited and unedited regions and generating each region locally in subsequent stages, while mitigating temporal redundancy via a velocity-decay cache across timesteps.\\nRegionE achieves 2.57√ó, 2.41√ó, and 2.06√ó end-to-end speedups on Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, while maintaining PSNR (30.520, 32.133, 31.115) and SSIM (0.939, 0.917, 0.937). Evaluations with GPT-4o further confirm that no quality degradation occurs.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Efficient Diffusion Model. Although few efficient methods have been developed specifically for IIE models, a variety of acceleration techniques have been proposed for diffusion models more generally. From the perspective of parameter redundancy, researchers have introduced pruning methods such as Diff-Pruning Fang et\\xa0al. ( 2023 ) and LD-Pruner (Castells et\\xa0al., 2024 ) , quantization methods such as PTQ4DM (Shang et\\xa0al., 2023 ) , FPQuant (Zhao et\\xa0al., 2025 ) , and SVDQuant (Li et\\xa0al., 2024a ) , distillation methods such as BK-SDM (Kim et\\xa0al., 2023 ) and CLEAR (Liu et\\xa0al., 2024 ) , sparse attention methods such as DiTFastAttn (Yuan et\\xa0al., 2024 ; Zhang et\\xa0al., 2025 ) , SVG (Xi et\\xa0al., 2025 ) , Sparse-vDiT (Chen et\\xa0al., 2025 ) , and VORTA (Sun et\\xa0al., 2025 ) , and early-stopping strategies such as ES-DDPM (Lyu et\\xa0al., 2022 ) . From the perspective of temporal redundancy, methods like DeepCache (Ma et\\xa0al., 2024 ) , Œî \\\\Delta -DiT (Chen et\\xa0al., 2024 ) , FORA (Selvaraju et\\xa0al., 2024 ) , and TeaCache (Liu et\\xa0al., 2025a ) reuse intermediate features across timesteps (Shen et\\xa0al., 2024 ; Liu et\\xa0al., 2025c ; b ) , while approaches such as LCM (Luo et\\xa0al., 2023 ) and ADD (Sauer et\\xa0al., 2024 ) reduce the number of timesteps through model distillation.\\nFrom the perspective of spatial redundancy, RAS (Liu et\\xa0al., 2025e ) observes that at each diffusion timestep, the model may focus only on semantically coherent regions; therefore, only those regions need to be updated, thereby accelerating image generation. Similarly, ToCa (Zou et\\xa0al., 2024a ) and DuCa (Zou et\\xa0al., 2024b ) note that during denoising, different tokens exhibit varying sensitivities, and dynamically updating only a subset of tokens at each timestep can further accelerate image generation.\\nIn contrast to the methods above, RegionE leverages the trajectory characteristics unique to IIE tasks, while simultaneously addressing both spatial and temporal redundancies in diffusion-based image editing to achieve acceleration.\\nImage Editing. Image editing is an essential task in the field of generative modeling. In the early U-Net (Ronneberger et\\xa0al., 2015 ) era, ControlNet (Zhang et\\xa0al., 2023b ) introduced a robust editing solution through a repeat-structure design. As research advanced, inversion-based methods (Pan et\\xa0al., 2023 ; Wang et\\xa0al., 2025 ) gradually became the dominant approach. These methods apply noise to the original image in the latent space and then recover the edited result through a denoising process. However, this paradigm involves both inversion and denoising stages, which increases complexity. At the same time, IIE models began to emerge. Approaches such as InstructEdit (Wang et\\xa0al., 2023 ) , MagicBrush (Zhang et\\xa0al., 2023a ) , and BrushEdit (Li et\\xa0al., 2024b ) employed modular pipelines, in which large language models generate prompts, spatial cues, or synthetic instruction‚Äìimage pairs to guide diffusion-based editing. Most of these approaches, however, are task-specific and lack generality. More recently, a new class of IIE has been developed to improve general-purpose editing. These models rely solely on textual instructions, without requiring masks or task-specific designs, and still achieve effective editing performance. Concretely, they leverage MLLMs or advanced text encoders to provide richer semantic control signals, and feed both the target image and noise into a DiT (Peebles & Xie, 2023 ) architecture to enhance image alignment. In this work, we propose an adaptive, region-aware acceleration method for this emerging IIE models.',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Preliminary',\n",
       "    'paragraphs': 'Flow Matching & Rectified Flow. Flow matching (Lipman et\\xa0al., 2022 ) has become a widely adopted training technique in advanced diffusion models. It facilitates the transfer from a source distribution œÄ 1 \\\\pi_{1} to a target distribution œÄ 0 \\\\pi_{0} by learning a time-dependent velocity field ùíó \\u200b ( ùíô , t ) \\\\bm{v}(\\\\bm{x},t) . This velocity field is used to construct the flow through the ordinary differential equation (ODE):\\nRectified Flow (Liu et\\xa0al., 2022 ) simplifies this process through a linear assumption. Given that ùëø 1 \\\\bm{X}_{1} follows a noise distribution œÄ 1 \\\\pi_{1} and ùëø 0 \\\\bm{X}_{0} follows the target image distribution œÄ 0 \\\\pi_{0} , the equation is\\nDifferentiating both ends with respect to timestep t t yields: d \\u200b ùëø t d \\u200b t = ùëø 1 ‚àí ùëø 0 \\\\frac{d\\\\bm{X}_{t}}{dt}=\\\\bm{X}_{1}-\\\\bm{X}_{0} . The velocity of the rectified flow ùíó \\u200b ( ùëø t , t ) \\\\bm{v}(\\\\bm{X}_{t},t) , always points in the direction of ùëø 1 ‚àí ùëø 0 \\\\bm{X}_{1}-\\\\bm{X}_{0} . Therefore, the training loss is minimized by reducing the deviation between the velocity and ùëø 1 ‚àí ùëø 0 \\\\bm{X}_{1}-\\\\bm{X}_{0} :\\nThe inference process involves starting from ùëø 1 \\\\bm{X}_{1} and iteratively solving for ùëø 0 \\\\bm{X}_{0} in reverse, using the learned velocity ùíó \\u200b ( ùëø t , t ) \\\\bm{v}(\\\\bm{X}_{t},t) . In practice, we typically use a discrete Euler sampler, which discretizes the timestep t i \\u200b ( i ‚àà ‚Ñï T , t T = 1 , t 0 = 0 ) t_{i}(i\\\\in\\\\mathbb{N}^{T},t_{T}=1,t_{0}=0) and approximates:\\nAfter T T iterations, the final target image ùëø 0 \\\\bm{X}_{0} is obtained. This paper, therefore, targets the IIE task and optimizes the inference process of T T iterations in Equation 4 .\\nInstruction-Based Editing Model. Recent IIE models, such as Step1X-Edit (Liu et\\xa0al., 2025d ) , FLUX.1 Kontext (Labs et\\xa0al., 2025 ) and Qwen-Image (Wu et\\xa0al., 2025 ) , follow the same paradigm, as shown in Figure 2 b. In these models, the velocity field is estimated using Instruction-DiT, the variants of DiT (Peebles & Xie, 2023 ) . The input to Instruction-DiT consists of three types of tokens: text (prompt) tokens ùëø P \\\\bm{X}^{P} , noise tokens ùëø t i \\\\bm{X}_{t_{i}} , and instruction tokens ùëø I \\\\bm{X}^{I} . The noise token corresponds to the generation of the target image, while the text token carries the instruction information. The instruction token is specific to the editing task, representing the part of the image to be edited. Notably, the counts of noise and instruction tokens are roughly comparable and substantially higher than that of text tokens. Temporally, the text and instruction tokens serve as static control signals throughout the denoising process, whereas the noise token evolves dynamically at each timestep. Since Instruction-DiT is designed to predict only the noise component, the model‚Äôs output corresponds exclusively to the portion represented by the noise token. To simplify the expression, the Instruction-DiT mentioned below will be referred to simply as DiT.',\n",
       "    'subsections': []},\n",
       "   {'title': '4 Methodology',\n",
       "    'paragraphs': 'This section introduces RegionE, a method that accelerates the IIE model without additional training. The workflow is shown in Figure 3 . RegionE consists of three stages: the Stabilization Stage (STS), the Region-Aware Generation Stage (RAGS), and the Smooth Stage (SMS).\\nStabilization Stage. In the early steps of denoising, the input ùëø t i \\\\bm{X}_{t_{i}} to DiT is close to Gaussian noise (i.e., the signal-to-noise ratio is low). This leads to oscillations in DiT‚Äôs velocity estimation (see Figure 2 d and 2 e). Since the estimates at this stage are inherently unstable, it is not suitable for acceleration. Therefore, we keep the original sampling process unchanged. Additionally, at the last step of this stage, we save the Key and Value in each attention layer of DiT, denoted as ùë≤ C \\\\bm{K}^{C} and ùëΩ C \\\\bm{V}^{C} .\\nRegion-Aware Generation Stage. This stage is the core component of RegionE and consists of three parts: adaptive region partition, region-aware generation, and adaptive velocity decay cache. The first two parts primarily address spatial redundancy in IIE, while the third further reduces temporal redundancy across timesteps.\\nAdaptive Region Partition. After the stabilization stage, the output of DiT becomes stable. As previously observed, the generation trajectories in the edited regions are curved, whereas those in the unedited regions are straight, as shown in Figure 1 and 2 f. Therefore, for the unedited regions ùëø t i U \\\\bm{X}^{U}_{t_{i}} , we can accurately estimate ùëø ^ t f U \\\\hat{\\\\bm{X}}^{U}_{t_{f}} at any timestep t f \\u200b ( f < i ) t_{f}(f<i) using one-step estimation:\\nWhen t f = 0 t_{f}=0 , this corresponds to estimating the final unedited regions ùëø ^ 0 U \\\\hat{\\\\bm{X}}^{U}_{0} , which is nearly identical to the true ùëø 0 U \\\\bm{X}_{0}^{U} .\\nHowever, using Equation 5 for the edited region does not accurately estimate ùëø ^ 0 E \\\\hat{\\\\bm{X}}^{E}_{0} . Based on this difference between the edited and unedited regions, we propose an adaptive region partition (ARP), as illustrated in the lower-left corner of Figure 3 . Given the velocity ùíó t i + 1 \\\\bm{v}_{t_{i+1}} at the beginning of the region-aware generation stage and the noisy image ùëø t i \\\\bm{X}_{t_{i}} , the final edited result ùëø ^ 0 \\\\hat{\\\\bm{X}}_{0} can be estimated in one step using Equation 5 . This estimate is reliable in unedited regions but less accurate in edited ones.\\nSince the unedited region undergoes minimal change before and after editing, we can compute the cosine similarity between the estimated image ùëø 0 ^ \\\\hat{\\\\bm{X}_{0}} and the instruction image ùëø I \\\\bm{X}^{I} along the token dimension. Regions with sufficiently high similarity ( > > threshold Œ∑ \\\\eta ), that is, small changes before and after editing, are considered unedited regions, while the remainder is treated as the edited region. To account for potential segmentation noise, morphological opening and closing operations are applied to make the two regions more continuous and accurate.\\nRegion-Aware Generation. After identifying the edited and unedited regions, we apply Equation 5 to the unedited region to directly estimate the denoised image X t f U X^{U}_{t_{f}} at the next timestep t f t_{f} in one step, thereby saving computation for the unedited region. For the edited region, our implementation is as follows: first, the input to DiT is changed from [ ùëø P , ùëø t i , ùëø I ] [\\\\bm{X}^{P},\\\\bm{X}_{t_{i}},\\\\bm{X}^{I}] to [ ùëø P , ùëø t i E ] [\\\\bm{X}^{P},\\\\bm{X}^{E}_{t_{i}}] , so that DiT only estimates the velocity of the edited region ùíó t i E \\\\bm{v}^{E}_{t_{i}} . However, since DiT contains attention layers that involve global token interactions, completely discarding the ùëø I \\\\bm{X}^{I} and ùëø t i U \\\\bm{X}^{U}_{t_{i}} inputs can gradually inject bias into the estimation of ùíó t i E \\\\bm{v}^{E}_{t_{i}} during global attention. To compensate for this loss of information, we propose a Region-Instruction KV Cache (RIKVCache). Specifically, the input to DiT remains [ ùëø P , ùëø t i E ] [\\\\bm{X}^{P},\\\\bm{X}^{E}_{t_{i}}] , but within the attention layers of DiT, it is modified as follows:\\nThe lower corner labels P P , E E , U U , and I I represent prompt token, edited region token, unedited region token, and instruction token, respectively. The superscript C C in the upper-right corner indicates that the value is taken from the cache of the previous complete computation. And the middle-lower part of Figure 3 visualizes this process. The feasibility of this approach is supported by the high similarity of the KV pairs between consecutive steps, as shown in Figure 2 g and 2 h.\\nAdaptive Velocity Decay Cache. As illustrated in the right part of Figure 1 , although the trajectory of the edited region is curved, the velocities between consecutive timesteps are actually similar. Focusing on the intermediate denoising phase, we observe from Figure 2 e that the velocity directions between adjacent steps are almost identical (cosine similarity approaches 1). At the same time, the magnitudes exhibit a gradual decay that varies across timesteps (Figure 2 d). Based on this observation, we propose an adaptive velocity decay cache (AVDCache). Specifically, the AVDCache introduces a decay factor:\\nHere, ( 1 ‚àí Œî \\u200b t t i + 1 , t i ) (1-\\\\Delta t_{t_{i+1},t_{i}}) represents the sample-aware component under discrete Euler solver, while Œ≥ t i \\\\gamma_{t_{i}} represents the timestep-aware component. The solver entirely determines the former, while the latter is obtained by fitting on a randomly sampled dataset. Since the decay factor in Eq. 7 characterizes the intrinsic differences between diffusion model timesteps, we introduce the AVDCache criterion:\\nHere, t s t_{s} and t e t_{e} denote the start and end timesteps of the cache, respectively, while the criterion measures the cumulative error of this process. The decision of whether to apply the cache is made using a threshold Œ¥ \\\\delta . The complete process is as follows:\\nThe right-lower part of Figure 3 visualizes this process.\\nIn fact, AVDCache is an improved version of the existing residual cache methods, with further details and analysis provided in the supplementary.\\nAfter the above process, we obtain the generated results for both the edited and unedited regions. We then re-gather these results according to their spatial positions to reconstruct the complete image tokens.\\nIt is worth noting that the similarity of the KV Cache decreases as the timestep increases. To address this issue, we periodically enforce full-image gathering at certain timesteps within the region-aware generation stage, performing a complete DiT computation to update the RIKVCache.\\nSmooth Stage. Small gaps may appear at the boundaries between edited and unedited regions after stitching. Although these gaps are often imperceptible in most cases, to ensure the generality of our method, we perform several steps of unaccelerated denoising on the merged full image to smooth these discontinuities. Empirically, two denoising steps are sufficient to eliminate the gaps effectively.',\n",
       "    'subsections': []},\n",
       "   {'title': '5 Experiment',\n",
       "    'paragraphs': 'Pretrained Model & Dataset. We evaluate RegionE on three open-source state-of-the-art IIE models: Step1X-Edit-v1p1 (Liu et\\xa0al., 2025d ) , FLUX.1 Kontext (Labs et\\xa0al., 2025 ) , and Qwen-Image-Edit (Wu et\\xa0al., 2025 ) . Step1X-Edit adopts a CFG (classifier-free guidance) (Ho & Salimans, 2022 ) scale of 6, FLUX.1 Kontext uses a scale of 2.5, and Qwen-Image-Edit applies a scale of 4. All models are evaluated with 28 sampling steps. For evaluation, we follow the dataset protocols described in the respective technical reports. Specifically, we use 606 image prompt pairs covering 11 tasks from GEdit-Bench English (Liu et\\xa0al., 2025d ) for Step1X-Edit and Qwen-Image-Edit, and 1026 image prompt pairs spanning five tasks from KontextBench (Labs et\\xa0al., 2025 ) for FLUX.1 Kontext.\\nEvaluation Metrics. We design a comprehensive evaluation framework to assess both the quality and efficiency of IIE models. For quality assessment, we adopt two complementary approaches. First, we evaluate reconstruction quality by measuring deviations before and after acceleration, using PSNR (Zhao et\\xa0al., 2024 ) , SSIM (Wang & Bovik, 2002 ) , and LPIPS (Zhang et\\xa0al., 2018 ) as metrics. Second, we conduct an editing evaluation using vision‚Äìlanguage models (VLMs), specifically GPT-4o, to assess image quality, semantic alignment, and overall performance (Ku et\\xa0al., 2024 ) , as shown in Table 1 . Evaluation dimensions are denoted by the suffixes SC, PQ, and O, consistent with (Liu et\\xa0al., 2025d ) and (Wu et\\xa0al., 2025 ) . For efficiency evaluation, we report actual runtime latency as well as the relative speedup compared to the vanilla pretrained models.\\nBaseline. Currently, there are no acceleration methods designed explicitly for IIEmodels. Therefore, we adapt several effective acceleration techniques initially developed for diffusion models as baselines, since they are also applicable to diffusion-based IIE tasks. From the perspective of timestep redundancy, Steoskip performs larger jumps in the sampling steps, FORA (Selvaraju et\\xa0al., 2024 ) employs block-level cache, and Œî \\\\Delta -DiT (Chen et\\xa0al., 2024 ) and TeaCache (Liu et\\xa0al., 2025a ) use residual cache. From the perspective of spatial redundancy, RAS (Liu et\\xa0al., 2025e ) and ToCa (Zou et\\xa0al., 2024a ) perform redundancy-reduction denoising at the token level.\\nImplementation Details. For all three models, RegionE uses six steps in the stabilization stage, enforces an update at step 16 in the region-aware generation stage, and adopts two steps in the smooth stage. For Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, the segmentation thresholds Œ∑ \\\\eta of ARP are 0.88, 0.93, and 0.80, respectively, while the decision thresholds Œ¥ \\\\delta of AVDCache are 0.02, 0.04, and 0.03, respectively. Latency is measured on a single NVIDIA H800 GPU, with each run editing one image at a time.\\nWe evaluate RegionE against several state-of-the-art acceleration methods on three prominent IIE models: Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. Our evaluation encompasses quantitative metrics, efficiency measurements, and visualization, demonstrating that RegionE achieves a superior balance between acceleration and quality preservation. The quantitative results are shown in Table 1 . Since both GEdit-Bench and KontextBench involve multiple editing tasks, the table reports results averaged over tasks, while the per-task quantitative results are provided in the supplementary.\\nDeviation Analysis Compared to Pre-trained Models. The ‚ÄùAgainst Vanilla‚Äù evaluation reveals RegionE‚Äôs exceptional fidelity to original model outputs across all evaluation metrics, significantly outperforming competing acceleration methods. RegionE achieves the highest PSNR values: 30.520 dB (Step1X-Edit), 32.133 dB (FLUX.1 Kontext), and 31.115 dB (Qwen-Image-Edit), representing substantial improvements of 2-4 dB over the next-best methods, indicating minimal pixel-level deviation from the original outputs. The SSIM scores of 0.939, 0.917, and 0.937 demonstrate superior preservation of structural coherence across different model architectures. In contrast, the LPIPS scores of 0.054, 0.057, and 0.046 represent 25-50% improvements over competing methods, indicating dramatically reduced perceptual differences that would be virtually indistinguishable to users. This consistent performance across three diverse model architectures validates RegionE‚Äôs architectural agnosticism. RegionE consistently maintains stable, high-quality results.\\nGPT-4o Editing Quality Assessment. The GPT-4o evaluation provides additional quality validation through automated semantic and perceptual analysis across three dimensions, consistently demonstrating RegionE‚Äôs superior performance. For semantic consistency (G-SC), RegionE achieves scores of 7.552, 7.278, and 8.242, matching or exceeding original models while maintaining substantial acceleration, with Qwen-Image-Edit showing perfect preservation (8.242) despite 2.059√ó speedup. The perceptual quality (G-PQ) scores of 7.405, 6.953, and 7.968 consistently outperform competing acceleration methods by 0.1 to 0.3 points, demonstrating the practical preservation of visual coherence through region-aware processing. Overall quality (G-O) scores of 6.948, 6.538, and 7.731 provide holistic assessment validation, with the alignment between GPT-4o assessments and quantitative metrics (PSNR, SSIM, LPIPS) strengthening confidence in RegionE‚Äôs comprehensive quality preservation across multiple evaluation dimensions and providing additional evidence of the hybrid temporal-spatial optimization approach‚Äôs effectiveness.\\nEfficiency Analysis. RegionE demonstrates substantial efficiency gains while maintaining superior quality, achieving an optimal balance between acceleration and performance preservation with impressive results across all evaluated models. The method achieves speedups of 2.572√ó, 2.409√ó, and 2.059√ó across Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit respectively, translating to significant absolute latency reductions: from 27.945s to 10.865s, from 14.682s to 6.096s, and from 32.125s to 15.604s respectively. RegionE occupies the optimal position on the efficiency-quality curve, maintaining the highest quality metrics while achieving competitive or superior acceleration compared to methods that sacrifice substantial quality for higher speedups.\\nVisualization. Figure 4 presents partial visualizations of different acceleration methods on Step1X-Edit. Among the baselines, RegionE produces edited outputs closest to the vanilla setting at higher speedups, preserving both details and contours. The last column shows ARP predictions of spatial regions in RegionE, where unedited regions are masked. These masked regions closely match human perception. Additional visualizations for other tasks and models are provided in the supplementary.\\nThe experimental results conclusively demonstrate the effectiveness of RegionE in addressing both spatial and temporal redundancies in IIE models. RegionE achieves superior quality preservation metrics while maintaining competitive acceleration. The consistent improvements across diverse model architectures validate the generalizability of the underlying insights about regional editing patterns and temporal similarities in diffusion-based IIE processes.\\nWe conduct ablation studies to investigate the contributions of different components in RegionE, primarily on the Step1X-Edit-v1p1. The quantitative results are summarized in Table 2 .\\nCache Design. We propose two key components: RIKVCache and AVDCache. Removing RIKVCache, i.e., performing local attention within the edited region without injecting instruction information or context from the unedited region, results in a 2.734√ó speed-up. However, this comes at a significant cost to editing quality, with PSNR dropping from 30.520 to 22.868 and G-O decreasing from 6.948 to 5.191. This demonstrates that global context supervision is crucial even during region generation. In contrast, removing AVDCache results in a slight improvement in editing quality (G-O increases from 6.948 to 7.023), but without eliminating redundancy across timesteps, the acceleration is limited to 1.733. This indicates that AVDCache significantly improves inference efficiency with minimal degradation in quality.\\nStage Design. We introduce two auxiliary stages: Stabilization Stage (STS) and Smooth Stage (SMS), as well as a forced step in the region-aware generation stage (RAGS). Removing STS causes substantial drops in editing quality (PSNR: 30.520 ‚Üí 21.441, LPIPS: 0.054 ‚Üí 0.161, G-O: 6.948 ‚Üí 6.325). As discussed in Section 4 , STS addresses the instability in speed estimation, and skipping it results in degraded performance. Removing SMS leads to smaller declines in both pixel-level (PSNR: 30.520 ‚Üí 28.857, SSIM: 0.939 ‚Üí 0.904) and perceptual metrics (G-O: 6.948 ‚Üí 6.773), reflecting its role in bridging the gap between edited and unedited regions. Finally, when the forced step in RAGS was removed, since its role was to mitigate the decay of KV similarity over time, its removal led to a 2-point drop in PSNR, further validating its necessity.',\n",
       "    'subsections': [{'title': '5.1 Experimental Settings',\n",
       "      'paragraphs': 'Pretrained Model & Dataset. We evaluate RegionE on three open-source state-of-the-art IIE models: Step1X-Edit-v1p1 (Liu et\\xa0al., 2025d ) , FLUX.1 Kontext (Labs et\\xa0al., 2025 ) , and Qwen-Image-Edit (Wu et\\xa0al., 2025 ) . Step1X-Edit adopts a CFG (classifier-free guidance) (Ho & Salimans, 2022 ) scale of 6, FLUX.1 Kontext uses a scale of 2.5, and Qwen-Image-Edit applies a scale of 4. All models are evaluated with 28 sampling steps. For evaluation, we follow the dataset protocols described in the respective technical reports. Specifically, we use 606 image prompt pairs covering 11 tasks from GEdit-Bench English (Liu et\\xa0al., 2025d ) for Step1X-Edit and Qwen-Image-Edit, and 1026 image prompt pairs spanning five tasks from KontextBench (Labs et\\xa0al., 2025 ) for FLUX.1 Kontext.\\nEvaluation Metrics. We design a comprehensive evaluation framework to assess both the quality and efficiency of IIE models. For quality assessment, we adopt two complementary approaches. First, we evaluate reconstruction quality by measuring deviations before and after acceleration, using PSNR (Zhao et\\xa0al., 2024 ) , SSIM (Wang & Bovik, 2002 ) , and LPIPS (Zhang et\\xa0al., 2018 ) as metrics. Second, we conduct an editing evaluation using vision‚Äìlanguage models (VLMs), specifically GPT-4o, to assess image quality, semantic alignment, and overall performance (Ku et\\xa0al., 2024 ) , as shown in Table 1 . Evaluation dimensions are denoted by the suffixes SC, PQ, and O, consistent with (Liu et\\xa0al., 2025d ) and (Wu et\\xa0al., 2025 ) . For efficiency evaluation, we report actual runtime latency as well as the relative speedup compared to the vanilla pretrained models.\\nBaseline. Currently, there are no acceleration methods designed explicitly for IIEmodels. Therefore, we adapt several effective acceleration techniques initially developed for diffusion models as baselines, since they are also applicable to diffusion-based IIE tasks. From the perspective of timestep redundancy, Steoskip performs larger jumps in the sampling steps, FORA (Selvaraju et\\xa0al., 2024 ) employs block-level cache, and Œî \\\\Delta -DiT (Chen et\\xa0al., 2024 ) and TeaCache (Liu et\\xa0al., 2025a ) use residual cache. From the perspective of spatial redundancy, RAS (Liu et\\xa0al., 2025e ) and ToCa (Zou et\\xa0al., 2024a ) perform redundancy-reduction denoising at the token level.\\nImplementation Details. For all three models, RegionE uses six steps in the stabilization stage, enforces an update at step 16 in the region-aware generation stage, and adopts two steps in the smooth stage. For Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, the segmentation thresholds Œ∑ \\\\eta of ARP are 0.88, 0.93, and 0.80, respectively, while the decision thresholds Œ¥ \\\\delta of AVDCache are 0.02, 0.04, and 0.03, respectively. Latency is measured on a single NVIDIA H800 GPU, with each run editing one image at a time.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Experimental Results Analysis',\n",
       "      'paragraphs': 'We evaluate RegionE against several state-of-the-art acceleration methods on three prominent IIE models: Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. Our evaluation encompasses quantitative metrics, efficiency measurements, and visualization, demonstrating that RegionE achieves a superior balance between acceleration and quality preservation. The quantitative results are shown in Table 1 . Since both GEdit-Bench and KontextBench involve multiple editing tasks, the table reports results averaged over tasks, while the per-task quantitative results are provided in the supplementary.\\nDeviation Analysis Compared to Pre-trained Models. The ‚ÄùAgainst Vanilla‚Äù evaluation reveals RegionE‚Äôs exceptional fidelity to original model outputs across all evaluation metrics, significantly outperforming competing acceleration methods. RegionE achieves the highest PSNR values: 30.520 dB (Step1X-Edit), 32.133 dB (FLUX.1 Kontext), and 31.115 dB (Qwen-Image-Edit), representing substantial improvements of 2-4 dB over the next-best methods, indicating minimal pixel-level deviation from the original outputs. The SSIM scores of 0.939, 0.917, and 0.937 demonstrate superior preservation of structural coherence across different model architectures. In contrast, the LPIPS scores of 0.054, 0.057, and 0.046 represent 25-50% improvements over competing methods, indicating dramatically reduced perceptual differences that would be virtually indistinguishable to users. This consistent performance across three diverse model architectures validates RegionE‚Äôs architectural agnosticism. RegionE consistently maintains stable, high-quality results.\\nGPT-4o Editing Quality Assessment. The GPT-4o evaluation provides additional quality validation through automated semantic and perceptual analysis across three dimensions, consistently demonstrating RegionE‚Äôs superior performance. For semantic consistency (G-SC), RegionE achieves scores of 7.552, 7.278, and 8.242, matching or exceeding original models while maintaining substantial acceleration, with Qwen-Image-Edit showing perfect preservation (8.242) despite 2.059√ó speedup. The perceptual quality (G-PQ) scores of 7.405, 6.953, and 7.968 consistently outperform competing acceleration methods by 0.1 to 0.3 points, demonstrating the practical preservation of visual coherence through region-aware processing. Overall quality (G-O) scores of 6.948, 6.538, and 7.731 provide holistic assessment validation, with the alignment between GPT-4o assessments and quantitative metrics (PSNR, SSIM, LPIPS) strengthening confidence in RegionE‚Äôs comprehensive quality preservation across multiple evaluation dimensions and providing additional evidence of the hybrid temporal-spatial optimization approach‚Äôs effectiveness.\\nEfficiency Analysis. RegionE demonstrates substantial efficiency gains while maintaining superior quality, achieving an optimal balance between acceleration and performance preservation with impressive results across all evaluated models. The method achieves speedups of 2.572√ó, 2.409√ó, and 2.059√ó across Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit respectively, translating to significant absolute latency reductions: from 27.945s to 10.865s, from 14.682s to 6.096s, and from 32.125s to 15.604s respectively. RegionE occupies the optimal position on the efficiency-quality curve, maintaining the highest quality metrics while achieving competitive or superior acceleration compared to methods that sacrifice substantial quality for higher speedups.\\nVisualization. Figure 4 presents partial visualizations of different acceleration methods on Step1X-Edit. Among the baselines, RegionE produces edited outputs closest to the vanilla setting at higher speedups, preserving both details and contours. The last column shows ARP predictions of spatial regions in RegionE, where unedited regions are masked. These masked regions closely match human perception. Additional visualizations for other tasks and models are provided in the supplementary.\\nThe experimental results conclusively demonstrate the effectiveness of RegionE in addressing both spatial and temporal redundancies in IIE models. RegionE achieves superior quality preservation metrics while maintaining competitive acceleration. The consistent improvements across diverse model architectures validate the generalizability of the underlying insights about regional editing patterns and temporal similarities in diffusion-based IIE processes.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Ablation Study',\n",
       "      'paragraphs': 'We conduct ablation studies to investigate the contributions of different components in RegionE, primarily on the Step1X-Edit-v1p1. The quantitative results are summarized in Table 2 .\\nCache Design. We propose two key components: RIKVCache and AVDCache. Removing RIKVCache, i.e., performing local attention within the edited region without injecting instruction information or context from the unedited region, results in a 2.734√ó speed-up. However, this comes at a significant cost to editing quality, with PSNR dropping from 30.520 to 22.868 and G-O decreasing from 6.948 to 5.191. This demonstrates that global context supervision is crucial even during region generation. In contrast, removing AVDCache results in a slight improvement in editing quality (G-O increases from 6.948 to 7.023), but without eliminating redundancy across timesteps, the acceleration is limited to 1.733. This indicates that AVDCache significantly improves inference efficiency with minimal degradation in quality.\\nStage Design. We introduce two auxiliary stages: Stabilization Stage (STS) and Smooth Stage (SMS), as well as a forced step in the region-aware generation stage (RAGS). Removing STS causes substantial drops in editing quality (PSNR: 30.520 ‚Üí 21.441, LPIPS: 0.054 ‚Üí 0.161, G-O: 6.948 ‚Üí 6.325). As discussed in Section 4 , STS addresses the instability in speed estimation, and skipping it results in degraded performance. Removing SMS leads to smaller declines in both pixel-level (PSNR: 30.520 ‚Üí 28.857, SSIM: 0.939 ‚Üí 0.904) and perceptual metrics (G-O: 6.948 ‚Üí 6.773), reflecting its role in bridging the gap between edited and unedited regions. Finally, when the forced step in RAGS was removed, since its role was to mitigate the decay of KV similarity over time, its removal led to a 2-point drop in PSNR, further validating its necessity.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Conclusion',\n",
       "    'paragraphs': 'Inspired by temporal and spatial redundancy in IIE, we propose RegionE, an adaptive, region-aware generation framework that accelerates the IIE process. Specifically, we perform early prediction on spatial regions using ARP and combine it with RIKVCache for region-wise editing to reduce spatial redundancy. We also use AVDCache to minimize temporal redundancy. Experiments show that RegionE achieves 2.57√ó, 2.41√ó, and 2.06√ó end-to-end speedups on Step1X-Edit and FLUX.1 Kontext, and Qwen-Image-Edit, respectively, while maintaining minimal bias (PSNR 30.52‚Äì32.13) and negligible quality loss (GPT-4o evaluation results remain comparable). These results demonstrate the effectiveness of RegionE in reducing redundancy in IIE.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Pseudocode of RegionE',\n",
       "    'paragraphs': '',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Analysis of Adaptive Velocity Decay Cache',\n",
       "    'paragraphs': 'In current research on diffusion model caching, many studies focus on residual-based caches (Chen et\\xa0al., 2024 ; Liu et\\xa0al., 2025a ; Zhou et\\xa0al., 2025 ; Bu et\\xa0al., 2025 ) , which store the Œî \\\\Delta shown in Figure 5 . Based on the sampling formula in Equation 4 and the definition of caching, we can derive the following expression:\\nIt can be solved as:\\nSimilarly, for the timestep t i ‚àí 2 t_{i-2} , we have:\\nTherefore, if we perform N N steps of residual caching, as illustrated in Figure 5 , we can obtain:\\nThis further indicates that the current residual cache and the velocity cache are equivalent. Since Œî \\u200b t i ‚àí m + 1 , i ‚àí m \\\\Delta t_{{i-m+1},{i-m}} is a minimal value approaching zero, the coefficient before ùíó t i \\\\bm{v}_{t_{i}} is less than one. Therefore, it can be seen that the current residual cache is essentially a decayed form of the velocity cache. Furthermore, we observe that the solver determines the decay coefficient in Equation LABEL:eq:deltacache . However, as shown in Figure 2 d, the decay of velocity exhibits a timestep-dependent behavior. To account for this, we introduce an external timestep correction coefficient Œ≥ t i \\\\gamma_{t_{i}} . Notably, the AVDCache proposed in this paper reduces to Equation LABEL:eq:deltacache when the correction coefficient Œ≥ t i \\\\gamma_{t_{i}} equals 1.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Per-Task Visualization Results in the Benchmark',\n",
       "    'paragraphs': 'Due to space limitations, we put the visualization results of some tasks in the manuscript. Here, we provide a visual comparison of additional tasks and models. Figure 6 and Figure 7 show the visualization results of 11 tasks on Step1X-Edit. Figure 9 and Figure 10 show the visualization results of 11 tasks on Qwen-Image-Edit. Figure 8 show the visualization results of 5 tasks on FLUX.1 Kontext.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Per-Task Quantitative Results in the Benchmark',\n",
       "    'paragraphs': 'In this section, we present the performance of RegionE and the baseline methods on each task in the benchmark. Table 3 -Table 13 show the performance on the 11 tasks: motion-change, ps-human, color-alter, material-alter, subject-add, subject-remove, style-change, tone-transfer, subject-replace, text-change, and background-change. Table 14 -Table 18 show the performance on the five tasks: Character Reference, Style Reference, Text Editing, Instruction Editing-Global, and Instruction Editing-Local.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Recently, instruction-based image editing (IIE) has received widespread attention.\\n In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition.\\n We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step.\\n Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas.\\n For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information.\\n 3) Adaptive Velocity Decay Cache.\\n Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process.\\n We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57√ó, 2.41√ó, and 2.06√ó, respectively, with minimal quality loss (PSNR: 30.520‚Äì32.133). Evaluations by GPT-4o also confirmed that semantic and perceptual fidelity were well preserved.'},\n",
       " '2510.25007v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Accurate and efficient coding of Evaluation and Management (E/M) services using the Current Procedural Terminology 1 1 1 https://www.ama-assn.org/practice-management/cpt/cpt-overview-and-code-approval (CPT) system is crucial for healthcare providers, as it directly impacts billing accuracy, regulatory compliance, and revenue cycles. Traditionally, CPT E/M coding is performed by physicians and trained human coders who review clinical encounter notes and electronic health record (EHR) data to assign appropriate codes. They often rely on detailed guidelines American Medical Association ( 2023 ) to navigate the complexity of Medical Decision Making (MDM) for a given encounter. However, this manual process is resource-intensive, prone to inconsistencies due to variation in coder‚Äôs expertise, and vulnerable to errors arising from intricate coding guidelines and the nuanced nature of clinical reasoning.\\nMany studies highlight the financial and compliance ramifications of coding inaccuracies. For example, the U.S. Office of Inspector General (OIG) estimated that in 2010, Medicare inappropriately paid US$6.7 billion (21% of total E/M payments) due to incorrect coding and/or insufficient documentation; 42% of claims were miscoded and 19% lacked proper documentation U.S. Department of Health & Human Services, Office of Inspector\\nGeneral ( 2014 ) . A Florida-specific analysis estimated that nearly 9% of primary care visits (2.6 million annually) were undercoded, resulting in approximately US$114 million in lost revenue for hospitals Tenpas and Dietrich ( 2023 ) . These findings underscore both widespread inconsistency and the high cost of coding errors.\\nAutomation of CPT E/M coding offers several compelling advantages, including reducing human error through consistent application of complex coding rules, achieving scalable efficiency by handling high-volume chart reviews without increasing workload on physicians and coders, enhancing timely revenue capture by improved turnaround time. Additionally, automated coding can provide transparent and traceable reasoning, crucial for audits and compliance risk mitigation and enhance coding consistency, and optimizes coder resources by allowing human coders to focus on more complex or ambiguous cases. Beyond its immediate healthcare impact, CPT E/M coding exemplifies a broader class of problems where models must map free-text to codified decisions under strict guideline constraints. Insights from this work therefore extend to other regulated domains that require auditable, rule-aligned predictions.\\nIn this paper, we share our practical experience in developing and deploying ProFees, an automated CPT E/M coding model within a production EHR system. Development of ProFees provides solutions to five main complexities of real-world E/M coding, summarized in Table 1 . Leveraging advanced large language model (LLM) techniques, ProFees employs a dynamic few-shot prompting strategy to align coding decisions with best practices contextually relevant to each patients‚Äô encounter notes. Furthermore, an LLM-based critic is integrated to validate reasoning across the MDM elements explicitly, ensuring robust and explainable outcomes. To mitigate the stochasticity inherent in generative models, ProFees also adopts a self-consistency strategy to achieve reliable and consistent coding predictions.\\nThis paper‚Äôs key contributions include:\\nDevelopment and deployment of ProFees, a sophisticated LLM-based framework employing dynamic few-shot prompting, explicit criticism, and self-consistency techniques for accurate CPT E/M coding and intermediate MDM level estimation.\\nEmpirical demonstration of ProFees‚Äôs efficacy on a proprietary test dataset 2 2 2 This test set was meticulously annotated by our internal expert in the same way as the development set of 117 encounters. Due to privacy and regulatory constraints, this test set is not publicly available. with comparison against a commercial CPT E/M coding system and single-prompt baselines. Our findings highlight superiority of ProFees in E/M coding accuracy over both the commerical system and our strongest baseline by around 36% and 5% respectively, validating its efficacy in CPT E/M coding automation.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'CPT coding has gone through three broad phases of modeling and technical development, which are classical machine learning, deep learning and large language modeling.\\nClassical ML models heavily rely on feature engineering, which demands significant pre-processing efforts to convert structured and/or unstructured text data into hand-crafted features. Supervised classifiers, such as support-vector machines and gradient-boosted trees, are then learned on these features to predict CPT codes Morey et\\xa0al. ( 2025 ); Khaleghi et\\xa0al. ( 2021 ) . While achieving moderate accuracy, these models struggle with the complexity and variability of clinical narratives.\\nDeep learning models for CPT coding mainly feature encoder-decoder architectures. The encoder learns to map medical text into dense embeddings, based on which the decoder learns to predict the codes. Despite the wide success of DL models in ICD-10 coding Ji et\\xa0al. ( 2024 ) , their application to CPT coding is limited and has not been reported with favorable results compared to classical ML models Burns et\\xa0al. ( 2020 ); Levy et\\xa0al. ( 2022 ); Kim et\\xa0al. ( 2023b ) , which is likely due to the unavailability of large datasets for effective training.\\nRecently, LLMs have broken new grounds in medical coding, where with prompt engineering, GPT-4 is reported to have achieved comparable performance to SOTA DL models for ICD-10 coding Boyle et\\xa0al. ( 2023 ) . However, other studies also found that na√Øvely feeding raw texts and code descriptions into LLMs for code generation can only yield sub-optimal results that fall short of the coding performance of human coders Soroush et\\xa0al. ( 2024 ) . Overall, the development of effective LLM-based frameworks for medical coding is still in its infancy. This paper presents the first systematic attempt to develop such a framework for CPT E/M coding.',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Task Overview',\n",
       "    'paragraphs': 'CPT E/M coding involves assigning standardized billing codes based on the type and complexity of the patient encounter. In this work, we focus on outpatient and preventive medicine encounters. Figure 1 illustrates the decision process used by human coders, starting from determining the encounter type, followed by key branching logic such as whether the patient is new or established, and finally selecting the appropriate CPT code based on either the Medical Decision Making (MDM) level or the patient age group.\\nAmong these branches, determining the MDM level is the most complex and error-prone step. MDM is assessed across three elements: (1) the number and complexity of problems addressed during patient visit, (2) the amount and/or complexity of data reviewed and analyzed, and (3) the risk of complications and/or morbidity or mortality. The final MDM level is determined using a 2-out-of-3 rule, that at least two of the three elements must meet the criteria for a given complexity level (i.e. ‚Äústraightforward‚Äù, ‚Äúlow‚Äù, ‚Äúmoderate‚Äù, or ‚Äúhigh‚Äù).\\nThis process requires structured clinical reasoning. For instance, for the ‚Äúproblems addressed‚Äù element, distinguishing between ‚Äústable, chronic illness‚Äù (i.e. ‚Äúlow‚Äù complexity) and ‚Äúchronic illnesses with exacerbation, progression, or side effects of treatment‚Äù (i.e. ‚Äúmoderate‚Äù complexity) depends on close reading of clinical descriptions and inferred disease trajectories. Similarly, assessing the ‚Äúdata complexity‚Äù element may involve identifying whether documentation reflects ‚Äúordering unique test(s)‚Äù or ‚Äúindependent interpretation of tests‚Äù, which can elevate the data complexity from ‚Äúlow‚Äù to ‚Äúmoderate‚Äù or ‚Äúextensive‚Äù. See Appendix A for a full view of the 2024 edition of MDM guidelines used for developing ProFees\\nand Appendix F for a synthetic example.\\nIn this paper, we report only strict exact-match accuracy, as payers adjudicate at the code level and incorrect CPT E/M codes may be denied, downcoded, or adjusted, affecting reimbursement timelines. Near-miss codes do not change clinical care (E/M is a billing construct), but they have operational and financial implications: miscoding can trigger denials and rework; systematic overcoding increases legal/compliance risk despite any short-term gain; systematic undercoding reduces revenue and understates provider workload. In addition, although business outcomes, e.g., revenue uplift, are ultimately important, they are much harder to be attributed directly to CPT E/M code choices due to other down-the-line factors, such as diagnosis‚Äìprocedure linkage, billing completeness, payer-specific rules, and documentation quality. We therefore focus our evaluation on exact-match accuracy.',\n",
       "    'subsections': []},\n",
       "   {'title': '4 The ProFees Model',\n",
       "    'paragraphs': 'As illustrated in Fig. 2 , our proposed ProFees model for CPT E/M coding integrates LLM-based classifiers and self-critics, few-shot in-context learning Brown et\\xa0al. ( 2020 ) boosted by retrieval with chain-of-thought Wei et\\xa0al. ( 2023 ) exemplars, and traditional rule-based decision trees. The architecture comprises two primary LLM-driven modules: the Encounter Type Classifier , which identifies the nature of the clinical visit (e.g., inpatient, outpatient and preventive medicine), and the MDM Complexity Classifier , which predicts the level of medical decision-making (MDM) involved.\\nThis modular decomposition is directly motivated by CPT E/M coding guidelines, which determine the final code based on three key inputs: encounter type, MDM level, and patient type, with the patient type being readily available from structured EHR data. Therefore, our model is designed to predict the two remaining components: encounter type and MDM level. The initial MDM prediction is further refined through a Recursive Criticism and Improvement (RCI) process Kim et\\xa0al. ( 2023a ) to ensure guideline compliance. The outputs of both classifiers are then combined and processed by a downstream rule-based decision tree to produce the final CPT E/M code together with the corresponding justification.\\nOur model design reflects iterative failure analysis in collaboration with medical coders, addressing limitations of early monolithic prompt architectures that lacked interpretability, guideline adherence, and reliability. By modularizing prediction, reasoning, and validation, the system ensures contextual relevance through retrieval-augmented prompting, enforces compliance via recursive self-critique, and reduces hallucination by rule-based logic (e.g., majority voting, 2-out-of-3 CPT rules) to deterministic decision trees. This architecture achieves high accuracy, robust explainability via CoT and critic reviews, and strong alignment with clinical workflows‚Äîsupporting trust, adoption, and real-world deployment.\\nThis component identifies the type of encounters using an LLM. The model receives relevant contextual information such as SOAP notes 3 3 3 SOAP notes are a highly structured and standard format for documenting patient encounters, containing the following parts: S ubjective, O bjective, A ssessment and P lan. See Appendix F for an example. , patient age, and patient type, and returns an appropriate encounter classification.\\nThis component assesses the complexity level involved in medical decision-making. The prediction occurs in two stages: an initial classification followed by self-refinement through RCI. The initial prediction leverages dynamic few-shot prompting with external annotated examples retrieved from a Vector Database (VDB), the content of which consist of justifications and reasoning steps that were curated and validated by human coding experts to align with MDM guidelines. Subsequently, the RCI process systematically audits and refines the initial prediction to ensure adherence to comprehensive guideline criteria. To further enhance robustness, the model employs self-consistency via majority voting across multiple LLM inferences.\\nTo enable the MDM classifier to leverage annotated external knowledge for reference in reasoning and decision making. We designed and created a VDB that indexes encounters of the development and test datasets described in Section 5 , each consisting of:\\nGold justifications per MDM element, provided by our internal coding experts.\\nModel justifications per MDM element, generated by an LLM, curated and verified to be correct by our internal experts.\\nChain-of-thought reasoning (with step-by-step verification on well-designed checklists) for each MDM element, generated by an LLM, curated and verified by our internal experts.\\nDuring retrieval, the SOAP note of an input encounter is used to query the VDB to select Top- N N relevant exemplars using semantic vector search.\\nDespite the careful prompt engineering to align the MDM classifier with the CPT coding guidelines, we found that it still remained limited in cases that involve criteria not explicitly addressed by the guidelines or require clinical knowledge beyond the current capabilities of LLMs.\\nTo overcome these limitations, we introduced additional self-critics to follow the initial MDM classification for all the MDM elements, i.e., the ‚Äúproblem‚Äù and ‚Äúdata‚Äù complexity, and ‚Äúrisk of complications‚Äù. Each of the critics is prompted to review and critically evaluate the intermediate predictions of the corresponding MDM elements, by following checklists designed to address common generic errors identified by our internal expert for each MDM element.\\nA key challenge with LLMs is their inherent stochasticity; that even with identical inputs and controlled conditions, predictions can vary. As discussed by Atil et\\xa0al. ( 2025 ), even with fixed random seeds and identical model settings, deterministic model outputs cannot be guaranteed due to factors such as input subjectivity, variability in the LLM backend infrastructure, etc. This issue can pose risks in sensitive clinical contexts, potentially undermining the credibility of ProFees.\\nTherefore, in this paper, we have implemented a self-consistency strategy to enhance model output determinism. Specifically, our proposed model, the RCI-enhanced MDM classifier, is executed K K times concurrently, which avoids latency increases, with costs scaling proportionally. Each inference run generates intermediate predictions for all the MDM elements. Subsequently, a majority vote is conducted across the intermediate predictions of each element from these parallel runs. The consensus prediction for each element is then fed into the decision tree module to determine the MDM level and then the final CPT code.\\nIn cases where no majority is reached for an element, ties are broken by selecting the first prediction in the sorted result list from the parallel runs. For example, if the result list is {1, 2, 2, 2, 3, 4, 4}, the final output should be 2, while for {1, 2, 2, 2, 4, 4, 4}, 2 will still be chosen over 4 due to its precedence in tie-breaking.\\nThis module considers outputs from both the Encounter Type Classifier and the RCI-enhanced MDM Classifier, along with additional EHR data, based on which specific MDM rules are applied to produce the final CPT E/M code. 4 4 4 An associated LLM justification of the MDM level prediction is also passed down from the MDM Classifier module and output together with the final E/M code.\\nWe do not release the full prompts verbatim due to compliance and business confidentiality reasons. Instead, we share the prompt templates to capture the essence of our prompts to facilitate further research and community adoption. They can be found in Appendix C .',\n",
       "    'subsections': [{'title': '4.1 Design Rationale and Practical Impact',\n",
       "      'paragraphs': 'Our model design reflects iterative failure analysis in collaboration with medical coders, addressing limitations of early monolithic prompt architectures that lacked interpretability, guideline adherence, and reliability. By modularizing prediction, reasoning, and validation, the system ensures contextual relevance through retrieval-augmented prompting, enforces compliance via recursive self-critique, and reduces hallucination by rule-based logic (e.g., majority voting, 2-out-of-3 CPT rules) to deterministic decision trees. This architecture achieves high accuracy, robust explainability via CoT and critic reviews, and strong alignment with clinical workflows‚Äîsupporting trust, adoption, and real-world deployment.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Encounter Type Classifier',\n",
       "      'paragraphs': 'This component identifies the type of encounters using an LLM. The model receives relevant contextual information such as SOAP notes 3 3 3 SOAP notes are a highly structured and standard format for documenting patient encounters, containing the following parts: S ubjective, O bjective, A ssessment and P lan. See Appendix F for an example. , patient age, and patient type, and returns an appropriate encounter classification.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 MDM Complexity Classifier',\n",
       "      'paragraphs': 'This component assesses the complexity level involved in medical decision-making. The prediction occurs in two stages: an initial classification followed by self-refinement through RCI. The initial prediction leverages dynamic few-shot prompting with external annotated examples retrieved from a Vector Database (VDB), the content of which consist of justifications and reasoning steps that were curated and validated by human coding experts to align with MDM guidelines. Subsequently, the RCI process systematically audits and refines the initial prediction to ensure adherence to comprehensive guideline criteria. To further enhance robustness, the model employs self-consistency via majority voting across multiple LLM inferences.\\nTo enable the MDM classifier to leverage annotated external knowledge for reference in reasoning and decision making. We designed and created a VDB that indexes encounters of the development and test datasets described in Section 5 , each consisting of:\\nGold justifications per MDM element, provided by our internal coding experts.\\nModel justifications per MDM element, generated by an LLM, curated and verified to be correct by our internal experts.\\nChain-of-thought reasoning (with step-by-step verification on well-designed checklists) for each MDM element, generated by an LLM, curated and verified by our internal experts.\\nDuring retrieval, the SOAP note of an input encounter is used to query the VDB to select Top- N N relevant exemplars using semantic vector search.\\nDespite the careful prompt engineering to align the MDM classifier with the CPT coding guidelines, we found that it still remained limited in cases that involve criteria not explicitly addressed by the guidelines or require clinical knowledge beyond the current capabilities of LLMs.\\nTo overcome these limitations, we introduced additional self-critics to follow the initial MDM classification for all the MDM elements, i.e., the ‚Äúproblem‚Äù and ‚Äúdata‚Äù complexity, and ‚Äúrisk of complications‚Äù. Each of the critics is prompted to review and critically evaluate the intermediate predictions of the corresponding MDM elements, by following checklists designed to address common generic errors identified by our internal expert for each MDM element.\\nA key challenge with LLMs is their inherent stochasticity; that even with identical inputs and controlled conditions, predictions can vary. As discussed by Atil et\\xa0al. ( 2025 ), even with fixed random seeds and identical model settings, deterministic model outputs cannot be guaranteed due to factors such as input subjectivity, variability in the LLM backend infrastructure, etc. This issue can pose risks in sensitive clinical contexts, potentially undermining the credibility of ProFees.\\nTherefore, in this paper, we have implemented a self-consistency strategy to enhance model output determinism. Specifically, our proposed model, the RCI-enhanced MDM classifier, is executed K K times concurrently, which avoids latency increases, with costs scaling proportionally. Each inference run generates intermediate predictions for all the MDM elements. Subsequently, a majority vote is conducted across the intermediate predictions of each element from these parallel runs. The consensus prediction for each element is then fed into the decision tree module to determine the MDM level and then the final CPT code.\\nIn cases where no majority is reached for an element, ties are broken by selecting the first prediction in the sorted result list from the parallel runs. For example, if the result list is {1, 2, 2, 2, 3, 4, 4}, the final output should be 2, while for {1, 2, 2, 2, 4, 4, 4}, 2 will still be chosen over 4 due to its precedence in tie-breaking.',\n",
       "      'subsections': [{'title': '4.3.1 Dynamic Few-Shot CoT Prompting',\n",
       "        'paragraphs': 'To enable the MDM classifier to leverage annotated external knowledge for reference in reasoning and decision making. We designed and created a VDB that indexes encounters of the development and test datasets described in Section 5 , each consisting of:\\nGold justifications per MDM element, provided by our internal coding experts.\\nModel justifications per MDM element, generated by an LLM, curated and verified to be correct by our internal experts.\\nChain-of-thought reasoning (with step-by-step verification on well-designed checklists) for each MDM element, generated by an LLM, curated and verified by our internal experts.\\nDuring retrieval, the SOAP note of an input encounter is used to query the VDB to select Top- N N relevant exemplars using semantic vector search.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.3.2 Recursive Criticism and Improvement',\n",
       "        'paragraphs': 'Despite the careful prompt engineering to align the MDM classifier with the CPT coding guidelines, we found that it still remained limited in cases that involve criteria not explicitly addressed by the guidelines or require clinical knowledge beyond the current capabilities of LLMs.\\nTo overcome these limitations, we introduced additional self-critics to follow the initial MDM classification for all the MDM elements, i.e., the ‚Äúproblem‚Äù and ‚Äúdata‚Äù complexity, and ‚Äúrisk of complications‚Äù. Each of the critics is prompted to review and critically evaluate the intermediate predictions of the corresponding MDM elements, by following checklists designed to address common generic errors identified by our internal expert for each MDM element.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.3.3 Self-Consistency via Majority Voting',\n",
       "        'paragraphs': 'A key challenge with LLMs is their inherent stochasticity; that even with identical inputs and controlled conditions, predictions can vary. As discussed by Atil et\\xa0al. ( 2025 ), even with fixed random seeds and identical model settings, deterministic model outputs cannot be guaranteed due to factors such as input subjectivity, variability in the LLM backend infrastructure, etc. This issue can pose risks in sensitive clinical contexts, potentially undermining the credibility of ProFees.\\nTherefore, in this paper, we have implemented a self-consistency strategy to enhance model output determinism. Specifically, our proposed model, the RCI-enhanced MDM classifier, is executed K K times concurrently, which avoids latency increases, with costs scaling proportionally. Each inference run generates intermediate predictions for all the MDM elements. Subsequently, a majority vote is conducted across the intermediate predictions of each element from these parallel runs. The consensus prediction for each element is then fed into the decision tree module to determine the MDM level and then the final CPT code.\\nIn cases where no majority is reached for an element, ties are broken by selecting the first prediction in the sorted result list from the parallel runs. For example, if the result list is {1, 2, 2, 2, 3, 4, 4}, the final output should be 2, while for {1, 2, 2, 2, 4, 4, 4}, 2 will still be chosen over 4 due to its precedence in tie-breaking.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.4 Decision Tree',\n",
       "      'paragraphs': 'This module considers outputs from both the Encounter Type Classifier and the RCI-enhanced MDM Classifier, along with additional EHR data, based on which specific MDM rules are applied to produce the final CPT E/M code. 4 4 4 An associated LLM justification of the MDM level prediction is also passed down from the MDM Classifier module and output together with the final E/M code.\\nWe do not release the full prompts verbatim due to compliance and business confidentiality reasons. Instead, we share the prompt templates to capture the essence of our prompts to facilitate further research and community adoption. They can be found in Appendix C .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Data Gathering and Annotation',\n",
       "    'paragraphs': 'We collected 216 real-world encounters from hospital production databases. The dataset comprises de-identified outpatient/preventative medicine encounters, each represented by a SOAP note, together with additional patient EHR data, such as demographics and medical orders.\\nEach encounter is annotated with a CPT E/M code assigned by the physician, along with a corresponding code provided by a professional coder. The coder‚Äôs code either confirms or corrects the physician‚Äôs selection and is used as the final billing code. However, the dataset lacks detailed justification for how the final CPT E/M code was determined, including the assessed complexity levels for the three MDM elements. Therefore, we engaged an internal medical expert to provide fine-grained annotations of both the E/M code and the MDM element complexity for each encounter.\\nThese encounters were sampled incrementally to reflect the real-world distribution of CPT E/M codes observed in production data, using a larger dataset with coarse-level CPT labels as the reference. The sample spans multiple physician specialties, including family practice, internal medicine, urology, hematology, oncology, and cardiology (see Figures 7 and 8 ), rather than being limited to a single domain. Subsequently, our internal expert coders reviewed the set to confirm that the inclusiveness and coverage of specialties is sufficiently broad.\\nFor each encounter, our internal expert annotates the problem, data, and risk of complexity levels together with free-text justifications. The expert also assigned an overall CPT E/M code along with a justification for the selection. Using this procedure, we annotated a total of 117 encounters for model development. The CPT codes in the test set were annotated by the same expert in the same way as the development set.\\nOur analysis revealed a high degree of disagreement between the coders. Specifically, the expert agreed with the professional coder‚Äôs CPT E/M codes on 52 encounters (44%), and disagreed on the remaining 65 (56%). Accordingly, we partitioned the development dataset into two subsets: Platinum , comprising encounters with agreement and thus more reliable ground-truth labels; and Disagreement , containing encounters where the coders diverged, reflecting more challenging coding decisions. Initial prompt tuning and critic-checklist design are performed exclusively on the Platinum subset. We then carry out iterative error analysis on both Platinum and Disagreement with our internal expert, refining prompts design while correcting any labelling errors identified.\\nOur final dataset comprises three subsets, as shown in Table 2 . The Platinum and Disagreement subsets serve as our primary development data. The separate Test subset is used for model evaluation. Given the stringent compliance, data safety and privacy requirements associated with collecting real-world patient data, large-scale data acquisition is challenging. Thus, we aimed to include a diverse range of CPT E/M codes across these subsets. Distributions of the CPT E/M codes (See Figures 4 , 5 and 6 ) and the data annotation interface (See Figure 9 ) can be found in Appendix D .',\n",
       "    'subsections': [{'title': 'Data Collection.',\n",
       "      'paragraphs': 'We collected 216 real-world encounters from hospital production databases. The dataset comprises de-identified outpatient/preventative medicine encounters, each represented by a SOAP note, together with additional patient EHR data, such as demographics and medical orders.\\nEach encounter is annotated with a CPT E/M code assigned by the physician, along with a corresponding code provided by a professional coder. The coder‚Äôs code either confirms or corrects the physician‚Äôs selection and is used as the final billing code. However, the dataset lacks detailed justification for how the final CPT E/M code was determined, including the assessed complexity levels for the three MDM elements. Therefore, we engaged an internal medical expert to provide fine-grained annotations of both the E/M code and the MDM element complexity for each encounter.\\nThese encounters were sampled incrementally to reflect the real-world distribution of CPT E/M codes observed in production data, using a larger dataset with coarse-level CPT labels as the reference. The sample spans multiple physician specialties, including family practice, internal medicine, urology, hematology, oncology, and cardiology (see Figures 7 and 8 ), rather than being limited to a single domain. Subsequently, our internal expert coders reviewed the set to confirm that the inclusiveness and coverage of specialties is sufficiently broad.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Data Annotation.',\n",
       "      'paragraphs': 'For each encounter, our internal expert annotates the problem, data, and risk of complexity levels together with free-text justifications. The expert also assigned an overall CPT E/M code along with a justification for the selection. Using this procedure, we annotated a total of 117 encounters for model development. The CPT codes in the test set were annotated by the same expert in the same way as the development set.\\nOur analysis revealed a high degree of disagreement between the coders. Specifically, the expert agreed with the professional coder‚Äôs CPT E/M codes on 52 encounters (44%), and disagreed on the remaining 65 (56%). Accordingly, we partitioned the development dataset into two subsets: Platinum , comprising encounters with agreement and thus more reliable ground-truth labels; and Disagreement , containing encounters where the coders diverged, reflecting more challenging coding decisions. Initial prompt tuning and critic-checklist design are performed exclusively on the Platinum subset. We then carry out iterative error analysis on both Platinum and Disagreement with our internal expert, refining prompts design while correcting any labelling errors identified.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Final Datasets.',\n",
       "      'paragraphs': 'Our final dataset comprises three subsets, as shown in Table 2 . The Platinum and Disagreement subsets serve as our primary development data. The separate Test subset is used for model evaluation. Given the stringent compliance, data safety and privacy requirements associated with collecting real-world patient data, large-scale data acquisition is challenging. Thus, we aimed to include a diverse range of CPT E/M codes across these subsets. Distributions of the CPT E/M codes (See Figures 4 , 5 and 6 ) and the data annotation interface (See Figure 9 ) can be found in Appendix D .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Evaluation and Results',\n",
       "    'paragraphs': 'We combine the Platinum and Disagreement subset, with a total of 117 encounters, and use them as the ‚Äútuning‚Äù dataset to refine prompts of both ProFees and the baselines. This ensures that both models are exposed to the same amount and type of generalizable knowledge, extracted by the internal expert from the error cases in the tuning set. The Test dataset, on the other hand, comprises the other 99 encounters.\\nWe build the VDB with both the tuning/development and test datasets with the indexing procedures specified in Section 4.3.1 . To ensure no information leakage occurs during the retrieval of dynamic few-shot exemplars from the VDB, we adopt the leave-one-out strategy, i.e., always filtering out the query‚Äôs corresponding example from the retrieved list. Additionally, semantic dense retrieval is performed with N = 3 N=3 for Top- N N exemplars retrieved (post leave-one-out filtering).\\nWe compare against (i) System A , a commercial rule-based coding tool, 5 5 5 The vendor name and detailed description are withheld to preserve anonymity. and\\n(ii) four Single prompt variants ranging from a single-prompt system to prompts augmented with CoT and full contextual information (i.e. EHR data).\\nNote that we use Azure GPT-4o-2024-05-13 as the foundation LLM for both the baseline Single Prompt variants and our Profees model.\\nWe conduct accuracy-based evaluation at different granularity levels of classification. These include both the final CPT accuracy and that of the intermediate MDM element complexity levels. For each metric, we report the average result as well as the standard deviation over 5 runs with different random seeds.\\nNote that the intermediate complexity prediction are not available for System A and are therefore omitted.\\nFor each of the 5 runs, we set the number of predictions made for each test encounter to be K = 3 K=3 to enforce the self-consistency majority vote (see Section 4.3.3 ), while balancing the cost and latency of LLM calling. In addition, we set the temperature for all LLM calls to be 0.\\nFor setting of RCI round, we found empirically that ProFees‚Äôs intermediate complexity prediction settled with 1 round of RCI for each MDM element. For simplicity, throughout the experiment, we also refer such 1-round setting as RCI.\\nTable 3 presents performance, as measured by accuracy improvements over the ‚ÄúSingle prompt‚Äù baseline, on the 99-encounter Test set.\\nOur full model ( Few-Shot + RCI ) achieves the highest CPT accuracy,\\n36.85% higher than the commercial System A,\\n33.73% higher than the Single prompt baseline, and 4.73% over the strongest baseline (i.e. Single prompt + Full Info + CoT).\\nNotably, adding RCI to our model lifts MDM accuracy by 4% in the few-shot setting and 2.33% in the zero-shot setting, confirming the value of structured self-critique.\\nMoreover, our full model significantly outperforms the strongest baseline on MDM, PC, DC and RC accuracy by 8‚Äì17 absolute points, further demonstrating its efficacy. Furthermore, a detailed ablation study can be found in Appendix B . We also found that the overall trend of performance improvements brought by ProFees preserved on the internal-external coder agreement subset of the Test set, confirming that ProFees coding capability is generalizable towards unseen real-world encounters. Detailed results on the agreement subset can be found in Appendix E .',\n",
       "    'subsections': [{'title': '6.1 Experiment Settings',\n",
       "      'paragraphs': 'We combine the Platinum and Disagreement subset, with a total of 117 encounters, and use them as the ‚Äútuning‚Äù dataset to refine prompts of both ProFees and the baselines. This ensures that both models are exposed to the same amount and type of generalizable knowledge, extracted by the internal expert from the error cases in the tuning set. The Test dataset, on the other hand, comprises the other 99 encounters.\\nWe build the VDB with both the tuning/development and test datasets with the indexing procedures specified in Section 4.3.1 . To ensure no information leakage occurs during the retrieval of dynamic few-shot exemplars from the VDB, we adopt the leave-one-out strategy, i.e., always filtering out the query‚Äôs corresponding example from the retrieved list. Additionally, semantic dense retrieval is performed with N = 3 N=3 for Top- N N exemplars retrieved (post leave-one-out filtering).\\nWe compare against (i) System A , a commercial rule-based coding tool, 5 5 5 The vendor name and detailed description are withheld to preserve anonymity. and\\n(ii) four Single prompt variants ranging from a single-prompt system to prompts augmented with CoT and full contextual information (i.e. EHR data).\\nNote that we use Azure GPT-4o-2024-05-13 as the foundation LLM for both the baseline Single Prompt variants and our Profees model.\\nWe conduct accuracy-based evaluation at different granularity levels of classification. These include both the final CPT accuracy and that of the intermediate MDM element complexity levels. For each metric, we report the average result as well as the standard deviation over 5 runs with different random seeds.\\nNote that the intermediate complexity prediction are not available for System A and are therefore omitted.\\nFor each of the 5 runs, we set the number of predictions made for each test encounter to be K = 3 K=3 to enforce the self-consistency majority vote (see Section 4.3.3 ), while balancing the cost and latency of LLM calling. In addition, we set the temperature for all LLM calls to be 0.\\nFor setting of RCI round, we found empirically that ProFees‚Äôs intermediate complexity prediction settled with 1 round of RCI for each MDM element. For simplicity, throughout the experiment, we also refer such 1-round setting as RCI.',\n",
       "      'subsections': [{'title': 'Dataset.',\n",
       "        'paragraphs': 'We combine the Platinum and Disagreement subset, with a total of 117 encounters, and use them as the ‚Äútuning‚Äù dataset to refine prompts of both ProFees and the baselines. This ensures that both models are exposed to the same amount and type of generalizable knowledge, extracted by the internal expert from the error cases in the tuning set. The Test dataset, on the other hand, comprises the other 99 encounters.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Vector Database.',\n",
       "        'paragraphs': 'We build the VDB with both the tuning/development and test datasets with the indexing procedures specified in Section 4.3.1 . To ensure no information leakage occurs during the retrieval of dynamic few-shot exemplars from the VDB, we adopt the leave-one-out strategy, i.e., always filtering out the query‚Äôs corresponding example from the retrieved list. Additionally, semantic dense retrieval is performed with N = 3 N=3 for Top- N N exemplars retrieved (post leave-one-out filtering).',\n",
       "        'subsections': []},\n",
       "       {'title': 'Baselines.',\n",
       "        'paragraphs': 'We compare against (i) System A , a commercial rule-based coding tool, 5 5 5 The vendor name and detailed description are withheld to preserve anonymity. and\\n(ii) four Single prompt variants ranging from a single-prompt system to prompts augmented with CoT and full contextual information (i.e. EHR data).\\nNote that we use Azure GPT-4o-2024-05-13 as the foundation LLM for both the baseline Single Prompt variants and our Profees model.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Evaluation Metrics.',\n",
       "        'paragraphs': 'We conduct accuracy-based evaluation at different granularity levels of classification. These include both the final CPT accuracy and that of the intermediate MDM element complexity levels. For each metric, we report the average result as well as the standard deviation over 5 runs with different random seeds.\\nNote that the intermediate complexity prediction are not available for System A and are therefore omitted.\\nFor each of the 5 runs, we set the number of predictions made for each test encounter to be K = 3 K=3 to enforce the self-consistency majority vote (see Section 4.3.3 ), while balancing the cost and latency of LLM calling. In addition, we set the temperature for all LLM calls to be 0.\\nFor setting of RCI round, we found empirically that ProFees‚Äôs intermediate complexity prediction settled with 1 round of RCI for each MDM element. For simplicity, throughout the experiment, we also refer such 1-round setting as RCI.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '6.2 Overall Results',\n",
       "      'paragraphs': 'Table 3 presents performance, as measured by accuracy improvements over the ‚ÄúSingle prompt‚Äù baseline, on the 99-encounter Test set.\\nOur full model ( Few-Shot + RCI ) achieves the highest CPT accuracy,\\n36.85% higher than the commercial System A,\\n33.73% higher than the Single prompt baseline, and 4.73% over the strongest baseline (i.e. Single prompt + Full Info + CoT).\\nNotably, adding RCI to our model lifts MDM accuracy by 4% in the few-shot setting and 2.33% in the zero-shot setting, confirming the value of structured self-critique.\\nMoreover, our full model significantly outperforms the strongest baseline on MDM, PC, DC and RC accuracy by 8‚Äì17 absolute points, further demonstrating its efficacy. Furthermore, a detailed ablation study can be found in Appendix B . We also found that the overall trend of performance improvements brought by ProFees preserved on the internal-external coder agreement subset of the Test set, confirming that ProFees coding capability is generalizable towards unseen real-world encounters. Detailed results on the agreement subset can be found in Appendix E .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 Conclusion and Future Work',\n",
       "    'paragraphs': 'This work presents the first systematic, production‚Äêoriented study of automating CPT E/M coding with large language models. We propose a modular architecture that (i) retrieves context-relevant chain-of-thought exemplars, (ii) predicts both encounter type and MDM element complexities, (iii) performs RCI to critic those intermediate decisions, and (iv) consolidates results with deterministic rule logic and a Self-Consistency ensemble.\\nOn a de-identified, expert-annotated Test dataset of 99 encounters, our model Profees improves CPT accuracy by 4.73 % over the strongest LLM baseline and by 30.61 % over the commercial CPT E/M coding software\\nSystem A.\\nOur future work includes extending the model to support multiple codes, CPT-modifiers and generating synthetic datasets for edge-case testing and enriching our VDB.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Limitations',\n",
       "    'paragraphs': 'ProFees currently predicts only one CPT E/M code per encounter, and we plan to support predicting multiple codes (e.g. for a preventive medicine visit, in which a problem-oriented service is also provided). It can be easily supported by extending the Encounter Type Classifier to output multiple encounter types. We also acknowledge the current lack of publicly available datasets for CPT E/M coding within the broader research community. To address this gap, we are curating and plan to release a synthetic dataset to facilitate research on improving model performance, robustness, and scalability in this domain.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Ethical Considerations',\n",
       "    'paragraphs': 'While ProFees demonstrates a significant improvement in CPT E/M coding accuracy, its integration into EHR systems requires careful consideration of several key ethical issues to ensure its responsible deployment.\\nPrivacy and data integrity. Our development and evaluation efforts were conducted in a secure, HIPAA-compliant environment. As mentioned in Section 5 , the development of ProFees utilize de-identified real-patient data. For deployment, ProFees will be deployed in a secure cloud environment with state-of-the-art safeguards to ensure patient confidentiality.\\nAccountability and liability. Automating clinical coding may introduce ambiguity on accountability for coding errors. Ultimately, ProFees acts as an assistive tool, but not an autonomous decision-making tool, that helps physicians and professional coders, who take the final responsibility.\\nAlgorithmic biases and fairness. Our model was tuned and evaluated on a small-scale real-world dataset, which may contain latent biases. Thus, there is a risk that the model could learn (e.g. through few-shot examples) to systematically assign different CPT E/M codes based on patterns in encounter notes correlated with patient demographics (e.g. race and gender). To mitigate this risk, we are curating a synthetic dataset to enable targeted evaluation of model behaviors.\\nInterpretability and transparency. Trust in a healthcare setting demands interpretability. While frontier LLMs are often considered blackboxes, we have developed ProFees to improve its interpretability. Specifically, through carefully designed prompts and architecture, we elicit sophisticated, multi-step reasoning from LLMs, leverage both internal and external clinical coding knowledge, and generate detailed rationales for coding outputs. This interpretability is not only crucial for gaining user trust and enabling human oversight, but also providing clear rationales for external audits.\\nAutomation bias and human oversight. ProFees‚Äô strong performance (a > 30 >30 -point accuracy improvement) may create a risk of automation bias . Specifically, it is plausible that human users might develop a degree of reliance higher than what is warranted. ProFees generates detailed rationales that justify suggested codes. The UI/UX and workflow design should enable and facilitate human review of the codes and corresponding rationales. Moreover, continued user education of their roles and responsibility is necessary.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgement',\n",
       "    'paragraphs': 'We would like to thank other members of Oracle Health & AI for their collaboration while deploying ProFees in production, and Irfan Bulu, Raefer Gabriel, Neil Hauge, Mark Johnson, Kiran Rama, Amitabh Saikia, Vishal Vishnoi, and Krishnaram Kenthapadi for insightful feedback and discussions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A MDM Guidelines',\n",
       "    'paragraphs': 'Table 4 elaborates the MDM guidelines from the 2024 edition of the CPT Professional Manual book American Medical Association ( 2023 ) that we have leveraged for building ProFees.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Ablation Studies',\n",
       "    'paragraphs': 'Replacing the verbose prompt with three retrieved CoT examples increases CPT accuracy by 4% relative to the zero-shot CoT variant, i.e. ProFees (Zero-Shot + RCI), and by 2.3% when comparing ProFees (Few-Shot) to ProFees (Zero-Shot).\\nAdding the RCI stage boosts all three intermediate complexity scores, yielding a 7.58% percentage-point gain in overall MDM accuracy and a 4% gain in final CPT accuracy. This confirms that a dedicated post-hoc critic effectively detects and corrects the most common intermediate errors.\\nFigure 3 reports relative accuracy gains (compared to ProFees with 1 vote/self-consistency run, i.e. K = 1 K{=}1 ), together with normalised cost. Increasing the vote count from one pass to K = 3 K{=}3 yields an additional + 1.2 % +1.2\\\\% CPT accuracy and + 2.7 % +2.7\\\\% MDM accuracy, but triples the unit cost.\\nRaising the count further to K = 5 K{=}5 adds only + 3.1 % +3.1\\\\% CPT and + 6.8 % +6.8\\\\% MDM over the 1-vote ProFees while pushing the cost above a four-fold increase.\\nIntermediate metrics show the following pattern: Data-Complexity gains + 11.8 % +11.8\\\\% at K = 3 K{=}3 and + 19.9 % +19.9\\\\% at K = 5 K{=}5 ; Problem-Complexity peaks at K = 3 K{=}3 then declines slightly, and Risk improves by just + 2.7 % +2.7\\\\% overall.\\nTo balance the accuracy improvements against the cost curve, we have chosen to fix K = 3 K{=}3 for all remaining experiments.',\n",
       "    'subsections': [{'title': 'Dynamic Few-Shot CoT Prompting.',\n",
       "      'paragraphs': 'Replacing the verbose prompt with three retrieved CoT examples increases CPT accuracy by 4% relative to the zero-shot CoT variant, i.e. ProFees (Zero-Shot + RCI), and by 2.3% when comparing ProFees (Few-Shot) to ProFees (Zero-Shot).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Recursive Criticism and Improvement (RCI).',\n",
       "      'paragraphs': 'Adding the RCI stage boosts all three intermediate complexity scores, yielding a 7.58% percentage-point gain in overall MDM accuracy and a 4% gain in final CPT accuracy. This confirms that a dedicated post-hoc critic effectively detects and corrects the most common intermediate errors.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Self-Consistency.',\n",
       "      'paragraphs': 'Figure 3 reports relative accuracy gains (compared to ProFees with 1 vote/self-consistency run, i.e. K = 1 K{=}1 ), together with normalised cost. Increasing the vote count from one pass to K = 3 K{=}3 yields an additional + 1.2 % +1.2\\\\% CPT accuracy and + 2.7 % +2.7\\\\% MDM accuracy, but triples the unit cost.\\nRaising the count further to K = 5 K{=}5 adds only + 3.1 % +3.1\\\\% CPT and + 6.8 % +6.8\\\\% MDM over the 1-vote ProFees while pushing the cost above a four-fold increase.\\nIntermediate metrics show the following pattern: Data-Complexity gains + 11.8 % +11.8\\\\% at K = 3 K{=}3 and + 19.9 % +19.9\\\\% at K = 5 K{=}5 ; Problem-Complexity peaks at K = 3 K{=}3 then declines slightly, and Risk improves by just + 2.7 % +2.7\\\\% overall.\\nTo balance the accuracy improvements against the cost curve, we have chosen to fix K = 3 K{=}3 for all remaining experiments.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix C High-Level Prompt Templates for CPT E/M Components',\n",
       "    'paragraphs': 'For compliance, privacy, and commercial reasons, we cannot release our full production prompts verbatim.\\nInstead, we share template skeletons that capture the role , task , and output schema of each LLM component while redacting institution-specific wording.\\nEach template specifies:\\nthe LLM role (e.g., ‚ÄúEncounter-Type Classifier‚Äù, ‚ÄúProblem-Complexity Critic‚Äù);\\nthe core task description and minimal guidelines needed for replication;\\nthe expected output format (always JSON); and\\nplaceholders for dynamic inputs such as {{soap_note}} or retrieved exemplars {{few_shot_examples}} .\\nResearchers may adapt these templates to their own datasets and policies while remaining compatible with the architecture in the main paper.',\n",
       "    'subsections': [{'title': 'C.1 Encounter-Type Prompt',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.2 MDM-Complexity Prompt (Initial Pass)',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.3 Critic Prompt for Problem Complexity',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.4 Critic Prompt for Data Complexity',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.5 Critic Prompt for Risk Assessment',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix D Datasets',\n",
       "    'paragraphs': 'This section presents more details of our datasets, including the CPT E/M code distribution over the three subsets and an example data annotation page.\\nFigures 4 , 5 and 6 illustrate the distribution of CPT E/M codes in the Platinum , Disagreement and Test datasets respectively.\\nFigures 7 and 8 illutrate the distribution of physician specialties in the combined Platinum + Disagreement , and Test datasets respectively.\\nFigure 9 shows our data annotation interface.',\n",
       "    'subsections': [{'title': 'D.1 Distribution of CPT E/M Codes',\n",
       "      'paragraphs': 'Figures 4 , 5 and 6 illustrate the distribution of CPT E/M codes in the Platinum , Disagreement and Test datasets respectively.',\n",
       "      'subsections': []},\n",
       "     {'title': 'D.2 Specialty Distribution',\n",
       "      'paragraphs': 'Figures 7 and 8 illutrate the distribution of physician specialties in the combined Platinum + Disagreement , and Test datasets respectively.',\n",
       "      'subsections': []},\n",
       "     {'title': 'D.3 Data Annotation Interface',\n",
       "      'paragraphs': 'Figure 9 shows our data annotation interface.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix E Additional Experiments',\n",
       "    'paragraphs': 'We present the performance of ProFees on the agreement subset of our Test dataset in Table 5 , on which our internal coder agrees with external coders‚Äô CPT E/M codes. Specifically, they agree on 54 out of the 99 test encounters.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix F Synthetic Example',\n",
       "    'paragraphs': 'This section shows an example of synthetically generated data, aligning with the data used in our work.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Evaluation and Management (E/M) coding, under the Current Procedural Terminology (CPT) taxonomy, documents medical services provided to patients by physicians. Used primarily for billing purposes, it is in physicians‚Äô best interest to provide accurate CPT E/M codes. Automating this coding task will help alleviate physicians‚Äô documentation burden, improve billing efficiency, and ultimately enable better patient care. However, a number of real-world complexities have made E/M encoding automation a challenging task. In this paper, we elaborate some of the key complexities and present ProFees, our LLM-based framework that tackles them, followed by a systematic evaluation. On an expert-curated real-world dataset, ProFees achieves an increase in coding accuracy of more than 36% over a commercial CPT E/M coding system and almost 5% over our strongest single-prompt baseline, demonstrating its effectiveness in addressing the real-world complexities.\\n'},\n",
       " '2510.25388v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Research into non-learning-based decision-making algorithms such as Monte Carlo Tree Search (MCTS) (Browne et\\xa0al., 2012 ; Kocsis & Szepesv√°ri, 2006 ) is an active field. On the one hand MCTS can be used for applications where a general on-the-fly applicable decision-making algorithm is needed such as Game Studios which rarely use Machine Learning (ML) based AI as they would have to be retrained whenever the game rules are modified (e.g. during development or patches). And on the other hand, though not the scope of this paper, foundational work in MCTS might potentially translate to improvements of ML algorithms such as Alpha Zero (Silver et\\xa0al., 2017 ) that are built on MCTS.\\nOne way to improve MCTS is to reduce the search space by grouping states and actions in the current MCTS search tree to enable an intra-layer information flow (Jiang et\\xa0al., 2014 ; Anand et\\xa0al., 2015 ; 2016 ) by averaging the visits and returns of all abstract action nodes in the same abstract node used for the Upper Confidence Bounds (UCB) formula in the tree policy, which increases the sample efficiency.\\nOne key strength of one of the state-of-the-art abstraction algorithms On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) (Anand et\\xa0al., 2016 ) is its exactness in the sense that if OGA-UCT groups two state-action pairs in a search tree where all possible successors of each state-action pair have been sampled, only state-action pairs are grouped that have the same Q ‚àó Q^{*} value, i.e., they have the same value under subsequent optimal play. This exactness condition, however, comes at the cost that state or state-action pairs that only differ slightly in their Q ‚àó Q^{*} value cannot be detected. This issue was slightly alleviated with the introduction of ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA (Schm√∂cker et\\xa0al., 2025c ) , which is equivalent to ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA in deterministic environments that allows for small errors in the immediate reward or transition function when building an abstraction. However, this also brings two downsides with it. Firstly, it introduces two parameters, which makes tuning harder, and secondly, grouping non-value equivalent state-action pairs might even be harmful to the performance, as they can make convergence to the optimal action impossible.\\nIn this work we propose Known Value Differences Abstractions UCT (KVDA-UCT), that relaxes the strict abstraction conditions of OGA-UCT to detect almost as many abstractions as ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA in deterministic environments but without losing the exactness condition. The novel idea that makes this possible is to deliberately group states or state-action pairs that do not have the same value if we know the difference between their values which is inferred by analysis of the search tree‚Äôs immediate rewards. When the abstractions are used, instead of averaging state-action pair values directly, their difference-accounted values are averaged.\\nThe contributions of this paper can be summarized as follows:\\n1. We introduce the Known-Value-Difference (KVDA) abstraction framework that extends the Abstractions of State-Action Pairs (ASAP) framework used by OGA-UCT. Fig. 1 is an example of a simple state-transition graph in which KVDA finds three non-trivial abstractions, while ASAP would detect none.\\n2. We propose and empirically evaluate KVDA-UCT, a modification of OGA-UCT that introduces no parameters and uses and builds KVDA abstractions. We show that KVDA-UCT outperforms OGA-UCT in most of the here-considered deterministic environments. We also compare KVDA-UCT with ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA and show that it either performs equally well or better than a parameter-optimized ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA agent.\\n3. We also consider the stochastic setting where we generalize KVDA-UCT to Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA which allows for errors in the transition function when building the abstraction. Furthermore, we compare it to ( Œµ t , Œµ a ) (\\\\varepsilon_{\\\\text{t}},\\\\varepsilon_{\\\\text{a}}) -OGA and show that, unlike in the deterministic setting, KVDA rarely performs better than ( Œµ t , Œµ a ) (\\\\varepsilon_{\\\\text{t}},\\\\varepsilon_{\\\\text{a}}) -OGA in stochastic environments.\\nThe paper is structured as follows. In Section 2 , the theoretical groundwork for automatic state and state-action pair abstractions is laid. In particular, we define OGA-UCT and ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA. Then, in Section 3 , our novel KVDA framework and both KVDA-UCT and Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA are introduced. Afterwards, in Section 4 , the experimental setup is defined, then in Section 5 , the experimental results using this setup are shown and discussed. The paper is concluded by a discussion of the limitations of KVDA-UCT and avenues for future work in Section 6 .',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Foundations of automatic abstractions',\n",
       "    'paragraphs': 'In this section, we will be laying the theoretical groundwork for this paper as well as introducing related work.\\nFor a comprehensive overview of non-learning-based abstractions, we refer to the survey paper by Schm√∂cker and Dockhorn (Schm√∂cker & Dockhorn, 2025 ) .\\nProblem model and optimization objective: For our purposes, finite Markov Decision Processes (Sutton & Barto, 2018 ) are used as the model for sequential, perfect-information decision-making tasks. Œî \\u200b ( X ) \\\\Delta(X) denotes the probability simplex of a finite, non-empty set X X and the power set of X X is denoted by ùí´ \\u200b ( X ) \\\\mathcal{P}(X) .\\nDefinition :\\nAn MDP is a 6-tuple ( S , Œº 0 , T , ùî∏ , ‚Ñô , R ) (S,\\\\mu_{0},T,\\\\mathbb{A},\\\\mathbb{P},R) where the components are as follows:\\nS ‚â† ‚àÖ S\\\\neq\\\\emptyset is the finite set of states.\\nŒº 0 ‚àà Œî \\u200b ( S ) \\\\mu_{0}\\\\in\\\\Delta(S) is the probability distribution for the initial state.\\n‚àÖ ‚â† T ‚´ã S \\\\emptyset\\\\neq T\\\\subsetneqq S is the (possibly empty) set of terminal states.\\nùî∏ : ( S ‚àñ T ) ‚Ü¶ A \\\\mathbb{A}\\\\colon(S\\\\setminus T)\\\\mapsto A maps each state s s to the available actions ‚àÖ ‚â† ùî∏ \\u200b ( s ) ‚äÜ A \\\\emptyset\\\\neq\\\\mathbb{A}(s)\\\\subseteq A at state s s where | A | < ‚àû |A|<\\\\infty .\\n‚Ñô : ( S ‚àñ T ) √ó A ‚Ü¶ Œî \\u200b ( S ) \\\\mathbb{P}\\\\colon(S\\\\setminus T)\\\\times A\\\\mapsto\\\\Delta(S) is the stochastic transition function where ‚Ñô \\u200b ( s ‚Ä≤ | s , a ) \\\\mathbb{P}(s^{\\\\prime}|\\\\>s,a) is used to denote the probability of transitioning from s ‚àà ( S ‚àñ T ) s\\\\in(S\\\\setminus T) to s ‚Ä≤ ‚àà S s^{\\\\prime}\\\\in S after taking action a ‚àà ùî∏ \\u200b ( s ) a\\\\in\\\\mathbb{A}(s) in s s .\\nR : ( S ‚àñ T ) √ó A ‚Ü¶ ‚Ñù R\\\\colon(S\\\\setminus T)\\\\times A\\\\mapsto\\\\mathbb{R} is the reward function.\\nLet M = ( S , Œº 0 , T , ùî∏ , ‚Ñô , R ) M=(S,\\\\mu_{0},T,\\\\mathbb{A},\\\\mathbb{P},R) be an MDP. We define P ‚âî { ( s , a ) | s ‚àà ( S ‚àñ T ) , a ‚àà ùî∏ \\u200b ( s ) } P\\\\coloneqq\\\\{(s,a)\\\\>|\\\\>s\\\\in(S\\\\setminus T),a\\\\in\\\\mathbb{A}(s)\\\\} as the set of all legal state-action pairs. The objective is to find a mapping (i.e. an agent) œÄ : S ‚Ü¶ Œî \\u200b ( A ) \\\\pi\\\\colon S\\\\mapsto\\\\Delta(A) such that œÄ \\\\pi maximizes the expected episode‚Äôs return where the (discounted) return of an episode s 0 , a 0 , r 0 , ‚Ä¶ , s n , a n , r n , s n + 1 s_{0},a_{0},r_{0},\\\\dots,s_{n},a_{n},r_{n},s_{n+1} with s n + 1 ‚àà T s_{n+1}\\\\in T is given by Œ≥ 0 \\u200b r 0 + ‚Ä¶ + Œ≥ n \\u200b r n \\\\gamma^{0}r_{0}+\\\\ldots+\\\\gamma^{n}r_{n} .\\nAbstractions of State-Action Pairs (ASAP): For MCTS-based abstraction research, the goal has been to detect state-action pairs with the same Q ‚àó Q^{*} value (the value under subsequent optimal play) in the search graph to increase sample efficiency by an intra-layer information flow (Jiang et\\xa0al., 2014 ; Anand et\\xa0al., 2015 ; 2016 ) . In general, by abstractions of either the states of state-action-pairs we refer to equivalence relations over the state set S S or state-action pair set P P . The equivalence classes are abstract states or state-action pairs.\\nThe current state of the art is the Abstraction of State-Action Pairs in UCT (ASAP) abstraction framework (Anand et\\xa0al., 2015 ) that proposes rules to detect value-equivalent states and state-action pairs given an MDP transition graph and applies it to the current MCTS search graph (for details on MCTS, see Section A.7 ). The core idea of ASAP is to alternatingly construct a state abstraction given a state-action pair abstraction and a state-action pair abstraction given a state abstraction.\\nAssume one is given a state abstraction ‚Ñ∞ ‚Ä≤ ‚äÜ S √ó S \\\\mathcal{E}^{\\\\prime}\\\\subseteq S\\\\times S . The corresponding ASAP state-action pair abstraction ‚Ñã ‚äÜ P √ó P \\\\mathcal{H}\\\\subseteq P\\\\times P is defined as grouping those state-action pairs with the same immediate reward and equal abstract successor distribution. Concretely,\\nany state-action-pair ( s 1 , a 1 ) , ( s 2 , a 2 ) (s_{1},a_{1}),(s_{2},a_{2}) is equivalent i.e. ( ( s 1 , a 1 ) , ( s 2 , a 2 ) ) ‚àà ‚Ñã ((s_{1},a_{1}),(s_{2},a_{2}))\\\\in\\\\mathcal{H} if and only if\\nwhere ùí≥ \\\\mathcal{X} are the equivalence classes of ‚Ñ∞ ‚Ä≤ \\\\mathcal{E}^{\\\\prime} . And given a state-action pair abstraction ‚Ñã ‚Ä≤ ‚äÜ P √ó P \\\\mathcal{H}^{\\\\prime}\\\\subseteq P\\\\times P , the corresponding ASAP state abstraction ‚Ñ∞ \\\\mathcal{E} groups all states whose actions can be mapped to each other, concretely:\\nTo obtain the ASAP abstraction for a given MDP, these two constructing steps are repeated alternatingly until convergence.\\nOGA-UCT: Anand et\\xa0al. ( 2016 ) proposed OGA-UCT, which builds an ASAP-like abstraction in parallel to running MCTS. When building the abstraction OGA starts with the initial state abstraction that groups all terminal states of the same layer and puts the remaining states in their own singleton equivalence class. Furthermore, when building the ASAP abstraction on the current search graph, OGA ignores non-yet-sampled successors of state-action pairs that appear in Equation 2 . To make the frequent recomputation of the ASAP abstraction feasible, OGA keeps track of a recency counter for each Q-node and once it surpasses a certain threshold, recomputes its abstraction. If the abstraction changed, the parent states are recomputed too (and possibly their Q-node parents if their abstraction changed). By only locally checking for errors in the abstraction, OGA is able to keep track of an ASAP-like abstraction that is always close to the true ASAP abstraction of the current search tree.\\nThe only MCTS component that OGA-UCT affects is the tree policy which is enhanced by using the aggregate returns and visits of a Q-node‚Äôs abstract node to enhance the UCB value. ( ùú∫ a , ùú∫ t ) \\\\bm{(\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}})} -OGA : The ASAP framework groups only value equivalent states and state-action pairs. This condition can be relaxed like already done by a predecessor of OGA-UCT, called AS-UCT (Jiang et\\xa0al., 2014 ) , by allowing the rewards in Equation 1 to differ by some threshold Œµ a > 0 \\\\varepsilon_{\\\\text{a}}>0 and the transition error F t F_{\\\\text{t}} of Equation 1 to lie in the interval [ 0 , Œµ t ] [0,\\\\varepsilon_{\\\\text{t}}] where 0 ‚â§ Œµ t ‚â§ 2 0\\\\leq\\\\varepsilon_{\\\\text{t}}\\\\leq 2 is another parameter which does not have any effect in deterministic environments . Since in general, positive threshold values do not induce an equivalence relation over state-action pairs, OGA-UCT has to be slightly modified to accommodate these approximate abstractions. This has been done by Schm√∂cker and Dockhorn (Schm√∂cker et\\xa0al., 2025c ) who introduced ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA.\\nAuther automatic abstraction algorithm: The ASAP framework is the direct successor of Abstraction of States (AS) by Jiang et\\xa0al. ( 2014 ) that abstracts states if and only if their actions are pairwise similar in the sense that Equation 2 only has to be approximately satisfied as described in the ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA section above. While this paper focuses on deterministic domains, ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA (and related algorithms) have been further improved for stochastic settings by defining intra-abstraction policies (Schm√∂cker et\\xa0al., 2025b ) , or dynamically abandoning the abstraction (Xu et\\xa0al., 2023 ; Schm√∂cker et\\xa0al., 2025c ) .\\nYet another paradigm is demonstrated by Hostetler et\\xa0al. ( 2015 ) and Schm√∂cker et\\xa0al. ( 2025a ) who optimistically construct abstractions by starting with a very coarse abstraction (e.g. grouping everything together) and then they refine this abstraction, for example by repeatedly splitting large node groups in half (Hostetler et\\xa0al., 2015 ) .\\nResearch effort has also been dedicated towards automatic abstractions of the transition function, which on an abstract level can be described as pruning certain successors from the transition function (Sokota et\\xa0al., 2021 ; Yoon et\\xa0al., 2008 ; 2007 ; Saisubramanian et\\xa0al., 2017 ) .',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Method',\n",
       "    'paragraphs': 'This section introduces our novel KVDA-UCT algorithm. First, we will provide an example of a concrete search graph in which ASAP misses abstractions that the Known Value Differences Abstractions (KVDA) framework would find which will be introduced after the search graph example. At the end of this section, we will be describing how KVDA is integrated into UCT to yield KVDA-UCT.\\nConsider the state-transition graph that is illustrated in Fig. 1 that consists of four states and four actions. The ASAP framework would not detect any equivalences. In fact, any framework that aims at finding value-equivalent states or state-action pairs would at most be able to detect that the two root actions are value-equivalent.\\nHowever, by analysing the state graph, one could derive that the Q ‚àó Q^{*} values of the action of node 2 differs only by 1 1 from the Q ‚àó Q^{*} value of the action of node 3 3 . This holds true even if the state-graph would extend past node 4 4 . Consequently, the V ‚àó V^{*} values of nodes 2 and 3 differ only by 1 1 . This in turn implies that the two actions of node 1 must have the same Q ‚àó Q^{*} value. This is the core idea behind our Known-Value-Difference-Abstractions (KVDA) framework which is to abstract states and state-action pairs if one can derive their value differences. Later, when using these abstractions to enhance MCTS, the differences only have to be subtracted when aggregating values. When viewed from the lenses of the KVDA framework, ASAP only groups states or state-action pairs with a value difference of 0 , i.e. detects only true equivalences.\\nNext, we will formalize the KVDA framework which both in theory and in our empirical evaluations (see Tab. 1 , more details are given in the experimental section) detects strictly more abstractions.\\nOur method, the Known-Value-Difference-Abstractions (KVDA) extends the ASAP definition by additionally grouping states or state-action-pairs whose V ‚àó V^{*} or Q ‚àó Q^{*} difference is known. While ASAP iteratively builds abstractions on abstractions, KVDA bootstraps of an abstraction, difference-function pair. More concretely, given a state abstraction ‚Ñ∞ ‚Ä≤ \\\\mathcal{E}^{\\\\prime} (or a state-action pair abstraction ‚Ñã ‚Ä≤ \\\\mathcal{H}^{\\\\prime} ) and a difference function d s ‚Ä≤ : S √ó S ‚Ü¶ ‚Ñù d_{\\\\text{s}}^{\\\\prime}\\\\colon S\\\\times S\\\\mapsto\\\\mathbb{R} (or d a ‚Ä≤ : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}^{\\\\prime}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} ), both a state-action-pair abstraction ‚Ñã \\\\mathcal{H} (or a state abstraction ‚Ñ∞ \\\\mathcal{E} ) is produced as well as a state-action pair difference function d a : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} (or a state difference function d s : S √ó S ‚Ü¶ ‚Ñù d_{\\\\text{s}}\\\\colon S\\\\times S\\\\mapsto\\\\mathbb{R} ). Both d a d_{\\\\text{a}} and d s d_{\\\\text{s}} will be constructed such that\\nfor any pair of states or state-action pairs in the same equivalence class (i.e., in the same abstract state). Even though in the experimental section, we will mostly consider deterministic environments, the now-to-be-described KVDA framework will be applicable to any MDP, including stochastic ones.\\nNext, starting with the base case, we will formalize how KVDA abstractions are built. The base case: Like ASAP, KVDA groups all terminal states into the same initial abstract node, the remaining states are singleton abstract nodes. The difference function between all nodes in the terminal abstract node is initialized with 0 . State-action-pair abstractions: Using the same notation as in Section 2 , let ‚Ñ∞ ‚Ä≤ ‚äÜ S √ó S \\\\mathcal{E}^{\\\\prime}\\\\subseteq S\\\\times S be a state abstraction and d s ‚Ä≤ : S √ó S ‚Ü¶ ‚Ñù d_{\\\\text{s}}^{\\\\prime}\\\\colon S\\\\times S\\\\mapsto\\\\mathbb{R} be a difference function. The corresponding KVDA state-action pair abstraction ‚Ñã ‚äÜ P √ó P \\\\mathcal{H}\\\\subseteq P\\\\times P is defined as follows:\\nAny state-action pair ( s 1 , a 1 ) , ( s 2 , a 2 ) (s_{1},a_{1}),(s_{2},a_{2}) is equivalent i.e. ( ( s 1 , a 1 ) , ( s 2 , a 2 ) ) ‚àà ‚Ñã ((s_{1},a_{1}),(s_{2},a_{2}))\\\\in\\\\mathcal{H} if and only if\\nwhere ùí≥ \\\\mathcal{X} are the equivalence classes of ‚Ñ∞ ‚Ä≤ \\\\mathcal{E}^{\\\\prime} . Note that this is almost identical to the ASAP definition except that the immediate rewards do not have to coincide. The difference function d a : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} for two ( p 1 , p 2 ) ‚àà ‚Ñã (p_{1},p_{2})\\\\in\\\\mathcal{H} is given by\\nwhere for each x ‚àà ùí≥ x\\\\in\\\\mathcal{X} , s x s_{x} is an arbitrarily chosen but fixed representative of ùí≥ \\\\mathcal{X} . We will later see that the value of d a d_{\\\\text{a}} is independent of this choice. State abstractions: Given a state-action pair abstraction ‚Ñã ‚Ä≤ ‚äÜ P √ó P \\\\mathcal{H}^{\\\\prime}\\\\subseteq P\\\\times P and a state-action pair difference function d a : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} the corresponding KVDA state abstraction ‚Ñ∞ ‚äÜ S √ó S \\\\mathcal{E}\\\\subseteq S\\\\times S groups all states whose actions can be mapped to each other, and whose mappings all have the same value difference:\\nThe difference function d s \\u200b ( s 1 , s 2 ) d_{\\\\text{s}}(s_{1},s_{2}) is defined as the value d d in the equation above. Theoretical guarantees:\\nConvergence: Given an MDP, the above-described construction steps can be repeated until convergence, which is guaranteed as in our MDP definition, there are finitely many states and state-action pairs and each construction step either leaves the abstraction unchanged or reduces the number of equivalence classes. The abstraction that one obtains at convergence is called the KVDA abstraction of an MDP.\\nSoundness of d a d_{\\\\text{a}} and d s d_{\\\\text{s}} : Both d a d_{\\\\text{a}} and d s d_{\\\\text{s}} at convergence are equal to the differences of their arguments Q ‚àó Q^{*} or V ‚àó V^{*} values as formulated in Eq. 3 . This is proven in the supplementary materials in Section A.1 .\\nIn this section, we will describe how the KVDA abstraction framework is integrated into MCTS, which is fully analogous to how ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA (which itself is equivalent to OGA-UCT for Œµ a = Œµ t = 0 \\\\varepsilon_{\\\\text{a}}=\\\\varepsilon_{\\\\text{t}}=0 ) integrates the ASAP framework. The usage of the following to-be-described modifications to ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA is called ùú∫ t \\\\bm{\\\\varepsilon_{\\\\text{t}}} -KVDA . For the case Œµ t = 0 \\\\varepsilon_{\\\\text{t}}=0 , the algorithm is simply called KVDA-UCT . ùú∫ t \\\\bm{\\\\varepsilon_{\\\\text{t}}} -KVDA does not depend on Œµ a \\\\varepsilon_{\\\\text{a}} unlike ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA.\\n1) Instead of (approximate) ASAP abstractions, (approximate) KVDA abstractions of the search tree are built. Both ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA and Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA allow the transition error F t F_{\\\\text{t}} of Equation 1 to be in the interval [ 0 , Œµ t ] [0,\\\\varepsilon_{\\\\text{t}}] .\\n2) In addition to abstract Q nodes, abstract state nodes now also keep track of a representative. In our case, this is the first original node added to the abstract node, and if that one is removed, a random new representative is chosen.\\n3) Each abstract Q node‚Äôs value is tracked as the value of its representative ùí¨ R \\\\mathcal{Q}_{\\\\text{R}} that encodes the state-action-pair p R ‚àà P p_{\\\\text{R}}\\\\in P . When another original node ùí¨ \\\\mathcal{Q} of the same abstract node representing p ‚àà P p\\\\in P is backed up in the MCTS backup phase with value v ‚àà ‚Ñù v\\\\in\\\\mathbb{R} , the value v + d a \\u200b ( p , p R ) v+d_{\\\\text{a}}(p,p_{\\\\text{R}}) is added to the abstract node‚Äôs statistics. In turn, ùí¨ \\\\mathcal{Q} extracts its aggregated returns from the abstract node by subtracting d a \\u200b ( p , p R ) d_{\\\\text{a}}(p,p_{\\\\text{R}}) from the abstract node‚Äôs value. If an abstract node‚Äôs representative changes to ùí¨ R ‚Ä≤ \\\\mathcal{Q}^{\\\\prime}_{\\\\text{R}} encoding p R ‚Ä≤ ‚àà P p_{\\\\text{R}}^{\\\\prime}\\\\in P , then n ‚ãÖ d a \\u200b ( p R , p R ‚Ä≤ ) n\\\\cdot d_{\\\\text{a}}(p_{\\\\text{R}},p_{\\\\text{R}}^{\\\\prime}) is added to the statistics where n n is the abstract visit count.\\n4) To reduce the computational load of finding a perfect match as the one required for the state abstractions in Equation 6 , we check for a stricter condition for states s 1 , s 2 s_{1},s_{2} . It is first checked that within s 1 s_{1} (and analogously s 2 s_{2} ) all actions within the same abstract node have a value difference of zero. Then, it is tested for all abstract Q nodes of s 1 s_{1} if the value difference between an arbitrarily chosen ground action of s 1 s_{1} and one of s 2 s_{2} is constant for all abstract nodes.\\n5) Finally, whenever a Q-node‚Äôs recency counter reaches the threshold, the value difference to its representative is recalculated. A change in this difference also results in a reevaluation (and subsequent recency counter reset) of the parent nodes‚Äô abstractions and difference functions.',\n",
       "    'subsections': [{'title': '3.1 Which abstractions ASAP misses',\n",
       "      'paragraphs': 'Consider the state-transition graph that is illustrated in Fig. 1 that consists of four states and four actions. The ASAP framework would not detect any equivalences. In fact, any framework that aims at finding value-equivalent states or state-action pairs would at most be able to detect that the two root actions are value-equivalent.\\nHowever, by analysing the state graph, one could derive that the Q ‚àó Q^{*} values of the action of node 2 differs only by 1 1 from the Q ‚àó Q^{*} value of the action of node 3 3 . This holds true even if the state-graph would extend past node 4 4 . Consequently, the V ‚àó V^{*} values of nodes 2 and 3 differ only by 1 1 . This in turn implies that the two actions of node 1 must have the same Q ‚àó Q^{*} value. This is the core idea behind our Known-Value-Difference-Abstractions (KVDA) framework which is to abstract states and state-action pairs if one can derive their value differences. Later, when using these abstractions to enhance MCTS, the differences only have to be subtracted when aggregating values. When viewed from the lenses of the KVDA framework, ASAP only groups states or state-action pairs with a value difference of 0 , i.e. detects only true equivalences.\\nNext, we will formalize the KVDA framework which both in theory and in our empirical evaluations (see Tab. 1 , more details are given in the experimental section) detects strictly more abstractions.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 The Known-Value-Difference-Abstractions framework',\n",
       "      'paragraphs': 'Our method, the Known-Value-Difference-Abstractions (KVDA) extends the ASAP definition by additionally grouping states or state-action-pairs whose V ‚àó V^{*} or Q ‚àó Q^{*} difference is known. While ASAP iteratively builds abstractions on abstractions, KVDA bootstraps of an abstraction, difference-function pair. More concretely, given a state abstraction ‚Ñ∞ ‚Ä≤ \\\\mathcal{E}^{\\\\prime} (or a state-action pair abstraction ‚Ñã ‚Ä≤ \\\\mathcal{H}^{\\\\prime} ) and a difference function d s ‚Ä≤ : S √ó S ‚Ü¶ ‚Ñù d_{\\\\text{s}}^{\\\\prime}\\\\colon S\\\\times S\\\\mapsto\\\\mathbb{R} (or d a ‚Ä≤ : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}^{\\\\prime}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} ), both a state-action-pair abstraction ‚Ñã \\\\mathcal{H} (or a state abstraction ‚Ñ∞ \\\\mathcal{E} ) is produced as well as a state-action pair difference function d a : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} (or a state difference function d s : S √ó S ‚Ü¶ ‚Ñù d_{\\\\text{s}}\\\\colon S\\\\times S\\\\mapsto\\\\mathbb{R} ). Both d a d_{\\\\text{a}} and d s d_{\\\\text{s}} will be constructed such that\\nfor any pair of states or state-action pairs in the same equivalence class (i.e., in the same abstract state). Even though in the experimental section, we will mostly consider deterministic environments, the now-to-be-described KVDA framework will be applicable to any MDP, including stochastic ones.\\nNext, starting with the base case, we will formalize how KVDA abstractions are built. The base case: Like ASAP, KVDA groups all terminal states into the same initial abstract node, the remaining states are singleton abstract nodes. The difference function between all nodes in the terminal abstract node is initialized with 0 . State-action-pair abstractions: Using the same notation as in Section 2 , let ‚Ñ∞ ‚Ä≤ ‚äÜ S √ó S \\\\mathcal{E}^{\\\\prime}\\\\subseteq S\\\\times S be a state abstraction and d s ‚Ä≤ : S √ó S ‚Ü¶ ‚Ñù d_{\\\\text{s}}^{\\\\prime}\\\\colon S\\\\times S\\\\mapsto\\\\mathbb{R} be a difference function. The corresponding KVDA state-action pair abstraction ‚Ñã ‚äÜ P √ó P \\\\mathcal{H}\\\\subseteq P\\\\times P is defined as follows:\\nAny state-action pair ( s 1 , a 1 ) , ( s 2 , a 2 ) (s_{1},a_{1}),(s_{2},a_{2}) is equivalent i.e. ( ( s 1 , a 1 ) , ( s 2 , a 2 ) ) ‚àà ‚Ñã ((s_{1},a_{1}),(s_{2},a_{2}))\\\\in\\\\mathcal{H} if and only if\\nwhere ùí≥ \\\\mathcal{X} are the equivalence classes of ‚Ñ∞ ‚Ä≤ \\\\mathcal{E}^{\\\\prime} . Note that this is almost identical to the ASAP definition except that the immediate rewards do not have to coincide. The difference function d a : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} for two ( p 1 , p 2 ) ‚àà ‚Ñã (p_{1},p_{2})\\\\in\\\\mathcal{H} is given by\\nwhere for each x ‚àà ùí≥ x\\\\in\\\\mathcal{X} , s x s_{x} is an arbitrarily chosen but fixed representative of ùí≥ \\\\mathcal{X} . We will later see that the value of d a d_{\\\\text{a}} is independent of this choice. State abstractions: Given a state-action pair abstraction ‚Ñã ‚Ä≤ ‚äÜ P √ó P \\\\mathcal{H}^{\\\\prime}\\\\subseteq P\\\\times P and a state-action pair difference function d a : P √ó P ‚Ü¶ ‚Ñù d_{\\\\text{a}}\\\\colon P\\\\times P\\\\mapsto\\\\mathbb{R} the corresponding KVDA state abstraction ‚Ñ∞ ‚äÜ S √ó S \\\\mathcal{E}\\\\subseteq S\\\\times S groups all states whose actions can be mapped to each other, and whose mappings all have the same value difference:\\nThe difference function d s \\u200b ( s 1 , s 2 ) d_{\\\\text{s}}(s_{1},s_{2}) is defined as the value d d in the equation above. Theoretical guarantees:\\nConvergence: Given an MDP, the above-described construction steps can be repeated until convergence, which is guaranteed as in our MDP definition, there are finitely many states and state-action pairs and each construction step either leaves the abstraction unchanged or reduces the number of equivalence classes. The abstraction that one obtains at convergence is called the KVDA abstraction of an MDP.\\nSoundness of d a d_{\\\\text{a}} and d s d_{\\\\text{s}} : Both d a d_{\\\\text{a}} and d s d_{\\\\text{s}} at convergence are equal to the differences of their arguments Q ‚àó Q^{*} or V ‚àó V^{*} values as formulated in Eq. 3 . This is proven in the supplementary materials in Section A.1 .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 KVDA-UCT and ùú∫t\\\\bm{\\\\varepsilon}_{\\\\text{t}}-KVDA',\n",
       "      'paragraphs': 'In this section, we will describe how the KVDA abstraction framework is integrated into MCTS, which is fully analogous to how ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA (which itself is equivalent to OGA-UCT for Œµ a = Œµ t = 0 \\\\varepsilon_{\\\\text{a}}=\\\\varepsilon_{\\\\text{t}}=0 ) integrates the ASAP framework. The usage of the following to-be-described modifications to ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA is called ùú∫ t \\\\bm{\\\\varepsilon_{\\\\text{t}}} -KVDA . For the case Œµ t = 0 \\\\varepsilon_{\\\\text{t}}=0 , the algorithm is simply called KVDA-UCT . ùú∫ t \\\\bm{\\\\varepsilon_{\\\\text{t}}} -KVDA does not depend on Œµ a \\\\varepsilon_{\\\\text{a}} unlike ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA.\\n1) Instead of (approximate) ASAP abstractions, (approximate) KVDA abstractions of the search tree are built. Both ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA and Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA allow the transition error F t F_{\\\\text{t}} of Equation 1 to be in the interval [ 0 , Œµ t ] [0,\\\\varepsilon_{\\\\text{t}}] .\\n2) In addition to abstract Q nodes, abstract state nodes now also keep track of a representative. In our case, this is the first original node added to the abstract node, and if that one is removed, a random new representative is chosen.\\n3) Each abstract Q node‚Äôs value is tracked as the value of its representative ùí¨ R \\\\mathcal{Q}_{\\\\text{R}} that encodes the state-action-pair p R ‚àà P p_{\\\\text{R}}\\\\in P . When another original node ùí¨ \\\\mathcal{Q} of the same abstract node representing p ‚àà P p\\\\in P is backed up in the MCTS backup phase with value v ‚àà ‚Ñù v\\\\in\\\\mathbb{R} , the value v + d a \\u200b ( p , p R ) v+d_{\\\\text{a}}(p,p_{\\\\text{R}}) is added to the abstract node‚Äôs statistics. In turn, ùí¨ \\\\mathcal{Q} extracts its aggregated returns from the abstract node by subtracting d a \\u200b ( p , p R ) d_{\\\\text{a}}(p,p_{\\\\text{R}}) from the abstract node‚Äôs value. If an abstract node‚Äôs representative changes to ùí¨ R ‚Ä≤ \\\\mathcal{Q}^{\\\\prime}_{\\\\text{R}} encoding p R ‚Ä≤ ‚àà P p_{\\\\text{R}}^{\\\\prime}\\\\in P , then n ‚ãÖ d a \\u200b ( p R , p R ‚Ä≤ ) n\\\\cdot d_{\\\\text{a}}(p_{\\\\text{R}},p_{\\\\text{R}}^{\\\\prime}) is added to the statistics where n n is the abstract visit count.\\n4) To reduce the computational load of finding a perfect match as the one required for the state abstractions in Equation 6 , we check for a stricter condition for states s 1 , s 2 s_{1},s_{2} . It is first checked that within s 1 s_{1} (and analogously s 2 s_{2} ) all actions within the same abstract node have a value difference of zero. Then, it is tested for all abstract Q nodes of s 1 s_{1} if the value difference between an arbitrarily chosen ground action of s 1 s_{1} and one of s 2 s_{2} is constant for all abstract nodes.\\n5) Finally, whenever a Q-node‚Äôs recency counter reaches the threshold, the value difference to its representative is recalculated. A change in this difference also results in a reevaluation (and subsequent recency counter reset) of the parent nodes‚Äô abstractions and difference functions.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Experiment setup',\n",
       "    'paragraphs': 'In this section, we describe the general experiment setup. Any deviations from this setup will be explicitly mentioned.\\nProblem models: For this paper, we ran our experiments on a variety of MDPs, all of which are either from the International Probabilistic Planning Conference (Grzes et\\xa0al., 2014 ) , are well-known board games, or are commonly used in the abstraction algorithm literature. Since we will compare KVDA-UCT to OGA-UCT we chose only environments with a non-constant and non-sparse reward function, as constant reward environments would imply that KVDA-UCT and OGA-UCT are semantically equivalent.\\nAll MDPs originally feature stochasticity, however, for some experiments we considered their deterministic versions which are obtained by sampling a single successor of each state-action pair. For each episode, new successors are sampled.\\nAll experiments were run on the finite horizon versions of the considered MDPs with a default horizon of 50 steps and 200 for the board games with a planning horizon of 50 and a discount factor Œ≥ = 1 \\\\gamma=1 .\\nThe board games are transformed into MDPs by inserting a deterministic One-Step-Lookahead agent as the opponent. Furthermore, since they are all sparse-reward (i.e. either win or lose) they were also transformed into dense-reward MDPs by using heuristics. For each board game we defined a heuristic V h V^{\\\\text{h}} that assigns each state a heuristic value for its state value. The reward of the (deterministic) transition s , a , s ‚Ä≤ s,a,s^{\\\\prime} is then given by V h \\u200b ( s ‚Ä≤ ) ‚àí V h \\u200b ( s ) V^{\\\\text{h}}(s^{\\\\prime})-V^{\\\\text{h}}(s) . The concrete heuristics used along with a brief description of all MDPs is provided in the supplementary materials in Section A.8 . The deterministic MDPs are denoted by adding the prefix d- and the stochastic ones are denoted by the prefix s- .\\nParameters: Since the problem domains have vastly different reward scales, we use the dynamic exploration factor Global Std (Schm√∂cker et\\xa0al., 2025d ) which has the form C ‚ãÖ œÉ C\\\\cdot\\\\sigma where œÉ \\\\sigma is the standard deviation of the Q values of all nodes in the search tree and C ‚àà ‚Ñù + C\\\\in\\\\mathbb{R}^{+} is some fixed parameter. Furthermore, we always use K = 3 K=3 as the recency counter which was proposed by Anand et\\xa0al. ( 2016 ) .\\nEvaluation: Each experiment is repeated at least 2000 times and all confidence that we denote in the following are 99 % 99\\\\% confidence intervals with range ‚âà 2 ‚ãÖ 2.33 \\\\approx 2\\\\cdot 2.33 times the standard error.\\nNormalized pairings score : We will later construct the normalized pairings score to test the generalization capabilities of KVDA-UCT. This is the same score as used in Schm√∂cker et\\xa0al. ( 2025d ) . The pairings score is constructed as follows. Let { œÄ 1 , ‚Ä¶ , œÄ n } \\\\{\\\\pi_{1},\\\\dots,\\\\pi_{n}\\\\} be n n agents (e.g. each KVDA-UCT agent along with its parameter setting is an agent) where each agent was evaluated on m m tasks (later, a task will be a given MCTS iteration budget and an environment). This induces a matrix of size n √ó n n\\\\times n with the entry ( i , j ) (i,j) being equal to the number of tasks where œÄ i \\\\pi_{i} achieved a higher performance than œÄ j \\\\pi_{j} subtracted by the number of times it performed worse, divided by m m .\\nThe normalized pairings score s i ‚â§ i ‚â§ n s_{i}\\\\leq i\\\\leq n is then obtained by taking the average of i i -th row when excluding the i i -th column.\\nReproducibility: For reproducibility, we released our implementation (Schm√∂cker, 2025 ) . Our code was compiled with g++ version 13.1.0 using the -O3 flag (i.e. aggressive optimization).',\n",
       "    'subsections': []},\n",
       "   {'title': '5 Experiments',\n",
       "    'paragraphs': 'This section presents the experimental results of KVDA-UCT. We considered two settings. Firstly, in Section 5 we measured the number of additional abstractions KVDA-UCT finds in comparison to OGA-UCT. Then, we evaluated KVDA-UCT on deterministic settings in which the losslessness of the abstraction is guaranteed in Section 1 and Section 2 . Lastly, we present the results for stochastic environments in Section 3 .\\nAbstractions that KVDA-UCT finds but OGA-UCT does not: Firstly, we empirically measured the number of non-trivial abstractions (i.e. those that aren‚Äôt of size one) that KVDA-UCT, OGA-UCT, and ( ‚àû , 0 ) (\\\\infty,0) -UCT find. For all deterministic environments, Tab. 1 denotes the average ratio of non-trivial abstract state-action pairs (synonymously abstract Q nodes) to the number of total abstract Q nodes in their respective search trees. In most environments, KVDA-UCT detects more abstractions than OGA-UCT, including environments where the abstraction rate more than doubles, such as SysAdmin or Wildfire. Furthermore, KVDA-UCT detects roughly as many abstractions as ( ‚àû , 0 ) (\\\\infty,0) -OGA in most environments, which fully ignores rewards when building abstractions.\\nParameter-optimized KVDA-UCT: Next, we compared KVDA-UCT, ( 0 , 0 ) (0,0) -OGA (i.e. standard OGA-UCT), and ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA with Œµ a > 0 \\\\varepsilon_{\\\\text{a}}>0 in terms of their parameter-optimized performances on deterministic environments.\\nFor all methods, we optimized over C ‚àà { 0.5 , 1 , 2 , 4 , 8 , 16 } C\\\\in\\\\{0.5,1,2,4,8,16\\\\} and some domain-specific values for Œµ a \\\\varepsilon_{\\\\text{a}} that are listed in the supplementary materials in Tab. 4 . Each parameter combination was evaluated on all here-considered deterministic environments with a budget of 100, 200, 500, and 1000 MCTS iterations.\\nTab. 2 list the performance for 100 and 1000 iterations, the tables for 200 and 500 iterations are found in the supplementary materials in Tab. 7 and 6 . The results clearly show that KVDA either outperforms all other competitor methods or is tightly within the confidence bounds. There are a number of environments such as Manufacturer, Tamarisk, or Push Your Luck where KVDA-UCT simultaneously outperforms all competitor methods at once. Figure 2 plots the performances in dependence of the iteration budget for these tasks. The performance graphs for all other environments are found in the supplementary materials in Section A.6 .\\nSingle-parameter KVDA-UCT: Next, the generalization capabilities of KVDA-UCT in comparison to OGA-UCT and ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA on deterministic environments were tested. Like last section, we ran all algorithms using 100, 200, 500, and 1000 MCTS iterations whilst varying C ‚àà { 0.5 , 1 , 2 , 4 , 8 , 16 } C\\\\in\\\\{0.5,1,2,4,8,16\\\\} and Œµ a \\\\varepsilon_{\\\\text{a}} with domain-specific values (see supplementary materials Tab. 4 ). Instead of considering the best-performances, we calculated the normalized pairings score for each parameter combination (except for ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA where we considered the maximum performance across all Œµ a > 0 \\\\varepsilon_{\\\\text{a}}>0 values a single parameter combination), constructed over the tasks which are the pairs of all environments and iteration budgets. Bar chart 3 shows the 6 agents with the highest score as well as the agent with the lowest score. Both the top spots are occupied by our KVDA-method, followed by OGA-UCT and ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA. The best performing algorithm overall is KVDA-UCT with C = 4 C=4 .\\nŒµ t \\\\varepsilon_{\\\\text{t}} -KVDA on stochastic environments: Lastly, the performance of Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA on stochastic environments were tested by running both Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA and ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA on the stochastic versions of the here-considered environments. We considered the iteration budgets of 100 and 1000 iterations and varied C ‚àà { 0.5 , 1 , 2 , 4 , 8 , 16 } C\\\\in\\\\{0.5,1,2,4,8,16\\\\} , Œµ t ‚àà { 0 , 0.4 , 0.8 , 1.2 , 1.6 } \\\\varepsilon_{\\\\text{t}}\\\\in\\\\{0,0.4,0.8,1.2,1.6\\\\} and the same domain-specific Œµ a \\\\varepsilon_{\\\\text{a}} that were used in the previous sections whose values are listed in the supplementary materials in Tab. 4 . For each environment, Tab. 5 of the supplementary materials lists the parameter-optimized performances of both algorithms. In contrast to the deterministic setting, KVDA does not outperform OGA. For most environments both perform equally well with some exceptions such as Manufacturer were Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA clearly performs best and Tamarisk were ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA performs best. We believe mediocre performance of Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA stems from the fact that since Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA essentially ignores immediate rewards when it builds abstractions, the number of faulty abstractions that occur in this approximate setting is increased. In the deterministic setting, no faulty abstractions are built.',\n",
       "    'subsections': []},\n",
       "   {'title': '6 Conclusion, Limitations and Future Work',\n",
       "    'paragraphs': 'This paper introduced KVDA-UCT, an extension to OGA-UCT, that additionally groups states and state-action pairs that do not have the same Q ‚àó Q^{*} or V ‚àó V^{*} value as long as the difference is known. We evaluated and compared KVDA-UCT to OGA-UCT and ( Œµ a , 0 ) (\\\\varepsilon_{\\\\text{a}},0) -OGA on a deterministic setting where KVDA-UCT outperforms all its competitors both in terms of generalization as well as the parameter-optimized performance. This does not hold true for the stochastic setting for which we generalized KVDA-UCT to Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA which is only clearly better than ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA in few environments.\\nA first avenue for future work will be to further investigate the reasons for the mediocre performance of the experiments in this paper of the stochastic setting and develop an extension of Œµ t \\\\varepsilon_{\\\\text{t}} -KVDA suited for this setting.\\nAnother limitation of KVDA abstractions in its current form is its limitation to MDPs (i.e. single-player games). Even though MCTS is in principle applicable to multi-player games, KVDA is not because in any state, there is in general, no unique V ‚àó V^{*} value from which differences can be built, as there are potentially many equilibria with different payoff vectors. Future work will be to extend KVDA to this setting.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Supplementary Materials',\n",
       "    'paragraphs': 'In this section, it will be shown that the KVDA difference functions d a d_{\\\\text{a}} and d s d_{\\\\text{s}} at any step (including the step of convergence)\\ncoincide with d a ‚àó d^{*}_{\\\\text{a}} and d s ‚àó d^{*}_{\\\\text{s}} for state or state-action pairs within the same abstract state. This will be proven inductively. First, note that this statement is true for the base case as all terminal states have a value of V ‚àó = 0 V^{*}=0 ; hence, their differences are also 0. For the following induction steps, assume that d a d_{\\\\text{a}} is bootstrapped by d s ‚Ä≤ d^{\\\\prime}_{\\\\text{s}} and d s d_{\\\\text{s}} is bootstrapped by d a ‚Ä≤ d^{\\\\prime}_{\\\\text{a}} .\\nNext, let ( p 1 , p 2 ) ‚àà ‚Ñã (p_{1},p_{2})\\\\in\\\\mathcal{H} . By definition, it holds that\\nBy rewriting V ‚àó \\u200b ( s ) V^{*}(s) in terms of the difference to its abstract representative, one obtains\\nHence d ‚àó \\u200b ( p 1 , p 2 ) = d a \\u200b ( p 1 , p 2 ) d^{*}(p_{1},p_{2})=d_{\\\\text{a}}(p_{1},p_{2}) .\\nNote that this proof holds for any choice of the abstract nodes‚Äô representatives. Lastly, let ( s 1 , s 2 ) ‚àà ‚Ñ∞ (s_{1},s_{2})\\\\in\\\\mathcal{E} and d s \\u200b ( s 1 , s 2 ) = d d_{\\\\text{s}}(s_{1},s_{2})=d . First note that if a ‚àà arg \\u200b max a ‚Ä≤ ‚àà ùî∏ \\u200b ( s 1 ) \\u2061 Q ‚àó \\u200b ( s 1 , a ‚Ä≤ ) a\\\\in\\\\text{arg}\\\\max\\\\limits_{a^{\\\\prime}\\\\in\\\\mathbb{A}(s_{1})}Q^{*}(s_{1},a^{\\\\prime}) and ( ( s 1 , a ) , ( s 2 , a ^ ) ) ‚àà ‚Ñã ‚Ä≤ ((s_{1},a),(s_{2},\\\\hat{a}))\\\\in\\\\mathcal{H}^{\\\\prime} with d a ‚àó \\u200b ( ( s 1 , a ) , ( s 2 , a ^ ) ) = d d^{*}_{\\\\text{a}}((s_{1},a),(s_{2},\\\\hat{a}))=d for some a ^ ‚àà ùî∏ \\u200b ( s 2 ) \\\\hat{a}\\\\in\\\\mathbb{A}(s_{2}) then a ^ ‚àà arg \\u200b max a ‚Ä≤ ‚àà ùî∏ \\u200b ( s 2 ) \\u2061 Q ‚àó \\u200b ( s 2 , a ‚Ä≤ ) \\\\hat{a}\\\\in\\\\text{arg}\\\\max\\\\limits_{a^{\\\\prime}\\\\in\\\\mathbb{A}(s_{2})}Q^{*}(s_{2},a^{\\\\prime}) because for all b ‚àà ùî∏ \\u200b ( s 2 ) b\\\\in\\\\mathbb{A}(s_{2}) it holds that\\nwhere ( ( s 1 , b ^ ) , ( s 2 , b ) ) ‚àà ‚Ñã ‚Ä≤ ((s_{1},\\\\hat{b}),(s_{2},b))\\\\in\\\\mathcal{H}^{\\\\prime} with d a ‚àó \\u200b ( ( s 1 , b ^ ) , ( s 2 , b ) ) = d d^{*}_{\\\\text{a}}((s_{1},\\\\hat{b}),(s_{2},b))=d .\\nAnd since V ‚àó \\u200b ( s ) = max a ‚àà ùî∏ \\u200b ( s ) \\u2061 Q ‚àó \\u200b ( s , a ) V^{*}(s)=\\\\max\\\\limits_{a\\\\in\\\\mathbb{A}(s)}Q^{*}(s,a) it directly follows that d = d s \\u200b ( s 1 , s 2 ) = d s ‚àó \\u200b ( s 1 , s 2 ) d=d_{\\\\text{s}}(s_{1},s_{2})=d^{*}_{\\\\text{s}}(s_{1},s_{2}) ‚àé.\\nTab. 3 lists the average decision-making times for each environment of KVDA-UCT compared to OGA-UCT for 100 and 2000 iterations on states sampled from a distribution induced by random walks. The numbers show a median runtime overhead of < 1 % <1\\\\% for 100 iterations and ‚âà 1 % \\\\approx 1\\\\% for 2000 iterations.\\nOne problem with ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA is that while Œµ t \\\\varepsilon_{\\\\text{t}} is neatly bounded by 0 and 2 2 , the Œµ a \\\\varepsilon_{\\\\text{a}} value has to be chosen on a per-environment basis since for example the value Œµ a = 0.5 \\\\varepsilon_{\\\\text{a}}=0.5 would be equivalent to Œµ a = 0 \\\\varepsilon_{\\\\text{a}}=0 for any environment with rewards that are all greater than 0.5 0.5 . Tab. 4 lists the hand picked values for each environment that we chose for the experiment section. The values were chosen to be at the boundary of rewards that actually occur in these environments.\\nThis section specifies the MCTS that was used for the experiments of this paper.\\nOur MCTS version builds a directed acyclic graph instead of a tree, i.e. different state-action pairs in the same layer can lead to the same node. This is a necessary condition for ASAP and KVDA to detect any abstractions since they bootstrap of the search graph converging.\\nWe use the Upper Confidence Bounds (UCB) tree policy which is defined as\\nwhere, s s is the state at which the decision has to be made, V a V_{a} is the sum of returns of the action under consideration, and N a N_{a} are its visits. The tree policy chooses the action that maximizes the UCB value.\\nWe use the greedy decision policy, which is choosing the root action, after the search statistics have been gathered, with the maximum Q value.\\nThe descriptions for all the problem models considered here are either found in Schm√∂cker & Dockhorn ( 2025 ) or in Schm√∂cker et\\xa0al. ( 2025d ) . Most models are parametrized (e.g. choosing a concrete racetrack for Racetrack), the concrete instance used for the experiments is found in the ExperimentConfigs folder in our publicly available implementation (Schm√∂cker, 2025 ) .\\nNext, we describe the heuristic functions used for the two-player games.\\nConnect 4 : As the heuristic from the perspective of player one is given by\\nwhere n i n_{i} is the total number of i i stones that are in one row/column/diagonal, divided by i i , but which are not part of a row/column/diagonal of size i + 1 i+1 .\\nConstrictor : The heuristics function used for player one is the number of grid cells that player one could reach before its opponent. If player one wins, the value 100 100 is added to the heuristic.\\nOthello : The heuristic function for any state for player one is given by a weighted sum of all the occupied grid cells. The weight w w of cell ( x , y ) (x,y) is given by 10 / ( 1 + d ) 10/(1+d) where d d is the distance to the closest corner cell. The weight w w is positive for any cells occupied by stones of player one and negative for those occupied by player two. In the round player one wins, 100 100 is added to the heuristics value and ‚àí 100 -100 if player two won.\\nPusher : We used the heuristic for player one that is equal to the difference of alive units of player one and player two in addition to the Manhatten distance between player one and player two‚Äôs units‚Äô center of mass. In the turn that player one wins, 100 100 is added to the heuristic value.',\n",
       "    'subsections': [{'title': 'A.1 Proof that da‚àó=dad^{*}_{a}=d_{a} and ds‚àó=dsd^{*}_{s}=d_{s}',\n",
       "      'paragraphs': 'In this section, it will be shown that the KVDA difference functions d a d_{\\\\text{a}} and d s d_{\\\\text{s}} at any step (including the step of convergence)\\ncoincide with d a ‚àó d^{*}_{\\\\text{a}} and d s ‚àó d^{*}_{\\\\text{s}} for state or state-action pairs within the same abstract state. This will be proven inductively. First, note that this statement is true for the base case as all terminal states have a value of V ‚àó = 0 V^{*}=0 ; hence, their differences are also 0. For the following induction steps, assume that d a d_{\\\\text{a}} is bootstrapped by d s ‚Ä≤ d^{\\\\prime}_{\\\\text{s}} and d s d_{\\\\text{s}} is bootstrapped by d a ‚Ä≤ d^{\\\\prime}_{\\\\text{a}} .\\nNext, let ( p 1 , p 2 ) ‚àà ‚Ñã (p_{1},p_{2})\\\\in\\\\mathcal{H} . By definition, it holds that\\nBy rewriting V ‚àó \\u200b ( s ) V^{*}(s) in terms of the difference to its abstract representative, one obtains\\nHence d ‚àó \\u200b ( p 1 , p 2 ) = d a \\u200b ( p 1 , p 2 ) d^{*}(p_{1},p_{2})=d_{\\\\text{a}}(p_{1},p_{2}) .\\nNote that this proof holds for any choice of the abstract nodes‚Äô representatives. Lastly, let ( s 1 , s 2 ) ‚àà ‚Ñ∞ (s_{1},s_{2})\\\\in\\\\mathcal{E} and d s \\u200b ( s 1 , s 2 ) = d d_{\\\\text{s}}(s_{1},s_{2})=d . First note that if a ‚àà arg \\u200b max a ‚Ä≤ ‚àà ùî∏ \\u200b ( s 1 ) \\u2061 Q ‚àó \\u200b ( s 1 , a ‚Ä≤ ) a\\\\in\\\\text{arg}\\\\max\\\\limits_{a^{\\\\prime}\\\\in\\\\mathbb{A}(s_{1})}Q^{*}(s_{1},a^{\\\\prime}) and ( ( s 1 , a ) , ( s 2 , a ^ ) ) ‚àà ‚Ñã ‚Ä≤ ((s_{1},a),(s_{2},\\\\hat{a}))\\\\in\\\\mathcal{H}^{\\\\prime} with d a ‚àó \\u200b ( ( s 1 , a ) , ( s 2 , a ^ ) ) = d d^{*}_{\\\\text{a}}((s_{1},a),(s_{2},\\\\hat{a}))=d for some a ^ ‚àà ùî∏ \\u200b ( s 2 ) \\\\hat{a}\\\\in\\\\mathbb{A}(s_{2}) then a ^ ‚àà arg \\u200b max a ‚Ä≤ ‚àà ùî∏ \\u200b ( s 2 ) \\u2061 Q ‚àó \\u200b ( s 2 , a ‚Ä≤ ) \\\\hat{a}\\\\in\\\\text{arg}\\\\max\\\\limits_{a^{\\\\prime}\\\\in\\\\mathbb{A}(s_{2})}Q^{*}(s_{2},a^{\\\\prime}) because for all b ‚àà ùî∏ \\u200b ( s 2 ) b\\\\in\\\\mathbb{A}(s_{2}) it holds that\\nwhere ( ( s 1 , b ^ ) , ( s 2 , b ) ) ‚àà ‚Ñã ‚Ä≤ ((s_{1},\\\\hat{b}),(s_{2},b))\\\\in\\\\mathcal{H}^{\\\\prime} with d a ‚àó \\u200b ( ( s 1 , b ^ ) , ( s 2 , b ) ) = d d^{*}_{\\\\text{a}}((s_{1},\\\\hat{b}),(s_{2},b))=d .\\nAnd since V ‚àó \\u200b ( s ) = max a ‚àà ùî∏ \\u200b ( s ) \\u2061 Q ‚àó \\u200b ( s , a ) V^{*}(s)=\\\\max\\\\limits_{a\\\\in\\\\mathbb{A}(s)}Q^{*}(s,a) it directly follows that d = d s \\u200b ( s 1 , s 2 ) = d s ‚àó \\u200b ( s 1 , s 2 ) d=d_{\\\\text{s}}(s_{1},s_{2})=d^{*}_{\\\\text{s}}(s_{1},s_{2}) ‚àé.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Runtime measurements',\n",
       "      'paragraphs': 'Tab. 3 lists the average decision-making times for each environment of KVDA-UCT compared to OGA-UCT for 100 and 2000 iterations on states sampled from a distribution induced by random walks. The numbers show a median runtime overhead of < 1 % <1\\\\% for 100 iterations and ‚âà 1 % \\\\approx 1\\\\% for 2000 iterations.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.3 Domain-specific Œµa\\\\varepsilon_{a} values',\n",
       "      'paragraphs': 'One problem with ( Œµ a , Œµ t ) (\\\\varepsilon_{\\\\text{a}},\\\\varepsilon_{\\\\text{t}}) -OGA is that while Œµ t \\\\varepsilon_{\\\\text{t}} is neatly bounded by 0 and 2 2 , the Œµ a \\\\varepsilon_{\\\\text{a}} value has to be chosen on a per-environment basis since for example the value Œµ a = 0.5 \\\\varepsilon_{\\\\text{a}}=0.5 would be equivalent to Œµ a = 0 \\\\varepsilon_{\\\\text{a}}=0 for any environment with rewards that are all greater than 0.5 0.5 . Tab. 4 lists the hand picked values for each environment that we chose for the experiment section. The values were chosen to be at the boundary of rewards that actually occur in these environments.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.4 Performances of Œµt\\\\varepsilon_{t}-KVDA',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.5 Performances of parameter-optimized KVDA-UCT for 200 and 500 iterations',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.6 Performance graphs of parameter-optimized KVDA-UCT',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.7 Monte Carlo Tree Search',\n",
       "      'paragraphs': 'This section specifies the MCTS that was used for the experiments of this paper.\\nOur MCTS version builds a directed acyclic graph instead of a tree, i.e. different state-action pairs in the same layer can lead to the same node. This is a necessary condition for ASAP and KVDA to detect any abstractions since they bootstrap of the search graph converging.\\nWe use the Upper Confidence Bounds (UCB) tree policy which is defined as\\nwhere, s s is the state at which the decision has to be made, V a V_{a} is the sum of returns of the action under consideration, and N a N_{a} are its visits. The tree policy chooses the action that maximizes the UCB value.\\nWe use the greedy decision policy, which is choosing the root action, after the search statistics have been gathered, with the maximum Q value.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.8 Problem descriptions',\n",
       "      'paragraphs': 'The descriptions for all the problem models considered here are either found in Schm√∂cker & Dockhorn ( 2025 ) or in Schm√∂cker et\\xa0al. ( 2025d ) . Most models are parametrized (e.g. choosing a concrete racetrack for Racetrack), the concrete instance used for the experiments is found in the ExperimentConfigs folder in our publicly available implementation (Schm√∂cker, 2025 ) .\\nNext, we describe the heuristic functions used for the two-player games.\\nConnect 4 : As the heuristic from the perspective of player one is given by\\nwhere n i n_{i} is the total number of i i stones that are in one row/column/diagonal, divided by i i , but which are not part of a row/column/diagonal of size i + 1 i+1 .\\nConstrictor : The heuristics function used for player one is the number of grid cells that player one could reach before its opponent. If player one wins, the value 100 100 is added to the heuristic.\\nOthello : The heuristic function for any state for player one is given by a weighted sum of all the occupied grid cells. The weight w w of cell ( x , y ) (x,y) is given by 10 / ( 1 + d ) 10/(1+d) where d d is the distance to the closest corner cell. The weight w w is positive for any cells occupied by stones of player one and negative for those occupied by player two. In the round player one wins, 100 100 is added to the heuristics value and ‚àí 100 -100 if player two won.\\nPusher : We used the heuristic for player one that is equal to the difference of alive units of player one and player two in addition to the Manhatten distance between player one and player two‚Äôs units‚Äô center of mass. In the turn that player one wins, 100 100 is added to the heuristic value.',\n",
       "      'subsections': []}]}],\n",
       "  'abstract': 'A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by grouping state-action pairs and using their aggregate statistics instead of single-node statistics. On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS abstraction algorithm for deterministic environments that builds its abstraction using the Abstractions of State-Action Pairs (ASAP) framework, which aims to detect states and state-action pairs with the same value under optimal play by analysing the search graph. ASAP, however, requires two state-action pairs to have the same immediate reward, which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency. In this paper, we break with the paradigm of grouping value-equivalent states or state-action pairs and instead group states and state-action pairs with possibly different values as long as the difference between their values can be inferred. We call this abstraction framework Known Value Difference Abstractions (KVDA), which infers the value differences by analysis of the immediate rewards and modifies OGA-UCT to use this framework instead. The modification is called KVDA-UCT, which detects significantly more abstractions than OGA-UCT, introduces no additional parameter, and outperforms OGA-UCT on a variety of deterministic environments and parameter settings.'},\n",
       " '2510.25770v1': {'content': [{'title': '1 INTRODUCTION',\n",
       "    'paragraphs': 'Generative models, large language models (LLMs) in particular, have gained widespread popularity, with millions of users around the world [OpenAI, 2024a , b , c ; Gemini Team, 2025 ] .\\nHowever, they are susceptible to generating incorrect outputs, or hallucinations [Huang et\\xa0al., 2025 ] , requiring caution in their use.\\nIndeed, in the example illustrated in Fig. Àú 1 of using an LLM for a math problem, 3 of the 5 steps/sub-responses generated by the LLM are incorrect.\\nSince correctness labels are unknown at test time, we need a mechanism to assess the (in)correctness of the generated responses.\\nA recent line of work aims to provide statistical guarantees for such LLM responses [Mohri and Hashimoto, 2024 ; Cherian et\\xa0al., 2024 ; Rubin-Toles et\\xa0al., 2025 ] .\\nThese methods rely on p-value based conformal prediction [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] to filter the response set such that the probability of including an incorrect response, or error, is capped at a user-defined tolerance level Œ± \\\\alpha .\\nImplicitly, this is done by computing a p-value based score for each response, or p-score ; then, the filtered response set is obtained by simply thresholding the p-scores ‚â§ Œ± \\\\leq\\\\alpha .\\nImportantly, Œ± \\\\alpha must be chosen independently of the data.\\nThis begs the question: what if we want a data-dependent Œ± \\\\alpha ?\\nLet us revisit the example in Fig. Àú 1 , and imagine the scores to be the p-scores in question.\\nIf the user pre-sets Œ± = 0.1 \\\\alpha=0.1 , the responses are filtered for p-scores ‚â§ 0.1 \\\\leq 0.1 , resulting in the first two sub-responses.\\nHowever, the p-scores of the filtered set are ‚â§ 0.01 \\\\leq 0.01 ; in fact, the user would have obtained the same filtered set if they had pre-set Œ± = 0.01 \\\\alpha=0.01 instead.\\nSince a tolerance level of 0.01 0.01 conveys a much higher assurance in the responses compared to 0.1 0.1 , the user would want to update Œ± = 0.1 ‚Üí 0.01 \\\\alpha=0.1\\\\rightarrow 0.01 .\\nThis necessitates a tolerance level Œ± \\\\alpha that is data-dependent, called post-hoc Œ± \\\\alpha .\\nUnfortunately, even though such post-hoc Œ± \\\\alpha ‚Äôs are commonly used in practice, the guarantees for p-score based methods are invalidated.\\nThis is due to the susceptibility of p-values, and hence p-scores, to p-hacking [Carney, 2016 ] .\\nIn our work, we propose e-scores as measures of incorrectness : they are low for correct and high for incorrect responses (also depicted in Fig. Àú 1 ).\\nThese scores, based on e-values , provide statistical guarantees on a post-hoc notion of error called size distortion [Koning, 2024 ] : the distortion between observing an error and the user‚Äôs post-hoc tolerance level.\\nThe non-post-hoc error guarantee mentioned before arises as a special case of this generalization.\\nAdditionally, we show that our theoretical guarantees remain valid for any generative model and for a super-set of the response sets considered by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nThis provides an avenue for more diverse applications and use-cases.\\nFurthermore, we experimentally demonstrate the efficacy of our e-scores in the practical assessment of LLM responses under two settings: (i) mathematical factuality, where we check responses for sound mathematical reasoning, and (ii) property constraints satisfaction, where we ensure responses satisfy certain desirable properties.\\nWe summarize the contributions of our work as follows:\\nWe study the problem of achieving statistical guarantees for a post-hoc notion of error, namely size distortion, for generative model outputs.\\nThis generalizes the non-post-hoc guarantees studied before for LLM outputs [Mohri and Hashimoto, 2024 ; Cherian et\\xa0al., 2024 ; Rubin-Toles et\\xa0al., 2025 ] .\\nWe propose e-scores, based on e-values, as measures of incorrectness.\\nOur theoretical results show that e-scores achieve the post-hoc statistical guarantees mentioned above, allowing users flexibility in choosing tolerance levels Œ± \\\\alpha after observing the e-scores themselves.\\nFurthermore, our experimental results provide practical verification for the same.\\nWe show that our theory extends to any generative model and a large response set, with those considered by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] being sub-sets.\\nWe begin by formulating our problem in Section Àú 2 and discussing further background and related works in Section Àú 3 .\\nWe define our e-scores in Section Àú 4 and demonstrate their practical efficacy in Section Àú 5 .\\nWe provide a theoretical analysis of our e-scores in Section Àú 6 .\\nWe finish with the concluding remarks in Section Àú 7 .',\n",
       "    'subsections': []},\n",
       "   {'title': '2 PROBLEM FORMULATION',\n",
       "    'paragraphs': 'We are interested in providing statistical guarantees pertaining to the (in)correctness of generative model outputs.\\nWe begin by defining (i) the generative model outputs we consider, (ii) their (in)correctness, (iii) the post-hoc statistical guarantees we aim to achieve, and (iv) examples of their practical post-hoc use.\\nWhile we use LLMs to provide concrete examples, our setup could be instantiated with any generative model.\\nWe define the prompt space ùí≥ \\\\mathcal{X} , a sub-space of language.\\nGiven a prompt x ‚àà ùí≥ x\\\\in\\\\mathcal{X} , an LLM œÄ \\\\pi generates a response,\\nan ordered set of sub-responses that collectively answer the prompt.\\nThese sub-responses could be sentences, steps in chain-of-thought reasoning [Wei et\\xa0al., 2022 ] , etc., akin to the example in Fig. Àú 1 .\\nWhile natural for auto-regressive models, we do not assume any particular dependency structure.\\nNote that singular responses of length | ùê≤ | = 1 \\\\lvert\\\\mathbf{y}\\\\rvert=1 are a special case.\\nWe define the sub-response space ùí¥ \\\\mathcal{Y} , also a sub-space of language; each sub-response y i ‚àà ùí¥ y_{i}\\\\in\\\\mathcal{Y} and the response ùê≤ ‚àà ‚à™ i ‚â• 1 ùí¥ i \\\\mathbf{y}\\\\in\\\\cup_{i\\\\geq 1}\\\\mathcal{Y}^{i} .\\nWhile we could assess the (in)correctness of g œÄ \\u200b ( x ) = ùê≤ g_{\\\\pi}\\\\left(x\\\\right)=\\\\mathbf{y} alone, we consider a larger set of responses.\\nNotice that a single generated response can form multiple responses: the partial responses ùê≤ ‚â§ i = ( y 1 , ‚Ä¶ , y i ) \\\\mathbf{y}_{\\\\leq i}=\\\\left(y_{1},\\\\ldots,y_{i}\\\\right) , for every i = 1 , ‚Ä¶ , | ùê≤ | i=1,\\\\ldots,\\\\lvert\\\\mathbf{y}\\\\rvert .\\nIn doing so, similar to the example in Fig. Àú 1 , the user can potentially find partial but correct responses.\\nWe define this response set,\\nThis includes the generated response itself g œÄ \\u200b ( x ) ‚àà ùïê \\u200b ( g œÄ \\u200b ( x ) ) g_{\\\\pi}\\\\left(x\\\\right)\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nThis response set differs from the ones considered by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nWe will generalize these response set definitions by using a super-set of responses in Section Àú 6.1 .\\nHowever, for now, we continue to use the definition in Eq. Àú 1 as it is suited for the experimental benchmarks we consider (cf. Section Àú 5 ).\\nThere are different notions of correctness that are of interest to us.\\nFactuality is a pertinent one, to verify whether responses are based on facts [Mohri and Hashimoto, 2024 ; Cherian et\\xa0al., 2024 ; Rubin-Toles et\\xa0al., 2025 ] .\\nAnother is property constraints satisfaction, to ensure responses have certain desirable properties [Dhillon et\\xa0al., 2025 ] .\\nTo adapt to such use-cases, we define (in)correctness with respect to an oracle o o ,\\ndetermining the correctness of ùê≤ \\\\mathbf{y} as a response to the prompt x x .\\nFurther, we define the labeled response set,\\nGiven an LLM œÄ \\\\pi and a prompt x ‚àà ùí≥ x\\\\in\\\\mathcal{X} , we construct its (unlabeled) response set ùïê \\u200b ( g œÄ \\u200b ( x ) ) \\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nOur goal then is to assess the (in)correctness of each response in this set, i.e., we want to reason about the unknown oracle labels o \\u200b ( x , ùê≤ ) o\\\\left(x,\\\\mathbf{y}\\\\right) for every response ùê≤ ‚àà ùïê \\u200b ( g œÄ \\u200b ( x ) ) \\\\mathbf{y}\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nWe do so by complementing each response with a non-negative score s \\u200b ( x , ùê≤ ) ‚àà ‚Ñù ‚â• 0 s\\\\left(x,\\\\mathbf{y}\\\\right)\\\\in\\\\mathds{R}_{\\\\geq 0} as a measure of incorrectness : low for correct responses and high for incorrect responses.\\nConsequently, we will provide the scored response set,\\nto facilitate the user in deciding which responses to include or exclude, i.e., which responses to use or not for their downstream task.\\nIn particular, they could decide to filter the scored response set at some Œ± ‚àà ‚Ñù ‚â• 0 \\\\alpha\\\\in\\\\mathds{R}_{\\\\geq 0} ,\\nSince we want to avoid any incorrect responses, we treat the inclusion of an incorrect response in the filtered set ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) as an error at Œ± \\\\alpha .\\nThen, a possible desideratum for our measure of incorrectness is to upper bound the probability of error at Œ± \\\\alpha by Œ± \\\\alpha itself,\\nIn doing so, Œ± \\\\alpha represents the user‚Äôs tolerance level.\\nThis is considered by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] , who use p-values based conformal prediction [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] to achieve this.\\nNote that the above requirement assumes that Œ± \\\\alpha is determined independently of the data (the prompt, the responses, and the scores).\\nHowever, in practice, users would want to use a data-dependent tolerance level Œ± \\\\alpha . Section Àú 1 highlights such a scenario pertaining to the example in Fig. Àú 1 .\\nOn realizing that they would obtain the same filtered set if they pre-set Œ± = 0.01 \\\\alpha=0.01 instead of 0.1 0.1 , the user wants to update Œ± = 0.1 ‚Üí 0.01 \\\\alpha=0.1\\\\rightarrow 0.01 , conveying higher assurance in the responses with a smaller tolerance level.\\nThis necessitates a data-dependent Œ± \\\\alpha , or a post-hoc Œ± \\\\alpha .\\nSpecifically, we want to enable the user to choose Œ± \\\\alpha after observing the prompt x x and the scored response set ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nSince Œ± \\\\alpha is now a random variable, Eq. Àú 2 cannot be applied directly.\\nWe therefore generalize our desideratum instead to a post-hoc notion of error,\\nThe ratio of observing an error at Œ± \\\\alpha and Œ± \\\\alpha itself is expected to be at most 1.\\nThe ratio captures the distortion between observing an error and the user‚Äôs tolerance level; furthermore, the bound ensures that the expected distortion is controlled. Koning [ 2024 ] uses such an expected distortion for hypothesis testing, calling it size distortion (with size referring to an error).\\nThis generalizes Eq. Àú 2 , recovered when Œ± \\\\alpha is a fixed pre-set value.\\nThus, Eq. Àú 3 will be our new desideratum.\\nTo aid us with our desideratum, we are given labeled calibration data that is assumed to be exchangeable with the test data, similar to Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nThis includes n n calibration prompts x i ‚àà ùí≥ x^{i}\\\\in\\\\mathcal{X} , for i = 1 , ‚Ä¶ , n i=1,\\\\ldots,n , each with their corresponding labeled response set ùïÜ \\u200b ( x i , g œÄ \\u200b ( x i ) ) \\\\mathds{O}\\\\left(x^{i},g_{\\\\pi}\\\\left(x^{i}\\\\right)\\\\right) .\\nNow, given the test prompt x n + 1 ‚àà ùí≥ x^{n+1}\\\\in\\\\mathcal{X} , we will complement each test response ùê≤ n + 1 ‚àà ùïê \\u200b ( g œÄ \\u200b ( x n + 1 ) ) \\\\mathbf{y}^{n+1}\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x^{n+1}\\\\right)\\\\right) with a non-negative test score s \\u200b ( x n + 1 , ùê≤ n + 1 ) ‚àà ‚Ñù ‚â• 0 s\\\\left(x^{n+1},\\\\mathbf{y}^{n+1}\\\\right)\\\\in\\\\mathds{R}_{\\\\geq 0} as a measure of incorrectness, which can additionally depend on the given calibration data.\\nFor simplicity and compactness, we will suppress the notation for the dependence of the scores on the calibration data; note that the requirement in Eq. Àú 3 (and in Eq. Àú 2 ) is now marginal over both the test and the calibration data .\\nWe provide two examples of post-hoc Œ± \\\\alpha strategies.\\nA user could choose either one of these strategies or any other one; in return, we will aim to satisfy Eq. Àú 3 .\\nThe user has a fixed pre-set maximum tolerance level Œ± max ‚àà [ 0 , 1 ] \\\\alpha_{\\\\max}\\\\in\\\\left[0,1\\\\right] .\\nSince the scores in the filtered set ùïä Œ± max \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha_{\\\\max}}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) could be ‚â§ Œ± max \\\\leq\\\\alpha_{\\\\max} , the user updates their tolerance level Œ± \\\\alpha to the maximum score in the corresponding filtered set,\\nAlternatively, the user wants to include Œª ‚àà [ 0 , 1 ] \\\\lambda\\\\in\\\\left[0,1\\\\right] fraction of the response set ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nThen, the user‚Äôs tolerance level Œ± \\\\alpha is the maximum score in the corresponding filtered set,\\nAfter choosing Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right) , the user gets the filtered set ùïä Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right)}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) for downstream use.\\nAn example of such use is to treat the longest response in the filtered set as the default response, as done by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] , now with added guarantees.',\n",
       "    'subsections': [{'title': '2.1 Generative Model Outputs',\n",
       "      'paragraphs': 'We define the prompt space ùí≥ \\\\mathcal{X} , a sub-space of language.\\nGiven a prompt x ‚àà ùí≥ x\\\\in\\\\mathcal{X} , an LLM œÄ \\\\pi generates a response,\\nan ordered set of sub-responses that collectively answer the prompt.\\nThese sub-responses could be sentences, steps in chain-of-thought reasoning [Wei et\\xa0al., 2022 ] , etc., akin to the example in Fig. Àú 1 .\\nWhile natural for auto-regressive models, we do not assume any particular dependency structure.\\nNote that singular responses of length | ùê≤ | = 1 \\\\lvert\\\\mathbf{y}\\\\rvert=1 are a special case.\\nWe define the sub-response space ùí¥ \\\\mathcal{Y} , also a sub-space of language; each sub-response y i ‚àà ùí¥ y_{i}\\\\in\\\\mathcal{Y} and the response ùê≤ ‚àà ‚à™ i ‚â• 1 ùí¥ i \\\\mathbf{y}\\\\in\\\\cup_{i\\\\geq 1}\\\\mathcal{Y}^{i} .\\nWhile we could assess the (in)correctness of g œÄ \\u200b ( x ) = ùê≤ g_{\\\\pi}\\\\left(x\\\\right)=\\\\mathbf{y} alone, we consider a larger set of responses.\\nNotice that a single generated response can form multiple responses: the partial responses ùê≤ ‚â§ i = ( y 1 , ‚Ä¶ , y i ) \\\\mathbf{y}_{\\\\leq i}=\\\\left(y_{1},\\\\ldots,y_{i}\\\\right) , for every i = 1 , ‚Ä¶ , | ùê≤ | i=1,\\\\ldots,\\\\lvert\\\\mathbf{y}\\\\rvert .\\nIn doing so, similar to the example in Fig. Àú 1 , the user can potentially find partial but correct responses.\\nWe define this response set,\\nThis includes the generated response itself g œÄ \\u200b ( x ) ‚àà ùïê \\u200b ( g œÄ \\u200b ( x ) ) g_{\\\\pi}\\\\left(x\\\\right)\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nThis response set differs from the ones considered by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nWe will generalize these response set definitions by using a super-set of responses in Section Àú 6.1 .\\nHowever, for now, we continue to use the definition in Eq. Àú 1 as it is suited for the experimental benchmarks we consider (cf. Section Àú 5 ).',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Oracle (In)Correctness',\n",
       "      'paragraphs': 'There are different notions of correctness that are of interest to us.\\nFactuality is a pertinent one, to verify whether responses are based on facts [Mohri and Hashimoto, 2024 ; Cherian et\\xa0al., 2024 ; Rubin-Toles et\\xa0al., 2025 ] .\\nAnother is property constraints satisfaction, to ensure responses have certain desirable properties [Dhillon et\\xa0al., 2025 ] .\\nTo adapt to such use-cases, we define (in)correctness with respect to an oracle o o ,\\ndetermining the correctness of ùê≤ \\\\mathbf{y} as a response to the prompt x x .\\nFurther, we define the labeled response set,',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3 Statistical Guarantees Desideratum',\n",
       "      'paragraphs': 'Given an LLM œÄ \\\\pi and a prompt x ‚àà ùí≥ x\\\\in\\\\mathcal{X} , we construct its (unlabeled) response set ùïê \\u200b ( g œÄ \\u200b ( x ) ) \\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nOur goal then is to assess the (in)correctness of each response in this set, i.e., we want to reason about the unknown oracle labels o \\u200b ( x , ùê≤ ) o\\\\left(x,\\\\mathbf{y}\\\\right) for every response ùê≤ ‚àà ùïê \\u200b ( g œÄ \\u200b ( x ) ) \\\\mathbf{y}\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nWe do so by complementing each response with a non-negative score s \\u200b ( x , ùê≤ ) ‚àà ‚Ñù ‚â• 0 s\\\\left(x,\\\\mathbf{y}\\\\right)\\\\in\\\\mathds{R}_{\\\\geq 0} as a measure of incorrectness : low for correct responses and high for incorrect responses.\\nConsequently, we will provide the scored response set,\\nto facilitate the user in deciding which responses to include or exclude, i.e., which responses to use or not for their downstream task.\\nIn particular, they could decide to filter the scored response set at some Œ± ‚àà ‚Ñù ‚â• 0 \\\\alpha\\\\in\\\\mathds{R}_{\\\\geq 0} ,\\nSince we want to avoid any incorrect responses, we treat the inclusion of an incorrect response in the filtered set ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) as an error at Œ± \\\\alpha .\\nThen, a possible desideratum for our measure of incorrectness is to upper bound the probability of error at Œ± \\\\alpha by Œ± \\\\alpha itself,\\nIn doing so, Œ± \\\\alpha represents the user‚Äôs tolerance level.\\nThis is considered by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] , who use p-values based conformal prediction [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] to achieve this.\\nNote that the above requirement assumes that Œ± \\\\alpha is determined independently of the data (the prompt, the responses, and the scores).\\nHowever, in practice, users would want to use a data-dependent tolerance level Œ± \\\\alpha . Section Àú 1 highlights such a scenario pertaining to the example in Fig. Àú 1 .\\nOn realizing that they would obtain the same filtered set if they pre-set Œ± = 0.01 \\\\alpha=0.01 instead of 0.1 0.1 , the user wants to update Œ± = 0.1 ‚Üí 0.01 \\\\alpha=0.1\\\\rightarrow 0.01 , conveying higher assurance in the responses with a smaller tolerance level.\\nThis necessitates a data-dependent Œ± \\\\alpha , or a post-hoc Œ± \\\\alpha .\\nSpecifically, we want to enable the user to choose Œ± \\\\alpha after observing the prompt x x and the scored response set ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nSince Œ± \\\\alpha is now a random variable, Eq. Àú 2 cannot be applied directly.\\nWe therefore generalize our desideratum instead to a post-hoc notion of error,\\nThe ratio of observing an error at Œ± \\\\alpha and Œ± \\\\alpha itself is expected to be at most 1.\\nThe ratio captures the distortion between observing an error and the user‚Äôs tolerance level; furthermore, the bound ensures that the expected distortion is controlled. Koning [ 2024 ] uses such an expected distortion for hypothesis testing, calling it size distortion (with size referring to an error).\\nThis generalizes Eq. Àú 2 , recovered when Œ± \\\\alpha is a fixed pre-set value.\\nThus, Eq. Àú 3 will be our new desideratum.\\nTo aid us with our desideratum, we are given labeled calibration data that is assumed to be exchangeable with the test data, similar to Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nThis includes n n calibration prompts x i ‚àà ùí≥ x^{i}\\\\in\\\\mathcal{X} , for i = 1 , ‚Ä¶ , n i=1,\\\\ldots,n , each with their corresponding labeled response set ùïÜ \\u200b ( x i , g œÄ \\u200b ( x i ) ) \\\\mathds{O}\\\\left(x^{i},g_{\\\\pi}\\\\left(x^{i}\\\\right)\\\\right) .\\nNow, given the test prompt x n + 1 ‚àà ùí≥ x^{n+1}\\\\in\\\\mathcal{X} , we will complement each test response ùê≤ n + 1 ‚àà ùïê \\u200b ( g œÄ \\u200b ( x n + 1 ) ) \\\\mathbf{y}^{n+1}\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x^{n+1}\\\\right)\\\\right) with a non-negative test score s \\u200b ( x n + 1 , ùê≤ n + 1 ) ‚àà ‚Ñù ‚â• 0 s\\\\left(x^{n+1},\\\\mathbf{y}^{n+1}\\\\right)\\\\in\\\\mathds{R}_{\\\\geq 0} as a measure of incorrectness, which can additionally depend on the given calibration data.\\nFor simplicity and compactness, we will suppress the notation for the dependence of the scores on the calibration data; note that the requirement in Eq. Àú 3 (and in Eq. Àú 2 ) is now marginal over both the test and the calibration data .',\n",
       "      'subsections': [{'title': 'The Role of Calibration Data',\n",
       "        'paragraphs': 'To aid us with our desideratum, we are given labeled calibration data that is assumed to be exchangeable with the test data, similar to Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nThis includes n n calibration prompts x i ‚àà ùí≥ x^{i}\\\\in\\\\mathcal{X} , for i = 1 , ‚Ä¶ , n i=1,\\\\ldots,n , each with their corresponding labeled response set ùïÜ \\u200b ( x i , g œÄ \\u200b ( x i ) ) \\\\mathds{O}\\\\left(x^{i},g_{\\\\pi}\\\\left(x^{i}\\\\right)\\\\right) .\\nNow, given the test prompt x n + 1 ‚àà ùí≥ x^{n+1}\\\\in\\\\mathcal{X} , we will complement each test response ùê≤ n + 1 ‚àà ùïê \\u200b ( g œÄ \\u200b ( x n + 1 ) ) \\\\mathbf{y}^{n+1}\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x^{n+1}\\\\right)\\\\right) with a non-negative test score s \\u200b ( x n + 1 , ùê≤ n + 1 ) ‚àà ‚Ñù ‚â• 0 s\\\\left(x^{n+1},\\\\mathbf{y}^{n+1}\\\\right)\\\\in\\\\mathds{R}_{\\\\geq 0} as a measure of incorrectness, which can additionally depend on the given calibration data.\\nFor simplicity and compactness, we will suppress the notation for the dependence of the scores on the calibration data; note that the requirement in Eq. Àú 3 (and in Eq. Àú 2 ) is now marginal over both the test and the calibration data .',\n",
       "        'subsections': []}]},\n",
       "     {'title': '2.4 The Use of Post-Hoc ùú∂\\\\boldsymbol{\\\\alpha}‚Äôs',\n",
       "      'paragraphs': 'We provide two examples of post-hoc Œ± \\\\alpha strategies.\\nA user could choose either one of these strategies or any other one; in return, we will aim to satisfy Eq. Àú 3 .\\nThe user has a fixed pre-set maximum tolerance level Œ± max ‚àà [ 0 , 1 ] \\\\alpha_{\\\\max}\\\\in\\\\left[0,1\\\\right] .\\nSince the scores in the filtered set ùïä Œ± max \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha_{\\\\max}}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) could be ‚â§ Œ± max \\\\leq\\\\alpha_{\\\\max} , the user updates their tolerance level Œ± \\\\alpha to the maximum score in the corresponding filtered set,\\nAlternatively, the user wants to include Œª ‚àà [ 0 , 1 ] \\\\lambda\\\\in\\\\left[0,1\\\\right] fraction of the response set ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nThen, the user‚Äôs tolerance level Œ± \\\\alpha is the maximum score in the corresponding filtered set,\\nAfter choosing Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right) , the user gets the filtered set ùïä Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right)}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) for downstream use.\\nAn example of such use is to treat the longest response in the filtered set as the default response, as done by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] , now with added guarantees.',\n",
       "      'subsections': [{'title': 'Max-Constrained Adaptive ùú∂\\\\boldsymbol{\\\\alpha}',\n",
       "        'paragraphs': 'The user has a fixed pre-set maximum tolerance level Œ± max ‚àà [ 0 , 1 ] \\\\alpha_{\\\\max}\\\\in\\\\left[0,1\\\\right] .\\nSince the scores in the filtered set ùïä Œ± max \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha_{\\\\max}}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) could be ‚â§ Œ± max \\\\leq\\\\alpha_{\\\\max} , the user updates their tolerance level Œ± \\\\alpha to the maximum score in the corresponding filtered set,',\n",
       "        'subsections': []},\n",
       "       {'title': 'Fractional Inclusion',\n",
       "        'paragraphs': 'Alternatively, the user wants to include Œª ‚àà [ 0 , 1 ] \\\\lambda\\\\in\\\\left[0,1\\\\right] fraction of the response set ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) .\\nThen, the user‚Äôs tolerance level Œ± \\\\alpha is the maximum score in the corresponding filtered set,\\nAfter choosing Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right) , the user gets the filtered set ùïä Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right)}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) for downstream use.\\nAn example of such use is to treat the longest response in the filtered set as the default response, as done by Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] , now with added guarantees.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '3 BACKGROUND AND RELATED WORK',\n",
       "    'paragraphs': 'Before we discuss related works, we define key concepts that we will rely on frequently.\\nConsider a non-negative random variable R ‚àà ‚Ñù ‚â• 0 R\\\\in\\\\mathds{R}_{\\\\geq 0} .\\nIt is a p-variable if ùêè \\u200b { R ‚â§ Œ± } ‚â§ Œ± \\\\mathbf{P}\\\\left\\\\{R\\\\leq\\\\alpha\\\\right\\\\}\\\\leq\\\\alpha , for all Œ± ‚àà ‚Ñù ‚â• 0 \\\\alpha\\\\in\\\\mathds{R}_{\\\\geq 0} .\\nAnd, it is an e-variable if ùêÑ \\u200b [ R ] ‚â§ 1 \\\\mathbf{E}\\\\left[R\\\\right]\\\\leq 1 (which, with Markov‚Äôs inequality, gives ùêè \\u200b { 1 / R ‚â§ Œ± } ‚â§ Œ± \\\\mathbf{P}\\\\left\\\\{\\\\nicefrac{{1}}{{R}}\\\\leq\\\\alpha\\\\right\\\\}\\\\leq\\\\alpha , for all Œ± ‚àà ‚Ñù ‚â• 0 \\\\alpha\\\\in\\\\mathds{R}_{\\\\geq 0} ).\\nFurthermore, its realized value is called a p- and e-value, respectively.\\nWhile p-values have been used for hypothesis testing for almost a century [Neyman and Pearson, 1933 ; Wald, 1939 ] , recent developments highlight the benefits of e-values [Shafer and Vovk, 2019 ; Wasserman et\\xa0al., 2020 ; Howard et\\xa0al., 2020 , 2021 ; Kaufmann and Koolen, 2021 ; Shafer, 2021 ; Vovk and Wang, 2021 ; Vovk et\\xa0al., 2022b ; Wang and Ramdas, 2022 ; Gr√ºnwald et\\xa0al., 2024 ; Ramdas and Wang, 2025 ] .\\nNotably, Gr√ºnwald [ 2024 ] emphasizes their use in scenarios with post-hoc Œ± \\\\alpha .\\nSince we are also interested in post-hoc Œ± \\\\alpha scenarios, we will base our scores on e-values to attain statistical guarantees.\\nClosest to our work is that of Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] . Mohri and Hashimoto [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] adapt ideas from conformal prediction [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] , typically used to construct prediction sets for supervised learning problems, to filter LLM outputs to construct response sets ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) for a given fixed Œ± \\\\alpha .\\nIn both these works, the dependence on p-values is implicit through their use of the nested conformal framework [Gupta et\\xa0al., 2022 ] .\\nAdditionally, rather than a single fixed Œ± \\\\alpha , Cherian et\\xa0al. [ 2024 ] consider a functional Œ± \\\\alpha that can vary to improve fractional inclusion, but is required to be independent of the scores.\\nConsequently, these works achieve Eq. Àú 2 , but not its post-hoc generalization in Eq. Àú 3 .\\nWe therefore design our scores to achieve the latter for any generative model and its outputs.\\nFurthermore, our theoretical results extend to the assessment of response sets that are larger than those of Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .',\n",
       "    'subsections': []},\n",
       "   {'title': '4 E-SCORES',\n",
       "    'paragraphs': 'We now describe the scores we propose to achieve Eq. Àú 3 .\\nWe defer the theoretical results that justify our design choices to Section Àú 6.2 ; but intuitively, our scores must be reciprocals of the corresponding e-values.\\nTherefore, we call our proposed scores the e-scores .\\nThe functional form of our e-scores is influenced by Gammerman et\\xa0al. [ 1998 ] , who construct e-values for supervised learning under exchangeable data (used by Balinsky and Balinsky [ 2024 ]; Vovk [ 2025 ]; Gauthier et\\xa0al. [ 2025b , a ] for the same).\\nSpecifically, we define our e-score for each test response ùê≤ n + 1 ‚àà ùïê \\u200b ( g œÄ \\u200b ( x n + 1 ) ) \\\\mathbf{y}^{n+1}\\\\in\\\\mathds{Y}\\\\left(g_{\\\\pi}\\\\left(x^{n+1}\\\\right)\\\\right) ,\\nwhere f f is any function mapping a prompt x x and response ùê≤ \\\\mathbf{y} to a non-negative value f \\u200b ( x , ùê≤ ) ‚àà ‚Ñù ‚â• 0 f\\\\left(x,\\\\mathbf{y}\\\\right)\\\\in\\\\mathds{R}_{\\\\geq 0} , and,\\nis the maximum incorrect response value (set to 0 in the absence of an incorrect response). 1 1 1 We follow the convention a / 0 = 0 \\\\nicefrac{{a}}{{0}}\\\\!=\\\\!0 if a = 0 a\\\\!=\\\\!0 otherwise ¬± ‚àû \\\\pm\\\\infty . Therefore, our e-scores compare a test response value to the incorrect calibration response values.\\nThe specific definition of f ‚àó f^{*} provides guarantees pertaining to the inclusion of an incorrect response (cf. Section Àú 6.2 ), which is similar to the non-conformity functions in Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] .\\nFor our e-scores to be measures of incorrectness, f \\u200b ( x , ùê≤ ) f\\\\left(x,\\\\mathbf{y}\\\\right) should intuitively be a proxy for the oracle o \\u200b ( x , ùê≤ ) o\\\\left(x,\\\\mathbf{y}\\\\right) : high for correct responses and low for incorrect responses.\\nIf the oracle were known, f o \\u200b ( x , ùê≤ ) f_{o}\\\\left(x,\\\\mathbf{y}\\\\right) could be any monotonically increasing transformation of the oracle.\\nThis includes, but is not limited to: (i) o \\u200b ( x , ùê≤ ) o\\\\left(x,\\\\mathbf{y}\\\\right) , (ii) ( 1 ‚àí o \\u200b ( x , ùê≤ ) ) ‚àí 1 \\\\left(1-o\\\\left(x,\\\\mathbf{y}\\\\right)\\\\right)^{-1} , and (iii) o \\u200b ( x , ùê≤ ) ‚ãÖ ( 1 ‚àí o \\u200b ( x , ùê≤ ) ) ‚àí 1 o\\\\left(x,\\\\mathbf{y}\\\\right)\\\\cdot\\\\left(1-o\\\\left(x,\\\\mathbf{y}\\\\right)\\\\right)^{-1} .\\nSince the oracle is unknown, we approximate it with o ^ \\\\hat{o} (with a range of [ 0 , 1 ] \\\\left[0,1\\\\right] ); this is discussed in detail below.\\nThen, the above options for f o ^ \\u200b ( x , ùê≤ ) f_{\\\\hat{o}}\\\\left(x,\\\\mathbf{y}\\\\right) translate to,\\neach providing an e-score with a different range.\\nTherefore, with an oracle estimator o ^ \\\\hat{o} and its transformation f o ^ f_{\\\\hat{o}} , we can compute our proposed e-scores in Eq. Àú 4 .\\nWe summarize this e-scoring mechanism in Algorithm Àú 1 .\\nFurthermore, note that our e-scores achieve our desideratum in Eq. Àú 3 ; this is despite the possible errors in the oracle approximation (cf. Section Àú 6.2 ).\\nThe oracle estimator o ^ \\\\hat{o} approximates the oracle o o , a binary classification problem.\\nTherefore, o ^ \\\\hat{o} is trained to predict the probability of correctness, with a range of [ 0 , 1 ] \\\\left[0,1\\\\right] .\\nThe data used to train the estimator should be independent of the test and the calibration data.\\nSo far, the estimator o ^ \\u200b ( x , ùê≤ ) \\\\hat{o}\\\\left(x,\\\\mathbf{y}\\\\right) operates at the response-level ùê≤ \\\\mathbf{y} .\\nRecall that a response consists of multiple sub-responses ùê≤ = ( y 1 , y 2 , ‚Ä¶ ) \\\\mathbf{y}=\\\\left(y_{1},y_{2},\\\\ldots\\\\right) .\\nIn some settings, like in Section Àú 5.1 , the oracle estimator operates at the sub-response-level y i y_{i} , predicting the probability of correctness of individual sub-responses.\\nTo translate this to a response-level prediction, we multiply the individual (conditional) predictions like conditional probabilities,\\nWith different options for the transformed oracle estimator in Eq. Àú 5 , the use-case would determine the choice in practice.\\nAlternatively, without making assumptions about or restricting the use-cases, one can opt to combine multiple e-scores by first combining the underlying e-values.\\nWe use the fact that simple averaging of e-values yields an admissible e-value [Vovk and Wang, 2021 ] .\\nLet s e-score ( i ) \\u200b ( x n + 1 , ùê≤ n + 1 ) s_{\\\\text{e-score ($i$)}}\\\\left(x^{n+1},\\\\mathbf{y}^{n+1}\\\\right) for i = 1 , 2 , 3 i=1,2,3 be the three e-scores corresponding to the options in Eq. Àú 5 .\\nThen, we combine them into one e-score,\\nWe will use this e-score by default, unless otherwise mentioned; this is also used for the example in Fig. Àú 1 .',\n",
       "    'subsections': [{'title': '4.1 Oracle Estimator',\n",
       "      'paragraphs': 'The oracle estimator o ^ \\\\hat{o} approximates the oracle o o , a binary classification problem.\\nTherefore, o ^ \\\\hat{o} is trained to predict the probability of correctness, with a range of [ 0 , 1 ] \\\\left[0,1\\\\right] .\\nThe data used to train the estimator should be independent of the test and the calibration data.\\nSo far, the estimator o ^ \\u200b ( x , ùê≤ ) \\\\hat{o}\\\\left(x,\\\\mathbf{y}\\\\right) operates at the response-level ùê≤ \\\\mathbf{y} .\\nRecall that a response consists of multiple sub-responses ùê≤ = ( y 1 , y 2 , ‚Ä¶ ) \\\\mathbf{y}=\\\\left(y_{1},y_{2},\\\\ldots\\\\right) .\\nIn some settings, like in Section Àú 5.1 , the oracle estimator operates at the sub-response-level y i y_{i} , predicting the probability of correctness of individual sub-responses.\\nTo translate this to a response-level prediction, we multiply the individual (conditional) predictions like conditional probabilities,',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Combining E-Scores',\n",
       "      'paragraphs': 'With different options for the transformed oracle estimator in Eq. Àú 5 , the use-case would determine the choice in practice.\\nAlternatively, without making assumptions about or restricting the use-cases, one can opt to combine multiple e-scores by first combining the underlying e-values.\\nWe use the fact that simple averaging of e-values yields an admissible e-value [Vovk and Wang, 2021 ] .\\nLet s e-score ( i ) \\u200b ( x n + 1 , ùê≤ n + 1 ) s_{\\\\text{e-score ($i$)}}\\\\left(x^{n+1},\\\\mathbf{y}^{n+1}\\\\right) for i = 1 , 2 , 3 i=1,2,3 be the three e-scores corresponding to the options in Eq. Àú 5 .\\nThen, we combine them into one e-score,\\nWe will use this e-score by default, unless otherwise mentioned; this is also used for the example in Fig. Àú 1 .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 EXPERIMENTAL RESULTS',\n",
       "    'paragraphs': 'We now demonstrate the efficacy of our proposed e-scores with two experimental use-cases.\\nWe discuss the use-cases individually and summarize the observed trends collectively.\\nWe begin by stating the baselines, the metrics for comparisons, the time and memory complexities, and the experimental setup.\\nWe also perform a worst-case analysis in Appendix Àú B , where Œ± \\\\alpha is chosen to maximize the size distortion in Eq. Àú 3 .\\nWe compare against p-value based scores, or p-scores .\\nAnalogous to our e-scores in Eq. Àú 4 , the p-scores can be defined as the corresponding p-values,\\ncomparing the test response value to the incorrect calibration response values as relative ranks [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] . Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] use such p-scores implicitly to achieve Eq. Àú 2 , which we make explicit in Appendix Àú C .\\nDue to the reliance on relative ranks, the choice of the transformed oracle estimator in Eq. Àú 5 does not matter, as they are monotonically increasing transformations of each other.\\nWe also compare against the transformed oracle estimators in Eq. Àú 5 directly, without conversion to e-/p-scores.\\nThese naive scores generally do not come with any statistical guarantees by themselves.\\nWe will use them for our worst-case analysis in Appendix Àú B .\\nOur comparisons are based on the following.\\nSize distortion. This is the most important metric, from our desideratum in Eq. Àú 3 .\\nWe report its empirical mean, which is expected to be ‚â§ 1 \\\\leq 1 .\\nError vs. Œ± \\\\alpha . While we aim to control size distortion, the expected error to Œ± \\\\alpha ratio, one might also be interested in the expected error and expected Œ± \\\\alpha individually.\\nWe report both empirical means.\\nWe ideally want the obtained error to be lower than the tolerance level (i.e., mean error ‚â§ \\\\leq mean Œ± \\\\alpha ).\\nPrecision vs. recall. We provide guarantees around the inclusion of an incorrect response.\\nSimultaneously, we want to experimentally validate that we do not exclude too many correct responses.\\nFor this reason, we report the empirical means of precision (fraction of correct responses among the ones filtered) and recall (fraction of correct responses that are filtered).\\nWe use the precision-recall curve for comparisons, where higher is better for both.\\nOur proposed e-scores are cheaper to compute, in memory and time, compared to the p-scores.\\nFor a given test prompt-response pair, p-scores compute relative ranks with the calibration data (cf. Eq. Àú 7 ).\\nThis requires memory and time that grows linearly in the amount of calibration data n n for every individual test prompt-response pair.\\nConversely, e-scores compute a sum over the calibration data (cf. the second term in the denominator of Eq. Àú 4 ).\\nThis requires constant memory and time that grows linearly in n n .\\nAdditionally, this is a one time cost, amortized over all test prompt-response pairs.\\nWe randomly split the data 50-50% into test and calibration (no training data is required as we use pre-trained oracle estimators).\\nThe metrics are averaged over 100 such random splits.\\nWe use an NVIDIA A100 GPU for the pre-trained oracle estimators; the remaining computations run on a CPU.\\nProcessBench [Zheng et\\xa0al., 2025 ] is a mathematical reasoning benchmark.\\nIt contains prompts from GSM8K [Cobbe et\\xa0al., 2021 ] , MATH [Hendrycks et\\xa0al., 2021 ] , OlympiadBench [He et\\xa0al., 2024 ] , and Omni-MATH [Gao et\\xa0al., 2025 ] .\\nThe responses are first generated by 12 open-source LLMs [LLaMA Team, 2024 ; Yang et\\xa0al., 2024a , b ; Qwen Team, 2025 ] , then separated into multiple steps/sub-responses using Qwen2.5-72B-Instruct [Qwen Team, 2025 ] .\\nLastly, human experts annotate the earliest-occurring incorrect sub-response.\\nLet ùê≤ = ( y 1 , y 2 , ‚Ä¶ ) \\\\mathbf{y}=\\\\left(y_{1},y_{2},\\\\ldots\\\\right) be a generated response and i i be its annotation.\\nThen, the responses ùê≤ ‚â§ j \\\\mathbf{y}_{\\\\leq j} for j = i , ‚Ä¶ , | ùê≤ | j=i,\\\\ldots,\\\\lvert\\\\mathbf{y}\\\\rvert are incorrect as they contain the i i ‚Äôth sub-response, whereas the responses for j = 1 , ‚Ä¶ , i ‚àí 1 j=1,\\\\ldots,i-1 are correct. Fig. Àú 1 illustrates one such labeled prompt-responses.\\nZheng et\\xa0al. [ 2025 ] also benchmark math-based process reward models, that predict the correctness of individual sub-responses.\\nWe consider two of them here: (i) Qwen2.5-Math-7B-PRM800K (or QwenPRM) proposed by them, and (ii) Math-Shepherd-PRM-7B (or MathShepherdPRM) [Wang et\\xa0al., 2024 ] .\\nWe use these pre-trained LLMs as oracle estimators, converting their outputs to response-level predictions (cf. Section Àú 4.1 ).\\nWe consider both post-hoc Œ± \\\\alpha strategies discussed in Section Àú 2.4 . Fig. Àú 2 illustrates the results.\\nFurthermore, the example provided in Fig. Àú 1 is from this setting.\\nUltraFeedback [Cui et\\xa0al., 2024 ] is a diverse and fine-grained preference dataset.\\nIt contains prompts from 6 benchmarks (we use Evol-Instruct [Xu et\\xa0al., 2024 ] and TruthfulQA [Lin et\\xa0al., 2022 ] ) and responses from 17 commercial and open-source LLMs [Chiang et\\xa0al., 2023 ; Tunstall et\\xa0al., 2023 ; Taori et\\xa0al., 2023 ; Touvron et\\xa0al., 2023 ; Biderman et\\xa0al., 2023 ; Almazrouei et\\xa0al., 2023 ; Ding et\\xa0al., 2023 ; OpenAI, 2024a ; Xu et\\xa0al., 2024 ] .\\nIt employs GPT-4 [OpenAI, 2024a ] to provide ratings (from 1 to 5) for 4 properties: instruction-following, truthfulness, honesty, and helpfulness.\\nIn practice, users are often interested in responses with certain desirable properties, which is equivalent to constraining the property ratings [Dhillon et\\xa0al., 2025 ] .\\nWe use this to define the (in)correctness of responses in two ways.\\nA response is correct if both its instruction-following and helpfulness ratings are ‚â• 4 \\\\geq 4 .\\nWe use the Evol-Instruct [Xu et\\xa0al., 2024 ] prompts. Fig. Àú 3(a) illustrates the results.\\nA response is deemed correct if both its truthfulness and honesty ratings are ‚â• 5 \\\\geq 5 .\\nWe use prompts from the TruthfulQA [Lin et\\xa0al., 2022 ] benchmark here. Fig. Àú 3(b) illustrates the results.\\nCui et\\xa0al. [ 2024 ] also provide a reward model, UltraRM, to predict response preference.\\nWe transform the range of this pre-trained LLM to [ 0 , 1 ] \\\\left[0,1\\\\right] by appending a sigmoid operator to it.\\nWe use this as the oracle estimator.\\nThis setting uses singular responses ( | ùê≤ | = 1 \\\\lvert\\\\mathbf{y}\\\\rvert=1 ).\\nAs a result, we only consider the max-constrained adaptive Œ± \\\\alpha strategies as the fractional inclusion strategy would either include or exclude all responses (cf. Section Àú 2.4 ).\\nThe trends we observe for the different experimental use-cases are consistent; we summarize them together.\\nOur proposed e-scores reliably upper bound size distortion by 1, verifying our theory in achieving Eq. Àú 3 (cf. Section Àú 6.2 ).\\nConversely, p-scores are unable to achieve this; the only time they experimentally satisfy Eq. Àú 3 is when all responses (correct and incorrect) are excluded, with 0 error by default.\\nOur proposed e-scores consistently obtain a mean error lower than or approximately equal to the mean tolerance Œ± \\\\alpha .\\nConversely, p-scores consistently obtain a mean error higher than the mean tolerance Œ± \\\\alpha .\\nThe precision-recall curves for our proposed e-scores overlap (partially or completely) with those of the p-scores.\\nIn attaining the post-hoc guarantees in Eq. Àú 3 , the e-scores are more conservative and prefer maintaining high precision over high recall.\\nConsequently, restricting Œ± \\\\alpha ‚Äôs to be ‚â§ 1 \\\\leq 1 (under the max-constrained adaptive Œ± \\\\alpha strategies) restricts the e-score recalls compared to the p-score recalls, resulting in partial overlap.\\nHowever, removing this restriction (under the fractional inclusion strategies) retains complete overlap of the e- and p-score precision-recall curves.\\nThe choice of the oracle estimator makes a difference.\\nThis is best depicted by the precision-recall curves in Fig. Àú 2(a) , where QwenPRM achieves higher precision and recall compared to MathShepherdPRM.\\nThis is expected as the former has comparatively higher accuracy [Zheng et\\xa0al., 2025 ] .',\n",
       "    'subsections': [{'title': 'Baselines',\n",
       "      'paragraphs': 'We compare against p-value based scores, or p-scores .\\nAnalogous to our e-scores in Eq. Àú 4 , the p-scores can be defined as the corresponding p-values,\\ncomparing the test response value to the incorrect calibration response values as relative ranks [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] . Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ]; Rubin-Toles et\\xa0al. [ 2025 ] use such p-scores implicitly to achieve Eq. Àú 2 , which we make explicit in Appendix Àú C .\\nDue to the reliance on relative ranks, the choice of the transformed oracle estimator in Eq. Àú 5 does not matter, as they are monotonically increasing transformations of each other.\\nWe also compare against the transformed oracle estimators in Eq. Àú 5 directly, without conversion to e-/p-scores.\\nThese naive scores generally do not come with any statistical guarantees by themselves.\\nWe will use them for our worst-case analysis in Appendix Àú B .',\n",
       "      'subsections': []},\n",
       "     {'title': 'Metrics',\n",
       "      'paragraphs': 'Our comparisons are based on the following.\\nSize distortion. This is the most important metric, from our desideratum in Eq. Àú 3 .\\nWe report its empirical mean, which is expected to be ‚â§ 1 \\\\leq 1 .\\nError vs. Œ± \\\\alpha . While we aim to control size distortion, the expected error to Œ± \\\\alpha ratio, one might also be interested in the expected error and expected Œ± \\\\alpha individually.\\nWe report both empirical means.\\nWe ideally want the obtained error to be lower than the tolerance level (i.e., mean error ‚â§ \\\\leq mean Œ± \\\\alpha ).\\nPrecision vs. recall. We provide guarantees around the inclusion of an incorrect response.\\nSimultaneously, we want to experimentally validate that we do not exclude too many correct responses.\\nFor this reason, we report the empirical means of precision (fraction of correct responses among the ones filtered) and recall (fraction of correct responses that are filtered).\\nWe use the precision-recall curve for comparisons, where higher is better for both.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Memory and Time Complexities',\n",
       "      'paragraphs': 'Our proposed e-scores are cheaper to compute, in memory and time, compared to the p-scores.\\nFor a given test prompt-response pair, p-scores compute relative ranks with the calibration data (cf. Eq. Àú 7 ).\\nThis requires memory and time that grows linearly in the amount of calibration data n n for every individual test prompt-response pair.\\nConversely, e-scores compute a sum over the calibration data (cf. the second term in the denominator of Eq. Àú 4 ).\\nThis requires constant memory and time that grows linearly in n n .\\nAdditionally, this is a one time cost, amortized over all test prompt-response pairs.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Experimental Setup',\n",
       "      'paragraphs': 'We randomly split the data 50-50% into test and calibration (no training data is required as we use pre-trained oracle estimators).\\nThe metrics are averaged over 100 such random splits.\\nWe use an NVIDIA A100 GPU for the pre-trained oracle estimators; the remaining computations run on a CPU.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.1 Mathematical Factuality',\n",
       "      'paragraphs': 'ProcessBench [Zheng et\\xa0al., 2025 ] is a mathematical reasoning benchmark.\\nIt contains prompts from GSM8K [Cobbe et\\xa0al., 2021 ] , MATH [Hendrycks et\\xa0al., 2021 ] , OlympiadBench [He et\\xa0al., 2024 ] , and Omni-MATH [Gao et\\xa0al., 2025 ] .\\nThe responses are first generated by 12 open-source LLMs [LLaMA Team, 2024 ; Yang et\\xa0al., 2024a , b ; Qwen Team, 2025 ] , then separated into multiple steps/sub-responses using Qwen2.5-72B-Instruct [Qwen Team, 2025 ] .\\nLastly, human experts annotate the earliest-occurring incorrect sub-response.\\nLet ùê≤ = ( y 1 , y 2 , ‚Ä¶ ) \\\\mathbf{y}=\\\\left(y_{1},y_{2},\\\\ldots\\\\right) be a generated response and i i be its annotation.\\nThen, the responses ùê≤ ‚â§ j \\\\mathbf{y}_{\\\\leq j} for j = i , ‚Ä¶ , | ùê≤ | j=i,\\\\ldots,\\\\lvert\\\\mathbf{y}\\\\rvert are incorrect as they contain the i i ‚Äôth sub-response, whereas the responses for j = 1 , ‚Ä¶ , i ‚àí 1 j=1,\\\\ldots,i-1 are correct. Fig. Àú 1 illustrates one such labeled prompt-responses.\\nZheng et\\xa0al. [ 2025 ] also benchmark math-based process reward models, that predict the correctness of individual sub-responses.\\nWe consider two of them here: (i) Qwen2.5-Math-7B-PRM800K (or QwenPRM) proposed by them, and (ii) Math-Shepherd-PRM-7B (or MathShepherdPRM) [Wang et\\xa0al., 2024 ] .\\nWe use these pre-trained LLMs as oracle estimators, converting their outputs to response-level predictions (cf. Section Àú 4.1 ).\\nWe consider both post-hoc Œ± \\\\alpha strategies discussed in Section Àú 2.4 . Fig. Àú 2 illustrates the results.\\nFurthermore, the example provided in Fig. Àú 1 is from this setting.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Property Constraints Satisfaction',\n",
       "      'paragraphs': 'UltraFeedback [Cui et\\xa0al., 2024 ] is a diverse and fine-grained preference dataset.\\nIt contains prompts from 6 benchmarks (we use Evol-Instruct [Xu et\\xa0al., 2024 ] and TruthfulQA [Lin et\\xa0al., 2022 ] ) and responses from 17 commercial and open-source LLMs [Chiang et\\xa0al., 2023 ; Tunstall et\\xa0al., 2023 ; Taori et\\xa0al., 2023 ; Touvron et\\xa0al., 2023 ; Biderman et\\xa0al., 2023 ; Almazrouei et\\xa0al., 2023 ; Ding et\\xa0al., 2023 ; OpenAI, 2024a ; Xu et\\xa0al., 2024 ] .\\nIt employs GPT-4 [OpenAI, 2024a ] to provide ratings (from 1 to 5) for 4 properties: instruction-following, truthfulness, honesty, and helpfulness.\\nIn practice, users are often interested in responses with certain desirable properties, which is equivalent to constraining the property ratings [Dhillon et\\xa0al., 2025 ] .\\nWe use this to define the (in)correctness of responses in two ways.\\nA response is correct if both its instruction-following and helpfulness ratings are ‚â• 4 \\\\geq 4 .\\nWe use the Evol-Instruct [Xu et\\xa0al., 2024 ] prompts. Fig. Àú 3(a) illustrates the results.\\nA response is deemed correct if both its truthfulness and honesty ratings are ‚â• 5 \\\\geq 5 .\\nWe use prompts from the TruthfulQA [Lin et\\xa0al., 2022 ] benchmark here. Fig. Àú 3(b) illustrates the results.\\nCui et\\xa0al. [ 2024 ] also provide a reward model, UltraRM, to predict response preference.\\nWe transform the range of this pre-trained LLM to [ 0 , 1 ] \\\\left[0,1\\\\right] by appending a sigmoid operator to it.\\nWe use this as the oracle estimator.\\nThis setting uses singular responses ( | ùê≤ | = 1 \\\\lvert\\\\mathbf{y}\\\\rvert=1 ).\\nAs a result, we only consider the max-constrained adaptive Œ± \\\\alpha strategies as the fractional inclusion strategy would either include or exclude all responses (cf. Section Àú 2.4 ).',\n",
       "      'subsections': [{'title': 'Instruction-Following and Helpfulness',\n",
       "        'paragraphs': 'A response is correct if both its instruction-following and helpfulness ratings are ‚â• 4 \\\\geq 4 .\\nWe use the Evol-Instruct [Xu et\\xa0al., 2024 ] prompts. Fig. Àú 3(a) illustrates the results.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Truthfulness and Honesty',\n",
       "        'paragraphs': 'A response is deemed correct if both its truthfulness and honesty ratings are ‚â• 5 \\\\geq 5 .\\nWe use prompts from the TruthfulQA [Lin et\\xa0al., 2022 ] benchmark here. Fig. Àú 3(b) illustrates the results.\\nCui et\\xa0al. [ 2024 ] also provide a reward model, UltraRM, to predict response preference.\\nWe transform the range of this pre-trained LLM to [ 0 , 1 ] \\\\left[0,1\\\\right] by appending a sigmoid operator to it.\\nWe use this as the oracle estimator.\\nThis setting uses singular responses ( | ùê≤ | = 1 \\\\lvert\\\\mathbf{y}\\\\rvert=1 ).\\nAs a result, we only consider the max-constrained adaptive Œ± \\\\alpha strategies as the fractional inclusion strategy would either include or exclude all responses (cf. Section Àú 2.4 ).',\n",
       "        'subsections': []}]},\n",
       "     {'title': '5.3 Observed Trends',\n",
       "      'paragraphs': 'The trends we observe for the different experimental use-cases are consistent; we summarize them together.\\nOur proposed e-scores reliably upper bound size distortion by 1, verifying our theory in achieving Eq. Àú 3 (cf. Section Àú 6.2 ).\\nConversely, p-scores are unable to achieve this; the only time they experimentally satisfy Eq. Àú 3 is when all responses (correct and incorrect) are excluded, with 0 error by default.\\nOur proposed e-scores consistently obtain a mean error lower than or approximately equal to the mean tolerance Œ± \\\\alpha .\\nConversely, p-scores consistently obtain a mean error higher than the mean tolerance Œ± \\\\alpha .\\nThe precision-recall curves for our proposed e-scores overlap (partially or completely) with those of the p-scores.\\nIn attaining the post-hoc guarantees in Eq. Àú 3 , the e-scores are more conservative and prefer maintaining high precision over high recall.\\nConsequently, restricting Œ± \\\\alpha ‚Äôs to be ‚â§ 1 \\\\leq 1 (under the max-constrained adaptive Œ± \\\\alpha strategies) restricts the e-score recalls compared to the p-score recalls, resulting in partial overlap.\\nHowever, removing this restriction (under the fractional inclusion strategies) retains complete overlap of the e- and p-score precision-recall curves.\\nThe choice of the oracle estimator makes a difference.\\nThis is best depicted by the precision-recall curves in Fig. Àú 2(a) , where QwenPRM achieves higher precision and recall compared to MathShepherdPRM.\\nThis is expected as the former has comparatively higher accuracy [Zheng et\\xa0al., 2025 ] .',\n",
       "      'subsections': [{'title': 'Size Distortion',\n",
       "        'paragraphs': 'Our proposed e-scores reliably upper bound size distortion by 1, verifying our theory in achieving Eq. Àú 3 (cf. Section Àú 6.2 ).\\nConversely, p-scores are unable to achieve this; the only time they experimentally satisfy Eq. Àú 3 is when all responses (correct and incorrect) are excluded, with 0 error by default.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Error vs. ùú∂\\\\boldsymbol{\\\\alpha}',\n",
       "        'paragraphs': 'Our proposed e-scores consistently obtain a mean error lower than or approximately equal to the mean tolerance Œ± \\\\alpha .\\nConversely, p-scores consistently obtain a mean error higher than the mean tolerance Œ± \\\\alpha .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Precision vs. Recall',\n",
       "        'paragraphs': 'The precision-recall curves for our proposed e-scores overlap (partially or completely) with those of the p-scores.\\nIn attaining the post-hoc guarantees in Eq. Àú 3 , the e-scores are more conservative and prefer maintaining high precision over high recall.\\nConsequently, restricting Œ± \\\\alpha ‚Äôs to be ‚â§ 1 \\\\leq 1 (under the max-constrained adaptive Œ± \\\\alpha strategies) restricts the e-score recalls compared to the p-score recalls, resulting in partial overlap.\\nHowever, removing this restriction (under the fractional inclusion strategies) retains complete overlap of the e- and p-score precision-recall curves.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Oracle Estimator',\n",
       "        'paragraphs': 'The choice of the oracle estimator makes a difference.\\nThis is best depicted by the precision-recall curves in Fig. Àú 2(a) , where QwenPRM achieves higher precision and recall compared to MathShepherdPRM.\\nThis is expected as the former has comparatively higher accuracy [Zheng et\\xa0al., 2025 ] .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '6 THEORETICAL RESULTS',\n",
       "    'paragraphs': 'We present our theoretical results here: (i) generalize the response set in Eq. Àú 1 to a larger one, and (ii) show that our e-scores achieve the desideratum in Eq. Àú 3 .\\nWe want to make the response set in Eq. Àú 1 as large as possible, while maintaining statistical guarantees.\\nThis enables the (in)correctness assessment of a larger set of responses, opening up possibilities for more diverse applications.\\nTherefore, we generalize Eq. Àú 1 to,\\nwhere œÉ \\u200b ( g œÄ \\u200b ( x ) ) \\\\sigma\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) is a permuted version of the generated response g œÄ \\u200b ( x ) g_{\\\\pi}\\\\left(x\\\\right) , and the union is over all permutations.\\nIf we fix œÉ \\\\sigma to the identity ordering only, we recover Eq. Àú 1 .\\nSimilarly, Rubin-Toles et\\xa0al. [ 2025 ] restrict œÉ \\\\sigma to orderings (what they call topological orderings of an approximate deducibility graph) obtained from GPT-4o [OpenAI, 2024b ] .\\nLastly, Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ] do not account for the inherent ordering of the sub-responses to make up a response.\\nTherefore, Eq. Àú 8 is a super-set of responses, containing all the response sets discussed above.\\nIn fact, we believe that it is the largest set of responses to consider when given a single generated response g œÄ \\u200b ( x ) g_{\\\\pi}\\\\left(x\\\\right) .\\nInstead of the full response set in Eq. Àú 8 , one might choose to use a sub-set depending on the use-case, while maintaining guarantees.\\nFor example, we use Eq. Àú 1 in Section Àú 5.1 as it is tailor-made for that benchmark.\\nWe are interested in achieving the desideratum in Eq. Àú 3 for any post-hoc Œ± \\\\alpha that a user might choose.\\nWithout restricting the user‚Äôs choice, we will analyze the setting where Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right) maximizes size distortion.\\nIf Eq. Àú 3 is satisfied under this worst-case setting, it will also be satisfied under any post-hoc Œ± \\\\alpha .\\nNote that a response is included in the filtered set ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) if and only if its score is ‚â§ Œ± \\\\leq\\\\alpha .\\nTherefore, we can re-write the inclusion of an incorrect response as at least one incorrect response score being ‚â§ Œ± \\\\leq\\\\alpha ,\\nThen, the worst-case size distortion simplifies to,\\nbecause Œ± \\\\alpha is set to the smallest value such that the indicator function evaluates to 1, otherwise the term is 0.\\nTo upper bound the above expectation by 1 is equivalent to requiring ( min ( ùê≤ , c ) ‚àà ùïÜ \\u200b ( x , g œÄ \\u200b ( x ) ) : c = 0 \\u2061 s \\u200b ( x , ùê≤ ) ) ‚àí 1 \\\\left(\\\\min_{\\\\left(\\\\mathbf{y},c\\\\right)\\\\in\\\\mathds{O}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right):c=0}s\\\\left(x,\\\\mathbf{y}\\\\right)\\\\right)^{-1} to be an e-value (by definition), hence the specific choice of our proposed e-scores in Eq. Àú 4 .\\nIndeed, if we use that definition here, the above term simplifies to,\\nwhich is an e-value under exchangeable data for any non-negative function f f [Gammerman et\\xa0al., 1998 ] .\\nLastly, since our e-scores satisfy Eq. Àú 3 under this worst-case setting, they also satisfy Eq. Àú 3 under any post-hoc Œ± \\\\alpha .\\nWe summarize this theoretical result below, and provide the detailed derivation in Appendix Àú A .\\nIf the test and the calibration prompts are exchangeable, then, our proposed e-scores in Eqs. Àú 4 and 6 upper bound the size distortion (marginal over the test and the calibration prompts) by 1, as in Eq. Àú 3 .',\n",
       "    'subsections': [{'title': '6.1 Super-Set of Responses',\n",
       "      'paragraphs': 'We want to make the response set in Eq. Àú 1 as large as possible, while maintaining statistical guarantees.\\nThis enables the (in)correctness assessment of a larger set of responses, opening up possibilities for more diverse applications.\\nTherefore, we generalize Eq. Àú 1 to,\\nwhere œÉ \\u200b ( g œÄ \\u200b ( x ) ) \\\\sigma\\\\left(g_{\\\\pi}\\\\left(x\\\\right)\\\\right) is a permuted version of the generated response g œÄ \\u200b ( x ) g_{\\\\pi}\\\\left(x\\\\right) , and the union is over all permutations.\\nIf we fix œÉ \\\\sigma to the identity ordering only, we recover Eq. Àú 1 .\\nSimilarly, Rubin-Toles et\\xa0al. [ 2025 ] restrict œÉ \\\\sigma to orderings (what they call topological orderings of an approximate deducibility graph) obtained from GPT-4o [OpenAI, 2024b ] .\\nLastly, Mohri and Hashimoto [ 2024 ]; Cherian et\\xa0al. [ 2024 ] do not account for the inherent ordering of the sub-responses to make up a response.\\nTherefore, Eq. Àú 8 is a super-set of responses, containing all the response sets discussed above.\\nIn fact, we believe that it is the largest set of responses to consider when given a single generated response g œÄ \\u200b ( x ) g_{\\\\pi}\\\\left(x\\\\right) .\\nInstead of the full response set in Eq. Àú 8 , one might choose to use a sub-set depending on the use-case, while maintaining guarantees.\\nFor example, we use Eq. Àú 1 in Section Àú 5.1 as it is tailor-made for that benchmark.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.2 Worst-Case Analysis and E-Values',\n",
       "      'paragraphs': 'We are interested in achieving the desideratum in Eq. Àú 3 for any post-hoc Œ± \\\\alpha that a user might choose.\\nWithout restricting the user‚Äôs choice, we will analyze the setting where Œ± \\u200b ( x , ùïä \\u200b ( x , g œÄ \\u200b ( x ) ) ) \\\\alpha\\\\left(x,\\\\mathds{S}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right) maximizes size distortion.\\nIf Eq. Àú 3 is satisfied under this worst-case setting, it will also be satisfied under any post-hoc Œ± \\\\alpha .\\nNote that a response is included in the filtered set ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) if and only if its score is ‚â§ Œ± \\\\leq\\\\alpha .\\nTherefore, we can re-write the inclusion of an incorrect response as at least one incorrect response score being ‚â§ Œ± \\\\leq\\\\alpha ,\\nThen, the worst-case size distortion simplifies to,\\nbecause Œ± \\\\alpha is set to the smallest value such that the indicator function evaluates to 1, otherwise the term is 0.\\nTo upper bound the above expectation by 1 is equivalent to requiring ( min ( ùê≤ , c ) ‚àà ùïÜ \\u200b ( x , g œÄ \\u200b ( x ) ) : c = 0 \\u2061 s \\u200b ( x , ùê≤ ) ) ‚àí 1 \\\\left(\\\\min_{\\\\left(\\\\mathbf{y},c\\\\right)\\\\in\\\\mathds{O}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right):c=0}s\\\\left(x,\\\\mathbf{y}\\\\right)\\\\right)^{-1} to be an e-value (by definition), hence the specific choice of our proposed e-scores in Eq. Àú 4 .\\nIndeed, if we use that definition here, the above term simplifies to,\\nwhich is an e-value under exchangeable data for any non-negative function f f [Gammerman et\\xa0al., 1998 ] .\\nLastly, since our e-scores satisfy Eq. Àú 3 under this worst-case setting, they also satisfy Eq. Àú 3 under any post-hoc Œ± \\\\alpha .\\nWe summarize this theoretical result below, and provide the detailed derivation in Appendix Àú A .\\nIf the test and the calibration prompts are exchangeable, then, our proposed e-scores in Eqs. Àú 4 and 6 upper bound the size distortion (marginal over the test and the calibration prompts) by 1, as in Eq. Àú 3 .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 CONCLUSIONS',\n",
       "    'paragraphs': 'We study the problem of achieving statistical guarantees for a post-hoc notion of error, namely size distortion, for generative model outputs.\\nWe propose e-scores, based on e-values, as measures of incorrectness.\\nWe show both theoretically and experimentally that our proposed e-scores achieve the desired post-hoc guarantees, allowing users flexibility in choosing tolerance levels Œ± \\\\alpha after observing the e-scores themselves.\\nWe also show that our guarantees extend to large response sets, with possibility for more diverse applications.\\nOur experiments show that the choice of the oracle estimator matters.\\nWhile we use pre-trained ones in our experiments (cf. Section Àú 5 ), they could be trained for specific use-cases to strengthen the e-scores.\\nAdditionally, while we consider size distortion as our post-hoc notion of error, other candidates exist, though not well studied. Koning [ 2024 ] discusses some alternatives, which could be investigated in the future.',\n",
       "    'subsections': [{'title': 'Future Work',\n",
       "      'paragraphs': 'Our experiments show that the choice of the oracle estimator matters.\\nWhile we use pre-trained ones in our experiments (cf. Section Àú 5 ), they could be trained for specific use-cases to strengthen the e-scores.\\nAdditionally, while we consider size distortion as our post-hoc notion of error, other candidates exist, though not well studied. Koning [ 2024 ] discusses some alternatives, which could be investigated in the future.',\n",
       "      'subsections': []},\n",
       "     {'title': 'References', 'paragraphs': '', 'subsections': []}]},\n",
       "   {'title': 'Appendix A THEORETICAL RESULTS',\n",
       "    'paragraphs': 'Here, we include the detailed derivation of Theorem Àú 1 .\\nFor convenience, we re-state the theorem below.\\nIf the test and the calibration prompts are exchangeable, then, our proposed e-scores in Eqs. Àú 4 and 6 upper bound the size distortion (marginal over the test and the calibration prompts) by 1, as in Eq. Àú 3 .\\nNote that a response is included in the filtered set ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) if and only if its score is ‚â§ Œ± \\\\leq\\\\alpha .\\nTherefore, we can re-write the inclusion of an incorrect response as at least one incorrect response score being ‚â§ Œ± \\\\leq\\\\alpha ,\\nThen, the size distortion for any post-hoc Œ± \\\\alpha strategy is upper bound by the worst-case size distortion,\\nwhere the equality ( i ) (i) is achieved by setting Œ± \\\\alpha to the smallest value such that the indicator function evaluates to 1, otherwise the term is 0.\\nWe are interested in upper bounding the above expectation by 1 to achieve Eq. Àú 3 .\\nWe plug-in the definition of our proposed e-scores from Eq. Àú 4 .\\nNote that our e-scores depend on the calibration prompts; we make this dependence explicit in the following.\\nThe worst-case size distortion simplifies to,\\nNote that for a , b ‚àà ‚Ñù ‚â• 0 a,b\\\\in\\\\mathds{R}_{\\\\geq 0} , the ratio a / ( a + b ) \\\\nicefrac{{a}}{{\\\\left(a+b\\\\right)}} is a monotonically non-decreasing transformation of a a because the derivative with respect to a a (i.e., b / ( a + b ) 2 \\\\nicefrac{{b}}{{\\\\left(a+b\\\\right)^{2}}} ) is non-negative.\\nConsequently, the above maximum is achieved at f ‚àó \\u200b ( x , ùïÜ \\u200b ( x , g œÄ \\u200b ( x ) ) ) = max ( ùê≤ , c ) ‚àà ùïÜ \\u200b ( x , g œÄ \\u200b ( x ) ) : c = 0 \\u2061 f \\u200b ( x , ùê≤ ) f^{*}\\\\left(x,\\\\mathds{O}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right)\\\\right)=\\\\max_{\\\\left(\\\\mathbf{y},c\\\\right)\\\\in\\\\mathds{O}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right):c=0}f\\\\left(x,\\\\mathbf{y}\\\\right) .\\nTherefore, the worst-case size distortion simplifies to,\\nLastly, we assume that the test and the calibration prompts are exchangeable, i.e., for any permutation œÉ \\\\sigma over the indices { 1 , ‚Ä¶ , n + 1 } \\\\left\\\\{1,\\\\ldots,n+1\\\\right\\\\} , the ordering of the permuted prompts is equal in distribution to the un-permuted prompts,\\nWe can follow arguments similar to those made by Gammerman et\\xa0al. [ 1998 ]; Balinsky and Balinsky [ 2024 ]; Vovk [ 2025 ] to show that the above expectation is ‚â§ 1 \\\\leq 1 under exchangeability.\\nSpecifically, we define random variables,\\nfor i = 1 , ‚Ä¶ , n + 1 i=1,\\\\ldots,n+1 .\\nUnder exchangeability of X 1 , ‚Ä¶ \\u200b X n + 1 X^{1},\\\\ldots X^{n+1} , the distributions of R 1 , ‚Ä¶ \\u200b R n + 1 R^{1},\\\\ldots R^{n+1} are identical.\\nThen,\\nwhere the inequality ( i \\u200b i ) (ii) accounts for the sum being 0, making 0 / 0 = 0 \\\\nicefrac{{0}}{{0}}=0 (by convention).\\nHence, our e-scores in Eq. Àú 4 upper bound the size distortion (marginal over the test and the calibration prompts) by 1, as in Eq. Àú 3 .\\nFurthermore, we can plug-in the definition of our proposed combined e-scores from Eq. Àú 6 .\\nNote that instead of combining three e-scores, we can combine any k ‚â• 1 k\\\\geq 1 .\\nThe worst-case size distortion from Eq. Àú 9 simplifies to,\\nWe have shown that the worst-case size distortion for the individual e-scores (the numerator) is ‚â§ 1 \\\\leq 1 . Then,\\nHence, our combined e-scores in Eq. Àú 6 upper bound the size distortion (marginally) by 1, as in Eq. Àú 3 .\\n‚àé',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B EXPERIMENTAL RESULTS',\n",
       "    'paragraphs': 'We include additional experimental results here, expanding on Section Àú 5 .\\nWe conduct a worst-case analysis where size distortion is maximized (cf. Eq. Àú 9 ) for the different use-cases.\\nWe begin by stating the common baselines.\\nIn addition to the p-scores defined in Eq. Àú 7 , we also compare against their randomized version,\\nwhere u ‚àº ùí∞ \\u200b ( 0 , 1 ) u\\\\sim\\\\mathcal{U}\\\\left(0,1\\\\right) is a uniform random sample in the range [ 0 , 1 ] \\\\left[0,1\\\\right] .\\nWe can recover the p-scores defined in Eq. Àú 7 as a special case of this definition by deterministically setting u = 1 u=1 .\\nWhile the non-randomized p-scores correspond to p-values, these randomized p-scores correspond to exact p-values [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] . 2 2 2 A non-negative random variable R ‚àà ‚Ñù ‚â• 0 R\\\\in\\\\mathds{R}_{\\\\geq 0} is an exact p-variable if ùêè \\u200b { R ‚â§ Œ± } = Œ± \\\\mathbf{P}\\\\left\\\\{R\\\\leq\\\\alpha\\\\right\\\\}=\\\\alpha , for all Œ± ‚àà [ 0 , 1 ] \\\\alpha\\\\in\\\\left[0,1\\\\right] .\\nWe also compare against the transformed oracle estimators in Eq. Àú 5 directly, without conversion to e-/p-scores using the calibration data.\\nSince we want our scores to be low for correct and high for incorrect responses (as measures of incorrectness), we define the naive scores to be the reciprocal of the transformed oracle estimators,\\nThese naive scores generally do not come with any statistical guarantees by themselves.\\nHowever, because the reciprocal of naive (1) is always ‚â§ 1 \\\\leq 1 , it happens to correspond to an uninformative e-value that is always ‚â§ 1 \\\\leq 1 (the expectation is ‚â§ 1 \\\\leq 1 by design).\\nTherefore, even though naive (1) achieves the size distortion bound in Eq. Àú 3 , it regularly excludes responses (correct and incorrect), and is extremely conservative compared to our e-scores.\\nWe analyze the worst-case setting that maximizes size distortion (cf. Eq. Àú 9 ). Table Àú 1 illustrates the results for both our experimental use-cases: mathematical factuality (cf. Section Àú 5.1 ) and property constraints satisfaction (cf. Section Àú 5.2 ).\\nOur proposed e-scores (and naive (1)) reliably upper bound the worst-case size distortion by 1, verifying our theory in achieving Eq. Àú 3 .\\nConversely, p-scores and other naive scores are unable to achieve this.',\n",
       "    'subsections': [{'title': 'Baselines',\n",
       "      'paragraphs': 'In addition to the p-scores defined in Eq. Àú 7 , we also compare against their randomized version,\\nwhere u ‚àº ùí∞ \\u200b ( 0 , 1 ) u\\\\sim\\\\mathcal{U}\\\\left(0,1\\\\right) is a uniform random sample in the range [ 0 , 1 ] \\\\left[0,1\\\\right] .\\nWe can recover the p-scores defined in Eq. Àú 7 as a special case of this definition by deterministically setting u = 1 u=1 .\\nWhile the non-randomized p-scores correspond to p-values, these randomized p-scores correspond to exact p-values [Shafer and Vovk, 2008 ; Vovk et\\xa0al., 2022a ] . 2 2 2 A non-negative random variable R ‚àà ‚Ñù ‚â• 0 R\\\\in\\\\mathds{R}_{\\\\geq 0} is an exact p-variable if ùêè \\u200b { R ‚â§ Œ± } = Œ± \\\\mathbf{P}\\\\left\\\\{R\\\\leq\\\\alpha\\\\right\\\\}=\\\\alpha , for all Œ± ‚àà [ 0 , 1 ] \\\\alpha\\\\in\\\\left[0,1\\\\right] .\\nWe also compare against the transformed oracle estimators in Eq. Àú 5 directly, without conversion to e-/p-scores using the calibration data.\\nSince we want our scores to be low for correct and high for incorrect responses (as measures of incorrectness), we define the naive scores to be the reciprocal of the transformed oracle estimators,\\nThese naive scores generally do not come with any statistical guarantees by themselves.\\nHowever, because the reciprocal of naive (1) is always ‚â§ 1 \\\\leq 1 , it happens to correspond to an uninformative e-value that is always ‚â§ 1 \\\\leq 1 (the expectation is ‚â§ 1 \\\\leq 1 by design).\\nTherefore, even though naive (1) achieves the size distortion bound in Eq. Àú 3 , it regularly excludes responses (correct and incorrect), and is extremely conservative compared to our e-scores.',\n",
       "      'subsections': []},\n",
       "     {'title': 'B.1 Worst-Case Size Distortion Analysis',\n",
       "      'paragraphs': 'We analyze the worst-case setting that maximizes size distortion (cf. Eq. Àú 9 ). Table Àú 1 illustrates the results for both our experimental use-cases: mathematical factuality (cf. Section Àú 5.1 ) and property constraints satisfaction (cf. Section Àú 5.2 ).\\nOur proposed e-scores (and naive (1)) reliably upper bound the worst-case size distortion by 1, verifying our theory in achieving Eq. Àú 3 .\\nConversely, p-scores and other naive scores are unable to achieve this.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix C IMPLICIT P-SCORES IN RELATED WORKS',\n",
       "    'paragraphs': 'Here we highlight the implicit role of p-scores (cf. Eq. Àú 7 ), and hence p-values, in the works most closely related to ours [Mohri and Hashimoto, 2024 ; Cherian et\\xa0al., 2024 ; Rubin-Toles et\\xa0al., 2025 ] , making it explicit.\\nTo begin with, these works compute the calibration values f ‚àó \\u200b ( x i , ùïÜ \\u200b ( x i , g œÄ \\u200b ( x i ) ) ) f^{*}\\\\left(x^{i},\\\\mathds{O}\\\\left(x^{i},g_{\\\\pi}\\\\left(x^{i}\\\\right)\\\\right)\\\\right) for i = 1 , ‚Ä¶ , n i=1,\\\\ldots,n .\\nGiven a fixed user-defined Œ± ‚àà [ 1 / ( n + 1 ) , 1 ] \\\\alpha\\\\in\\\\left[\\\\nicefrac{{1}}{{\\\\left(n+1\\\\right)}},1\\\\right] , they compute a threshold œÑ Œ± \\\\tau_{\\\\alpha} set to the ‚åà ( 1 ‚àí Œ± ) ‚ãÖ ( n + 1 ) ‚åâ \\\\lceil\\\\left(1-\\\\alpha\\\\right)\\\\cdot\\\\left(n+1\\\\right)\\\\rceil -th smallest of the calibration values above.\\nThen, a test response ùê≤ n + 1 \\\\mathbf{y}^{n+1} is included in the returned set if f \\u200b ( x n + 1 , ùê≤ n + 1 ) f\\\\left(x^{n+1},\\\\mathbf{y}^{n+1}\\\\right) is larger than this threshold,\\nIn our setup, this is equivalent to returning the filtered set ùïä Œ± \\u200b ( x , g œÄ \\u200b ( x ) ) \\\\mathds{S}_{\\\\alpha}\\\\left(x,g_{\\\\pi}\\\\left(x\\\\right)\\\\right) using p-scores.\\nWe highlight again that such approaches achieve Eq. Àú 2 , but not its post-hoc generalization in Eq. Àú 3 ; for that we propose our e-scores.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'While generative models, especially large language models (LLMs), are ubiquitous in today‚Äôs world, principled mechanisms to assess their (in)correctness are limited. Using the conformal prediction framework, previous works construct sets of LLM responses where the probability of including an incorrect response, or error, is capped at a desired user-defined tolerance level. However, since these methods are based on p-values, they are susceptible to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the guarantees. We therefore leverage e-values to complement generative model outputs with e-scores as a measure of incorrectness. In addition to achieving the same statistical guarantees as before, e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves, by upper bounding a post-hoc notion of error called size distortion. We experimentally demonstrate their efficacy in assessing LLM outputs for different correctness types: mathematical factuality and property constraints satisfaction.'},\n",
       " '2510.25662v1': {'content': [{'title': '1. Introduction',\n",
       "    'paragraphs': 'Programming assistants powered by large language models (LLMs) have rapidly proliferated across software development practices. Several studies indicate their potential to improve developer productivity (Ziegler et\\xa0al., 2022 ; Weber et\\xa0al., 2024 ) , though this finding is far from universal (Stray et\\xa0al., 2025 ; Becker et\\xa0al., [n.\\u2009d.] ) . Users employ these tools for diverse programming tasks, including information retrieval, code generation, debugging, documentation writing, and learning new technologies (Liang et\\xa0al., 2024 ) . The landscape of available tools is also complex and growing: in addition to the variety of LLMs powering programming assistants, interfaces can range from auto-complete engines embedded in developer‚Äôs environments to chatbot interfaces that mimic natural language conversations, (Treude and Gerosa, 2025 ) , with the latter proving particularly accessible to programmers across skill levels.\\nA risk for all users of LLM-based tools, especially for less experienced programmers, is over-reliance, potentially leading to unproductive programming practices or insufficient quality control for generated programs (Barke et\\xa0al., 2023 ; Vaithilingam et\\xa0al., 2022 ; Chen et\\xa0al., 2021 ) . For example, literature suggests that some programmers rely on asking the model to explain its code choices as their entire checking procedure (Fawzy et\\xa0al., 2025 ; O‚ÄôBrien, 2025 ) , even though LLM-generated explanations may be incorrect (Kabir et\\xa0al., 2024 ; Sarsa et\\xa0al., 2022 ) ‚Äî even about code the LLM has just produced. In some contexts, users of LLMs may not take steps to validate the outputs at all (Prather et\\xa0al., 2024 ; Fawzy et\\xa0al., 2025 ; Kazemitabaar et\\xa0al., 2024 ; Prather et\\xa0al., 2023 ) , a pattern that has been observed in other knowledge work contexts when users are particularly confident in a model‚Äôs abilities (Lee et\\xa0al., 2025 ) . Observing student programmers decline to validate generated code blocks, Kazemitabaar et\\xa0al. ( 2024 ) noted that ‚Äùthis indicates that some students might believe that AI code generators are fully capable of providing flawless code.‚Äù\\nHow programmers perceive the abilities and mechanisms behind LLM-based programming tools likely determines their degree of confidence in these tools, a phenomenon that extends beyond programming to other domains of AI use as well (Kelly et\\xa0al., 2023 ; Bansal et\\xa0al., 2019 ) . This includes not only positive conceptions of what tools can do reliably, but also forming mental models of what they do not do well (Prather et\\xa0al., 2024 ) . In the present study, we are interested in what users of conversational assistants (like ChatGPT (Treude and Gerosa, 2025 ) ) may get wrong about how these tools work.\\nIn this study, we investigate how users seeking programming support might misunderstand the properties of conversational LLM-based assistants through a two-phase approach. First, we conduct a brainstorming activity, drawing on existing literature and our experiences observing and teaching programmers to catalog and thematically organize potential misconceptions. We categorize these into misconceptions that are about the features of specific tools that involve LLMs, and misconceptions about LLMs as a class of models. Second, we perform a qualitative analysis of 500 Python programming-related conversations from the openly available WildChat dataset to identify which misconceptions are visible in user logs of conversational interactions with a chatbot.\\nWe see evidence for misconceptions about the availability of features like web access and code execution, the knowledge cutoff of the underlying model, the potential for non-text outputs and access to the users‚Äô machine. During certain programming activities, particularly debugging, we also observe prompt strategies that may reflect conceptual issues about what runtime and environment information is needed to diagnose and repair programs. We conclude by discussing implications for designing LLM-based programming tools that better surface their capabilities to users.',\n",
       "    'subsections': []},\n",
       "   {'title': '2. Related Work',\n",
       "    'paragraphs': 'There is a well-documented tendency towards over-trust in automated systems (‚Äùautomation bias‚Äù) beyond and within programming contexts (Ara√∫jo et\\xa0al., 2017 ; Goddard et\\xa0al., 2012 ; Zi et\\xa0al., 2025 ) . Indeed, work long before the era of LLMs on spreadsheet programming and automated programming assessments, Ko et\\xa0al. ( [n.\\u2009d.] ) suggests that less experienced programmers tend to be uncritical of automated feedback. Similarly, user studies with program synthesizers (both neural and rule-based) indicate a propensity for overconfidence in the ability of tools, which has been called the ‚Äùuser-synthesizer gap‚Äù (Jayagopal et\\xa0al., 2022 ; Ferdowsifard et\\xa0al., 2020 ) .\\nFerdowsifard et\\xa0al. ( 2020 ) concluded that ‚Äùif a programmer broadly understands (through trial and error) what kinds of tasks the synthesizer can do, they will know when to invoke the synthesizer and when to try something else. However, if the programmer has a poor mental model of the synthesizer‚Äôs ability (e.g., one that overestimates the synthesizer‚Äôs ability), then the programmer might waste time and energy trying to get the synthesizer to do something that it simply cannot.‚Äù Correspondingly, several studies of AI-aided decision making find that users with more accurate mental models of AI tools tend to achieve more successful outcomes (Gero et\\xa0al., 2020 ; Kulesza et\\xa0al., 2012 ; Bansal et\\xa0al., 2019 ) .\\nPrevious studies from the user-interaction literature have raised the possibility that neural-network-based tools pose an exceptionally high barrier challenges for users to develop working mental models. One possible reason owes to factors like the opaque rules governing their responses, high sensitivity to small changes in prompt wording, and dependency on training data that may be obscure to the user (Zamfirescu-Pereira et\\xa0al., 2023 ; Tankelevitch et\\xa0al., 2024 ; Gero et\\xa0al., 2020 ) . These demands are compounded by the fact that LLMs can misrepresent their own processes‚Äîas one recent analysis noted, ‚Äùwe found that o3 frequently fabricates actions it took to fulfill user requests, and elaborately justifies the fabrications when confronted by the user‚Äù (Chowdhury et\\xa0al., 2025 ) .\\nThe computer science education literature also provides a lens to understand how programmers, especially relatively inexperienced ones, can misconceive of LLM-based tools. Sorva et al. defined misconceptions as ‚Äùunderstandings that are deficient or inadequate for many practical programming contexts.‚Äù (Sorva, 2013 ) Programmer misconceptions about computers, languages, and complex systems have been studied as a target for educational intervention (Iii et\\xa0al., 1994 ; Qian et\\xa0al., 2017 ; Oliveira et\\xa0al., 2023 ) , as well as to understand barriers to student achievement.',\n",
       "    'subsections': []},\n",
       "   {'title': '3. Research Method',\n",
       "    'paragraphs': 'In the present study, we aim to better understand the misconceptions that programmers may have about LLM-based tools. We begin by articulating our assumptions and approach.\\nFor our purposes, we define user ‚Äùmisconceptions‚Äù as instances where a user believes, erroneously, that a given tool has an affordance that it does not actually have (or fails to perceive a property that is critical to their use case). In the study of tool design and use, affordances are ‚Äùthose fundamental properties that determine just how the thing could possibly be used‚Äù (Norman, 1988 ) . Affordances may be real or erroneously perceived, if a users‚Äô mental model is incomplete or faulty (Norman, 1999 ) .\\nIn the context of a user prompting a conversational assistant for programming support, a user might perceive the tool has the affordance of a web plugin that allows it to retrieve information from official software documentation online before generating a response. If the tool they were using did not have a plugin that would enable web search, we would consider this a misconception. We expect that this misconception would be relevant to the programmers‚Äô confidence in the LLM because they may weight the certainty of the result differently if they believe it is a direct summary of the most recent official documentation for the library, rather than purely the generation from an LLM based on frozen training data.\\nBy our definition, misconceptions are context-specific because the affordances of conversational programming assistants vary, both across providers and assistant versions. For example, OpenAI released a Python interpreter module for GPT-4 in 2023, which was initially available to Plus subscribers as an opt-in feature. Later, this module was transitioned to a default part of ChatGPT interactions, and opt-in was no longer needed. Thus, if a user expects a code interpreter module to be present in their interactions with a conversational assistant, it may or may not be a misconception depending on which tool they are using (and when, and possibly at what subscription tier).\\nA misconception could also look like the absence of sensitivity to a property of a tool. One example might be a user not perceiving that a given LLM-based tool is non-deterministic, and therefore becoming confused that the same prompt results in different responses.\\nOur study involved two activities: first, we conducted a brainstorming activity to identify potential types of misconceptions that could occur in LLM-programmer interactions, drawing on existing literature as well as our own observations and experiences with programmers. This was necessary because we did not know of many studies directly targeting programmers‚Äô misconceptions around LLM tools‚Äìwe typically encountered reports of misconceptions as brief comments in user interaction studies.\\nSecond, we use an openly available dataset of user interactions with GPT models through a conversational interface (the WildChat dataset (Zhao et\\xa0al., 2024 ) ) to perform a qualitative analysis. Using a subset of conversation logs featuring Python code snippets, we identify which misconceptions are apparent and in what programming contexts they appear. By focusing on a conversational assistant (to use the terminology established by Treude and Gerosa ( 2025 ) ) rather than a developer tool triggered within a dedicated programming environment like GitHub Copilot, we expect conversations may skew towards vibe coding interactions.\\n‚ÄùVibe coding‚Äù is an activity where users rely heavily on natural language to specify their programming requirements, often trading off the ability to quickly prototype with a detailed understanding of what their code base does (Fawzy et\\xa0al., 2025 ; Pimenova et\\xa0al., 2025 ; Karpathy, 2025 ) . These users may be less experienced in software development and computing, as conversational assistants like ChatGPT offer are often viewed as especially accessible to beginners (Fawzy et\\xa0al., 2025 ; Treude and Gerosa, 2025 ) . However, because the WildChat dataset is anonymous, we cannot be certain of users‚Äô professions or programming backgrounds. Therefore, we take care to also document what kind of programming activities are present in the dataset.\\nThe approach we use here is indirect, because whether or not a user understands that a conversational assistant has a given affordance (or not) cannot be measured directly. It is also impossible to be sure what misconceptions a user might have from their conversational logs alone, as users might experiment with prompts that are impossible to fulfill as part of their exploration of the tool. Therefore, in our exploration of user behaviors in programming-related contexts, we are only able to identify potential misconceptions ‚Äîinteractions where the users‚Äô prompts indicates the possibility of a misconception (e.g., providing a URL and asking for a model without browser access to use the link). For brevity, we will sometimes use the term ‚Äùmisconception‚Äù while reporting our results, but within the context of our study it should be understood that this language reflects an inference rather than a measurement. Acknowledging this, we believe that there is still value in cataloging the kinds of potential misconceptions users may have with conversational assistants and what programming contexts these might arise in.',\n",
       "    'subsections': [{'title': '3.1. Defining Misconceptions',\n",
       "      'paragraphs': 'For our purposes, we define user ‚Äùmisconceptions‚Äù as instances where a user believes, erroneously, that a given tool has an affordance that it does not actually have (or fails to perceive a property that is critical to their use case). In the study of tool design and use, affordances are ‚Äùthose fundamental properties that determine just how the thing could possibly be used‚Äù (Norman, 1988 ) . Affordances may be real or erroneously perceived, if a users‚Äô mental model is incomplete or faulty (Norman, 1999 ) .\\nIn the context of a user prompting a conversational assistant for programming support, a user might perceive the tool has the affordance of a web plugin that allows it to retrieve information from official software documentation online before generating a response. If the tool they were using did not have a plugin that would enable web search, we would consider this a misconception. We expect that this misconception would be relevant to the programmers‚Äô confidence in the LLM because they may weight the certainty of the result differently if they believe it is a direct summary of the most recent official documentation for the library, rather than purely the generation from an LLM based on frozen training data.\\nBy our definition, misconceptions are context-specific because the affordances of conversational programming assistants vary, both across providers and assistant versions. For example, OpenAI released a Python interpreter module for GPT-4 in 2023, which was initially available to Plus subscribers as an opt-in feature. Later, this module was transitioned to a default part of ChatGPT interactions, and opt-in was no longer needed. Thus, if a user expects a code interpreter module to be present in their interactions with a conversational assistant, it may or may not be a misconception depending on which tool they are using (and when, and possibly at what subscription tier).\\nA misconception could also look like the absence of sensitivity to a property of a tool. One example might be a user not perceiving that a given LLM-based tool is non-deterministic, and therefore becoming confused that the same prompt results in different responses.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2. Study Design',\n",
       "      'paragraphs': 'Our study involved two activities: first, we conducted a brainstorming activity to identify potential types of misconceptions that could occur in LLM-programmer interactions, drawing on existing literature as well as our own observations and experiences with programmers. This was necessary because we did not know of many studies directly targeting programmers‚Äô misconceptions around LLM tools‚Äìwe typically encountered reports of misconceptions as brief comments in user interaction studies.\\nSecond, we use an openly available dataset of user interactions with GPT models through a conversational interface (the WildChat dataset (Zhao et\\xa0al., 2024 ) ) to perform a qualitative analysis. Using a subset of conversation logs featuring Python code snippets, we identify which misconceptions are apparent and in what programming contexts they appear. By focusing on a conversational assistant (to use the terminology established by Treude and Gerosa ( 2025 ) ) rather than a developer tool triggered within a dedicated programming environment like GitHub Copilot, we expect conversations may skew towards vibe coding interactions.\\n‚ÄùVibe coding‚Äù is an activity where users rely heavily on natural language to specify their programming requirements, often trading off the ability to quickly prototype with a detailed understanding of what their code base does (Fawzy et\\xa0al., 2025 ; Pimenova et\\xa0al., 2025 ; Karpathy, 2025 ) . These users may be less experienced in software development and computing, as conversational assistants like ChatGPT offer are often viewed as especially accessible to beginners (Fawzy et\\xa0al., 2025 ; Treude and Gerosa, 2025 ) . However, because the WildChat dataset is anonymous, we cannot be certain of users‚Äô professions or programming backgrounds. Therefore, we take care to also document what kind of programming activities are present in the dataset.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3. Methodological Limitations',\n",
       "      'paragraphs': 'The approach we use here is indirect, because whether or not a user understands that a conversational assistant has a given affordance (or not) cannot be measured directly. It is also impossible to be sure what misconceptions a user might have from their conversational logs alone, as users might experiment with prompts that are impossible to fulfill as part of their exploration of the tool. Therefore, in our exploration of user behaviors in programming-related contexts, we are only able to identify potential misconceptions ‚Äîinteractions where the users‚Äô prompts indicates the possibility of a misconception (e.g., providing a URL and asking for a model without browser access to use the link). For brevity, we will sometimes use the term ‚Äùmisconception‚Äù while reporting our results, but within the context of our study it should be understood that this language reflects an inference rather than a measurement. Acknowledging this, we believe that there is still value in cataloging the kinds of potential misconceptions users may have with conversational assistants and what programming contexts these might arise in.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4. Misconceptions Brainstorming',\n",
       "    'paragraphs': 'This brainstorming began as a collaborative activity at an international workshop for researchers who study software engineering, ¬°redacted for anonymous review¬ø . After the workshop, we continued to work asynchronously on a catalog of misconceptions, drawing from peer-reviewed literature we had read and our own encounters from teaching and research.\\nNote that this exercise is in no way exhaustive and we do not intend it to be: the set of potential misconceptions is infinitely large, but not all are necessarily interesting. We are focused, as in the definition of programmer‚Äôs misconceptions put forward by Sorva ( 2013 ) on beliefs that we would expect to impact the choices a programmer would make about how to use an LLM-based assistant. We then organized the misconceptions into major themes, which we present next. Importantly, some misconceptions are about properties of LLMs as a class of models, whereas others are about properties of specific tools that involve LLM components.\\nUsers may struggle to form mental models about how LLMs store, retrieve, and use information to produce responses. We are aware of several studies in which programmers using conversational LLMs believed them to work via a database (O‚ÄôBrien, 2025 ; Nguyen et\\xa0al., 2024 ; Feldman and Anderson, 2024 ) . Nguyen et\\xa0al. ( 2024 ) reported this was the most common ‚Äùmental model‚Äù students had of a ChatGPT-like programming assistant, and noted that students often expected access to occur via keyword lookup (based on keywords in the user‚Äôs prompt). Of course, some tools with LLM components also have a database component, as in retrieval augmented generation (RAG) architectures. Extending from this, we could easily imagine user misconceptions about whether a given tool uses RAG to retrieve information before generating a response, or whether responses are solely conditioned on the model‚Äôs training data. Relatedly, a user might have confusion about which knowledge bases are available to a RAG system. One author reported an encounter in which a colleague expected that an LLM-based tool provided by their employer had access to all centralized data sources at the organization, when this could not be easily confirmed.\\nBecause of the well-documented tendency to anthropomorphize conversational interfaces (Prather et\\xa0al., 2023 ) , we expect that a common misconception would be ascribing agentic capabilties to tools that do not have these affordances. For example, users might expect by default that LLM-based tools can conduct web searches or execute programs in a code interpreter. Several such examples have been reported in user studies of programmers: Prather et\\xa0al. ( 2023 ) documented a participant who believed GitHub Copilot would proactively validate their program‚Äôs correctness, similar to an automated assessment tool. Other potential cases include programmers describing ChatGPT as conducting Google searches prior to generating responses, despite user logs confirming that web search functionality was not enabled during those interactions, or testing generated code for runability before returning it to the user (O‚ÄôBrien, 2025 ) .\\nIn most conversational LLM tools, conversations occur in discrete sessions. This may confuse users about when information from previous conversations is available during a new conversation. This has been observed in at least one user study: Zamfirescu-Pereira et\\xa0al. ( 2023 ) reported that ‚ÄùThe idea that the chatbot would take direction from preamble instructions but was ‚Äôhard reset‚Äô for every new conversation‚Äìeffectively forced to forget all previous conversations when starting a new one‚Äìwas not at all intuitive, especially to non-programmers.‚Äù Relatedly, we expect that users of conversational tools could have misconceptions about how code versions are tracked within a session. Multi-turn interactions with conversational programming assistants are common, in which users explore alternative specifications for programs or repairs to accomplish debugging goals. Users might expect that a tool stores ‚Äùcheckpoints‚Äù of code as a kind of version control, which can be accessed deterministically by prompting the tool to return code to a previous state.\\nUsers might have misconceptions about whether a tool can continue ‚Äùprocessing‚Äù after a conversation ends. In workshop discussions, one author shared an experience where a conversational tool told a user a task would take several weeks. The user then returned to the tool several weeks later and asked for a progress update.\\nWe expect that users of tools that integrate into a developer‚Äôs environment, or are accessed via desktop apps, may have misconceptions about which data on their local machine the tool can use to generate output. For example, GitHub Copilot users may be unsure which files and directories are ‚Äùavailable‚Äù to Copilot. Users may also have unexpected beliefs about how to hide information from a tool. A workshop participant reported encountering a misconception that Copilot will ignore any in-line code comments when generating suggestions.\\nBecause LLMs typically have a ‚Äùknowledge cutoff‚Äù corresponding to when their training data was collected, users may have misconceptions about when this cutoff occurred for a given tool‚Äîor that a cutoff exists at all. This will have consequences for the reliability of LLM-based tools for answering questions about software libraries, which may evolve after the cutoff. We also imagine misconceptions around the model‚Äôs use of user interactions to train in real-time. For example, users might expect that a tool ‚Äùlearns‚Äù from their coding habits or feedback to the model during a session in the absence of any such mechanism (as in (Zamfirescu-Pereira et\\xa0al., 2023 ) ).\\nAlthough LLMs may behave deterministically in settings where random seeds can be fixed, many user-facing tools do not allow direct control of hyperparameters that would ensure this. Users may have misunderstand that functionally, many tools with LLM components will not always give the same response to the same prompts. Several studies reported this as a source of confusion and frustration: Feldman and Anderson ( 2024 ) reported that student programmers using a conversational interface to an LLM expected the responses to be ‚Äùhard-coded‚Äù. In a related study with the tool (Nguyen et\\xa0al., 2024 ) , students reported being ‚Äùalarmed to find that resubmitting the same prompt could generate different programs‚Äù. Lau and Guo ( 2023 ) also expected that the unpredictability of LLMs could pose a challenge for learners.\\nAlthough we are primarily interested in misconceptions about LLM-based conversational assistants, we note that users could have a fundamental misconception about which conversational interfaces use LLMs on the ‚Äùback end‚Äù to produce responses. Users might encounter tools with interfaces that are visually indistinguishable from common LLM-based conversational tools, but that run a rule-based system instead.\\nIt is well documented that LLM outputs are sensitive to changes that are perceptually small to prompt authors, like adding white space or distractor sentences (for example, Mirzadeh et\\xa0al. ( 2024 ) )\\nThis is likely to be challenging for users to grasp, particularly because users may not experiment much with varying prompt language before forming judgments about a model‚Äôs capabilities (Zamfirescu-Pereira et\\xa0al., 2023 ) . Failure to anticipate the effects of minor, semantically meaningless modifications to code or prompt language has been observed in user studies of GitHub Copilot (Jayagopal et\\xa0al., 2022 ) as well as conversational tools (Zamfirescu-Pereira et\\xa0al., 2023 ) .\\nUsers may expect that there are circumstances in which LLM hallucinations are not possible, when this cannot be guaranteed. For example, one author had encountered a situation where a colleague expressed that if they uploaded a data file to a university-provided conversational LLM, the LLM would not hallucinate in any follow-up questions about the uploaded data. We noted that users might also expect that LLMs cannot hallucinate about meta-information about the LLM itself, for example, in response to questions like, ‚ÄùWhich model are you?‚Äù If users think of LLMs as a kind of translator between natural language and code, this may be analogous to expecting that there exist certain ‚Äùsystem commands‚Äù.\\nUsers may expect that LLMs cannot hallucinate when answering questions about code they have generated in the same conversation, when this is not true (Lehtinen et\\xa0al., 2024 ) . This misconception may contribute to the phenomenon in which users ask LLMs to justify code suggestions as a verification strategy (Ferdowsifard et\\xa0al., 2020 ) . One author had observed a developer expecting that a conversational LLM had ‚Äùnative explainability‚Äù: the user asked an LLM to explain why it had suggested a certain code solution, confident that the answer would faithfully represent the reasoning process by which the model had worked to create the code. The user did not discern that while the explanations might ‚Äùmake sense‚Äù they do not reflect explainable AI capabilities regarding the previously generated code.\\nUsers may have confusions that LLMs as a class sometimes respond to prompts, including algebra or logic problems, with correct answers. Still, they do not arrive at such responses by the same mechanism as a calculator (and have many unintuitive sensitivities to how math problems are presented in prompting (Mirzadeh et\\xa0al., 2024 ) ).\\nJayagopal et\\xa0al. ( 2022 ) described a ‚Äùmonotonicity belief‚Äù, in which users believed that giving more information to code synthesizers would always improve their performance. In fact, managing the context window of LLMs is a potential meta-cognitive challenge for users Tankelevitch et\\xa0al. ( 2024 ) , especially because there are highly non-linear relationships between LLM recall of information from the context window and its position Liu et\\xa0al. ( 2024 ) . Context window size can also differ across models and tools, so tracking this information across the various tools available can challenge the user as well.',\n",
       "    'subsections': [{'title': '4.1. Tool-Specific Misconceptions',\n",
       "      'paragraphs': 'Users may struggle to form mental models about how LLMs store, retrieve, and use information to produce responses. We are aware of several studies in which programmers using conversational LLMs believed them to work via a database (O‚ÄôBrien, 2025 ; Nguyen et\\xa0al., 2024 ; Feldman and Anderson, 2024 ) . Nguyen et\\xa0al. ( 2024 ) reported this was the most common ‚Äùmental model‚Äù students had of a ChatGPT-like programming assistant, and noted that students often expected access to occur via keyword lookup (based on keywords in the user‚Äôs prompt). Of course, some tools with LLM components also have a database component, as in retrieval augmented generation (RAG) architectures. Extending from this, we could easily imagine user misconceptions about whether a given tool uses RAG to retrieve information before generating a response, or whether responses are solely conditioned on the model‚Äôs training data. Relatedly, a user might have confusion about which knowledge bases are available to a RAG system. One author reported an encounter in which a colleague expected that an LLM-based tool provided by their employer had access to all centralized data sources at the organization, when this could not be easily confirmed.\\nBecause of the well-documented tendency to anthropomorphize conversational interfaces (Prather et\\xa0al., 2023 ) , we expect that a common misconception would be ascribing agentic capabilties to tools that do not have these affordances. For example, users might expect by default that LLM-based tools can conduct web searches or execute programs in a code interpreter. Several such examples have been reported in user studies of programmers: Prather et\\xa0al. ( 2023 ) documented a participant who believed GitHub Copilot would proactively validate their program‚Äôs correctness, similar to an automated assessment tool. Other potential cases include programmers describing ChatGPT as conducting Google searches prior to generating responses, despite user logs confirming that web search functionality was not enabled during those interactions, or testing generated code for runability before returning it to the user (O‚ÄôBrien, 2025 ) .\\nIn most conversational LLM tools, conversations occur in discrete sessions. This may confuse users about when information from previous conversations is available during a new conversation. This has been observed in at least one user study: Zamfirescu-Pereira et\\xa0al. ( 2023 ) reported that ‚ÄùThe idea that the chatbot would take direction from preamble instructions but was ‚Äôhard reset‚Äô for every new conversation‚Äìeffectively forced to forget all previous conversations when starting a new one‚Äìwas not at all intuitive, especially to non-programmers.‚Äù Relatedly, we expect that users of conversational tools could have misconceptions about how code versions are tracked within a session. Multi-turn interactions with conversational programming assistants are common, in which users explore alternative specifications for programs or repairs to accomplish debugging goals. Users might expect that a tool stores ‚Äùcheckpoints‚Äù of code as a kind of version control, which can be accessed deterministically by prompting the tool to return code to a previous state.\\nUsers might have misconceptions about whether a tool can continue ‚Äùprocessing‚Äù after a conversation ends. In workshop discussions, one author shared an experience where a conversational tool told a user a task would take several weeks. The user then returned to the tool several weeks later and asked for a progress update.\\nWe expect that users of tools that integrate into a developer‚Äôs environment, or are accessed via desktop apps, may have misconceptions about which data on their local machine the tool can use to generate output. For example, GitHub Copilot users may be unsure which files and directories are ‚Äùavailable‚Äù to Copilot. Users may also have unexpected beliefs about how to hide information from a tool. A workshop participant reported encountering a misconception that Copilot will ignore any in-line code comments when generating suggestions.\\nBecause LLMs typically have a ‚Äùknowledge cutoff‚Äù corresponding to when their training data was collected, users may have misconceptions about when this cutoff occurred for a given tool‚Äîor that a cutoff exists at all. This will have consequences for the reliability of LLM-based tools for answering questions about software libraries, which may evolve after the cutoff. We also imagine misconceptions around the model‚Äôs use of user interactions to train in real-time. For example, users might expect that a tool ‚Äùlearns‚Äù from their coding habits or feedback to the model during a session in the absence of any such mechanism (as in (Zamfirescu-Pereira et\\xa0al., 2023 ) ).\\nAlthough LLMs may behave deterministically in settings where random seeds can be fixed, many user-facing tools do not allow direct control of hyperparameters that would ensure this. Users may have misunderstand that functionally, many tools with LLM components will not always give the same response to the same prompts. Several studies reported this as a source of confusion and frustration: Feldman and Anderson ( 2024 ) reported that student programmers using a conversational interface to an LLM expected the responses to be ‚Äùhard-coded‚Äù. In a related study with the tool (Nguyen et\\xa0al., 2024 ) , students reported being ‚Äùalarmed to find that resubmitting the same prompt could generate different programs‚Äù. Lau and Guo ( 2023 ) also expected that the unpredictability of LLMs could pose a challenge for learners.\\nAlthough we are primarily interested in misconceptions about LLM-based conversational assistants, we note that users could have a fundamental misconception about which conversational interfaces use LLMs on the ‚Äùback end‚Äù to produce responses. Users might encounter tools with interfaces that are visually indistinguishable from common LLM-based conversational tools, but that run a rule-based system instead.',\n",
       "      'subsections': [{'title': 'Information retrieval mechanism',\n",
       "        'paragraphs': 'Users may struggle to form mental models about how LLMs store, retrieve, and use information to produce responses. We are aware of several studies in which programmers using conversational LLMs believed them to work via a database (O‚ÄôBrien, 2025 ; Nguyen et\\xa0al., 2024 ; Feldman and Anderson, 2024 ) . Nguyen et\\xa0al. ( 2024 ) reported this was the most common ‚Äùmental model‚Äù students had of a ChatGPT-like programming assistant, and noted that students often expected access to occur via keyword lookup (based on keywords in the user‚Äôs prompt). Of course, some tools with LLM components also have a database component, as in retrieval augmented generation (RAG) architectures. Extending from this, we could easily imagine user misconceptions about whether a given tool uses RAG to retrieve information before generating a response, or whether responses are solely conditioned on the model‚Äôs training data. Relatedly, a user might have confusion about which knowledge bases are available to a RAG system. One author reported an encounter in which a colleague expected that an LLM-based tool provided by their employer had access to all centralized data sources at the organization, when this could not be easily confirmed.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Agentic actions',\n",
       "        'paragraphs': 'Because of the well-documented tendency to anthropomorphize conversational interfaces (Prather et\\xa0al., 2023 ) , we expect that a common misconception would be ascribing agentic capabilties to tools that do not have these affordances. For example, users might expect by default that LLM-based tools can conduct web searches or execute programs in a code interpreter. Several such examples have been reported in user studies of programmers: Prather et\\xa0al. ( 2023 ) documented a participant who believed GitHub Copilot would proactively validate their program‚Äôs correctness, similar to an automated assessment tool. Other potential cases include programmers describing ChatGPT as conducting Google searches prior to generating responses, despite user logs confirming that web search functionality was not enabled during those interactions, or testing generated code for runability before returning it to the user (O‚ÄôBrien, 2025 ) .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Session memory',\n",
       "        'paragraphs': 'In most conversational LLM tools, conversations occur in discrete sessions. This may confuse users about when information from previous conversations is available during a new conversation. This has been observed in at least one user study: Zamfirescu-Pereira et\\xa0al. ( 2023 ) reported that ‚ÄùThe idea that the chatbot would take direction from preamble instructions but was ‚Äôhard reset‚Äô for every new conversation‚Äìeffectively forced to forget all previous conversations when starting a new one‚Äìwas not at all intuitive, especially to non-programmers.‚Äù Relatedly, we expect that users of conversational tools could have misconceptions about how code versions are tracked within a session. Multi-turn interactions with conversational programming assistants are common, in which users explore alternative specifications for programs or repairs to accomplish debugging goals. Users might expect that a tool stores ‚Äùcheckpoints‚Äù of code as a kind of version control, which can be accessed deterministically by prompting the tool to return code to a previous state.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Session persistence',\n",
       "        'paragraphs': 'Users might have misconceptions about whether a tool can continue ‚Äùprocessing‚Äù after a conversation ends. In workshop discussions, one author shared an experience where a conversational tool told a user a task would take several weeks. The user then returned to the tool several weeks later and asked for a progress update.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Scope of access',\n",
       "        'paragraphs': 'We expect that users of tools that integrate into a developer‚Äôs environment, or are accessed via desktop apps, may have misconceptions about which data on their local machine the tool can use to generate output. For example, GitHub Copilot users may be unsure which files and directories are ‚Äùavailable‚Äù to Copilot. Users may also have unexpected beliefs about how to hide information from a tool. A workshop participant reported encountering a misconception that Copilot will ignore any in-line code comments when generating suggestions.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Continuous training',\n",
       "        'paragraphs': 'Because LLMs typically have a ‚Äùknowledge cutoff‚Äù corresponding to when their training data was collected, users may have misconceptions about when this cutoff occurred for a given tool‚Äîor that a cutoff exists at all. This will have consequences for the reliability of LLM-based tools for answering questions about software libraries, which may evolve after the cutoff. We also imagine misconceptions around the model‚Äôs use of user interactions to train in real-time. For example, users might expect that a tool ‚Äùlearns‚Äù from their coding habits or feedback to the model during a session in the absence of any such mechanism (as in (Zamfirescu-Pereira et\\xa0al., 2023 ) ).',\n",
       "        'subsections': []},\n",
       "       {'title': 'Deterministic behavior',\n",
       "        'paragraphs': 'Although LLMs may behave deterministically in settings where random seeds can be fixed, many user-facing tools do not allow direct control of hyperparameters that would ensure this. Users may have misunderstand that functionally, many tools with LLM components will not always give the same response to the same prompts. Several studies reported this as a source of confusion and frustration: Feldman and Anderson ( 2024 ) reported that student programmers using a conversational interface to an LLM expected the responses to be ‚Äùhard-coded‚Äù. In a related study with the tool (Nguyen et\\xa0al., 2024 ) , students reported being ‚Äùalarmed to find that resubmitting the same prompt could generate different programs‚Äù. Lau and Guo ( 2023 ) also expected that the unpredictability of LLMs could pose a challenge for learners.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Model family.',\n",
       "        'paragraphs': 'Although we are primarily interested in misconceptions about LLM-based conversational assistants, we note that users could have a fundamental misconception about which conversational interfaces use LLMs on the ‚Äùback end‚Äù to produce responses. Users might encounter tools with interfaces that are visually indistinguishable from common LLM-based conversational tools, but that run a rule-based system instead.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.2. LLM Misconceptions',\n",
       "      'paragraphs': 'It is well documented that LLM outputs are sensitive to changes that are perceptually small to prompt authors, like adding white space or distractor sentences (for example, Mirzadeh et\\xa0al. ( 2024 ) )\\nThis is likely to be challenging for users to grasp, particularly because users may not experiment much with varying prompt language before forming judgments about a model‚Äôs capabilities (Zamfirescu-Pereira et\\xa0al., 2023 ) . Failure to anticipate the effects of minor, semantically meaningless modifications to code or prompt language has been observed in user studies of GitHub Copilot (Jayagopal et\\xa0al., 2022 ) as well as conversational tools (Zamfirescu-Pereira et\\xa0al., 2023 ) .\\nUsers may expect that there are circumstances in which LLM hallucinations are not possible, when this cannot be guaranteed. For example, one author had encountered a situation where a colleague expressed that if they uploaded a data file to a university-provided conversational LLM, the LLM would not hallucinate in any follow-up questions about the uploaded data. We noted that users might also expect that LLMs cannot hallucinate about meta-information about the LLM itself, for example, in response to questions like, ‚ÄùWhich model are you?‚Äù If users think of LLMs as a kind of translator between natural language and code, this may be analogous to expecting that there exist certain ‚Äùsystem commands‚Äù.\\nUsers may expect that LLMs cannot hallucinate when answering questions about code they have generated in the same conversation, when this is not true (Lehtinen et\\xa0al., 2024 ) . This misconception may contribute to the phenomenon in which users ask LLMs to justify code suggestions as a verification strategy (Ferdowsifard et\\xa0al., 2020 ) . One author had observed a developer expecting that a conversational LLM had ‚Äùnative explainability‚Äù: the user asked an LLM to explain why it had suggested a certain code solution, confident that the answer would faithfully represent the reasoning process by which the model had worked to create the code. The user did not discern that while the explanations might ‚Äùmake sense‚Äù they do not reflect explainable AI capabilities regarding the previously generated code.\\nUsers may have confusions that LLMs as a class sometimes respond to prompts, including algebra or logic problems, with correct answers. Still, they do not arrive at such responses by the same mechanism as a calculator (and have many unintuitive sensitivities to how math problems are presented in prompting (Mirzadeh et\\xa0al., 2024 ) ).\\nJayagopal et\\xa0al. ( 2022 ) described a ‚Äùmonotonicity belief‚Äù, in which users believed that giving more information to code synthesizers would always improve their performance. In fact, managing the context window of LLMs is a potential meta-cognitive challenge for users Tankelevitch et\\xa0al. ( 2024 ) , especially because there are highly non-linear relationships between LLM recall of information from the context window and its position Liu et\\xa0al. ( 2024 ) . Context window size can also differ across models and tools, so tracking this information across the various tools available can challenge the user as well.',\n",
       "      'subsections': [{'title': 'Stability of results',\n",
       "        'paragraphs': 'It is well documented that LLM outputs are sensitive to changes that are perceptually small to prompt authors, like adding white space or distractor sentences (for example, Mirzadeh et\\xa0al. ( 2024 ) )\\nThis is likely to be challenging for users to grasp, particularly because users may not experiment much with varying prompt language before forming judgments about a model‚Äôs capabilities (Zamfirescu-Pereira et\\xa0al., 2023 ) . Failure to anticipate the effects of minor, semantically meaningless modifications to code or prompt language has been observed in user studies of GitHub Copilot (Jayagopal et\\xa0al., 2022 ) as well as conversational tools (Zamfirescu-Pereira et\\xa0al., 2023 ) .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Groundedness',\n",
       "        'paragraphs': 'Users may expect that there are circumstances in which LLM hallucinations are not possible, when this cannot be guaranteed. For example, one author had encountered a situation where a colleague expressed that if they uploaded a data file to a university-provided conversational LLM, the LLM would not hallucinate in any follow-up questions about the uploaded data. We noted that users might also expect that LLMs cannot hallucinate about meta-information about the LLM itself, for example, in response to questions like, ‚ÄùWhich model are you?‚Äù If users think of LLMs as a kind of translator between natural language and code, this may be analogous to expecting that there exist certain ‚Äùsystem commands‚Äù.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Native explainability',\n",
       "        'paragraphs': 'Users may expect that LLMs cannot hallucinate when answering questions about code they have generated in the same conversation, when this is not true (Lehtinen et\\xa0al., 2024 ) . This misconception may contribute to the phenomenon in which users ask LLMs to justify code suggestions as a verification strategy (Ferdowsifard et\\xa0al., 2020 ) . One author had observed a developer expecting that a conversational LLM had ‚Äùnative explainability‚Äù: the user asked an LLM to explain why it had suggested a certain code solution, confident that the answer would faithfully represent the reasoning process by which the model had worked to create the code. The user did not discern that while the explanations might ‚Äùmake sense‚Äù they do not reflect explainable AI capabilities regarding the previously generated code.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Symbolic logic',\n",
       "        'paragraphs': 'Users may have confusions that LLMs as a class sometimes respond to prompts, including algebra or logic problems, with correct answers. Still, they do not arrive at such responses by the same mechanism as a calculator (and have many unintuitive sensitivities to how math problems are presented in prompting (Mirzadeh et\\xa0al., 2024 ) ).',\n",
       "        'subsections': []},\n",
       "       {'title': 'Context window',\n",
       "        'paragraphs': 'Jayagopal et\\xa0al. ( 2022 ) described a ‚Äùmonotonicity belief‚Äù, in which users believed that giving more information to code synthesizers would always improve their performance. In fact, managing the context window of LLMs is a potential meta-cognitive challenge for users Tankelevitch et\\xa0al. ( 2024 ) , especially because there are highly non-linear relationships between LLM recall of information from the context window and its position Liu et\\xa0al. ( 2024 ) . Context window size can also differ across models and tools, so tracking this information across the various tools available can challenge the user as well.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '5. Qualitative Analysis of Conversational Logs',\n",
       "    'paragraphs': 'Next, we investigated if and how these potential misconceptions manifest in user interactions with an LLM-based conversational assistant in programming contexts. We follow an approach similar to other studies of programmer misconceptions, in which a dataset of user interaction logs is analyzed qualitatively (Oliveira et\\xa0al., 2023 ; Sirki√§ and Sorva, 2012 ) . Analysis of conversational logs from interactions with LLM-based chatbots has also been used to study prompt engineering strategies (Trippas et\\xa0al., 2024 ) .\\nA challenge for studying how users interact with conversational LLM-based tools in realistic settings is that many such datasets are proprietary. We therefore selected an openly available dataset designed to facilitate research on naturalistic interactions, the WildChat corpus of over 1 million ‚Äùconversations‚Äù between anonymous users and a free, publicly available chatbot (Zhao et\\xa0al., 2024 ) . It‚Äôs worth mentioning that we also considered a similar dataset, LMSYS-Chat-1M, but this dataset emphasized a ‚Äùgamified‚Äù interface for users, inviting head-to-head comparisons of 25 different LLM models. We selected WildChat because the user interface for data collection (detailed below) seemed more ethologically valid for our purposes.\\nWildChat data was collected between April 2023 May 2024, with a total of 2,713,695 turns in 1,039,785 full conversations. Users interacted with a chat interface hosted on Hugging Face 1 1 1 https://huggingface.co/spaces supported by OpenAI‚Äôs GPT-3.5-Turbo and GPT-4 APIs. As a mandatory condition to access the chatbot, users agreed to share their chat transcripts, IP addresses, and request headers. Users who then affirmatively consented to sharing their data for publication were able to access ChatGPT via the service free of charge.\\nCritically for our purposes, the API calls used in the tool give us complete visibility into the plugins enabled during interactions. These API calls did not allow the chatbot to support features like web search, file uploads, non-text inputs or outputs, or ‚Äùtool calling‚Äù (such as instantiating a code interpreter). Each conversation was handled as a distinct session; users could not create accounts, so the chatbot did not have ‚Äùmemory‚Äù of a user‚Äôs previous conversations. Additionally, the models supporting chat were text-only.\\nBased on IP addresses, users were located predominantly in the US (21.60%), Russia (15.55%), and China (10.02%). Analysis of a random subset of 1,000 prompts indicated 6.7% were related to coding (other prominent categories were assisting/creative writing (61.9%, analysis/decision explanation (13.6%), and factual info (6.3%)). English was by far the most common prompt language (53%), followed by Chinese (13%) and Russian (12%).\\nThe goal of our study is to understand user interactions in programming contexts, so we focused on coding-related conversations. Although the WildChat authors estimated the fraction of prompts related to coding, they did not publish topic labels for specific conversations. We therefore created a pipeline to filter relevant conversations as presented in Figure 1 .\\nFirst, we identified coding-related conversations (Figure 1 a). We applied a regular expression filter looking for code block formatting characters (e.g., ‚Äò‚Äò‚Äò ) in both user prompts and chatbot responses. Notably, when the GPT models supporting the chat respond with code formatting, the formatting string also typically includes a language tag (such as ‚Äò‚Äò‚Äòpython‚Äò‚Äò‚Äò ). We were therefore able to estimate the rough number of conversations containing various popular programming languages, including the top 10 languages reported in Stack Overflow‚Äôs 2025 developer survey (202, 2025 ) . Out of 65,590 conversations containing code block formatting, the most common language was Python (23,353), followed by Java (11,852), SQL (11,852), and JavaScript (7,312). Considering that all authors were familiar with Python and it was the most common programming language in the dataset, we decided to restrict our analysis to this subset of coding-related conversations only.\\nIn addition to filtering for conversations involving Python code blocks, we also filtered for English-language conversations (Figure 1 b). The WildChat dataset includes a machine-generated label for probable language. We used this feature to restrict conversations to English-language only for qualitative coding. This left 11,418 conversations, which were candidates for qualitative coding, of which we selected 500 at random for qualitative analysis.\\nWe created a custom annotation tool for labeling conversations so that we could visualize the turn-by-turn interactions of the user and model responses easily (see Figure 2 ). Annotators could mark both the intention of a conversation as well as any potential user misconceptions using our pre-defined set of tags, or create a new tag. There was also an option to leave open-ended comments about a conversation. The tool with our final labelings is available at https://wildchat-website.hpcc.eu.org/ .\\nStarting from the misconceptions we collected in the first activity, we aimed to create a codebook that could be applied to a collection of real conversations with a conversational LLM. However, not all the misconceptions we brainstormed would be relevant to conversational assistants like the WildChat chatbot. Others were relevant only to specific architectures, like retrieval augmented generation (‚ÄùIf you upload a document to this tool and ask a question about it, the tool will only use information in the document to answer.‚Äù)\\nWe therefore began with a simplified initial codebook, which could be applied deductively to conversations. Authors 1 and 2 (abbreviated A1 and A2) both used the initial codebook to do a round of open-coding on 250 unique conversations each, totaling 500 unique conversations. Additionally, we created a codebook for the inferred user‚Äôs intention on the conversation (for example, ‚Äùgenerate code‚Äù, ‚Äùexplain user-submitted code‚Äù, ‚Äùrefactor code‚Äù). Note that for both kinds of codes, multiple labels could apply for a given conversation. This was common, as conversations frequently involved multiple ‚Äùturns‚Äù‚Äîfor example, requesting code to be generated, then explained, followed by a request for debugging assistance.\\nDuring open coding, A1 and A2 added potential codes based on notable observations. We also added codes for conversations that were: (i) mostly not in English, which likely occurred due to errors in WildChat‚Äôs automated language detection; (ii) did not have a discernible user intention in the prompt (for example, entering a block of code with no instructions); and (iii) not a Python-related conversation, that could occur given the limitations of our regular expression filter. We did not attempt to infer the intentions or misconceptions for 85 non-English conversations and 7 non-Python ones.\\nAfter individually labeling 250 unique conversations each, A1 and A2 reviewed one anothers‚Äô conversations that were flagged as misconceptions (78 total) and met to discuss any disagreements in misconception labels. As possible misconceptions were a distinct minority class, we considered reviewing these conversations more informative than reviewing the entire dataset together. During this meeting, we performed axial coding to create a finalized set of codes for both user potential misconceptions and intentions. This involved de-duplicating codes, merging codes that were insufficiently distinct, and removing codes from the initial codebook that we did not observe (for example, we did not see any instances of assumed database access that were not better categorized as assumed web access). A6 resolved any outstanding disagreements in either codebooks (intentions or misconceptions) as a tie-breaker to create a ‚ÄùGold‚Äù dataset of 50 conversations that at least two annotators agreed contained a potential misconception. This dataset with labels is provided as an artifact.\\nFinally, we did a validation round of coding involving the remaining authors who were not involved in the creation of the Gold dataset (A3, A4, and A5). We created a stratified validation set of 29 conversations to ensure there was at least one example of each misconception from the codebook present (conversations in the validation set are labeled in the paper artifacts). Annotators were advised that they could choose multiple codes per conversation (for both intention and misconceptions) and that each conversation they would review had been flagged as a potential misconception by at least two other authors. The outcomes of this final round of coding and ensuing discussion are presented in the Results.\\nThe axial codebook for intentions, with counts, is given in Table 1 . The axial codebook for misconceptions is provided in Table 2 , and the conversations labeled as misconceptions are discussed in Results. Note that we also provide as an artifact a dictionary mapping conversation indices (C1, C2, etc.) to identifying hashes from the WildChat dataset.\\nWe used a modified Cohen‚Äôs Kappa ( Œ∫ \\\\kappa ) to summarise inter-rater reliability. Because a conversation could have multiple non-exclusive labels, we considered ‚Äùagreement‚Äù to occur when two annotators assigned at least one overlapping label. For example, if A1 tagged a conversation with the intentions ‚ÄùDebug‚Äù and ‚ÄùValidate code‚Äù and A2 tagged it as ‚ÄùDebug‚Äù, this would be considered agreement. Cohen‚Äôs Œ∫ \\\\kappa was calculated for all conversations using this definition of agreement. This provides a (somewhat optimistic) estimate of inter-rater reliability. Reliability statistics are presented in the Results section.',\n",
       "    'subsections': [{'title': '5.1. The WildChat Dataset',\n",
       "      'paragraphs': 'A challenge for studying how users interact with conversational LLM-based tools in realistic settings is that many such datasets are proprietary. We therefore selected an openly available dataset designed to facilitate research on naturalistic interactions, the WildChat corpus of over 1 million ‚Äùconversations‚Äù between anonymous users and a free, publicly available chatbot (Zhao et\\xa0al., 2024 ) . It‚Äôs worth mentioning that we also considered a similar dataset, LMSYS-Chat-1M, but this dataset emphasized a ‚Äùgamified‚Äù interface for users, inviting head-to-head comparisons of 25 different LLM models. We selected WildChat because the user interface for data collection (detailed below) seemed more ethologically valid for our purposes.\\nWildChat data was collected between April 2023 May 2024, with a total of 2,713,695 turns in 1,039,785 full conversations. Users interacted with a chat interface hosted on Hugging Face 1 1 1 https://huggingface.co/spaces supported by OpenAI‚Äôs GPT-3.5-Turbo and GPT-4 APIs. As a mandatory condition to access the chatbot, users agreed to share their chat transcripts, IP addresses, and request headers. Users who then affirmatively consented to sharing their data for publication were able to access ChatGPT via the service free of charge.\\nCritically for our purposes, the API calls used in the tool give us complete visibility into the plugins enabled during interactions. These API calls did not allow the chatbot to support features like web search, file uploads, non-text inputs or outputs, or ‚Äùtool calling‚Äù (such as instantiating a code interpreter). Each conversation was handled as a distinct session; users could not create accounts, so the chatbot did not have ‚Äùmemory‚Äù of a user‚Äôs previous conversations. Additionally, the models supporting chat were text-only.\\nBased on IP addresses, users were located predominantly in the US (21.60%), Russia (15.55%), and China (10.02%). Analysis of a random subset of 1,000 prompts indicated 6.7% were related to coding (other prominent categories were assisting/creative writing (61.9%, analysis/decision explanation (13.6%), and factual info (6.3%)). English was by far the most common prompt language (53%), followed by Chinese (13%) and Russian (12%).\\nThe goal of our study is to understand user interactions in programming contexts, so we focused on coding-related conversations. Although the WildChat authors estimated the fraction of prompts related to coding, they did not publish topic labels for specific conversations. We therefore created a pipeline to filter relevant conversations as presented in Figure 1 .\\nFirst, we identified coding-related conversations (Figure 1 a). We applied a regular expression filter looking for code block formatting characters (e.g., ‚Äò‚Äò‚Äò ) in both user prompts and chatbot responses. Notably, when the GPT models supporting the chat respond with code formatting, the formatting string also typically includes a language tag (such as ‚Äò‚Äò‚Äòpython‚Äò‚Äò‚Äò ). We were therefore able to estimate the rough number of conversations containing various popular programming languages, including the top 10 languages reported in Stack Overflow‚Äôs 2025 developer survey (202, 2025 ) . Out of 65,590 conversations containing code block formatting, the most common language was Python (23,353), followed by Java (11,852), SQL (11,852), and JavaScript (7,312). Considering that all authors were familiar with Python and it was the most common programming language in the dataset, we decided to restrict our analysis to this subset of coding-related conversations only.\\nIn addition to filtering for conversations involving Python code blocks, we also filtered for English-language conversations (Figure 1 b). The WildChat dataset includes a machine-generated label for probable language. We used this feature to restrict conversations to English-language only for qualitative coding. This left 11,418 conversations, which were candidates for qualitative coding, of which we selected 500 at random for qualitative analysis.\\nWe created a custom annotation tool for labeling conversations so that we could visualize the turn-by-turn interactions of the user and model responses easily (see Figure 2 ). Annotators could mark both the intention of a conversation as well as any potential user misconceptions using our pre-defined set of tags, or create a new tag. There was also an option to leave open-ended comments about a conversation. The tool with our final labelings is available at https://wildchat-website.hpcc.eu.org/ .',\n",
       "      'subsections': [{'title': '5.1.1. Pre-processing',\n",
       "        'paragraphs': 'The goal of our study is to understand user interactions in programming contexts, so we focused on coding-related conversations. Although the WildChat authors estimated the fraction of prompts related to coding, they did not publish topic labels for specific conversations. We therefore created a pipeline to filter relevant conversations as presented in Figure 1 .\\nFirst, we identified coding-related conversations (Figure 1 a). We applied a regular expression filter looking for code block formatting characters (e.g., ‚Äò‚Äò‚Äò ) in both user prompts and chatbot responses. Notably, when the GPT models supporting the chat respond with code formatting, the formatting string also typically includes a language tag (such as ‚Äò‚Äò‚Äòpython‚Äò‚Äò‚Äò ). We were therefore able to estimate the rough number of conversations containing various popular programming languages, including the top 10 languages reported in Stack Overflow‚Äôs 2025 developer survey (202, 2025 ) . Out of 65,590 conversations containing code block formatting, the most common language was Python (23,353), followed by Java (11,852), SQL (11,852), and JavaScript (7,312). Considering that all authors were familiar with Python and it was the most common programming language in the dataset, we decided to restrict our analysis to this subset of coding-related conversations only.\\nIn addition to filtering for conversations involving Python code blocks, we also filtered for English-language conversations (Figure 1 b). The WildChat dataset includes a machine-generated label for probable language. We used this feature to restrict conversations to English-language only for qualitative coding. This left 11,418 conversations, which were candidates for qualitative coding, of which we selected 500 at random for qualitative analysis.',\n",
       "        'subsections': []},\n",
       "       {'title': '5.1.2. Annotation tool',\n",
       "        'paragraphs': 'We created a custom annotation tool for labeling conversations so that we could visualize the turn-by-turn interactions of the user and model responses easily (see Figure 2 ). Annotators could mark both the intention of a conversation as well as any potential user misconceptions using our pre-defined set of tags, or create a new tag. There was also an option to leave open-ended comments about a conversation. The tool with our final labelings is available at https://wildchat-website.hpcc.eu.org/ .',\n",
       "        'subsections': []}]},\n",
       "     {'title': '5.2. Codebooks for Conversational Logs',\n",
       "      'paragraphs': 'Starting from the misconceptions we collected in the first activity, we aimed to create a codebook that could be applied to a collection of real conversations with a conversational LLM. However, not all the misconceptions we brainstormed would be relevant to conversational assistants like the WildChat chatbot. Others were relevant only to specific architectures, like retrieval augmented generation (‚ÄùIf you upload a document to this tool and ask a question about it, the tool will only use information in the document to answer.‚Äù)\\nWe therefore began with a simplified initial codebook, which could be applied deductively to conversations. Authors 1 and 2 (abbreviated A1 and A2) both used the initial codebook to do a round of open-coding on 250 unique conversations each, totaling 500 unique conversations. Additionally, we created a codebook for the inferred user‚Äôs intention on the conversation (for example, ‚Äùgenerate code‚Äù, ‚Äùexplain user-submitted code‚Äù, ‚Äùrefactor code‚Äù). Note that for both kinds of codes, multiple labels could apply for a given conversation. This was common, as conversations frequently involved multiple ‚Äùturns‚Äù‚Äîfor example, requesting code to be generated, then explained, followed by a request for debugging assistance.\\nDuring open coding, A1 and A2 added potential codes based on notable observations. We also added codes for conversations that were: (i) mostly not in English, which likely occurred due to errors in WildChat‚Äôs automated language detection; (ii) did not have a discernible user intention in the prompt (for example, entering a block of code with no instructions); and (iii) not a Python-related conversation, that could occur given the limitations of our regular expression filter. We did not attempt to infer the intentions or misconceptions for 85 non-English conversations and 7 non-Python ones.\\nAfter individually labeling 250 unique conversations each, A1 and A2 reviewed one anothers‚Äô conversations that were flagged as misconceptions (78 total) and met to discuss any disagreements in misconception labels. As possible misconceptions were a distinct minority class, we considered reviewing these conversations more informative than reviewing the entire dataset together. During this meeting, we performed axial coding to create a finalized set of codes for both user potential misconceptions and intentions. This involved de-duplicating codes, merging codes that were insufficiently distinct, and removing codes from the initial codebook that we did not observe (for example, we did not see any instances of assumed database access that were not better categorized as assumed web access). A6 resolved any outstanding disagreements in either codebooks (intentions or misconceptions) as a tie-breaker to create a ‚ÄùGold‚Äù dataset of 50 conversations that at least two annotators agreed contained a potential misconception. This dataset with labels is provided as an artifact.\\nFinally, we did a validation round of coding involving the remaining authors who were not involved in the creation of the Gold dataset (A3, A4, and A5). We created a stratified validation set of 29 conversations to ensure there was at least one example of each misconception from the codebook present (conversations in the validation set are labeled in the paper artifacts). Annotators were advised that they could choose multiple codes per conversation (for both intention and misconceptions) and that each conversation they would review had been flagged as a potential misconception by at least two other authors. The outcomes of this final round of coding and ensuing discussion are presented in the Results.\\nThe axial codebook for intentions, with counts, is given in Table 1 . The axial codebook for misconceptions is provided in Table 2 , and the conversations labeled as misconceptions are discussed in Results. Note that we also provide as an artifact a dictionary mapping conversation indices (C1, C2, etc.) to identifying hashes from the WildChat dataset.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3. Measuring Inter-Rater Reliability',\n",
       "      'paragraphs': 'We used a modified Cohen‚Äôs Kappa ( Œ∫ \\\\kappa ) to summarise inter-rater reliability. Because a conversation could have multiple non-exclusive labels, we considered ‚Äùagreement‚Äù to occur when two annotators assigned at least one overlapping label. For example, if A1 tagged a conversation with the intentions ‚ÄùDebug‚Äù and ‚ÄùValidate code‚Äù and A2 tagged it as ‚ÄùDebug‚Äù, this would be considered agreement. Cohen‚Äôs Œ∫ \\\\kappa was calculated for all conversations using this definition of agreement. This provides a (somewhat optimistic) estimate of inter-rater reliability. Reliability statistics are presented in the Results section.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6. Results',\n",
       "    'paragraphs': 'Briefly, we will review the most common prompt intentions that were inferred by annotators. The intention codebook, plus counts for all conversations, are provided in Table 1 . Annotators A1 and A2 compared intention codes on the Gold dataset of 50 conversations and had strong agreement ( Œ∫ = 0.75 \\\\kappa=0.75 during open coding and Œ∫ = 0.87 \\\\kappa=0.87 using the axial codebook). We therefore considered this coding strategy sufficiently reliable to report labels from the full dataset of 500 conversations.\\nThe most common prompt intention was ‚Äùgenerate code‚Äù (243 conversations). Prompts of this form typically specified a desired program in natural language and requested a code implementation. The next most common intention was debugging (126), which involved help-seeking behaviors to resolve issues with programs. Interestingly, debugging prompts ranged widely in the scope of information about the program behavior provided by the user: some mentioned only that ‚Äùan error‚Äù occurred, whereas others included keywords from a traceback or even the entire traceback. Some also included error message information, but not the associated program.\\nA theme that often co-occurred with debugging was code validation, where users asked if a program met certain requirements. This included requests to confirm that modifications to a buggy program would allow it to run without error (or ‚Äùwork‚Äù). Other common intentions were brainstorming ideas about how to solve a computational problem, asking questions about common Python functions and libraries, and updating requirements for programs generated earlier in a conversation.\\nThe user misconception codebook, plus counts for all conversations as marked in the Gold dataset, are provided in Table 1. However, the inter-rater reliability on the misconception labels was not as high as for intentions: after open coding, A1 and A2 reached Œ∫ = 0.54 \\\\kappa=0.54 and after axial coding, Œ∫ = 0.80 \\\\kappa=0.80 on the Gold dataset. In the validation round of coding by the remaining annotators (A3, A4 and A5), individual agreement with the validation set labels was even lower ( 0.45 <= Œ∫ <= 0.51 0.45<=\\\\kappa<=0.51 ). While there is no widely agreed upon standard for interpreting inter-rater reliability, by the common benchmark of Landis and Koch ( 1977 ) , this would be ‚Äùmoderate‚Äù agreement (though note that our scoring strategy is by definition optimistic; see Section 5.3 ).\\nAfter the final round, we discussed the conversations and reviewed one anothers‚Äô notes from the annotation process to better understand disagreements. There were a few common causes: some prompts had language that was difficult to parse, which seemed related to non-native English speaking users. This could be related due the minority of WildChat users have IP addresses from primarily English-speaking countries (Zhao et\\xa0al., 2024 ) ). Other conversations appeared to involve prompts that were copy pastes of homework instructions, which prompted some disagreement about how to treat instructions that were likely not written by the user.\\nPerhaps most substantially, during the creation of the Gold dataset, A1, A2, and A6 tended to err on the side of flagging any conversation where a misconception seemed plausible‚Äîif not certain. During the validation round, the remaining annotators preferred to reserve the ‚Äùmisconception‚Äù labels for more clear-cut cases. However, there was strong agreement by all annotators in discussions that the codebook of misconceptions was reasonable. In other words, there were some conversations where misconceptions were confidently identified by multiple annotators, and there were also misconceptions we think likely exist but could not be confidently labeled at the level of individual conversations in the dataset.\\nWe therefore present the results of our annotations and discussions in two sections: first, we present key themes from conversations with reasonably high agreement. These were all cases where at least one annotator not involved in the Gold dataset creation agreed with the Gold label, providing evidence for reliability. Then, we discuss common themes emerging from our discussions of more ambiguous conversations where this agreement did not naturally occur during annotation.\\nOverwhelmingly, the most frequently agreed-upon misconception was about web access capabilities.\\nIn C1, a user provided a link to a dataset and requirements for a system that would process the linked set of images and detect fractures. The user further indicated in a follow-up prompt ‚Äùjust coding‚Äù and the chatbot returned a Python program. However, without any further details about the format of the linked dataset or its file structure, the chatbot would not have sufficient information to construct a program that could successfully read the dataset and any attached labels.\\nIn C2, the user provided the name of a .csv file and a link to a GitHub repository where the file was hosted. They asked for a line plot of a specific column contained in that file (this was also flagged as a user expecting non-text output).\\nIn C14, the user instructed the chatbot to ‚Äùwrite a code [sic] to interact with ¬°link to a GitHub page¬ø with python.‚Äù\\nIn C18, the user prefaced a prompt with, ‚Äùfrom the data provided here‚Äù with a link to a GitHub repository. They proceeded to ask controlling Wi-Fi enabled smart devices: ‚Äùcan i just run the breathe_all.py and i got control over my lifx bulbs in my network or do i need any api connection or something extra‚Äù.\\nIn C24, a user prompted the chatbot to ‚Äùuse a genetic algorithm to optimize the features and parameters of a machine learning model\\nfor predicting the survival of passengers aboard the Titanic.‚Äù They then provided a link to the Kaggle titanic dataset.\\nNote that in the case of the Titanic dataset prompt (C24), the annotators were all quite confident that the dataset would be well represented in the models‚Äô training data. We considered this a misconception because the user included a URL to access the dataset. An edge case we observed (C28, C29) was users asking for programs that would perform web scraping. For example, C28 prompted ‚Äùweb scrape ¬°link to an IMDb list of top 250 movies¬ø with python and beautifulsoup‚Äù. Outside of the trivial case of simply pulling all the HTML from a website, the model would presumably need information about that website‚Äôs HTML to know which tags are present and related to the movie list. In this example, the chatbot returned a script that would scrape movie titles, years, and ratings based on presumed class tags in the HTML, but it is unclear if these would be correct.\\nThe models used for the WildChat data collection were all text-only for both inputs and outputs, so prompts that requested figure generation (absent any indication that code to produce the figure would be acceptable) were flagged as potential misconceptions. In one example, C7 asked, ‚Äùgive me a meteogram in port of genoa on June 7, 2020.‚Äù Similarly, C2 asked for a ‚Äùline plot‚Äù of a column in a dataset without specifying they wanted code that would do this. We note, though, that this appeared to be a homework assignment copy and paste, and the user did appear to accept a chatbot response providing code.\\nWe observed one clear example of a user referencing a previous session, which would not be a supported feature in WildChat. C4 asked, ‚ÄùOkay thank you please from the previous programming problem you solved please redo it and round the answers in dollars and cents‚Äù.\\nTwo conversations involved prompts that appear to request the chatbot to execute programs. C3 began with the user specifying a program and some sample input-output pairs. The chatbot responded with a Python program, and the user followed up with ‚Äùmake the code run and check for input 5‚Äù. In C26, the user prompted with what looks like a homework assignment containing a short Python program and the instructions ‚ÄùClick Run and watch the stage to see what‚Äôs wrong.‚Äù The chatbot responded with an explanation for why the program ‚Äùis not running‚Äù. The user then followed up with the prompt, ‚Äùdid you call the function to make it run?‚Äù We also note, curiously, a case where a user did not ask for a script to be run at all, but the chatbot responded by printing out simulated output of the program (C334).\\nA user trying to debug a program (C12) began the session with ‚Äùplease help‚Äù and a traceback from their computer (interestingly, they did not include the program in question). The chatbot responded, ‚ÄùIt looks like your script encountered a timeout error while trying to download a model from Hugging Face‚Äôs model hub‚Äù and suggested a few possible solutions, including checking the internet connection and firewall settings. The user followed up, ‚Äùwhere did it download the file to or attempt to?‚Äù\\nMost annotators considered this to be an example of the user expecting the chatbot to have access to their local machine. But An alternative interpretation may be that the user is asking what is the default download location for a HuggingFace model hub file, in which case they are attempting a kind of documentation lookup.\\nThe models that users interacted with in the WildChat dataset have knowledge cutoffs, so we considered requests to use the ‚Äùlatest‚Äù version of libraries or APIs to be misconceptions. Two examples are C6, in which a user prompted ‚ÄùUse latest telethon to delete messges in group that are sent by me ‚Äù, and C13, where a user instructed the chatbot to ‚ÄùMigrate this code to the latest openai api‚Äù.\\nC22 prompted, ‚Äùwhat does 75% expentional [sic] equate to‚Äù. Interestingly, the chatbot followed up with an example of how to calculate this value in Python, and then simulated printing out the output of the program (‚ÄùOutput: 2.117‚Ä¶‚Äù)\\nC28, midway through a conversation, prompted the chatbot to ‚Äùclear this page‚Äù. The bot returns with instructions to clear a Jupyter notebook output, and the user then changed topic in their next prompt. There was some disagreement among annotators whether this represented a common context-window-management strategy or a misunderstanding that a ‚Äùclear the chat‚Äù system command exists.\\nDuring review of the conversation logs, we encountered several challenging themes that emerged as discussion topics between annotators. Generally, these were misconceptions we agreed were possible, but could not be confidently labeled at the level of individual conversations without knowing the users‚Äô intentions. We discuss them here, organized by programming context.\\nA frequent pattern in debugging-related conversations was users requesting that all bugs be removed from a program. For example:\\n‚ÄúFix any bugs in this code and return the full fixed code assembled and ready‚Äù (C397)\\n‚ÄúBut I getting ERROR, please give me code which will doesn‚Äôt give me ERROR‚Äù (C19)\\n‚ÄúJust make sure the function is error proof.‚Äù (C278)\\n‚ÄúNothing happens when I press Ctrl + Alt + Shift and nothing happens when I right-click the icon in my system tray. Fix and give me full, working code‚Äù (C10)\\nTaken literally, it is generally not possible to ensure that a program will run without errors only by inspecting it statically‚Äìfor example, a program could throw an error because a user does not have a dependency installed on their computer. Observing this pattern, we wondered how often users expected that the model was capable of dynamic analysis, which would imply an ability to execute programs. However, we did not feel confident labeling these conversations as misconceptions because this could also reflect prompt engineering rather than the user‚Äôs beliefs.\\nRelatedly, we observed conversations (again, typically in the debugging context) in which users asked if code would either work or resolve a previously-discussed bug:\\n‚Äúif i restart the original script again will it fix it?‚Äù (C12)\\n‚Äúis the below code functionable , if not then give me the code to modify it‚Äù (C23)\\n‚Äúwill this one work or no?‚Äù (C434, providing a code block)\\n‚ÄúReview my code now and tell me if it ill work now‚Äù (C397)\\nIn some cases, it may be reasonable to expect that a machine with only access to the text of a program can provide useful input about a bug repair‚Äîfor example, if the user has provided a traceback message indicating a syntactical error in a certain line along with the repaired program. However, there are also a large number of potential debugging scenarios in which it would not be feasible to answer whether a program will run without dynamic information.\\nWe also note that users often include traceback messages as part of their debugging conversations, which provides some runtime information to the model. This behavior may be evidence that users do not expect a tool can reproduce this kind of information itself.\\nA number of conversations focused on optimizing programs for speed or memory usage. For example:\\n‚Äúcould you please speed up this function as much as possible?‚Äù (C157)\\n‚Äúgive me time estimation for this code‚Ä¶with ryzen 5 5600x‚Ä¶for this batch_size = 1024‚Ä¶give me time estimation with 3 minute of margin error for you‚Ä¶16 gb of memory ?‚Äù (C258)\\n‚Äúmake my thread to process the number faster‚Äù (C307)\\nWhile none of these conversations seem necessarily ‚Äùmisconceived‚Äù, we wondered what expectations users might have about how an LLM-based tool would produce a response. For example, in Python, certain libraries for vectorized computations are almost always faster than algebra on values stored in base Python types like lists. This kind of information is almost surely very salient in the GPT models‚Äô training data, as it is a common subject for help-seeking in online forums. Users seeking optimization help may therefore be motivated by their mental models about an LLM‚Äôs training data, or alternatively from incorrect assumptions about the capabilities of the tool to perform memory profiling.\\nA relatively common prompt theme was seeking validation that code would meet some requirements (see Table 1 ). Some of these prompts were worded ambiguously, which raised questions about what kinds of validation behaviors were expected. For example, one user (C5) wrote, ‚Äúi have historical data of crypto as csv files [sic] i have following code to train a model on them without merging them, check if code has any problem or anything wrong‚Äù. In another example (C397), a user prompted ‚ÄùReview this an tell me what‚Äôs missing‚Äù, followed by a code block.\\nBecause these prompts are rather underspecified, we cannot be confident exactly what kind of ‚Äùproblems‚Äù or ‚Äùmissing‚Äù elements the user hoped the tool would be able to check for. This style of prompting could possibly stem from about misconceptions about the tool having access to some kind of ‚Äùground truth‚Äù about programs to grade them, similar to an autograder typically found in computer science educational assessments.\\nWhile iterating on versions of a program specified by prompting, users often asked for modifications. Particularly in the context of longer programs, the default behavior of the WildChat models is typically to provide a snippet of code with the modification and instruct the user to update their program. Sometimes, users would follow up with a prompt requesting not just for the snippet, but the ‚Äùfull code‚Äù (for examples, C155 and C306)‚Äîwhich appears to mean, the original program with the modification made in-line.\\nThis request seems to imply a certain degree of trust that the model will reliably reproduce the entire program, only making the intended modification. Because the previous version of the program is only represented in the model‚Äôs context window, it is not actually guaranteed that a deterministic ‚Äùdiff‚Äù will be made (changing only the indicated section). This style of prompting could reflect a user‚Äôs expectation that the model explicitly stores snapshots of code as a kind of version control. On the other hand, it could also be explained as the user trusting that the tool will be sufficiently reliable to reproduce their code, perhaps based on reasoning about the length of the program to modify and the size of the context window.\\nWe noted that some conversations involved very long prompts, fully incorporating copy-pastes of previous conversations or very long programs. For example, C11 prompted with a combination of code and what appears to be copies of previous conversation logs totaling 6,021 words (45,345 characters). The model serving this chat was gpt-4-0125-preview , which has a context window of 128,000 tokens‚Äîas such, the prompt would surely fit within the window (in English, a common benchmark is 100 tokens to 75 words). However, model performance is known to be highly sensitive to use of the context window, and using more of the context window is not necessarily better. Prompts of this size could be an informed and deliberate choice by the user, or may be related to the ‚Äùmonotonicity belief‚Äù reported by Jayagopal et\\xa0al. ( 2022 ) .',\n",
       "    'subsections': [{'title': '6.1. Programming-Related Conversations',\n",
       "      'paragraphs': 'Briefly, we will review the most common prompt intentions that were inferred by annotators. The intention codebook, plus counts for all conversations, are provided in Table 1 . Annotators A1 and A2 compared intention codes on the Gold dataset of 50 conversations and had strong agreement ( Œ∫ = 0.75 \\\\kappa=0.75 during open coding and Œ∫ = 0.87 \\\\kappa=0.87 using the axial codebook). We therefore considered this coding strategy sufficiently reliable to report labels from the full dataset of 500 conversations.\\nThe most common prompt intention was ‚Äùgenerate code‚Äù (243 conversations). Prompts of this form typically specified a desired program in natural language and requested a code implementation. The next most common intention was debugging (126), which involved help-seeking behaviors to resolve issues with programs. Interestingly, debugging prompts ranged widely in the scope of information about the program behavior provided by the user: some mentioned only that ‚Äùan error‚Äù occurred, whereas others included keywords from a traceback or even the entire traceback. Some also included error message information, but not the associated program.\\nA theme that often co-occurred with debugging was code validation, where users asked if a program met certain requirements. This included requests to confirm that modifications to a buggy program would allow it to run without error (or ‚Äùwork‚Äù). Other common intentions were brainstorming ideas about how to solve a computational problem, asking questions about common Python functions and libraries, and updating requirements for programs generated earlier in a conversation.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.2. Results of Labeling Misconceptions',\n",
       "      'paragraphs': 'The user misconception codebook, plus counts for all conversations as marked in the Gold dataset, are provided in Table 1. However, the inter-rater reliability on the misconception labels was not as high as for intentions: after open coding, A1 and A2 reached Œ∫ = 0.54 \\\\kappa=0.54 and after axial coding, Œ∫ = 0.80 \\\\kappa=0.80 on the Gold dataset. In the validation round of coding by the remaining annotators (A3, A4 and A5), individual agreement with the validation set labels was even lower ( 0.45 <= Œ∫ <= 0.51 0.45<=\\\\kappa<=0.51 ). While there is no widely agreed upon standard for interpreting inter-rater reliability, by the common benchmark of Landis and Koch ( 1977 ) , this would be ‚Äùmoderate‚Äù agreement (though note that our scoring strategy is by definition optimistic; see Section 5.3 ).\\nAfter the final round, we discussed the conversations and reviewed one anothers‚Äô notes from the annotation process to better understand disagreements. There were a few common causes: some prompts had language that was difficult to parse, which seemed related to non-native English speaking users. This could be related due the minority of WildChat users have IP addresses from primarily English-speaking countries (Zhao et\\xa0al., 2024 ) ). Other conversations appeared to involve prompts that were copy pastes of homework instructions, which prompted some disagreement about how to treat instructions that were likely not written by the user.\\nPerhaps most substantially, during the creation of the Gold dataset, A1, A2, and A6 tended to err on the side of flagging any conversation where a misconception seemed plausible‚Äîif not certain. During the validation round, the remaining annotators preferred to reserve the ‚Äùmisconception‚Äù labels for more clear-cut cases. However, there was strong agreement by all annotators in discussions that the codebook of misconceptions was reasonable. In other words, there were some conversations where misconceptions were confidently identified by multiple annotators, and there were also misconceptions we think likely exist but could not be confidently labeled at the level of individual conversations in the dataset.\\nWe therefore present the results of our annotations and discussions in two sections: first, we present key themes from conversations with reasonably high agreement. These were all cases where at least one annotator not involved in the Gold dataset creation agreed with the Gold label, providing evidence for reliability. Then, we discuss common themes emerging from our discussions of more ambiguous conversations where this agreement did not naturally occur during annotation.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.3. High-Agreement Misconceptions',\n",
       "      'paragraphs': 'Overwhelmingly, the most frequently agreed-upon misconception was about web access capabilities.\\nIn C1, a user provided a link to a dataset and requirements for a system that would process the linked set of images and detect fractures. The user further indicated in a follow-up prompt ‚Äùjust coding‚Äù and the chatbot returned a Python program. However, without any further details about the format of the linked dataset or its file structure, the chatbot would not have sufficient information to construct a program that could successfully read the dataset and any attached labels.\\nIn C2, the user provided the name of a .csv file and a link to a GitHub repository where the file was hosted. They asked for a line plot of a specific column contained in that file (this was also flagged as a user expecting non-text output).\\nIn C14, the user instructed the chatbot to ‚Äùwrite a code [sic] to interact with ¬°link to a GitHub page¬ø with python.‚Äù\\nIn C18, the user prefaced a prompt with, ‚Äùfrom the data provided here‚Äù with a link to a GitHub repository. They proceeded to ask controlling Wi-Fi enabled smart devices: ‚Äùcan i just run the breathe_all.py and i got control over my lifx bulbs in my network or do i need any api connection or something extra‚Äù.\\nIn C24, a user prompted the chatbot to ‚Äùuse a genetic algorithm to optimize the features and parameters of a machine learning model\\nfor predicting the survival of passengers aboard the Titanic.‚Äù They then provided a link to the Kaggle titanic dataset.\\nNote that in the case of the Titanic dataset prompt (C24), the annotators were all quite confident that the dataset would be well represented in the models‚Äô training data. We considered this a misconception because the user included a URL to access the dataset. An edge case we observed (C28, C29) was users asking for programs that would perform web scraping. For example, C28 prompted ‚Äùweb scrape ¬°link to an IMDb list of top 250 movies¬ø with python and beautifulsoup‚Äù. Outside of the trivial case of simply pulling all the HTML from a website, the model would presumably need information about that website‚Äôs HTML to know which tags are present and related to the movie list. In this example, the chatbot returned a script that would scrape movie titles, years, and ratings based on presumed class tags in the HTML, but it is unclear if these would be correct.\\nThe models used for the WildChat data collection were all text-only for both inputs and outputs, so prompts that requested figure generation (absent any indication that code to produce the figure would be acceptable) were flagged as potential misconceptions. In one example, C7 asked, ‚Äùgive me a meteogram in port of genoa on June 7, 2020.‚Äù Similarly, C2 asked for a ‚Äùline plot‚Äù of a column in a dataset without specifying they wanted code that would do this. We note, though, that this appeared to be a homework assignment copy and paste, and the user did appear to accept a chatbot response providing code.\\nWe observed one clear example of a user referencing a previous session, which would not be a supported feature in WildChat. C4 asked, ‚ÄùOkay thank you please from the previous programming problem you solved please redo it and round the answers in dollars and cents‚Äù.\\nTwo conversations involved prompts that appear to request the chatbot to execute programs. C3 began with the user specifying a program and some sample input-output pairs. The chatbot responded with a Python program, and the user followed up with ‚Äùmake the code run and check for input 5‚Äù. In C26, the user prompted with what looks like a homework assignment containing a short Python program and the instructions ‚ÄùClick Run and watch the stage to see what‚Äôs wrong.‚Äù The chatbot responded with an explanation for why the program ‚Äùis not running‚Äù. The user then followed up with the prompt, ‚Äùdid you call the function to make it run?‚Äù We also note, curiously, a case where a user did not ask for a script to be run at all, but the chatbot responded by printing out simulated output of the program (C334).\\nA user trying to debug a program (C12) began the session with ‚Äùplease help‚Äù and a traceback from their computer (interestingly, they did not include the program in question). The chatbot responded, ‚ÄùIt looks like your script encountered a timeout error while trying to download a model from Hugging Face‚Äôs model hub‚Äù and suggested a few possible solutions, including checking the internet connection and firewall settings. The user followed up, ‚Äùwhere did it download the file to or attempt to?‚Äù\\nMost annotators considered this to be an example of the user expecting the chatbot to have access to their local machine. But An alternative interpretation may be that the user is asking what is the default download location for a HuggingFace model hub file, in which case they are attempting a kind of documentation lookup.\\nThe models that users interacted with in the WildChat dataset have knowledge cutoffs, so we considered requests to use the ‚Äùlatest‚Äù version of libraries or APIs to be misconceptions. Two examples are C6, in which a user prompted ‚ÄùUse latest telethon to delete messges in group that are sent by me ‚Äù, and C13, where a user instructed the chatbot to ‚ÄùMigrate this code to the latest openai api‚Äù.\\nC22 prompted, ‚Äùwhat does 75% expentional [sic] equate to‚Äù. Interestingly, the chatbot followed up with an example of how to calculate this value in Python, and then simulated printing out the output of the program (‚ÄùOutput: 2.117‚Ä¶‚Äù)\\nC28, midway through a conversation, prompted the chatbot to ‚Äùclear this page‚Äù. The bot returns with instructions to clear a Jupyter notebook output, and the user then changed topic in their next prompt. There was some disagreement among annotators whether this represented a common context-window-management strategy or a misunderstanding that a ‚Äùclear the chat‚Äù system command exists.',\n",
       "      'subsections': [{'title': 'Web access',\n",
       "        'paragraphs': 'Overwhelmingly, the most frequently agreed-upon misconception was about web access capabilities.\\nIn C1, a user provided a link to a dataset and requirements for a system that would process the linked set of images and detect fractures. The user further indicated in a follow-up prompt ‚Äùjust coding‚Äù and the chatbot returned a Python program. However, without any further details about the format of the linked dataset or its file structure, the chatbot would not have sufficient information to construct a program that could successfully read the dataset and any attached labels.\\nIn C2, the user provided the name of a .csv file and a link to a GitHub repository where the file was hosted. They asked for a line plot of a specific column contained in that file (this was also flagged as a user expecting non-text output).\\nIn C14, the user instructed the chatbot to ‚Äùwrite a code [sic] to interact with ¬°link to a GitHub page¬ø with python.‚Äù\\nIn C18, the user prefaced a prompt with, ‚Äùfrom the data provided here‚Äù with a link to a GitHub repository. They proceeded to ask controlling Wi-Fi enabled smart devices: ‚Äùcan i just run the breathe_all.py and i got control over my lifx bulbs in my network or do i need any api connection or something extra‚Äù.\\nIn C24, a user prompted the chatbot to ‚Äùuse a genetic algorithm to optimize the features and parameters of a machine learning model\\nfor predicting the survival of passengers aboard the Titanic.‚Äù They then provided a link to the Kaggle titanic dataset.\\nNote that in the case of the Titanic dataset prompt (C24), the annotators were all quite confident that the dataset would be well represented in the models‚Äô training data. We considered this a misconception because the user included a URL to access the dataset. An edge case we observed (C28, C29) was users asking for programs that would perform web scraping. For example, C28 prompted ‚Äùweb scrape ¬°link to an IMDb list of top 250 movies¬ø with python and beautifulsoup‚Äù. Outside of the trivial case of simply pulling all the HTML from a website, the model would presumably need information about that website‚Äôs HTML to know which tags are present and related to the movie list. In this example, the chatbot returned a script that would scrape movie titles, years, and ratings based on presumed class tags in the HTML, but it is unclear if these would be correct.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Non-text output',\n",
       "        'paragraphs': 'The models used for the WildChat data collection were all text-only for both inputs and outputs, so prompts that requested figure generation (absent any indication that code to produce the figure would be acceptable) were flagged as potential misconceptions. In one example, C7 asked, ‚Äùgive me a meteogram in port of genoa on June 7, 2020.‚Äù Similarly, C2 asked for a ‚Äùline plot‚Äù of a column in a dataset without specifying they wanted code that would do this. We note, though, that this appeared to be a homework assignment copy and paste, and the user did appear to accept a chatbot response providing code.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Session memory',\n",
       "        'paragraphs': 'We observed one clear example of a user referencing a previous session, which would not be a supported feature in WildChat. C4 asked, ‚ÄùOkay thank you please from the previous programming problem you solved please redo it and round the answers in dollars and cents‚Äù.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Code execution',\n",
       "        'paragraphs': 'Two conversations involved prompts that appear to request the chatbot to execute programs. C3 began with the user specifying a program and some sample input-output pairs. The chatbot responded with a Python program, and the user followed up with ‚Äùmake the code run and check for input 5‚Äù. In C26, the user prompted with what looks like a homework assignment containing a short Python program and the instructions ‚ÄùClick Run and watch the stage to see what‚Äôs wrong.‚Äù The chatbot responded with an explanation for why the program ‚Äùis not running‚Äù. The user then followed up with the prompt, ‚Äùdid you call the function to make it run?‚Äù We also note, curiously, a case where a user did not ask for a script to be run at all, but the chatbot responded by printing out simulated output of the program (C334).',\n",
       "        'subsections': []},\n",
       "       {'title': 'Local machine access',\n",
       "        'paragraphs': 'A user trying to debug a program (C12) began the session with ‚Äùplease help‚Äù and a traceback from their computer (interestingly, they did not include the program in question). The chatbot responded, ‚ÄùIt looks like your script encountered a timeout error while trying to download a model from Hugging Face‚Äôs model hub‚Äù and suggested a few possible solutions, including checking the internet connection and firewall settings. The user followed up, ‚Äùwhere did it download the file to or attempt to?‚Äù\\nMost annotators considered this to be an example of the user expecting the chatbot to have access to their local machine. But An alternative interpretation may be that the user is asking what is the default download location for a HuggingFace model hub file, in which case they are attempting a kind of documentation lookup.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Continuous training',\n",
       "        'paragraphs': 'The models that users interacted with in the WildChat dataset have knowledge cutoffs, so we considered requests to use the ‚Äùlatest‚Äù version of libraries or APIs to be misconceptions. Two examples are C6, in which a user prompted ‚ÄùUse latest telethon to delete messges in group that are sent by me ‚Äù, and C13, where a user instructed the chatbot to ‚ÄùMigrate this code to the latest openai api‚Äù.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Algebra',\n",
       "        'paragraphs': 'C22 prompted, ‚Äùwhat does 75% expentional [sic] equate to‚Äù. Interestingly, the chatbot followed up with an example of how to calculate this value in Python, and then simulated printing out the output of the program (‚ÄùOutput: 2.117‚Ä¶‚Äù)',\n",
       "        'subsections': []},\n",
       "       {'title': 'Clear chat',\n",
       "        'paragraphs': 'C28, midway through a conversation, prompted the chatbot to ‚Äùclear this page‚Äù. The bot returns with instructions to clear a Jupyter notebook output, and the user then changed topic in their next prompt. There was some disagreement among annotators whether this represented a common context-window-management strategy or a misunderstanding that a ‚Äùclear the chat‚Äù system command exists.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '6.4. Potential But Unconfirmed Misconceptions',\n",
       "      'paragraphs': 'During review of the conversation logs, we encountered several challenging themes that emerged as discussion topics between annotators. Generally, these were misconceptions we agreed were possible, but could not be confidently labeled at the level of individual conversations without knowing the users‚Äô intentions. We discuss them here, organized by programming context.\\nA frequent pattern in debugging-related conversations was users requesting that all bugs be removed from a program. For example:\\n‚ÄúFix any bugs in this code and return the full fixed code assembled and ready‚Äù (C397)\\n‚ÄúBut I getting ERROR, please give me code which will doesn‚Äôt give me ERROR‚Äù (C19)\\n‚ÄúJust make sure the function is error proof.‚Äù (C278)\\n‚ÄúNothing happens when I press Ctrl + Alt + Shift and nothing happens when I right-click the icon in my system tray. Fix and give me full, working code‚Äù (C10)\\nTaken literally, it is generally not possible to ensure that a program will run without errors only by inspecting it statically‚Äìfor example, a program could throw an error because a user does not have a dependency installed on their computer. Observing this pattern, we wondered how often users expected that the model was capable of dynamic analysis, which would imply an ability to execute programs. However, we did not feel confident labeling these conversations as misconceptions because this could also reflect prompt engineering rather than the user‚Äôs beliefs.\\nRelatedly, we observed conversations (again, typically in the debugging context) in which users asked if code would either work or resolve a previously-discussed bug:\\n‚Äúif i restart the original script again will it fix it?‚Äù (C12)\\n‚Äúis the below code functionable , if not then give me the code to modify it‚Äù (C23)\\n‚Äúwill this one work or no?‚Äù (C434, providing a code block)\\n‚ÄúReview my code now and tell me if it ill work now‚Äù (C397)\\nIn some cases, it may be reasonable to expect that a machine with only access to the text of a program can provide useful input about a bug repair‚Äîfor example, if the user has provided a traceback message indicating a syntactical error in a certain line along with the repaired program. However, there are also a large number of potential debugging scenarios in which it would not be feasible to answer whether a program will run without dynamic information.\\nWe also note that users often include traceback messages as part of their debugging conversations, which provides some runtime information to the model. This behavior may be evidence that users do not expect a tool can reproduce this kind of information itself.\\nA number of conversations focused on optimizing programs for speed or memory usage. For example:\\n‚Äúcould you please speed up this function as much as possible?‚Äù (C157)\\n‚Äúgive me time estimation for this code‚Ä¶with ryzen 5 5600x‚Ä¶for this batch_size = 1024‚Ä¶give me time estimation with 3 minute of margin error for you‚Ä¶16 gb of memory ?‚Äù (C258)\\n‚Äúmake my thread to process the number faster‚Äù (C307)\\nWhile none of these conversations seem necessarily ‚Äùmisconceived‚Äù, we wondered what expectations users might have about how an LLM-based tool would produce a response. For example, in Python, certain libraries for vectorized computations are almost always faster than algebra on values stored in base Python types like lists. This kind of information is almost surely very salient in the GPT models‚Äô training data, as it is a common subject for help-seeking in online forums. Users seeking optimization help may therefore be motivated by their mental models about an LLM‚Äôs training data, or alternatively from incorrect assumptions about the capabilities of the tool to perform memory profiling.\\nA relatively common prompt theme was seeking validation that code would meet some requirements (see Table 1 ). Some of these prompts were worded ambiguously, which raised questions about what kinds of validation behaviors were expected. For example, one user (C5) wrote, ‚Äúi have historical data of crypto as csv files [sic] i have following code to train a model on them without merging them, check if code has any problem or anything wrong‚Äù. In another example (C397), a user prompted ‚ÄùReview this an tell me what‚Äôs missing‚Äù, followed by a code block.\\nBecause these prompts are rather underspecified, we cannot be confident exactly what kind of ‚Äùproblems‚Äù or ‚Äùmissing‚Äù elements the user hoped the tool would be able to check for. This style of prompting could possibly stem from about misconceptions about the tool having access to some kind of ‚Äùground truth‚Äù about programs to grade them, similar to an autograder typically found in computer science educational assessments.\\nWhile iterating on versions of a program specified by prompting, users often asked for modifications. Particularly in the context of longer programs, the default behavior of the WildChat models is typically to provide a snippet of code with the modification and instruct the user to update their program. Sometimes, users would follow up with a prompt requesting not just for the snippet, but the ‚Äùfull code‚Äù (for examples, C155 and C306)‚Äîwhich appears to mean, the original program with the modification made in-line.\\nThis request seems to imply a certain degree of trust that the model will reliably reproduce the entire program, only making the intended modification. Because the previous version of the program is only represented in the model‚Äôs context window, it is not actually guaranteed that a deterministic ‚Äùdiff‚Äù will be made (changing only the indicated section). This style of prompting could reflect a user‚Äôs expectation that the model explicitly stores snapshots of code as a kind of version control. On the other hand, it could also be explained as the user trusting that the tool will be sufficiently reliable to reproduce their code, perhaps based on reasoning about the length of the program to modify and the size of the context window.\\nWe noted that some conversations involved very long prompts, fully incorporating copy-pastes of previous conversations or very long programs. For example, C11 prompted with a combination of code and what appears to be copies of previous conversation logs totaling 6,021 words (45,345 characters). The model serving this chat was gpt-4-0125-preview , which has a context window of 128,000 tokens‚Äîas such, the prompt would surely fit within the window (in English, a common benchmark is 100 tokens to 75 words). However, model performance is known to be highly sensitive to use of the context window, and using more of the context window is not necessarily better. Prompts of this size could be an informed and deliberate choice by the user, or may be related to the ‚Äùmonotonicity belief‚Äù reported by Jayagopal et\\xa0al. ( 2022 ) .',\n",
       "      'subsections': [{'title': 'Debugging',\n",
       "        'paragraphs': 'A frequent pattern in debugging-related conversations was users requesting that all bugs be removed from a program. For example:\\n‚ÄúFix any bugs in this code and return the full fixed code assembled and ready‚Äù (C397)\\n‚ÄúBut I getting ERROR, please give me code which will doesn‚Äôt give me ERROR‚Äù (C19)\\n‚ÄúJust make sure the function is error proof.‚Äù (C278)\\n‚ÄúNothing happens when I press Ctrl + Alt + Shift and nothing happens when I right-click the icon in my system tray. Fix and give me full, working code‚Äù (C10)\\nTaken literally, it is generally not possible to ensure that a program will run without errors only by inspecting it statically‚Äìfor example, a program could throw an error because a user does not have a dependency installed on their computer. Observing this pattern, we wondered how often users expected that the model was capable of dynamic analysis, which would imply an ability to execute programs. However, we did not feel confident labeling these conversations as misconceptions because this could also reflect prompt engineering rather than the user‚Äôs beliefs.\\nRelatedly, we observed conversations (again, typically in the debugging context) in which users asked if code would either work or resolve a previously-discussed bug:\\n‚Äúif i restart the original script again will it fix it?‚Äù (C12)\\n‚Äúis the below code functionable , if not then give me the code to modify it‚Äù (C23)\\n‚Äúwill this one work or no?‚Äù (C434, providing a code block)\\n‚ÄúReview my code now and tell me if it ill work now‚Äù (C397)\\nIn some cases, it may be reasonable to expect that a machine with only access to the text of a program can provide useful input about a bug repair‚Äîfor example, if the user has provided a traceback message indicating a syntactical error in a certain line along with the repaired program. However, there are also a large number of potential debugging scenarios in which it would not be feasible to answer whether a program will run without dynamic information.\\nWe also note that users often include traceback messages as part of their debugging conversations, which provides some runtime information to the model. This behavior may be evidence that users do not expect a tool can reproduce this kind of information itself.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Optimization',\n",
       "        'paragraphs': 'A number of conversations focused on optimizing programs for speed or memory usage. For example:\\n‚Äúcould you please speed up this function as much as possible?‚Äù (C157)\\n‚Äúgive me time estimation for this code‚Ä¶with ryzen 5 5600x‚Ä¶for this batch_size = 1024‚Ä¶give me time estimation with 3 minute of margin error for you‚Ä¶16 gb of memory ?‚Äù (C258)\\n‚Äúmake my thread to process the number faster‚Äù (C307)\\nWhile none of these conversations seem necessarily ‚Äùmisconceived‚Äù, we wondered what expectations users might have about how an LLM-based tool would produce a response. For example, in Python, certain libraries for vectorized computations are almost always faster than algebra on values stored in base Python types like lists. This kind of information is almost surely very salient in the GPT models‚Äô training data, as it is a common subject for help-seeking in online forums. Users seeking optimization help may therefore be motivated by their mental models about an LLM‚Äôs training data, or alternatively from incorrect assumptions about the capabilities of the tool to perform memory profiling.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Code validation',\n",
       "        'paragraphs': 'A relatively common prompt theme was seeking validation that code would meet some requirements (see Table 1 ). Some of these prompts were worded ambiguously, which raised questions about what kinds of validation behaviors were expected. For example, one user (C5) wrote, ‚Äúi have historical data of crypto as csv files [sic] i have following code to train a model on them without merging them, check if code has any problem or anything wrong‚Äù. In another example (C397), a user prompted ‚ÄùReview this an tell me what‚Äôs missing‚Äù, followed by a code block.\\nBecause these prompts are rather underspecified, we cannot be confident exactly what kind of ‚Äùproblems‚Äù or ‚Äùmissing‚Äù elements the user hoped the tool would be able to check for. This style of prompting could possibly stem from about misconceptions about the tool having access to some kind of ‚Äùground truth‚Äù about programs to grade them, similar to an autograder typically found in computer science educational assessments.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Code diffs',\n",
       "        'paragraphs': 'While iterating on versions of a program specified by prompting, users often asked for modifications. Particularly in the context of longer programs, the default behavior of the WildChat models is typically to provide a snippet of code with the modification and instruct the user to update their program. Sometimes, users would follow up with a prompt requesting not just for the snippet, but the ‚Äùfull code‚Äù (for examples, C155 and C306)‚Äîwhich appears to mean, the original program with the modification made in-line.\\nThis request seems to imply a certain degree of trust that the model will reliably reproduce the entire program, only making the intended modification. Because the previous version of the program is only represented in the model‚Äôs context window, it is not actually guaranteed that a deterministic ‚Äùdiff‚Äù will be made (changing only the indicated section). This style of prompting could reflect a user‚Äôs expectation that the model explicitly stores snapshots of code as a kind of version control. On the other hand, it could also be explained as the user trusting that the tool will be sufficiently reliable to reproduce their code, perhaps based on reasoning about the length of the program to modify and the size of the context window.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Sensitivity to context window',\n",
       "        'paragraphs': 'We noted that some conversations involved very long prompts, fully incorporating copy-pastes of previous conversations or very long programs. For example, C11 prompted with a combination of code and what appears to be copies of previous conversation logs totaling 6,021 words (45,345 characters). The model serving this chat was gpt-4-0125-preview , which has a context window of 128,000 tokens‚Äîas such, the prompt would surely fit within the window (in English, a common benchmark is 100 tokens to 75 words). However, model performance is known to be highly sensitive to use of the context window, and using more of the context window is not necessarily better. Prompts of this size could be an informed and deliberate choice by the user, or may be related to the ‚Äùmonotonicity belief‚Äù reported by Jayagopal et\\xa0al. ( 2022 ) .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '7. Discussion',\n",
       "    'paragraphs': 'We begin our discussion with a brief summary of our work. First, we brainstormed potential misconceptions that\\nusers of conversational LLM-assistants may have in programming contexts and identified a crucial distinction: misconceptions may be about the affordances of specific tools with LLM-components, or about LLMs as a class of models. Several papers have demonstrated how LLMs pose challenges for mental model formation (Tankelevitch et\\xa0al., 2024 ; Zamfirescu-Pereira et\\xa0al., 2023 ; Nguyen et\\xa0al., 2024 ) ; here, we propose that the variance in affordances of tools built around LLMs represents an entirely new frontier of complexity for users to navigate.\\nA user might have reasonable awareness of core properties of LLMs, like sensitivity to prompt language and context window usage, but have mistaken ideas about the tools and plugins available to a particular chatbot using an LLM as part of its response mechanism. In our analysis of Python-programming-related conversations from the WildChat dataset, these tool-level misconceptions were most readily apparent. We specifically noted potential confusions around the availability of web access, non-text outputs, code execution, algebraic computations, cross-session memory, system commands to clear the chat and continuous model updates.\\nWe did not observe as strong evidence for what we consider ‚Äùmodel-level‚Äù misconceptions in the log analysis, like confusions about how context windows work. However, this may be because such conceptual errors are less likely to manifest in prompt language directly. Although we note prompts that could indicate misconceptions (or at least suboptimal strategies) about the context window, for example, the user‚Äôs expectations are not as readily clear from very long prompts as when a user instructs the chatbot to use information from a linked GitHub repository.\\nConsidering the kind of misconceptions we observed about the affordances of the WildChat chatbot, it seems that many user confusions could be avoided with clearer communication about some commonly-available features. Importantly, this must be distinct from the model card for the models on the backend‚Äîthe OpenAI API supporting the WildChat conversations could optionally use features like web search or what OpenAI calls ‚Äùtools‚Äù, but these may or may not be configured in the context of a given chatbot. If there are certain features that are reasonably common, like web search or code execution, then a set of icons or a standardized ‚Äùspecs sheet‚Äù clearly indicating them might be present somewhere on the chatbot interface. Importantly, it may not be enough to indicate when a feature is present ‚Äîif users tend to over-estimate the capabilities of chatbots (Prather et\\xa0al., 2023 ; Kazemitabaar et\\xa0al., 2024 ; Ko et\\xa0al., [n.\\u2009d.] ; Goddard et\\xa0al., 2012 ) , it may be more important to signal the absence of commonly assumed features.\\nUsers may attempt to gain this information by attempting to run ‚Äùsystem commands‚Äù through prompts, like asking models ‚ÄùWhat version of GPT are you?‚Äù (our observation that users may instruct a chatbot to ‚Äùclear the chat‚Äù may be another example of a user inferring certain system commands exist). However, these kinds of prompts are unreliable 2 2 2 Interestingly, the WildChat creators note that GPT-4 ‚Äùself-identifies‚Äù differently depending on how it is accessed. Their FAQ states, ‚ÄùDuring most of our data collection period, GPT-4 was trained with information available up to September 2021, preceding its release. Consequently, the chatbot might self-identify as GPT-3 due to the training data cut-off. However, a notable observation is that when interacting through the ChatGPT web interface, GPT-4 accurately recognizes and states its version. This accuracy could be attributed to a system prompt or a hidden context underlying the web interface.‚Äù , and models may ‚Äùself-report‚Äù fabricated information (Chowdhury et\\xa0al., 2025 ) . As such, this information must be clearly communicated through a medium besides LLM-mediated chat interactions.',\n",
       "    'subsections': []},\n",
       "   {'title': '8. Limitations',\n",
       "    'paragraphs': 'Several threats to validity warrant consideration. First, our brainstormed misconceptions are biased by the authors‚Äô professional backgrounds and positionality; researchers from different contexts may prioritize different misconceptions. Second, the WildChat dataset is non-representative by design. Its creators note: ‚ÄùSince our chatbot is hosted on Hugging Face Spaces, the majority of users are likely associated with the IT community.‚Äù Additionally, inferring user intentions or misconceptions from interaction logs is imperfect‚Äîwe lack direct access to user expectations at the time of prompting.\\nFinally, our findings‚Äô usefulness may be limited by the dataset‚Äôs age (collection ended in May 2024). If conversational LLM-based tools have since converged on standard affordances (web access, code execution, cross-session memory), historical user misconceptions may be less relevant. However, we anticipate continued variance in conversational assistant affordances as organizations develop internal tools with distinct cost, security, and user requirements.',\n",
       "    'subsections': []},\n",
       "   {'title': '9. Conclusions',\n",
       "    'paragraphs': 'Based on our present work, we suggest a few directions for further research. To study misconceptions about specific tool affordances (like web search, image generation, code execution), surveying users with true/false questions could gauge the prevalence of specific beliefs. Such surveys must be tailored to specific tools and ensure participants respond about that tool rather than ‚ÄùAI‚Äù generally. Data collection from user logs could also be scaled up with automated detection, using an LLM for one-shot or few-shot classification with our codebook. This kind of analysis could address the prevalence of language suggestive of misconceptions in a corpus.\\nFor other misconceptions, interviews or open-ended surveys may be essential. Asking users directly how they expect an LLM tool to arrive at a response during a debugging session would likely reveal more than log analysis. Behavioral experiments could measure user sensitivity to LLM properties (similar to (Bansal et\\xa0al., 2019 ) ). For instance, users could predict effects of prompt modifications or context window variations, with predictions compared against actual model performance.\\nA critical challenge is that LLM capabilities, tool availability, and user literacy change rapidly. Developing generalizable theories about how users form and test assumptions about programming tools will therefore be essential.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Programming assistants powered by large language models (LLMs) have become widely available, with conversational assistants like ChatGPT proving particularly accessible to less experienced programmers. However, the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search, code execution, or retrieval-augmented generation create opportunities for user misconceptions about what systems can and cannot do. Such misconceptions may lead to over-reliance, unproductive practices, or insufficient quality control in LLM-assisted programming. Here, we aim to characterize misconceptions that users of conversational LLM-based assistants may have in programming contexts. Using a two-phase approach, we first brainstorm and catalog user misconceptions that may occur, and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic Python-programming conversations with an LLM-based chatbot drawn from an openly available dataset. Indeed, we see evidence that some users have misplaced expectations about the availability of LLM-based chatbot features like web access, code execution, or non-text output generation. We also see potential evidence for\\n deeper conceptual issues around the scope of information required to debug, validate, and optimize programs. Our findings reinforce the need for designing LLM-based tools that more clearly communicate their programming capabilities to users.'},\n",
       " '2510.25729v1': {'content': [{'title': 'I Introduction',\n",
       "    'paragraphs': 'Quantitative Microwave Imaging (MWI) is a non-invasive technique with applications in a wide range of areas such as medical diagnostics, and non-destructive testing in agricultural and industrial settings [ 1 , 2 , 3 ] . At its core, MWI is a wavefield modality that has associated with it an electromagnetic inverse scattering problem (ISCP). In an electromagnetic ISCP the objective is to quantitatively reconstruct a map of the dielectric properties, and potentially the dielectric loss, of the inaccessible interior of an object or region of interest (OI/ROI). The data used for this reconstruction are the measured scattered fields when the target OI/ROI is interrogated by a known, and usually controllable, impinging incident field.\\nThe electromagnetic ISCP has associated with it an ill-posed wavefield inverse source problem (ISP), that in addition to being highly sensitive to noise in the measurement data, means that more than one permittivity map will produce the same scattered field, i.e. , the solution to the ISP is non-unique (see, e.g. , [ 4 , 5 ] ). This makes accurate reconstructions particularly challenging and usually requires the augmentation of the acquired scattered-field data with some sort of prior information, e.g. , the use of regularization methods [ 4 , 6 ] . A question arises as to whether the non-uniqueness of the ill-posed wavefield inversion problem can be mirrored in an algorithmic procedure to some benefit, as it has, for example, in the formulation of design problems as an inverse problem [ 7 , 8 , 9 , 10 , 11 ] .\\nTraditional algorithms for the ISCP, such as Contrast Source Inversion (CSI) [ 12 ] and Gauss-Newton Inversion [ 13 , 14 ] , rely on iterative optimization. These approaches typically incorporate a physically rigorous model of the data-acquisition system and discretization of the permittivity map of the target being imaged [ 15 ] makes these techniques computationally expensive. Even with the use of calibration techniques that lessen demands on accurate models of the data-acquisition system, these optimization-based inversion methods require many iterations of a forward solver before converging. In addition, the non-uniqueness of the underlying ISCP means that they don‚Äôt always converge to the true permittivity map for the OI/ROI.\\nIn recent years, machine learning‚Äîparticularly deep learning‚Äîhas gained significant traction for MWI [ 16 ] . Deep learning methods have been applied across nearly all stages of the imaging pipeline, including calibration, post-processing, image enhancement, and inverse problem solving [ 17 ] . Among these tasks, addressing the associated ISCP is clearly the most challenging but can have the greatest impact. Supervised learning approaches attempt to bypass iterative optimization by directly mapping scattered-field measurements to target reconstructions using large datasets of measurement/target pairs [ 18 , 19 , 20 ] . Studies have shown that deep networks can achieve high-quality reconstructions and even enable real-time imaging [ 20 ] , particularly when combined with physics-informed pre-processing such as the Born approximation or electromagnetic backpropagation [ 21 , 22 , 23 ] .\\nA critical limitation of both traditional inversion techniques and deterministic Machine-Learning (ML) frameworks is that, by definition, they output a single reconstruction for a given input, neglecting the fundamental non-uniqueness of the electromagnetic inverse problem. In reality, multiple plausible target permittivity maps may correspond to the same measurement data [ 5 , 4 ] . In deterministic machine-learning frameworks this one-to-one map is ultimately created by the training data set and thereby limits the trained ML model‚Äôs generalizability to making predictions on scattered-field data corresponding to unseen targets that are too far away from the training data. That is, it is widely recognized that such ML models are highly sensitive to ‚Äúdomain gaps‚Äù [ 24 ] .\\nIn addition to the deterministic nature of many supervised ML models, there is the related issue that these approaches require large datasets for training. To address this, researchers have employed generative models such as Generative Adversarial Networks (GANs) and diffusion models to create large labeled datasets for subsequent use in supervised learning [ 25 ] . For instance, Shao et al. [ 26 ] combined a GAN with a deep neural network to approximate electromagnetic scattering in microwave breast imaging, thereby generating additional training data.\\nAlthough synthetic datasets are convenient, they often fail to capture practical complexities such as antenna coupling, calibration errors, and hardware imperfections [ 27 ] . Therefore, although large synthetically generated datasets have indeed been used for training, resulting ML models typically require either augmenting the training set with experimental data or ‚Äúcalibrating out‚Äù the experimental setup to make the data more representative of synthetic conditions. Otherwise, networks trained exclusively on synthetic data often struggle to generalize to experimental measurements acquired using unique lab-specific systems. It should be commented that currently there does not exist a standardized MWI data-acquisition setup‚Äìmost researchers have developed there own unique systems.\\nThus, the gap between synthetic training data and experimental measurements remains a major barrier to deploying generalizable deep learning‚Äìbased MWI systems. On the other hand, generative approaches are inherently less sensitive to domain discrepancies, as they learn the underlying data manifold rather than a deterministic mapping, thereby mitigating the impact of distributional shifts between training and testing datasets.\\nMost recently, researchers have leveraged the inherent non-uniqueness of generative models to tackle the non-unique nature of inverse problems. Early efforts predominantly employed GANs and VAEs. For example, Bhadra et al. introduced an image-adaptive GAN framework that allows high-fidelity reconstruction of under-sampled MRI data and improved data consistency in ill-posed inverse imaging problems [ 28 ] . Similarly, Yao et al. [ 29 ] introduced a conditional GAN framework to map scattered electromagnetic fields directly to dielectric contrasts, enabling real-time image reconstruction. Despite these advancements, GAN-based approaches often suffer from training instabilities, mode collapse, and limited control over output diversity.\\nTo overcome these limitations, diffusion and score-based generative models have emerged as more robust, theoretically grounded alternatives. These models have demonstrated state-of-the-art performance across a broad range of imaging tasks, including synthesis, reconstruction, and enhancement [ 30 , 31 , 32 ] . Compared with GANs and VAEs, diffusion-based approaches exhibit stable optimization dynamics, provide well-defined likelihood formulations, and yield superior generative fidelity. Accumulating evidence indicates that they consistently surpass earlier generative paradigms in various medical imaging applications [ 30 , 31 , 33 , 34 ] .\\nIn the domain of medical image reconstruction, Song et al. [ 34 ] proposed a score-based generative framework capable of reconstructing medical images from partial CT and MRI measurements in an unsupervised manner, achieving strong generalization across diverse measurement processes. Nonetheless, applications of such models as inverse solvers for MWI remain limited. Recently, Bi et al. [ 35 ] introduced DiffusionEMIS, a diffusion-based method that iteratively refines 3-D point clouds to reconstruct scatterer geometries conditioned on measured scattered fields.\\nBuilding on these advances, we propose a conditional diffusion‚Äëbased generative model that explicitly incorporates the non‚Äëunique nature of the microwave inverse scattering problem. Unlike deterministic supervised networks that produce a single estimate for a given set of measurements, our framework generates multiple plausible reconstructions consistent with the same data, thereby reflecting the inherent non-uniqueness of the ill-posed problem and leveraging the strengths of diffusion models to produce physically meaningful solutions. The core innovation of our approach lies in the integration of a physics-based selection mechanism, transforming the framework into a physics-informed generative system. After the diffusion model produces multiple candidate reconstructions, a forward electromagnetic solver is applied to each candidate to predict the corresponding scattered fields. The reconstruction yielding the lowest data discrepancy with respect to the measured fields is reported as the final solution. This physics-guided validation ensures that the chosen reconstruction is not only statistically plausible, based on the conditioning data, but also physically consistent with the underlying electromagnetic principles.\\nThis integration of a forward solver marks a departure from purely data-driven approaches, forming a hybrid architecture that combines the generative flexibility of diffusion models with the physical accuracy of electromagnetic modeling. By embedding physics-based validation directly within the inference process, our method effectively bridges the gap between machine learning efficiency and the physics of electromagnetic interactions with the target, addressing one of the major limitations of deep learning-based inverse solvers.\\nThe proposed methodology is particularly well-suited for data collected using actual MWI systems, where measurement noise, calibration errors, and model mismatches often degrade performance. By generating multiple candidate solutions and selecting the most physically consistent one, our approach achieves enhanced robustness and generalization. We validate the method using both synthetic and experimental datasets, demonstrating improved reconstruction accuracy and stability compared to deterministic baselines.\\nTo address the challenges outlined above, the remainder of this paper is organized as follows. Section II provides an overview of the methodology and fundamental concepts behind diffusion models, including the forward and reverse processes and the use of diffusion priors for inverse problems. Section III introduces the proposed latent diffusion framework, which integrates a latent autoencoder representation, a physics-aware conditioning mechanism, and an end-to-end inversion architecture augmented by a physics-based selection strategy. Section IV describes the dataset used for training and evaluation, followed by Section V, which presents the experimental results, including reconstruction consistency on synthetic data, generalization to experimental measurements, multi-frequency inversion, and performance evaluation of model error. Finally, Section VI concludes the paper with key findings and future directions, and Section VII acknowledges supporting contributions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'II Methodology',\n",
       "    'paragraphs': 'Diffusion models aim to approximate a target data distribution by constructing a forward process that incrementally adds Gaussian noise and a reverse process that learns to remove this noise. Given a clean sample x 0 ‚àº p data x_{0}\\\\sim p_{\\\\mathrm{data}} , the forward Markov chain { x t } t = 1 T \\\\{x_{t}\\\\}_{t=1}^{T} evolves as\\nwhere the noise schedule { Œ≤ t } \\\\{\\\\beta_{t}\\\\} governs how quickly the signal is corrupted. As t t increases, x t x_{t} approaches an isotropic Gaussian.\\nGeneration proceeds by starting from a Gaussian x T rev ‚àº ùí© \\u200b ( 0 , I ) x^{\\\\mathrm{rev}}_{T}\\\\sim\\\\mathcal{N}(0,I) and iteratively denoising back to x 0 rev x^{\\\\mathrm{rev}}_{0} :\\nSampling algorithms for this reverse chain‚Äîdeterministic or stochastic‚Äîcan be interpreted as discretizations of underlying ODEs or SDEs. They rely on the score function s t ‚ãÜ \\u200b ( x ) = ‚àá x log \\u2061 p x t \\u200b ( x ) s_{t}^{\\\\star}(x)=\\\\nabla_{x}\\\\log p_{x_{t}}(x) of the intermediate distributions. Tweedie‚Äôs formula provides an explicit expression,\\nwith Œ± t := 1 ‚àí Œ≤ t \\\\alpha_{t}:=1-\\\\beta_{t} and Œ± t := ‚àè k = 1 t Œ± k \\\\alpha_{t}:=\\\\prod_{k=1}^{t}\\\\alpha_{k} . In practice, one trains a neural network to approximate these score functions via score matching, enabling the learned model to generate samples from p data p_{\\\\mathrm{data}} .\\nThe flexibility of diffusion models makes them attractive as priors for ill‚Äëposed inverse problems. Suppose we observe data y y generated from an unknown signal x ‚ãÜ x^{\\\\star} through a known forward operator and additive noise. A Bayesian formulation samples from the posterior\\nwhere p prior \\u200b ( x ) p_{\\\\mathrm{prior}}(x) is the diffusion‚Äëmodel prior and p \\u200b ( y ‚à£ x ) p(y\\\\mid x) is the likelihood. Sampling from this posterior combines the learned score functions with the physics of the measurement process.\\nWe consider the electromagnetic inverse problem of reconstructing two-dimensional permittivity distributions from scattered field measurements. The scattered fields are represented as two channels, corresponding to the real and imaginary components of the electric field. To overcome the non-uniqueness of the inverse problem, we design a generative inversion framework based on a conditional diffusion model operating in a compact latent space of permittivity maps. This framework produces multiple plausible reconstructions that are driven by, i.e. , consistent with, the measured fields, after which a post-processing step is applied to identify the most physically meaningful solution.\\nDirectly applying a diffusion model to full-resolution permittivity maps is computationally expensive, particularly when extending this work to 3D medical imaging applications, which represent the ultimate goal of our research. To address this, we first train an autoencoder (AE) to learn a compact latent representation, following a design similar to the one used in [ 36 ] .\\nEncoder (E): compresses the permittivity grid x x into a latent representation z = E \\u200b ( x ) z=E(x) using convolution and downsampling layers.\\nDecoder (D): reconstructs the grid from the latent code as x ~ = D \\u200b ( z ) = D \\u200b ( E \\u200b ( x ) ) \\\\tilde{x}=D(z)=D(E(x)) using upsampling operations.\\nThe AE is trained with a composite loss function:\\nReconstruction loss (pixelwise error) ensures quantitative fidelity to the input.\\nPerceptual loss [ 37 ] , computed from intermediate features of a pretrained VGG16 network [ 38 ] , encourages preservation of edges and structural features.\\nAdversarial loss [ 39 ] penalizes overly smooth reconstructions and promotes realistic textural detail.\\nThe relative weights of these terms are tuned to balance numerical accuracy with perceptual quality. By compressing a 100√ó100 grid into a 16√ó24√ó24 latent vector, the AE reduces the computational cost of the subsequent diffusion process.\\nThe learned latent space serves as the domain for our diffusion model. The model is conditioned on measured scattered fields collected in the scenario depicted in Fig. xxxx where a dielectric target is located at the center of a ring of transmitter and receivers. The transmitters and receivers are located at equidistant points on a circle of radius XXX m surrounding the target. A complete descriprion can be found in [ 40 ] . This condition ensures that the generated permittivity maps are physically consistent with the observations. The data is composed of a 24√ó24 scattered-field matrix of complex-valued data representing the real and imaginary parts of the received frequency-domain phasor. Data consists of up to five frequencies collected at 3.0, 3.5, 4.0, 4.5, and 5.0 GHz.\\nForward diffusion: Gaussian noise is added to latent vectors over a predefined schedule.\\nReverse denoising: A U-Net‚Äìbased denoiser is trained to iteratively predict and remove this noise. The denoiser is conditioned on the scattered fields via cross-attention layers.\\nConditioning mechanism: In the proposed framework, the scattered electromagnetic fields are initially processed through convolutional layers to project them into a feature space. Because electromagnetic inverse problems depend on the spatial arrangement of transmitters, receivers, and objects, it is vital to retain positional information‚Äîsomething that standard convolution and pooling layers tend to lose. To address this, we augment the 24√ó24 scattered-field inputs with sinusoidal positional encodings that explicitly encode transmitters coordinates. Injecting these encodings into the network provides an absolute reference frame, enabling the model to distinguish sensor locations and object orientations. This spatial awareness is especially important for microwave imaging, where the geometry of the scene strongly influences the observed scattering.\\nThese features are then injected back into the diffusion model‚Äôs denoiser at every step: they are concatenated with intermediate feature maps and linked via cross-attention blocks so that the denoising operation remains conditioned on the measured electromagnetic response. By coupling the generative model to the observed physics in this way, we encourage it to produce reconstructions that are physically consistent with the actual scattered fields.\\nArchitecture: The U-Net consists of downsampling and upsampling blocks with intermediate attention [ 41 ] . Time embeddings indicate the current noise level, while projected scattered field features provide physical guidance.\\nThe denoiser is trained using a Mean-Squared-Error (MSE) between the true and predicted noise. This objective allows the network to implicitly learn the conditional distribution of latent permittivity representations.\\nThe full reconstruction pipeline proceeds as follows:\\nEncoding: The AE encoder maps the permittivity grid to a latent representation.\\nDiffusion training: The conditional diffusion model learns to denoise noisy latent codes, conditioned on scattered fields.\\nSampling: For unseen measurements, the diffusion model generates latent permittivity representations driven by the observed fields.\\nDecoding: The AE decoder reconstructs the full-resolution permittivity grid.\\nThis framework, illustrated in Fig. 1 , combines the representational efficiency of autoencoders with the generative power of diffusion models, resulting in a distribution of generated high-quality reconstructions of permittivity maps from electromagnetic data.\\nThe conditional diffusion model produces a set of reconstructions for one scattered field data that are broadly consistent with the measured scattered fields, mirroring the non-uniqueness of the ill-posed inverse problem. That is, after training, the diffusion model is used to make several inferences for each measurement, generating a set of candidate permittivity maps. To choose between the reconstructed candidates, we introduce a post-processing stage designed to select the most physically meaningful reconstruction. Specifically, each candidate is passed through a forward solver to compute the scattered fields it would produce, illustrated in part b of Fig. 1 . We compute the MSE between the simulated and measured scattered fields, choosing the candidate permittivity map that produces the minimum scattered-field MSE, providing a global measure of accuracy. By relying exclusively on scattered-field MSE we do not use any prior information regarding the actual permittivity map.\\nIn summary, the conditional diffusion model ensures that the candidate permittivity maps appear physically reasonable, and the forward-solver selection process ensures that the corresponding scattered fields closely match the experimental measurements. This paradigm opens the door for post-processing selection procedures based on different criteria and/or other available prior information, e.g , smoothness of the permitivity map.\\nTwo datasets are used in this study. The first dataset, DataSet1, is the publicly available benchmark presented by Cathers et al. [ 42 ] , which includes both synthetic and experimental measurements. It contains scattering data for Nylon-66 cylinders with diameters of 3.8 cm and 10.2 cm (reported permittivity: œµ r = 3.03 ‚àí j \\u200b 0.03 \\\\epsilon_{r}=3.03-j0.03 ) and a complex-shaped object referred to as the E-phantom, machined from an ultra-high molecular weight polyethylene (UHMWP) block (reported permittivity: œµ r = 2.3 \\\\epsilon_{r}=2.3 [ 43 ] ). The cylinders were translated spatially to produce multiple configurations within the imaging domain, while the E-phantom was both translated and rotated to generate a diverse set of measurement data. For each target, the dataset provides both synthetic scattered fields, computed using a 2D scalar Method-of-Moments (MoM) forward solver based on Richmond‚Äôs method [ 44 ] on a 100 √ó 100 permittivity grid, and calibrated experimental measurements acquired under comparable conditions [ 45 , 40 ] . Representative examples of the synthetic and experimental data for each target are illustrated in first column of Figure 2 .\\nAlthough DataSet1, obtained from [ 45 , 42 ] , provides valuable benchmarking data, it includes a limited number targets displaying a restricted class of scattering features. This limits one‚Äôs ability to properly investigate and evaluate an machine-learning model‚Äôs robustness and generalizability as an inverse solver. To address this limitation and assess the robustness of the proposed inverse solver, we created a new dataset comprising 2000 samples of dielectric targets (DataSet2). The scattered fields were generated under ideal 2D point-source illumination (3D line sources) using the same 2D scalar MoM forward solver on a 100 √ó 100 permittivity grid. This dataset introduces composite dielectric targets made up of random configurations of basic canonical dielectric shapes. The canonical dielectric shapes are circles of varying sizes, hollow circles, and U-shapes of different dimensions. Although the permittivity of each is uniform across the shape, one of two possible values of permittivity are chosen, either Œµ r = 2.3 \\\\varepsilon_{r}=2.3 or Œµ r = 3.03 \\\\varepsilon_{r}=3.03 , for any particular shape. Each canonical shape introduces challenging scattering features of its own, e.g. , the hollow circles introduces the feature of penetrability of energy, and multiple scattering from within the target. The U-shaped targets introduce direction-based, or anisotropic, scattering. Both have stronger frequency-dependent scattering features than the simple solid circles of DataSet1. Although the E-phantom of DataSet1 is approximately a combination of overlapping canonical U-shapes, in the E-phantom these are fixed with respect to their relative positions.\\nThe positions, sizes, and orientations of the canonical shapes were randomly varied to create diverse spatial configurations. In addition, each composite target was created using up to four canonical shapes positioned at random within the grid. This produces random overlapping intersections of the shapes, further increasing the overall geometric complexity of the final composite scatterer. Some representative composite targets in DataSet2 are depicted in Figs. 6 , 7 and 8 . These two datasets enable a comprehensive variation of scattered fields allowing us to evaluate the performance of the proposed ML model.',\n",
       "    'subsections': [{'title': 'II-A Diffusion Models',\n",
       "      'paragraphs': 'Diffusion models aim to approximate a target data distribution by constructing a forward process that incrementally adds Gaussian noise and a reverse process that learns to remove this noise. Given a clean sample x 0 ‚àº p data x_{0}\\\\sim p_{\\\\mathrm{data}} , the forward Markov chain { x t } t = 1 T \\\\{x_{t}\\\\}_{t=1}^{T} evolves as\\nwhere the noise schedule { Œ≤ t } \\\\{\\\\beta_{t}\\\\} governs how quickly the signal is corrupted. As t t increases, x t x_{t} approaches an isotropic Gaussian.\\nGeneration proceeds by starting from a Gaussian x T rev ‚àº ùí© \\u200b ( 0 , I ) x^{\\\\mathrm{rev}}_{T}\\\\sim\\\\mathcal{N}(0,I) and iteratively denoising back to x 0 rev x^{\\\\mathrm{rev}}_{0} :\\nSampling algorithms for this reverse chain‚Äîdeterministic or stochastic‚Äîcan be interpreted as discretizations of underlying ODEs or SDEs. They rely on the score function s t ‚ãÜ \\u200b ( x ) = ‚àá x log \\u2061 p x t \\u200b ( x ) s_{t}^{\\\\star}(x)=\\\\nabla_{x}\\\\log p_{x_{t}}(x) of the intermediate distributions. Tweedie‚Äôs formula provides an explicit expression,\\nwith Œ± t := 1 ‚àí Œ≤ t \\\\alpha_{t}:=1-\\\\beta_{t} and Œ± t := ‚àè k = 1 t Œ± k \\\\alpha_{t}:=\\\\prod_{k=1}^{t}\\\\alpha_{k} . In practice, one trains a neural network to approximate these score functions via score matching, enabling the learned model to generate samples from p data p_{\\\\mathrm{data}} .\\nThe flexibility of diffusion models makes them attractive as priors for ill‚Äëposed inverse problems. Suppose we observe data y y generated from an unknown signal x ‚ãÜ x^{\\\\star} through a known forward operator and additive noise. A Bayesian formulation samples from the posterior\\nwhere p prior \\u200b ( x ) p_{\\\\mathrm{prior}}(x) is the diffusion‚Äëmodel prior and p \\u200b ( y ‚à£ x ) p(y\\\\mid x) is the likelihood. Sampling from this posterior combines the learned score functions with the physics of the measurement process.',\n",
       "      'subsections': [{'title': 'Forward and reverse processes.',\n",
       "        'paragraphs': 'Diffusion models aim to approximate a target data distribution by constructing a forward process that incrementally adds Gaussian noise and a reverse process that learns to remove this noise. Given a clean sample x 0 ‚àº p data x_{0}\\\\sim p_{\\\\mathrm{data}} , the forward Markov chain { x t } t = 1 T \\\\{x_{t}\\\\}_{t=1}^{T} evolves as\\nwhere the noise schedule { Œ≤ t } \\\\{\\\\beta_{t}\\\\} governs how quickly the signal is corrupted. As t t increases, x t x_{t} approaches an isotropic Gaussian.\\nGeneration proceeds by starting from a Gaussian x T rev ‚àº ùí© \\u200b ( 0 , I ) x^{\\\\mathrm{rev}}_{T}\\\\sim\\\\mathcal{N}(0,I) and iteratively denoising back to x 0 rev x^{\\\\mathrm{rev}}_{0} :\\nSampling algorithms for this reverse chain‚Äîdeterministic or stochastic‚Äîcan be interpreted as discretizations of underlying ODEs or SDEs. They rely on the score function s t ‚ãÜ \\u200b ( x ) = ‚àá x log \\u2061 p x t \\u200b ( x ) s_{t}^{\\\\star}(x)=\\\\nabla_{x}\\\\log p_{x_{t}}(x) of the intermediate distributions. Tweedie‚Äôs formula provides an explicit expression,\\nwith Œ± t := 1 ‚àí Œ≤ t \\\\alpha_{t}:=1-\\\\beta_{t} and Œ± t := ‚àè k = 1 t Œ± k \\\\alpha_{t}:=\\\\prod_{k=1}^{t}\\\\alpha_{k} . In practice, one trains a neural network to approximate these score functions via score matching, enabling the learned model to generate samples from p data p_{\\\\mathrm{data}} .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Diffusion priors for inverse problems.',\n",
       "        'paragraphs': 'The flexibility of diffusion models makes them attractive as priors for ill‚Äëposed inverse problems. Suppose we observe data y y generated from an unknown signal x ‚ãÜ x^{\\\\star} through a known forward operator and additive noise. A Bayesian formulation samples from the posterior\\nwhere p prior \\u200b ( x ) p_{\\\\mathrm{prior}}(x) is the diffusion‚Äëmodel prior and p \\u200b ( y ‚à£ x ) p(y\\\\mid x) is the likelihood. Sampling from this posterior combines the learned score functions with the physics of the measurement process.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'II-B Proposed Latent Diffusion Model',\n",
       "      'paragraphs': 'We consider the electromagnetic inverse problem of reconstructing two-dimensional permittivity distributions from scattered field measurements. The scattered fields are represented as two channels, corresponding to the real and imaginary components of the electric field. To overcome the non-uniqueness of the inverse problem, we design a generative inversion framework based on a conditional diffusion model operating in a compact latent space of permittivity maps. This framework produces multiple plausible reconstructions that are driven by, i.e. , consistent with, the measured fields, after which a post-processing step is applied to identify the most physically meaningful solution.\\nDirectly applying a diffusion model to full-resolution permittivity maps is computationally expensive, particularly when extending this work to 3D medical imaging applications, which represent the ultimate goal of our research. To address this, we first train an autoencoder (AE) to learn a compact latent representation, following a design similar to the one used in [ 36 ] .\\nEncoder (E): compresses the permittivity grid x x into a latent representation z = E \\u200b ( x ) z=E(x) using convolution and downsampling layers.\\nDecoder (D): reconstructs the grid from the latent code as x ~ = D \\u200b ( z ) = D \\u200b ( E \\u200b ( x ) ) \\\\tilde{x}=D(z)=D(E(x)) using upsampling operations.\\nThe AE is trained with a composite loss function:\\nReconstruction loss (pixelwise error) ensures quantitative fidelity to the input.\\nPerceptual loss [ 37 ] , computed from intermediate features of a pretrained VGG16 network [ 38 ] , encourages preservation of edges and structural features.\\nAdversarial loss [ 39 ] penalizes overly smooth reconstructions and promotes realistic textural detail.\\nThe relative weights of these terms are tuned to balance numerical accuracy with perceptual quality. By compressing a 100√ó100 grid into a 16√ó24√ó24 latent vector, the AE reduces the computational cost of the subsequent diffusion process.\\nThe learned latent space serves as the domain for our diffusion model. The model is conditioned on measured scattered fields collected in the scenario depicted in Fig. xxxx where a dielectric target is located at the center of a ring of transmitter and receivers. The transmitters and receivers are located at equidistant points on a circle of radius XXX m surrounding the target. A complete descriprion can be found in [ 40 ] . This condition ensures that the generated permittivity maps are physically consistent with the observations. The data is composed of a 24√ó24 scattered-field matrix of complex-valued data representing the real and imaginary parts of the received frequency-domain phasor. Data consists of up to five frequencies collected at 3.0, 3.5, 4.0, 4.5, and 5.0 GHz.\\nForward diffusion: Gaussian noise is added to latent vectors over a predefined schedule.\\nReverse denoising: A U-Net‚Äìbased denoiser is trained to iteratively predict and remove this noise. The denoiser is conditioned on the scattered fields via cross-attention layers.\\nConditioning mechanism: In the proposed framework, the scattered electromagnetic fields are initially processed through convolutional layers to project them into a feature space. Because electromagnetic inverse problems depend on the spatial arrangement of transmitters, receivers, and objects, it is vital to retain positional information‚Äîsomething that standard convolution and pooling layers tend to lose. To address this, we augment the 24√ó24 scattered-field inputs with sinusoidal positional encodings that explicitly encode transmitters coordinates. Injecting these encodings into the network provides an absolute reference frame, enabling the model to distinguish sensor locations and object orientations. This spatial awareness is especially important for microwave imaging, where the geometry of the scene strongly influences the observed scattering.\\nThese features are then injected back into the diffusion model‚Äôs denoiser at every step: they are concatenated with intermediate feature maps and linked via cross-attention blocks so that the denoising operation remains conditioned on the measured electromagnetic response. By coupling the generative model to the observed physics in this way, we encourage it to produce reconstructions that are physically consistent with the actual scattered fields.\\nArchitecture: The U-Net consists of downsampling and upsampling blocks with intermediate attention [ 41 ] . Time embeddings indicate the current noise level, while projected scattered field features provide physical guidance.\\nThe denoiser is trained using a Mean-Squared-Error (MSE) between the true and predicted noise. This objective allows the network to implicitly learn the conditional distribution of latent permittivity representations.\\nThe full reconstruction pipeline proceeds as follows:\\nEncoding: The AE encoder maps the permittivity grid to a latent representation.\\nDiffusion training: The conditional diffusion model learns to denoise noisy latent codes, conditioned on scattered fields.\\nSampling: For unseen measurements, the diffusion model generates latent permittivity representations driven by the observed fields.\\nDecoding: The AE decoder reconstructs the full-resolution permittivity grid.\\nThis framework, illustrated in Fig. 1 , combines the representational efficiency of autoencoders with the generative power of diffusion models, resulting in a distribution of generated high-quality reconstructions of permittivity maps from electromagnetic data.\\nThe conditional diffusion model produces a set of reconstructions for one scattered field data that are broadly consistent with the measured scattered fields, mirroring the non-uniqueness of the ill-posed inverse problem. That is, after training, the diffusion model is used to make several inferences for each measurement, generating a set of candidate permittivity maps. To choose between the reconstructed candidates, we introduce a post-processing stage designed to select the most physically meaningful reconstruction. Specifically, each candidate is passed through a forward solver to compute the scattered fields it would produce, illustrated in part b of Fig. 1 . We compute the MSE between the simulated and measured scattered fields, choosing the candidate permittivity map that produces the minimum scattered-field MSE, providing a global measure of accuracy. By relying exclusively on scattered-field MSE we do not use any prior information regarding the actual permittivity map.\\nIn summary, the conditional diffusion model ensures that the candidate permittivity maps appear physically reasonable, and the forward-solver selection process ensures that the corresponding scattered fields closely match the experimental measurements. This paradigm opens the door for post-processing selection procedures based on different criteria and/or other available prior information, e.g , smoothness of the permitivity map.',\n",
       "      'subsections': [{'title': 'II-B1 Latent Representation with Autoencoder',\n",
       "        'paragraphs': 'Directly applying a diffusion model to full-resolution permittivity maps is computationally expensive, particularly when extending this work to 3D medical imaging applications, which represent the ultimate goal of our research. To address this, we first train an autoencoder (AE) to learn a compact latent representation, following a design similar to the one used in [ 36 ] .\\nEncoder (E): compresses the permittivity grid x x into a latent representation z = E \\u200b ( x ) z=E(x) using convolution and downsampling layers.\\nDecoder (D): reconstructs the grid from the latent code as x ~ = D \\u200b ( z ) = D \\u200b ( E \\u200b ( x ) ) \\\\tilde{x}=D(z)=D(E(x)) using upsampling operations.\\nThe AE is trained with a composite loss function:\\nReconstruction loss (pixelwise error) ensures quantitative fidelity to the input.\\nPerceptual loss [ 37 ] , computed from intermediate features of a pretrained VGG16 network [ 38 ] , encourages preservation of edges and structural features.\\nAdversarial loss [ 39 ] penalizes overly smooth reconstructions and promotes realistic textural detail.\\nThe relative weights of these terms are tuned to balance numerical accuracy with perceptual quality. By compressing a 100√ó100 grid into a 16√ó24√ó24 latent vector, the AE reduces the computational cost of the subsequent diffusion process.',\n",
       "        'subsections': []},\n",
       "       {'title': 'II-B2 Physics‚ÄëAware Conditioning Mechanism',\n",
       "        'paragraphs': 'The learned latent space serves as the domain for our diffusion model. The model is conditioned on measured scattered fields collected in the scenario depicted in Fig. xxxx where a dielectric target is located at the center of a ring of transmitter and receivers. The transmitters and receivers are located at equidistant points on a circle of radius XXX m surrounding the target. A complete descriprion can be found in [ 40 ] . This condition ensures that the generated permittivity maps are physically consistent with the observations. The data is composed of a 24√ó24 scattered-field matrix of complex-valued data representing the real and imaginary parts of the received frequency-domain phasor. Data consists of up to five frequencies collected at 3.0, 3.5, 4.0, 4.5, and 5.0 GHz.\\nForward diffusion: Gaussian noise is added to latent vectors over a predefined schedule.\\nReverse denoising: A U-Net‚Äìbased denoiser is trained to iteratively predict and remove this noise. The denoiser is conditioned on the scattered fields via cross-attention layers.\\nConditioning mechanism: In the proposed framework, the scattered electromagnetic fields are initially processed through convolutional layers to project them into a feature space. Because electromagnetic inverse problems depend on the spatial arrangement of transmitters, receivers, and objects, it is vital to retain positional information‚Äîsomething that standard convolution and pooling layers tend to lose. To address this, we augment the 24√ó24 scattered-field inputs with sinusoidal positional encodings that explicitly encode transmitters coordinates. Injecting these encodings into the network provides an absolute reference frame, enabling the model to distinguish sensor locations and object orientations. This spatial awareness is especially important for microwave imaging, where the geometry of the scene strongly influences the observed scattering.\\nThese features are then injected back into the diffusion model‚Äôs denoiser at every step: they are concatenated with intermediate feature maps and linked via cross-attention blocks so that the denoising operation remains conditioned on the measured electromagnetic response. By coupling the generative model to the observed physics in this way, we encourage it to produce reconstructions that are physically consistent with the actual scattered fields.\\nArchitecture: The U-Net consists of downsampling and upsampling blocks with intermediate attention [ 41 ] . Time embeddings indicate the current noise level, while projected scattered field features provide physical guidance.\\nThe denoiser is trained using a Mean-Squared-Error (MSE) between the true and predicted noise. This objective allows the network to implicitly learn the conditional distribution of latent permittivity representations.',\n",
       "        'subsections': []},\n",
       "       {'title': 'II-B3 End-to-End Inversion Framework',\n",
       "        'paragraphs': 'The full reconstruction pipeline proceeds as follows:\\nEncoding: The AE encoder maps the permittivity grid to a latent representation.\\nDiffusion training: The conditional diffusion model learns to denoise noisy latent codes, conditioned on scattered fields.\\nSampling: For unseen measurements, the diffusion model generates latent permittivity representations driven by the observed fields.\\nDecoding: The AE decoder reconstructs the full-resolution permittivity grid.\\nThis framework, illustrated in Fig. 1 , combines the representational efficiency of autoencoders with the generative power of diffusion models, resulting in a distribution of generated high-quality reconstructions of permittivity maps from electromagnetic data.',\n",
       "        'subsections': []},\n",
       "       {'title': 'II-B4 Physic Based Selection',\n",
       "        'paragraphs': 'The conditional diffusion model produces a set of reconstructions for one scattered field data that are broadly consistent with the measured scattered fields, mirroring the non-uniqueness of the ill-posed inverse problem. That is, after training, the diffusion model is used to make several inferences for each measurement, generating a set of candidate permittivity maps. To choose between the reconstructed candidates, we introduce a post-processing stage designed to select the most physically meaningful reconstruction. Specifically, each candidate is passed through a forward solver to compute the scattered fields it would produce, illustrated in part b of Fig. 1 . We compute the MSE between the simulated and measured scattered fields, choosing the candidate permittivity map that produces the minimum scattered-field MSE, providing a global measure of accuracy. By relying exclusively on scattered-field MSE we do not use any prior information regarding the actual permittivity map.\\nIn summary, the conditional diffusion model ensures that the candidate permittivity maps appear physically reasonable, and the forward-solver selection process ensures that the corresponding scattered fields closely match the experimental measurements. This paradigm opens the door for post-processing selection procedures based on different criteria and/or other available prior information, e.g , smoothness of the permitivity map.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'II-C Dataset',\n",
       "      'paragraphs': 'Two datasets are used in this study. The first dataset, DataSet1, is the publicly available benchmark presented by Cathers et al. [ 42 ] , which includes both synthetic and experimental measurements. It contains scattering data for Nylon-66 cylinders with diameters of 3.8 cm and 10.2 cm (reported permittivity: œµ r = 3.03 ‚àí j \\u200b 0.03 \\\\epsilon_{r}=3.03-j0.03 ) and a complex-shaped object referred to as the E-phantom, machined from an ultra-high molecular weight polyethylene (UHMWP) block (reported permittivity: œµ r = 2.3 \\\\epsilon_{r}=2.3 [ 43 ] ). The cylinders were translated spatially to produce multiple configurations within the imaging domain, while the E-phantom was both translated and rotated to generate a diverse set of measurement data. For each target, the dataset provides both synthetic scattered fields, computed using a 2D scalar Method-of-Moments (MoM) forward solver based on Richmond‚Äôs method [ 44 ] on a 100 √ó 100 permittivity grid, and calibrated experimental measurements acquired under comparable conditions [ 45 , 40 ] . Representative examples of the synthetic and experimental data for each target are illustrated in first column of Figure 2 .\\nAlthough DataSet1, obtained from [ 45 , 42 ] , provides valuable benchmarking data, it includes a limited number targets displaying a restricted class of scattering features. This limits one‚Äôs ability to properly investigate and evaluate an machine-learning model‚Äôs robustness and generalizability as an inverse solver. To address this limitation and assess the robustness of the proposed inverse solver, we created a new dataset comprising 2000 samples of dielectric targets (DataSet2). The scattered fields were generated under ideal 2D point-source illumination (3D line sources) using the same 2D scalar MoM forward solver on a 100 √ó 100 permittivity grid. This dataset introduces composite dielectric targets made up of random configurations of basic canonical dielectric shapes. The canonical dielectric shapes are circles of varying sizes, hollow circles, and U-shapes of different dimensions. Although the permittivity of each is uniform across the shape, one of two possible values of permittivity are chosen, either Œµ r = 2.3 \\\\varepsilon_{r}=2.3 or Œµ r = 3.03 \\\\varepsilon_{r}=3.03 , for any particular shape. Each canonical shape introduces challenging scattering features of its own, e.g. , the hollow circles introduces the feature of penetrability of energy, and multiple scattering from within the target. The U-shaped targets introduce direction-based, or anisotropic, scattering. Both have stronger frequency-dependent scattering features than the simple solid circles of DataSet1. Although the E-phantom of DataSet1 is approximately a combination of overlapping canonical U-shapes, in the E-phantom these are fixed with respect to their relative positions.\\nThe positions, sizes, and orientations of the canonical shapes were randomly varied to create diverse spatial configurations. In addition, each composite target was created using up to four canonical shapes positioned at random within the grid. This produces random overlapping intersections of the shapes, further increasing the overall geometric complexity of the final composite scatterer. Some representative composite targets in DataSet2 are depicted in Figs. 6 , 7 and 8 . These two datasets enable a comprehensive variation of scattered fields allowing us to evaluate the performance of the proposed ML model.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'III Results',\n",
       "    'paragraphs': 'The latent diffusion model was first trained using only synthetic data from DataSet1 and subsequently evaluated also using only unseen synthetic samples that differed in position and orientation from those in the training set. Figure 3 presents four independent reconstructions conditioned on scattered-field data at 5 GHz from a single instance of the E-phantom. Each of the four candidate solutions shown was generated from a distinct random noise initialization of the diffusion model. As illustrated, the model consistently reproduces the target permittivity distribution with minimal reconstruction error, demonstrating its stability and robustness to stochastic variation in the sampling process.\\nTo identify the best reconstruction, the diffusion model was used to generated 100 candidate permittivity maps, each evaluated using a forward electromagnetic solver to compute the corresponding scattered fields. It should be noted that the choice of 100 reconstructions is arbitrary and can be adjusted according to the time or computational constraints of each application. The reconstruction that was selected as the final result was the one yielding the lowest mean squared error (MSE) between the predicted and ground-truth scattered fields. As illustrated in Figure 2 , the selected reconstruction not only achieves a low MSE in the scattered field domain but also exhibits a low MSE in the permittivity distribution, indicating that the reconstructed permittivity grid closely matches the ground truth in both material properties and resulting electromagnetic behavior.The corresponding MSE values for this part can be found in Table I, row 3.\\nTo assess the proposed model‚Äôs ability to generalize beyond synthetic training data, the trained model was subsequently evaluated on experimental scattered-field measurements from DataSet1. As shown in Figure 4 , the model achieved promising reconstruction performance, successfully recovering key structural features from previously unseen experimental samples. Although the reconstructed permittivity maps were less accurate than those obtained when testing synthetic data, the results demonstrate the model‚Äôs capacity to adapt to experimental measurements, even though it was trained solely on synthetic data.\\nThis generalization result is notable given the significant differences between synthetic and experimental measurements, particularly for the near-field electromagnetic imaging system from which the experimental data was obtained [ 45 ] . Near-field systems, especially those utilizing co-resident antennas, exhibit field behaviors that can differ significantly from the fields generated using, necessarily approximate, computational models. For example, the synthetic training data was generated using the incident fields of idealized point sources (line-soures) and the received fields were simply taken to be scattered fields at precise point-locations. In contrast, the experimental data was collected as microwave scattering parameters (S-parameters) at the antenna ports. An antenna effectively integrates the spatially varying fields across its aperture. In addition, the mutual-coupling with nonactive antennas is ignored in the synthetically generated data. Despite these discrepancies, our ML model was able to deliver strong reconstruction performance. Of course, this is partly due to the scattered-field calibration procedure that was implemented [ 46 , 45 ] . Incorporating the calibration procedure into a machine learning model of its own has been attempted in the past and is a research subject of the greatest importance.\\nResults shown so far were obtained by conditioning the ML model on single frequency scattered-field data. It is well-known that using data from multiple frequencies, either simultaneously or via frequency-hopping, can enhance reconstruction performance especially for non-dispersive targets [ 47 , 48 ] . To investigate how incorporating multiple frequencies improves the ML model‚Äôs ability to learn the relationship between the scattered-field data and the corresponding permittivity distribution, we conditioned the diffusion model on scattered-field measurements acquired at five distinct frequencies‚Äî3.0, 3.5, 4.0, 4.5, and 5.0 GHz‚Äîinstead of a single frequency.\\nIn addition, as previously mentioned, DataSet1 includes a limited range of object types‚Äîspecifically, solid circularly-cylindrical targets and the E-phantom. Each sample in DataSet1 consists of a single object, albeit represented across multiple positions and orientations within the imaging grid. To ensure that the ML model was not over-fitted to a narrow set of object geometries‚Äîa limitation commonly observed in prior studies‚Äîwe developed the more diverse synthetic dataset, Dataset2. It provides a more challenging evaluation of the model‚Äôs generalization ability.\\nThe model was re-trained using this newly generated dataset and tested on previously unseen samples. Representative reconstructions and their corresponding ground-truth targets are shown in Fig. 7 . As expected, the inverse problem associated with this more complex, multi-object dataset presents increased difficulty compared to Dataset\\xa01. Nevertheless, the model successfully reconstructed the underlying permittivity distributions, effectively capturing both the global structure and spatial organization of the objects.\\nTo further enhance the model‚Äôs capacity to learn the mapping between scattered-field measurements and corresponding permittivity distributions, we investigated the integration of multi-frequency data. Specifically, scattered-field measurements were acquired at five frequencies: 3.0, 3.5, 4.0, 4.5, and 5.0\\xa0GHz. Figure 8 illustrates the model‚Äôs performance when conditioned on multi-frequency data, demonstrating improved reconstruction consistency and sharper structural recovery across test samples.\\nThe results indicate that while the single-frequency model occasionally struggles to accurately reconstruct the shapes of closely spaced or sharp-edged objects‚Äîsometimes confusing U-shaped targets with hollow circles (see rows\\xa02 and\\xa04 of Fig. 7 )‚Äîthe multi-frequency model performs substantially better. By leveraging additional frequency-domain information, it accurately resolves fine structural details and preserves object boundaries even in complex spatial configurations.\\nThis improvement is quantitatively reflected in the last two rows of Table I , where the average MSE image \\\\text{MSE}_{\\\\text{image}} values for the multi-frequency case decrease to approximately one-third of the single-frequency error. The averages were computed over 10 test cases, with each case reconstructed 100 times to select the best reconstruction based on the MSE data \\\\text{MSE}_{\\\\text{data}} .\\nThese findings highlight the potential of incorporating richer physical information‚Äîbeyond frequency diversity‚Äîto improve data representation and model robustness. Future research will explore additional labeled datasets encompassing a more comprehensive set of composite scattering targets to further enhance the model‚Äôs capacity for accurate and stable inversion in challenging electromagnetic imaging scenarios.\\nWhile the qualitative evaluation demonstrates substantial improvements over both traditional inverse solvers such as Contrast Source Inversion (CSI) and previously reported state-of-the-art models‚Äîspecifically, the reconstructed images presented in Figures 11‚Äì14 of [ 42 ] ‚Äîa quantitative assessment was also conducted to provide a more comprehensive performance analysis. This evaluation includes comparisons with the state-of-the-art CNN-based supervised inverse solvers previously reported in the literature [ 42 ] and the proposed generative diffusion-based approach. Specifically, the mean squared error (MSE) was evaluated in both the scattered-field domain ( MSE data \\\\text{MSE}_{\\\\text{data}} ) and the image domain ( MSE image \\\\text{MSE}_{\\\\text{image}} ), with averages computed over 10 test samples. The results, summarized in Table I , clearly indicate that the proposed method outperforms state-of-the-art inverse solvers, achieves notable performance gains with multi-frequency data, and exhibits enhanced generalization capability when evaluated on Dataset\\xa02.\\nAs the architecture of the designed ML model is quite complicated it is important to carefully analyze where the inference errors ar occuring. This analysis revealed that the reconstruction error originates not only from the diffusion component but also from the autoencoder (AE) reconstruction stage. The AE-induced error was found to be relatively minor when the model was trained on the simpler DataSet1 containing a limited number of object types, but became more pronounced when trained on the more diverse Dataset2. Table II presents a quantitative comparison of AE performance across both datasets, reporting the average MSE computed over 100 random samples. As shown, the reconstruction error increases for the more complex dataset, reflecting the greater difficulty of compressing and reconstructing diverse geometrical structures. As the diffusion model works on the latent space of the AE, it is inevitable that errors in compressing the permittivity maps into the latent space would degrade the overall performance.\\nFigure 6 qualitatively illustrates the compression error obtained when only the AE component of the model is applied to both datasets. While improving the AE architecture lies beyond the scope of this study, the results show that the compression-related error (1.39%) accounts for approximately 25% of the total reconstruction error (3.99%). In an attempt to mitigate this effect, the AE performance was optimized through hyperparameter tuning, particularly by adjusting the relative weighting of loss terms to balance numerical accuracy with perceptual reconstruction quality. The reported results are for performance after this tuning was implemented.',\n",
       "    'subsections': [{'title': 'III-A Reconstruction Consistency on Synthetic Data',\n",
       "      'paragraphs': 'The latent diffusion model was first trained using only synthetic data from DataSet1 and subsequently evaluated also using only unseen synthetic samples that differed in position and orientation from those in the training set. Figure 3 presents four independent reconstructions conditioned on scattered-field data at 5 GHz from a single instance of the E-phantom. Each of the four candidate solutions shown was generated from a distinct random noise initialization of the diffusion model. As illustrated, the model consistently reproduces the target permittivity distribution with minimal reconstruction error, demonstrating its stability and robustness to stochastic variation in the sampling process.\\nTo identify the best reconstruction, the diffusion model was used to generated 100 candidate permittivity maps, each evaluated using a forward electromagnetic solver to compute the corresponding scattered fields. It should be noted that the choice of 100 reconstructions is arbitrary and can be adjusted according to the time or computational constraints of each application. The reconstruction that was selected as the final result was the one yielding the lowest mean squared error (MSE) between the predicted and ground-truth scattered fields. As illustrated in Figure 2 , the selected reconstruction not only achieves a low MSE in the scattered field domain but also exhibits a low MSE in the permittivity distribution, indicating that the reconstructed permittivity grid closely matches the ground truth in both material properties and resulting electromagnetic behavior.The corresponding MSE values for this part can be found in Table I, row 3.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-B Generalization to Experimental Data',\n",
       "      'paragraphs': 'To assess the proposed model‚Äôs ability to generalize beyond synthetic training data, the trained model was subsequently evaluated on experimental scattered-field measurements from DataSet1. As shown in Figure 4 , the model achieved promising reconstruction performance, successfully recovering key structural features from previously unseen experimental samples. Although the reconstructed permittivity maps were less accurate than those obtained when testing synthetic data, the results demonstrate the model‚Äôs capacity to adapt to experimental measurements, even though it was trained solely on synthetic data.\\nThis generalization result is notable given the significant differences between synthetic and experimental measurements, particularly for the near-field electromagnetic imaging system from which the experimental data was obtained [ 45 ] . Near-field systems, especially those utilizing co-resident antennas, exhibit field behaviors that can differ significantly from the fields generated using, necessarily approximate, computational models. For example, the synthetic training data was generated using the incident fields of idealized point sources (line-soures) and the received fields were simply taken to be scattered fields at precise point-locations. In contrast, the experimental data was collected as microwave scattering parameters (S-parameters) at the antenna ports. An antenna effectively integrates the spatially varying fields across its aperture. In addition, the mutual-coupling with nonactive antennas is ignored in the synthetically generated data. Despite these discrepancies, our ML model was able to deliver strong reconstruction performance. Of course, this is partly due to the scattered-field calibration procedure that was implemented [ 46 , 45 ] . Incorporating the calibration procedure into a machine learning model of its own has been attempted in the past and is a research subject of the greatest importance.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-C Multi-Frequency Inversion and Dataset Enhancement',\n",
       "      'paragraphs': 'Results shown so far were obtained by conditioning the ML model on single frequency scattered-field data. It is well-known that using data from multiple frequencies, either simultaneously or via frequency-hopping, can enhance reconstruction performance especially for non-dispersive targets [ 47 , 48 ] . To investigate how incorporating multiple frequencies improves the ML model‚Äôs ability to learn the relationship between the scattered-field data and the corresponding permittivity distribution, we conditioned the diffusion model on scattered-field measurements acquired at five distinct frequencies‚Äî3.0, 3.5, 4.0, 4.5, and 5.0 GHz‚Äîinstead of a single frequency.\\nIn addition, as previously mentioned, DataSet1 includes a limited range of object types‚Äîspecifically, solid circularly-cylindrical targets and the E-phantom. Each sample in DataSet1 consists of a single object, albeit represented across multiple positions and orientations within the imaging grid. To ensure that the ML model was not over-fitted to a narrow set of object geometries‚Äîa limitation commonly observed in prior studies‚Äîwe developed the more diverse synthetic dataset, Dataset2. It provides a more challenging evaluation of the model‚Äôs generalization ability.\\nThe model was re-trained using this newly generated dataset and tested on previously unseen samples. Representative reconstructions and their corresponding ground-truth targets are shown in Fig. 7 . As expected, the inverse problem associated with this more complex, multi-object dataset presents increased difficulty compared to Dataset\\xa01. Nevertheless, the model successfully reconstructed the underlying permittivity distributions, effectively capturing both the global structure and spatial organization of the objects.\\nTo further enhance the model‚Äôs capacity to learn the mapping between scattered-field measurements and corresponding permittivity distributions, we investigated the integration of multi-frequency data. Specifically, scattered-field measurements were acquired at five frequencies: 3.0, 3.5, 4.0, 4.5, and 5.0\\xa0GHz. Figure 8 illustrates the model‚Äôs performance when conditioned on multi-frequency data, demonstrating improved reconstruction consistency and sharper structural recovery across test samples.\\nThe results indicate that while the single-frequency model occasionally struggles to accurately reconstruct the shapes of closely spaced or sharp-edged objects‚Äîsometimes confusing U-shaped targets with hollow circles (see rows\\xa02 and\\xa04 of Fig. 7 )‚Äîthe multi-frequency model performs substantially better. By leveraging additional frequency-domain information, it accurately resolves fine structural details and preserves object boundaries even in complex spatial configurations.\\nThis improvement is quantitatively reflected in the last two rows of Table I , where the average MSE image \\\\text{MSE}_{\\\\text{image}} values for the multi-frequency case decrease to approximately one-third of the single-frequency error. The averages were computed over 10 test cases, with each case reconstructed 100 times to select the best reconstruction based on the MSE data \\\\text{MSE}_{\\\\text{data}} .\\nThese findings highlight the potential of incorporating richer physical information‚Äîbeyond frequency diversity‚Äîto improve data representation and model robustness. Future research will explore additional labeled datasets encompassing a more comprehensive set of composite scattering targets to further enhance the model‚Äôs capacity for accurate and stable inversion in challenging electromagnetic imaging scenarios.\\nWhile the qualitative evaluation demonstrates substantial improvements over both traditional inverse solvers such as Contrast Source Inversion (CSI) and previously reported state-of-the-art models‚Äîspecifically, the reconstructed images presented in Figures 11‚Äì14 of [ 42 ] ‚Äîa quantitative assessment was also conducted to provide a more comprehensive performance analysis. This evaluation includes comparisons with the state-of-the-art CNN-based supervised inverse solvers previously reported in the literature [ 42 ] and the proposed generative diffusion-based approach. Specifically, the mean squared error (MSE) was evaluated in both the scattered-field domain ( MSE data \\\\text{MSE}_{\\\\text{data}} ) and the image domain ( MSE image \\\\text{MSE}_{\\\\text{image}} ), with averages computed over 10 test samples. The results, summarized in Table I , clearly indicate that the proposed method outperforms state-of-the-art inverse solvers, achieves notable performance gains with multi-frequency data, and exhibits enhanced generalization capability when evaluated on Dataset\\xa02.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-D Performance Evaluation of ML Model‚Äôs Error',\n",
       "      'paragraphs': 'As the architecture of the designed ML model is quite complicated it is important to carefully analyze where the inference errors ar occuring. This analysis revealed that the reconstruction error originates not only from the diffusion component but also from the autoencoder (AE) reconstruction stage. The AE-induced error was found to be relatively minor when the model was trained on the simpler DataSet1 containing a limited number of object types, but became more pronounced when trained on the more diverse Dataset2. Table II presents a quantitative comparison of AE performance across both datasets, reporting the average MSE computed over 100 random samples. As shown, the reconstruction error increases for the more complex dataset, reflecting the greater difficulty of compressing and reconstructing diverse geometrical structures. As the diffusion model works on the latent space of the AE, it is inevitable that errors in compressing the permittivity maps into the latent space would degrade the overall performance.\\nFigure 6 qualitatively illustrates the compression error obtained when only the AE component of the model is applied to both datasets. While improving the AE architecture lies beyond the scope of this study, the results show that the compression-related error (1.39%) accounts for approximately 25% of the total reconstruction error (3.99%). In an attempt to mitigate this effect, the AE performance was optimized through hyperparameter tuning, particularly by adjusting the relative weighting of loss terms to balance numerical accuracy with perceptual reconstruction quality. The reported results are for performance after this tuning was implemented.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'IV Conclusion',\n",
       "    'paragraphs': 'This work presented a physics-informed conditional generative framework for solving the electromagnetic inverse scattering problem in microwave imaging. Leveraging the generative nature of the diffusion model, the proposed approach explicitly captures the inherent uncertainty and non-uniqueness of the inverse problem by producing multiple plausible reconstructions of the permittivity distribution. A forward-solver-based physical evaluation is then employed to select the most consistent reconstruction, ensuring that the final solution aligns with both the measurement data and the underlying electromagnetic physics.\\nThe results demonstrated that the model can accurately reconstruct complex permittivity distributions from synthetic data, achieving stable and low-error reconstructions. When applied to experimental scattered-field measurements‚Äîdespite being trained exclusively on synthetic data‚Äîthe model successfully recovered key structural features, confirming its robustness and generalization capability across measurement domains. The proposed approach outperformed the state-of-the-art deep learning‚Äìbased inverse solvers reported in [ 42 ] , achieving lower reconstruction error and improved structural fidelity in both synthetic and experimental evaluations.\\nIn order to ensure that our model was not overfitted to a limited number of object types‚Äîan issue commonly observed in previous studies‚Äîwe developed a more diverse synthetic dataset. This expanded dataset enabled the model to better generalize to previously unseen geometries.\\nIncorporating multi-frequency scattered-field data further improved reconstruction accuracy, yielding finer structural details and enhanced consistency across test samples. These findings are consistent with prior literature, underscoring the benefits of multi-frequency illumination in improving stability and resolution in microwave imaging.\\nThe results also revealed an increase in the reconstruction error attributable to the autoencoder (AE) component. Although optimizing the compression AE network lies beyond the scope of this study, preliminary hyperparameter tuning and loss-weight adjustments helped balance numerical accuracy and perceptual reconstruction quality.\\nOverall, the proposed diffusion-based framework demonstrates strong potential for robust, data-driven microwave imaging. It achieves superior reconstruction quality relative to existing deep learning baselines and exhibits strong cross-domain generalization from synthetic to experimental data. Future work will extend this approach to full three-dimensional medical imaging and further refine the underlying AE architecture to enhance reconstruction fidelity and computational efficiency.',\n",
       "    'subsections': []},\n",
       "   {'title': 'V Acknowledgment',\n",
       "    'paragraphs': 'The authors acknowledge the help of Mr. Seth Cathers who supplied the forward solver.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'A conditional latent-diffusion based framework for solving the electromagnetic inverse scattering problem associated with microwave imaging is introduced. This generative machine-learning model explicitly mirrors the non-uniqueness of the ill-posed inverse problem. Unlike existing inverse solvers utilizing deterministic machine learning techniques that produce a single reconstruction, the proposed latent-diffusion model generates multiple plausible permittivity maps conditioned on measured scattered-field data, thereby generating several potential instances in the range-space of the non-unique inverse mapping. A forward electromagnetic solver is integrated into the reconstruction pipeline as a physics-based evaluation mechanism. The space of candidate reconstructions form a distribution of possibilities consistent with the conditioning data and the member of this space yielding the lowest scattered-field data discrepancy between the predicted and measured scattered fields is reported as the final solution. Synthetic and experimental labeled datasets are used for training and evaluation of the model. An innovative labeled synthetic dataset is created that exemplifies a varied set of scattering features. Training of the model using this new dataset produces high-quality permittivity reconstructions achieving improved generalization with excellent fidelity to shape recognition. The results highlight the potential of hybrid generative‚Äìphysics frameworks as a promising direction for robust, data-driven microwave imaging.'},\n",
       " '2510.25005v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Most research on counterfactual reasoning (Pawlowski et al. , 2020 ; Sanchez and Tsaftaris, 2022 ; Saha and Garain, 2022 ; Komanduri et al. , 2024 ; Melistas et al. , 2024 ; Wu et al. , 2025 ; K√ºgelgen et al. , 2023 ; Kladny et al. , 2024 ) assumes that the underlying causal structure among variables can be represented by a Directed Acyclic Graph (DAG). However, this acyclicity assumption is often violated in real-world systems. For instance, gene regulatory networks frequently exhibit feedback loops, leading to cyclic dependencies that DAGs cannot capture. Such cycles are integral to the dynamic behavior of biological systems and are crucial for understanding processes like cell differentiation and immune responses.\\nGiven the prevalence of cycles in such systems and the availability of detailed perturbation data, there is a compelling case for extending counterfactual inference frameworks to accommodate cyclic structures.\\nHowever, progress in this area has stalled due to the need for new theoretical breakthroughs, as many properties that hold in acyclic models no longer apply when feedback loops are present.\\nA fundamental issue is that a set of structural equations with cycles may not have a unique solution for the endogenous variables. Solvability refers to the existence of at least one solution (equilibrium), and unique solvability means there is exactly one solution (almost surely). If an SCM is not uniquely solvable ‚Äîi.e., not a simple SCM‚Äîit might generate multiple different outcome distributions or undefined behavior under interventions. Although the class of simple SCMs includes acyclic SCMs as a special case, the theory remains much less developed for the cyclic setting.\\nIn causality, a hard (structural) intervention (Pearl‚Äôs do-operator) (Pearl et al. , 2016 ) sets a variable to a fixed value, severing its dependence on its usual causes. In contrast, shift and scale interventions are types of soft (parametric) interventions that modify the value of a variable by some function (such as adding or multiplying by a constant) without removing its original input links. In particular, a hard intervention is just a degenerate case of a soft intervention (Massidda et al. , 2023 ) .\\nThis generality means one can ask nuanced ‚Äúwhat-if‚Äù questions: What if everyone received 20 % 20\\\\% more of the drug? What if we lowered each student‚Äôs class size by 5? Such policies cannot be represented as a simple d \\u200b o \\u200b ( X = x ) do(X=x) since they depend on individuals‚Äô original X X . Soft interventions are strictly more expressive in defining counterfactual worlds. More specifically, a shift can implement dynamic-like policies (‚Äúincrease dose for those who had high risk‚Äù) that static d \\u200b o do -interventions cannot capture.\\nShift interventions have been used to learn causal cyclic graphs (Rothenh√§usler et al. , 2015 ) , to match a desired causal state (Zhang et al. , 2021 ) . Lorch et al. ( 2024 ) use shift-scale intervention in causal modeling with stationary diffusions.\\nHowever, the theoretical foundations supporting their use remain underdeveloped.\\nIn this work, we develop a theoretical framework for counterfactual inference in cyclic causal models under shift‚Äìscale interventions . Our main contributions are as follows:\\nWe show that SCMs satisfying a global contraction condition are simple ‚Äîi.e., uniquely solvable with respect to every subset of variables‚Äîeven in the presence of cycles.\\nWe prove that under shift‚Äìscale interventions with bounded scale coefficients (i.e., | a j | ‚â§ 1 |a_{j}|\\\\leq 1 ), the intervened twin SCM remains uniquely solvable , ensuring the well-posedness of counterfactual queries.\\nWe establish that this class of shift‚Äìscale interventions is closed under composition , making it algebraically stable for sequential interventional analysis.\\nUnder an additional Lipschitz regularity condition in the exogenous noise , we derive sub-Gaussian tail bounds for counterfactual functionals, showing that the distribution of counterfactual outcomes concentrates sharply around their mean.',\n",
       "    'subsections': [{'title': 'Contributions.',\n",
       "      'paragraphs': 'In this work, we develop a theoretical framework for counterfactual inference in cyclic causal models under shift‚Äìscale interventions . Our main contributions are as follows:\\nWe show that SCMs satisfying a global contraction condition are simple ‚Äîi.e., uniquely solvable with respect to every subset of variables‚Äîeven in the presence of cycles.\\nWe prove that under shift‚Äìscale interventions with bounded scale coefficients (i.e., | a j | ‚â§ 1 |a_{j}|\\\\leq 1 ), the intervened twin SCM remains uniquely solvable , ensuring the well-posedness of counterfactual queries.\\nWe establish that this class of shift‚Äìscale interventions is closed under composition , making it algebraically stable for sequential interventional analysis.\\nUnder an additional Lipschitz regularity condition in the exogenous noise , we derive sub-Gaussian tail bounds for counterfactual functionals, showing that the distribution of counterfactual outcomes concentrates sharply around their mean.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '2 Background and Problem Setup',\n",
       "    'paragraphs': 'This section provides a brief overview of SCMs and establishes the notational framework adopted throughout the paper. Our exposition aligns with the formalism presented in Bongers et al. ( 2021 ) .\\nA Structural Causal Model (SCM) is defined as a tuple\\nwhere ‚Ñê \\\\mathcal{I} denotes a finite set indexing the endogenous variables , ùí• \\\\mathcal{J} denotes a disjoint finite set indexing the exogenous variables , ùí≥ = ‚àè i ‚àà ‚Ñê ùí≥ i \\\\mathcal{X}=\\\\prod_{i\\\\in\\\\mathcal{I}}\\\\mathcal{X}_{i} is the joint domain of the endogenous variables, with each ùí≥ i \\\\mathcal{X}_{i} being a standard measurable space, ‚Ñ∞ = ‚àè j ‚àà ùí• ‚Ñ∞ j \\\\mathcal{E}=\\\\prod_{j\\\\in\\\\mathcal{J}}\\\\mathcal{E}_{j} is the joint domain of the exogenous variables, with each ‚Ñ∞ j \\\\mathcal{E}_{j} also a standard measurable space, f : ùí≥ √ó ‚Ñ∞ ‚Üí ùí≥ f:\\\\mathcal{X}\\\\times\\\\mathcal{E}\\\\to\\\\mathcal{X} is a measurable function representing the causal mechanisms that determine the values of endogenous variables from both endogenous and exogenous inputs, ‚Ñô ‚Ñ∞ = ‚àè j ‚àà ùí• ‚Ñô ‚Ñ∞ j \\\\mathbb{P}_{\\\\mathcal{E}}=\\\\prod_{j\\\\in\\\\mathcal{J}}\\\\mathbb{P}_{\\\\mathcal{E}_{j}} is a product probability measure over ‚Ñ∞ \\\\mathcal{E} , describing the joint distribution of the exogenous variables.\\nA solution of ‚Ñ≥ \\\\mathcal{M} is a pair of random variables ( ùêó , ùêÑ ) (\\\\mathbf{X},\\\\mathbf{E}) defined on a common probability space ( Œ© , ‚Ñ± , ‚Ñô ) (\\\\Omega,\\\\mathcal{F},\\\\mathbb{P}) such that:\\nùêÑ : Œ© ‚Üí ‚Ñ∞ \\\\mathbf{E}:\\\\Omega\\\\!\\\\to\\\\!\\\\mathcal{E} has distribution ‚Ñô ‚Ñ∞ \\\\mathbb{P}_{\\\\mathcal{E}} ;\\nùêó : Œ© ‚Üí ùí≥ \\\\mathbf{X}:\\\\Omega\\\\!\\\\to\\\\!\\\\mathcal{X} satisfies the structural\\nequations\\nFor convenience, we say that a random variable X X is a solution of ‚Ñ≥ \\\\mathcal{M} if there exists an exogenous random variable ùêÑ \\\\mathbf{E} such that the pair ( ùêó , ùêÑ ) (\\\\mathbf{X},\\\\mathbf{E}) constitutes a solution of ‚Ñ≥ \\\\mathcal{M}\\nFor i ‚àà ‚Ñê i\\\\in\\\\mathcal{I} , an index k ‚àà ‚Ñê ‚à™ ùí• k\\\\in\\\\mathcal{I}\\\\cup\\\\mathcal{J} is called a parent of i i iff there does not exist a measurable map f ~ i : ùí≥ ‚àñ k √ó ‚Ñ∞ ‚àñ k ‚Üí ùí≥ i \\\\tilde{f}_{i}:\\\\mathcal{X}_{\\\\setminus k}\\\\times\\\\mathcal{E}_{\\\\setminus k}\\\\to\\\\mathcal{X}_{i} such that for P ‚Ñ∞ P_{\\\\mathcal{E}} -almost every e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} and all x ‚àà ùí≥ x\\\\in\\\\mathcal{X} ,\\nExogenous variables have no parents. We write pa \\u200b ( i ) \\\\mathrm{pa}(i) for the set of parents of i i and extend to sets by pa \\u200b ( O ) := ‚ãÉ i ‚àà O pa \\u200b ( i ) \\\\mathrm{pa}(O)\\\\!:=\\\\!\\\\bigcup_{i\\\\in O}\\\\mathrm{pa}(i) .\\nAn SCM ‚Ñ≥ \\\\mathcal{M} is uniquely solvable with respect to a subset ùí™ ‚äÜ ‚Ñê \\\\mathcal{O}\\\\subseteq\\\\mathcal{I} if there exists a measurable function\\nsuch that for all x ‚àà ùí≥ x\\\\in\\\\mathcal{X} and ‚Ñô ‚Ñ∞ \\\\mathbb{P}_{\\\\mathcal{E}} -almost every e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} ,\\nAn SCM ‚Ñ≥ \\\\mathcal{M} is called simple if it is uniquely solvable with respect to every subset ùí™ ‚äÜ ‚Ñê \\\\mathcal{O}\\\\subseteq\\\\mathcal{I} .\\nAcyclic SCMs are simple.\\nLet ‚Ñ≥ \\\\mathcal{M} be a structural causal model.\\nThe twin SCM associated with ‚Ñ≥ \\\\mathcal{M} is defined as\\nwhere ‚Ñê ‚Ä≤ := { i ‚Ä≤ : i ‚àà ‚Ñê } \\\\mathcal{I}^{\\\\prime}:=\\\\{i^{\\\\prime}:i\\\\in\\\\mathcal{I}\\\\} is a disjoint copy of the endogenous index set, and f ~ : ùí≥ √ó ùí≥ √ó ‚Ñ∞ ‚Üí ùí≥ √ó ùí≥ \\\\tilde{f}:\\\\mathcal{X}\\\\times\\\\mathcal{X}\\\\times\\\\mathcal{E}\\\\to\\\\mathcal{X}\\\\times\\\\mathcal{X} is the measurable function defined by\\nwith x , x ‚Ä≤ ‚àà ùí≥ x,x^{\\\\prime}\\\\in\\\\mathcal{X} and e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} .\\nFor any noise realisation e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} and stacked endogenous state ( x , x ‚Ä≤ ) = ( x 1 , ‚Ä¶ , x | ‚Ñê | , x 1 ‚Ä≤ , ‚Ä¶ , x | ‚Ñê | ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ , (x,x^{\\\\prime})\\\\;=\\\\;\\\\bigl(x_{1},\\\\dots,x_{|\\\\mathcal{I}|},\\\\;x_{1}^{\\\\prime},\\\\dots,x_{|\\\\mathcal{I}|}^{\\\\prime}\\\\bigr)\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X}, the twin‚ÄêSCM mechanism f ~ : ùí≥ √ó ùí≥ √ó ‚Ñ∞ ‚ü∂ ùí≥ √ó ùí≥ \\\\tilde{f}:\\\\mathcal{X}\\\\times\\\\mathcal{X}\\\\times\\\\mathcal{E}\\\\;\\\\longrightarrow\\\\;\\\\mathcal{X}\\\\times\\\\mathcal{X} is defined by f ~ \\u200b ( x , x ‚Ä≤ , e ) := ( f \\u200b ( x , e ) , f \\u200b ( x ‚Ä≤ , e ) ) . \\\\tilde{f}(x,x^{\\\\prime},e):=\\\\bigl(f(x,e),\\\\,f(x^{\\\\prime},e)\\\\bigr). Written coordinate-wise:\\ni.e. the first copy (un-primed) follows the original mechanism X j ‚Üê f j \\u200b ( x , e ) X_{j}\\\\!\\\\leftarrow\\\\!f_{j}(x,e) ,\\nwhile the primed copy applies the same function f i f_{i} to its own state x ‚Ä≤ x^{\\\\prime} . In compact notation\\nLet ‚Ñ≥ = ‚ü® ‚Ñê , ùí• , ùí≥ , ‚Ñ∞ , f , ‚Ñô ‚Ñ∞ ‚ü© \\\\mathcal{M}=\\\\langle\\\\mathcal{I},\\\\mathcal{J},\\\\mathcal{X},\\\\mathcal{E},f,\\\\mathbb{P}_{\\\\mathcal{E}}\\\\rangle be an SCM and let ‚Ñ≥ twin \\\\mathcal{M}^{\\\\mathrm{twin}} be its twin SCM.\\nConsider a perfect intervention\\napplied to ‚Ñ≥ twin \\\\mathcal{M}^{\\\\mathrm{twin}} , and denote the intervened model by ( ‚Ñ≥ twin ) do \\u200b ( ‚Ñê ~ , Œæ ‚Ñê ~ ) . \\\\bigl(\\\\mathcal{M}^{\\\\mathrm{twin}}\\\\bigr)_{\\\\!\\\\mathrm{do}(\\\\tilde{\\\\mathcal{I}},\\\\xi_{\\\\tilde{\\\\mathcal{I}}})}. If this intervened twin SCM admits a (measurable) solution ( X , X ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ (X,X^{\\\\prime})\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X} ,\\nthen the joint distribution ‚Ñô ( X , X ‚Ä≤ ) \\\\mathbb{P}_{(X,X^{\\\\prime})} is called the counterfactual distribution of ‚Ñ≥ \\\\mathcal{M} under\\nthe perfect intervention do \\u200b ( ‚Ñê ~ , Œæ ‚Ñê ~ ) \\\\mathrm{do}(\\\\tilde{\\\\mathcal{I}},\\\\xi_{\\\\tilde{\\\\mathcal{I}}}) associated with the pair of random variables ( X , X ‚Ä≤ ) (X,X^{\\\\prime}) .\\nIn the appendix, we delineate a clear correspondence between the twin-network formulation of structural causal models and Pearl‚Äôs canonical action‚Äìabduction‚Äìprediction schema for counterfactual inference.\\nFor example, instead of forcing a treatment X X to a set value, a shift intervention might increase each individual‚Äôs natural treatment dose by a fixed amount Œ¥ \\\\delta , and a scale intervention might multiply each dose by a factor (e.g. 10 % 10\\\\% increase) ‚Äì all while allowing X X to remain influenced by its usual causes (confounders, prior variables, etc.). This preserves the causal edges into X X but changes the conditional distribution or structural equation of X X .\\nLet ‚Ñ≥ = ‚ü® ‚Ñê , ùí• , ùí≥ , ‚Ñ∞ , f , ‚Ñô ‚Ñ∞ ‚ü© \\\\mathcal{M}=\\\\langle\\\\mathcal{I},\\\\mathcal{J},\\\\mathcal{X},\\\\mathcal{E},f,\\\\mathbb{P}_{\\\\mathcal{E}}\\\\rangle be an SCM and fix a non-empty subset ‚Ñê ~ ‚äÜ ‚Ñê \\\\tilde{\\\\mathcal{I}}\\\\subseteq\\\\mathcal{I} .\\nFor each j ‚àà ‚Ñê ~ j\\\\in\\\\tilde{\\\\mathcal{I}} choose\\nscale a j ‚àà ‚Ñù a_{j}\\\\in\\\\mathbb{R} and shift b j ‚àà ‚Ñù b_{j}\\\\in\\\\mathbb{R} .\\nThe shift‚Äìscale intervention\\nproduces a new SCM\\nPerfect or do \\\\operatorname{do} -intervention is recovered as the special case a j = 0 , b j = Œæ j a_{j}=0,\\\\;b_{j}=\\\\xi_{j} .\\nLet ‚Ñ≥ twin \\\\mathcal{M}^{\\\\mathrm{twin}} be the twin SCM of ‚Ñ≥ \\\\mathcal{M} .\\nApply the shift‚Äìscale intervention only to the first copy:\\nIf this intervened twin SCM admits a measurable solution ( X , X ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ (X,X^{\\\\prime})\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X} ,\\nwe call the joint law ‚Ñô ( X , X ‚Ä≤ ) \\\\mathbb{P}_{\\\\,(X,X^{\\\\prime})} the shift‚Äìscale counterfactual distribution of ‚Ñ≥ \\\\mathcal{M} under the intervention ss \\u200b ( ‚Ñê ~ , a ‚Ñê ~ , b ‚Ñê ~ ) \\\\mathrm{ss}\\\\bigl(\\\\tilde{\\\\mathcal{I}},a_{\\\\tilde{\\\\mathcal{I}}},b_{\\\\tilde{\\\\mathcal{I}}}\\\\bigr) .\\nGiven a subset a set of coordinates ‚Ñê ~ ‚äÜ ‚Ñê ‚à™ ‚Ñê ‚Ä≤ , \\\\tilde{\\\\mathcal{I}}\\\\subseteq\\\\mathcal{I}\\\\cup\\\\mathcal{I}^{\\\\prime}, and parameters a ‚Ñê ~ , b ‚Ñê ~ ‚àà ‚Ñù | ‚Ñê ~ | a_{\\\\tilde{\\\\mathcal{I}}},b_{\\\\tilde{\\\\mathcal{I}}}\\\\in\\\\mathbb{R}^{|\\\\tilde{\\\\mathcal{I}}|} ,\\nwe want to replace the structural equation\\nfor each j ‚àà ‚Ñê ~ j\\\\in\\\\tilde{\\\\mathcal{I}} .\\nAll other coordinates stay unchanged.\\nEncoding these modifications into a single map\\ngives the intervened twin update rule.\\nSpecifically\\nThus each copy (un-primed and primed) is modified only on the\\nrequested coordinates, allowing independent interventions on the\\ntwo worlds.\\nOne natural interpretation of cyclic structural equations is by assuming an underlying\\ndiscrete-time dynamical system, where the equations act as update rules: the value of\\neach variable at time t + 1 t{+}1 is computed from the values at time t t . The system is then\\nanalyzed in the limit as t ‚Üí ‚àû t\\\\to\\\\infty , focusing on the fixed points to which the dynamics\\nconverge. Mooij et al. ( 2013 ) demonstrate that an alternative, yet natural,\\ninterpretation of SCMs emerges when considering systems of ordinary differential\\nequations (ODEs). By examining the equilibrium (steady-state) solutions of such ODEs,\\none arrives at a structural causal model that is time-independent, but still retains\\nmeaningful causal semantics with respect to interventions. Specifically, the semantics\\nof interventions and counterfactuals (see also Appendix A.1 ) remain valid and well-defined in this steady-state\\ncontext, as rigorously formalized by Mooij et al. ( 2013 ) and further extended\\nby Bongers et al. ( 2021 ) . These are by no means the only routes to structural causal models; indeed, SCMs may arise through a variety of alternative constructions\\nand representations, depending on the nature of the system under consideration.\\nAlthough many physical processes exhibit inertia, static cyclic structural causal models (SCMs) remain appropriate when we focus on equilibrium behavior or sample at a temporal resolution coarser than the fastest feedback loop. Examples include gene‚Äëregulatory networks in single‚Äëcell genomics (Rohbeck et al. , 2024 ) ; market‚Äëequilibrium models (Bongers et al. , 2021 ) ; predator‚Äìprey ecological systems; Thyroid or reproductive hormone axes exhibit feedback loops (Clarke et al. , 2014 ) , etc.\\nOur mathematical framework is agnostic about the interpretation of cycles:\\nwe formulate everything directly at the level of structural equations with exogenous noise.',\n",
       "    'subsections': [{'title': 'How the twin map f~\\\\tilde{f} is constructed',\n",
       "      'paragraphs': 'For any noise realisation e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} and stacked endogenous state ( x , x ‚Ä≤ ) = ( x 1 , ‚Ä¶ , x | ‚Ñê | , x 1 ‚Ä≤ , ‚Ä¶ , x | ‚Ñê | ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ , (x,x^{\\\\prime})\\\\;=\\\\;\\\\bigl(x_{1},\\\\dots,x_{|\\\\mathcal{I}|},\\\\;x_{1}^{\\\\prime},\\\\dots,x_{|\\\\mathcal{I}|}^{\\\\prime}\\\\bigr)\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X}, the twin‚ÄêSCM mechanism f ~ : ùí≥ √ó ùí≥ √ó ‚Ñ∞ ‚ü∂ ùí≥ √ó ùí≥ \\\\tilde{f}:\\\\mathcal{X}\\\\times\\\\mathcal{X}\\\\times\\\\mathcal{E}\\\\;\\\\longrightarrow\\\\;\\\\mathcal{X}\\\\times\\\\mathcal{X} is defined by f ~ \\u200b ( x , x ‚Ä≤ , e ) := ( f \\u200b ( x , e ) , f \\u200b ( x ‚Ä≤ , e ) ) . \\\\tilde{f}(x,x^{\\\\prime},e):=\\\\bigl(f(x,e),\\\\,f(x^{\\\\prime},e)\\\\bigr). Written coordinate-wise:\\ni.e. the first copy (un-primed) follows the original mechanism X j ‚Üê f j \\u200b ( x , e ) X_{j}\\\\!\\\\leftarrow\\\\!f_{j}(x,e) ,\\nwhile the primed copy applies the same function f i f_{i} to its own state x ‚Ä≤ x^{\\\\prime} . In compact notation\\nLet ‚Ñ≥ = ‚ü® ‚Ñê , ùí• , ùí≥ , ‚Ñ∞ , f , ‚Ñô ‚Ñ∞ ‚ü© \\\\mathcal{M}=\\\\langle\\\\mathcal{I},\\\\mathcal{J},\\\\mathcal{X},\\\\mathcal{E},f,\\\\mathbb{P}_{\\\\mathcal{E}}\\\\rangle be an SCM and let ‚Ñ≥ twin \\\\mathcal{M}^{\\\\mathrm{twin}} be its twin SCM.\\nConsider a perfect intervention\\napplied to ‚Ñ≥ twin \\\\mathcal{M}^{\\\\mathrm{twin}} , and denote the intervened model by ( ‚Ñ≥ twin ) do \\u200b ( ‚Ñê ~ , Œæ ‚Ñê ~ ) . \\\\bigl(\\\\mathcal{M}^{\\\\mathrm{twin}}\\\\bigr)_{\\\\!\\\\mathrm{do}(\\\\tilde{\\\\mathcal{I}},\\\\xi_{\\\\tilde{\\\\mathcal{I}}})}. If this intervened twin SCM admits a (measurable) solution ( X , X ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ (X,X^{\\\\prime})\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X} ,\\nthen the joint distribution ‚Ñô ( X , X ‚Ä≤ ) \\\\mathbb{P}_{(X,X^{\\\\prime})} is called the counterfactual distribution of ‚Ñ≥ \\\\mathcal{M} under\\nthe perfect intervention do \\u200b ( ‚Ñê ~ , Œæ ‚Ñê ~ ) \\\\mathrm{do}(\\\\tilde{\\\\mathcal{I}},\\\\xi_{\\\\tilde{\\\\mathcal{I}}}) associated with the pair of random variables ( X , X ‚Ä≤ ) (X,X^{\\\\prime}) .\\nIn the appendix, we delineate a clear correspondence between the twin-network formulation of structural causal models and Pearl‚Äôs canonical action‚Äìabduction‚Äìprediction schema for counterfactual inference.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.1 Shift‚ÄìScale intervention',\n",
       "      'paragraphs': 'For example, instead of forcing a treatment X X to a set value, a shift intervention might increase each individual‚Äôs natural treatment dose by a fixed amount Œ¥ \\\\delta , and a scale intervention might multiply each dose by a factor (e.g. 10 % 10\\\\% increase) ‚Äì all while allowing X X to remain influenced by its usual causes (confounders, prior variables, etc.). This preserves the causal edges into X X but changes the conditional distribution or structural equation of X X .\\nLet ‚Ñ≥ = ‚ü® ‚Ñê , ùí• , ùí≥ , ‚Ñ∞ , f , ‚Ñô ‚Ñ∞ ‚ü© \\\\mathcal{M}=\\\\langle\\\\mathcal{I},\\\\mathcal{J},\\\\mathcal{X},\\\\mathcal{E},f,\\\\mathbb{P}_{\\\\mathcal{E}}\\\\rangle be an SCM and fix a non-empty subset ‚Ñê ~ ‚äÜ ‚Ñê \\\\tilde{\\\\mathcal{I}}\\\\subseteq\\\\mathcal{I} .\\nFor each j ‚àà ‚Ñê ~ j\\\\in\\\\tilde{\\\\mathcal{I}} choose\\nscale a j ‚àà ‚Ñù a_{j}\\\\in\\\\mathbb{R} and shift b j ‚àà ‚Ñù b_{j}\\\\in\\\\mathbb{R} .\\nThe shift‚Äìscale intervention\\nproduces a new SCM\\nPerfect or do \\\\operatorname{do} -intervention is recovered as the special case a j = 0 , b j = Œæ j a_{j}=0,\\\\;b_{j}=\\\\xi_{j} .\\nLet ‚Ñ≥ twin \\\\mathcal{M}^{\\\\mathrm{twin}} be the twin SCM of ‚Ñ≥ \\\\mathcal{M} .\\nApply the shift‚Äìscale intervention only to the first copy:\\nIf this intervened twin SCM admits a measurable solution ( X , X ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ (X,X^{\\\\prime})\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X} ,\\nwe call the joint law ‚Ñô ( X , X ‚Ä≤ ) \\\\mathbb{P}_{\\\\,(X,X^{\\\\prime})} the shift‚Äìscale counterfactual distribution of ‚Ñ≥ \\\\mathcal{M} under the intervention ss \\u200b ( ‚Ñê ~ , a ‚Ñê ~ , b ‚Ñê ~ ) \\\\mathrm{ss}\\\\bigl(\\\\tilde{\\\\mathcal{I}},a_{\\\\tilde{\\\\mathcal{I}}},b_{\\\\tilde{\\\\mathcal{I}}}\\\\bigr) .\\nGiven a subset a set of coordinates ‚Ñê ~ ‚äÜ ‚Ñê ‚à™ ‚Ñê ‚Ä≤ , \\\\tilde{\\\\mathcal{I}}\\\\subseteq\\\\mathcal{I}\\\\cup\\\\mathcal{I}^{\\\\prime}, and parameters a ‚Ñê ~ , b ‚Ñê ~ ‚àà ‚Ñù | ‚Ñê ~ | a_{\\\\tilde{\\\\mathcal{I}}},b_{\\\\tilde{\\\\mathcal{I}}}\\\\in\\\\mathbb{R}^{|\\\\tilde{\\\\mathcal{I}}|} ,\\nwe want to replace the structural equation\\nfor each j ‚àà ‚Ñê ~ j\\\\in\\\\tilde{\\\\mathcal{I}} .\\nAll other coordinates stay unchanged.\\nEncoding these modifications into a single map\\ngives the intervened twin update rule.\\nSpecifically\\nThus each copy (un-primed and primed) is modified only on the\\nrequested coordinates, allowing independent interventions on the\\ntwo worlds.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Semantics of Cyclic SCMs',\n",
       "      'paragraphs': 'One natural interpretation of cyclic structural equations is by assuming an underlying\\ndiscrete-time dynamical system, where the equations act as update rules: the value of\\neach variable at time t + 1 t{+}1 is computed from the values at time t t . The system is then\\nanalyzed in the limit as t ‚Üí ‚àû t\\\\to\\\\infty , focusing on the fixed points to which the dynamics\\nconverge. Mooij et al. ( 2013 ) demonstrate that an alternative, yet natural,\\ninterpretation of SCMs emerges when considering systems of ordinary differential\\nequations (ODEs). By examining the equilibrium (steady-state) solutions of such ODEs,\\none arrives at a structural causal model that is time-independent, but still retains\\nmeaningful causal semantics with respect to interventions. Specifically, the semantics\\nof interventions and counterfactuals (see also Appendix A.1 ) remain valid and well-defined in this steady-state\\ncontext, as rigorously formalized by Mooij et al. ( 2013 ) and further extended\\nby Bongers et al. ( 2021 ) . These are by no means the only routes to structural causal models; indeed, SCMs may arise through a variety of alternative constructions\\nand representations, depending on the nature of the system under consideration.\\nAlthough many physical processes exhibit inertia, static cyclic structural causal models (SCMs) remain appropriate when we focus on equilibrium behavior or sample at a temporal resolution coarser than the fastest feedback loop. Examples include gene‚Äëregulatory networks in single‚Äëcell genomics (Rohbeck et al. , 2024 ) ; market‚Äëequilibrium models (Bongers et al. , 2021 ) ; predator‚Äìprey ecological systems; Thyroid or reproductive hormone axes exhibit feedback loops (Clarke et al. , 2014 ) , etc.\\nOur mathematical framework is agnostic about the interpretation of cycles:\\nwe formulate everything directly at the level of structural equations with exogenous noise.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Theory',\n",
       "    'paragraphs': 'For any subset ùí™ ‚äÜ ‚Ñê \\\\mathcal{O}\\\\subseteq\\\\mathcal{I} let ùí¨ := pa \\u200b ( ùí™ ) ‚àñ ùí™ \\\\mathcal{Q}:=\\\\mathrm{pa}(\\\\mathcal{O})\\\\setminus\\\\mathcal{O} .\\nBecause every ùí≥ i \\\\mathcal{X}_{i} is closed in ‚Ñù \\\\mathbb{R} (hence complete) and ùí™ \\\\mathcal{O} is finite,\\nthe product ùí≥ ùí™ = ‚àè i ‚àà ùí™ ùí≥ i \\\\mathcal{X}_{\\\\mathcal{O}}=\\\\prod_{i\\\\in\\\\mathcal{O}}\\\\mathcal{X}_{i} is complete under the ‚Ñì p \\\\ell^{p} -metric;\\nthe same holds for the full space ùí≥ \\\\mathcal{X} .\\nFor each pair ( x ùí¨ , e pa \\u200b ( ùí™ ) ) (x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}) define\\nwhere the ‚Äúdummy‚Äù coordinates x ‚Ñê ‚àñ ( ùí™ ‚à™ ùí¨ ) x_{\\\\mathcal{I}\\\\setminus(\\\\mathcal{O}\\\\cup\\\\mathcal{Q})} and e ùí• ‚àñ pa \\u200b ( ùí™ ) e_{\\\\mathcal{J}\\\\setminus\\\\mathrm{pa}(\\\\mathcal{O})} may be chosen arbitrarily because they do not influence f ùí™ f_{\\\\mathcal{O}} .\\nFor u , v ‚àà ùí≥ ùí™ u,v\\\\in\\\\mathcal{X}_{\\\\mathcal{O}} define u ~ := ( u , x ùí¨ , x ‚Ñê ‚àñ ( ùí™ ‚à™ ùí¨ ) ) , v ~ := ( v , x ùí¨ , x ‚Ñê ‚àñ ( ùí™ ‚à™ ùí¨ ) ) ‚àà ùí≥ . \\\\tilde{u}:=(u,x_{\\\\mathcal{Q}},x_{\\\\mathcal{I}\\\\setminus(\\\\mathcal{O}\\\\cup\\\\mathcal{Q})}),\\\\ \\\\tilde{v}:=(v,x_{\\\\mathcal{Q}},x_{\\\\mathcal{I}\\\\setminus(\\\\mathcal{O}\\\\cup\\\\mathcal{Q})})\\\\in\\\\mathcal{X}. Then ‚Äñ u ~ ‚àí v ~ ‚Äñ p = ‚Äñ u ‚àí v ‚Äñ p \\\\|\\\\tilde{u}-\\\\tilde{v}\\\\|_{p}=\\\\|u-v\\\\|_{p} because u ~ , v ~ \\\\tilde{u},\\\\tilde{v} coincide outside ùí™ \\\\mathcal{O} .\\nUsing ( 1 ),\\nThus each map h x ùí¨ , e pa \\u200b ( ùí™ ) h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}} is a Œ∫ \\\\kappa -contraction on the complete metric space ( ùí≥ ùí™ , ‚à• ‚ãÖ ‚à• p ) (\\\\mathcal{X}_{\\\\mathcal{O}},\\\\|\\\\cdot\\\\|_{p}) .\\nBy the Banach fixed-point theorem, for every ( x ùí¨ , e pa \\u200b ( ùí™ ) ) (x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}) there exists a unique element u ‚àó \\u200b ( x ùí¨ , e pa \\u200b ( ùí™ ) ) ‚àà ùí≥ ùí™ u^{*}(x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})})\\\\in\\\\mathcal{X}_{\\\\mathcal{O}} satisfying u ‚àó = h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( u ‚àó ) . u^{*}=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}(u^{*}).\\nFix any u ¬Ø ‚àà ùí≥ ùí™ \\\\bar{u}\\\\in\\\\mathcal{X}_{\\\\mathcal{O}} .\\nDefining Picard iterates u ( 0 ) ‚â° u ¬Ø u^{(0)}\\\\equiv\\\\bar{u} and u ( n + 1 ) := h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( u ( n ) ) u^{(n+1)}:=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}\\\\bigl(u^{(n)}\\\\bigr) ,\\none obtains a sequence of measurable functions converging pointwise to u ‚àó u^{*} .\\nLimits of measurable functions are measurable, hence the map\\nis measurable.\\n( ‚áí \\\\Rightarrow ) By definition, g ùí™ \\u200b ( x ùí¨ , e pa \\u200b ( ùí™ ) ) g_{\\\\mathcal{O}}(x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}) is the unique fixed point u ‚àó u^{*} of the map\\nHence, if x ùí™ = g ùí™ \\u200b ( x ùí¨ , e pa \\u200b ( ùí™ ) ) x_{\\\\mathcal{O}}=g_{\\\\mathcal{O}}(x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}) ,\\nthen x ùí™ = h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( x ùí™ ) = f ùí™ \\u200b ( x , e ) x_{\\\\mathcal{O}}=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}(x_{\\\\mathcal{O}})=f_{\\\\mathcal{O}}(x,e) .\\n( ‚áê \\\\Leftarrow ) Suppose x ùí™ = f ùí™ \\u200b ( x , e ) x_{\\\\mathcal{O}}=f_{\\\\mathcal{O}}(x,e) .\\nThen x ùí™ x_{\\\\mathcal{O}} satisfies x ùí™ = f ùí™ \\u200b ( x , e ) = h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( x ùí™ ) , x_{\\\\mathcal{O}}=f_{\\\\mathcal{O}}(x,e)=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}(x_{\\\\mathcal{O}}), so it is a fixed point of h x ùí¨ , e pa \\u200b ( ùí™ ) h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}} .\\nBy Banach‚Äôs theorem, the fixed point is unique, so\\nTherefore,\\nestablishing the defining equivalence in the notion of unique solvability\\nholds for all x x and for ‚Ñô ‚Ñ∞ \\\\mathbb{P}_{\\\\mathcal{E}} -almost every e e .\\nBecause ùí™ ‚äÜ ‚Ñê \\\\mathcal{O}\\\\subseteq\\\\mathcal{I} was arbitrary,\\nthe same reasoning applies to every subset, establishing that ‚Ñ≥ \\\\mathcal{M} is uniquely solvable with respect to all subsets and\\nhence is a simple SCM.\\n‚àé\\nThe global contraction condition implies unique solvability for every subset; hence the models are simple SCMs in the sense of Bongers et al. ( 2021 ) . This places us directly inside the setting where their closure results for do-interventions, marginalization and twin networks apply.\\nLet ( x , x ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ (x,x^{\\\\prime})\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X} be the endogenous variables of the twin SCM and the twin map f ~ \\u200b ( x , x ‚Ä≤ , e ) := ( f \\u200b ( x , e ) , f \\u200b ( x ‚Ä≤ , e ) ) . \\\\tilde{f}(x,x^{\\\\prime},e):=(f(x,e),f(x^{\\\\prime},e)). is a global Œ∫ \\\\kappa -contraction on ùí≥ √ó ùí≥ \\\\mathcal{X}\\\\times\\\\mathcal{X} :\\nNow define the shift‚Äìscale intervened map g ~ \\u200b ( x , x ‚Ä≤ , e ) \\\\tilde{g}(x,x^{\\\\prime},e) by:\\nWe now show that g ~ \\u200b ( ‚ãÖ , ‚ãÖ , e ) \\\\tilde{g}(\\\\cdot,\\\\cdot,e) is a global contraction in the ‚Ñì p \\\\ell^{p} -norm with the same constant Œ∫ \\\\kappa . Let u := ( x , x ‚Ä≤ ) u:=(x,x^{\\\\prime}) , v := ( y , y ‚Ä≤ ) v:=(y,y^{\\\\prime}) . Then for any j ‚àà ‚Ñê ‚à™ ‚Ñê ‚Ä≤ j\\\\in\\\\mathcal{I}\\\\cup\\\\mathcal{I}^{\\\\prime} ,\\nTherefore,\\nSince a max ‚â§ 1 a_{\\\\max}\\\\leq 1 , we have:\\nThus g ~ \\u200b ( ‚ãÖ , e ) \\\\tilde{g}(\\\\cdot,e) is also a global Œ∫ \\\\kappa -contraction on ( ùí≥ √ó ùí≥ , ‚à• ‚ãÖ ‚à• p ) (\\\\mathcal{X}\\\\times\\\\mathcal{X},\\\\|\\\\cdot\\\\|_{p}) , a complete metric space since ùí≥ \\\\mathcal{X} is closed and finite-dimensional.\\nBy Banach‚Äôs fixed-point theorem (as used in Theorem 1 ), the shift‚Äìscale intervened twin SCM has a unique solution ( X , X ‚Ä≤ ) (X,X^{\\\\prime}) for each e e , and the solution map is measurable.\\nSince the SCM is uniquely solvable with respect to all subsets of ‚Ñê ‚à™ ‚Ñê ‚Ä≤ \\\\mathcal{I}\\\\cup\\\\mathcal{I}^{\\\\prime} , the intervened twin SCM is simple. Pushing forward ‚Ñô ‚Ñ∞ \\\\mathbb{P}_{\\\\mathcal{E}} through this solution map defines the shift‚Äìscale counterfactual distribution ‚Ñô ( X , X ‚Ä≤ ) \\\\mathbb{P}_{(X,X^{\\\\prime})} .\\n‚àé\\nProposition 8.2 in Bongers et al. ( 2021 ) shows that the class of simple SCMs is closed under (i) marginalization, (ii) perfect interventions (do), and (iii) the twin construction. It does not give a sufficient condition for simplicity; it presupposes simplicity.\\nTheorem 2 gives sufficient analytic conditions for unique solvability of the twin under a class of soft, parametric shift‚Äìscale interventions, thereby ensuring well-posed counterfactuals beyond hard do.\\n(1) Composition reduces to a single affine map. For any coordinate j ‚àà ‚Ñê j\\\\in\\\\mathcal{I} the successive\\nupdates act as x j ‚Ü¶ a j ( m ) \\u200b ( a j ( m ‚àí 1 ) \\u200b ( ‚Ä¶ \\u200b a j ( 1 ) \\u200b f j \\u200b ( x , e ) + b j ( 1 ) \\u200b ‚Ä¶ ) + b j ( m ‚àí 1 ) ) + b j ( m ) , x_{j}\\\\;\\\\mapsto\\\\;a^{(m)}_{j}\\\\!\\\\bigl(a^{(m-1)}_{j}(\\\\dots a^{(1)}_{j}f_{j}(x,e)+b^{(1)}_{j}\\\\dots)+b^{(m-1)}_{j}\\\\bigr)+b^{(m)}_{j}, which can be written a j comp \\u200b f j \\u200b ( x , e ) + b j comp a^{\\\\mathrm{comp}}_{j}f_{j}(x,e)+b^{\\\\mathrm{comp}}_{j} with a j comp := ‚àè r : j ‚àà ‚Ñê ~ ( r ) a j ( r ) a^{\\\\mathrm{comp}}_{j}:=\\\\prod_{r:\\\\;j\\\\in\\\\tilde{\\\\mathcal{I}}^{(r)}}a^{(r)}_{j} and a corresponding affine drift b j comp b^{\\\\mathrm{comp}}_{j} .\\nSince each factor satisfies | a j ( r ) | ‚â§ 1 |a^{(r)}_{j}|\\\\leq 1 ,\\nwe have | a j comp | ‚â§ 1 \\\\lvert a^{\\\\mathrm{comp}}_{j}\\\\rvert\\\\leq 1 .\\nHence the final system coincides with g \\u200b ( x , e ) = a comp ‚äô f \\u200b ( x , e ) + b comp g(x,e)=a^{\\\\mathrm{comp}}\\\\!\\\\odot f(x,e)+b^{\\\\mathrm{comp}} .\\n(2) Contraction constant is preserved. Let D := diag \\u2061 ( a comp ) D:=\\\\operatorname{diag}(a^{\\\\mathrm{comp}}) denote the diagonal\\nmatrix of multiplicative factors.\\nFor any u ‚àà ‚Ñù | ‚Ñê | u\\\\in\\\\mathbb{R}^{|\\\\mathcal{I}|} , ‚Äñ D \\u200b u ‚Äñ p ‚â§ ‚Äñ u ‚Äñ p \\\\|D\\\\,u\\\\|_{p}\\\\leq\\\\|u\\\\|_{p} because every diagonal entry satisfies | a j comp | ‚â§ 1 |a^{\\\\mathrm{comp}}_{j}|\\\\leq 1 .\\nHence for all x , y x,y and any e e\\nThus g g inherits the same global contraction constant Œ∫ \\\\kappa .\\n(3) Simplicity after intervention. Because the intervened mechanism is still Œ∫ \\\\kappa -contractive with Œ∫ < 1 \\\\kappa<1 , Theorem 1 applies verbatim:\\nthe intervened model is uniquely solvable with respect to every subset\\nof endogenous variables, i.e. it is simple.\\n‚àé\\nThe proof of Proposition 1 used the bound | a j | ‚â§ 1 |a_{j}|\\\\leq 1 to conclude that the diagonal scaling D = diag \\u2061 ( a j ) D=\\\\operatorname{diag}(a_{j}) obeys ‚Äñ D \\u200b u ‚Äñ p ‚â§ ‚Äñ u ‚Äñ p \\\\|Du\\\\|_{p}\\\\leq\\\\|u\\\\|_{p} and therefore does not enlarge the\\nglobal Lipschitz constant.\\nIf some multipliers satisfy | a j | > 1 |a_{j}|>1 , simplicity can still hold, but\\none must verify an additional criterion:\\nLet Œ∫ < 1 \\\\kappa<1 be the original contraction constant of f f and define\\nBecause ‚Äñ D ‚Äñ p = max j \\u2061 | a j | \\\\|D\\\\|_{p}=\\\\max_{j}|a_{j}| , the intervened mechanism is\\nglobally Œ∫ max \\\\kappa_{\\\\max} -Lipschitz: ‚Äñ D \\u200b f \\u200b ( x , e ) ‚àí D \\u200b f \\u200b ( y , e ) ‚Äñ p ‚â§ Œ∫ max \\u200b ‚Äñ x ‚àí y ‚Äñ p \\\\|Df(x,e)-Df(y,e)\\\\|_{p}\\\\leq\\\\kappa_{\\\\max}\\\\|x-y\\\\|_{p} .\\nIf Œ∫ max < 1 \\\\kappa_{\\\\max}<1 the model remains a contraction and is therefore\\nsimple; if Œ∫ max ‚â• 1 \\\\kappa_{\\\\max}\\\\geq 1 the contraction proof no longer ensures\\nuniqueness and additional analysis is required.\\nLet Œ¶ : ‚Ñ∞ ‚Üí ùí≥ √ó ùí≥ \\\\Phi:\\\\mathcal{E}\\\\to\\\\mathcal{X}\\\\times\\\\mathcal{X} denote the unique measurable solution map of the shift‚Äìscale‚Äìintervened twin SCM. That is,\\nwhere ùêÑ ‚àº ‚Ñô ‚Ñ∞ \\\\mathbf{E}\\\\sim\\\\mathbb{P}_{\\\\mathcal{E}} is the exogenous random variable. By the global Œ∫ \\\\kappa -contraction in x x and the 1 1 -Lipschitz property in e e (both in ‚Ñì 2 \\\\ell^{2} ), Œ¶ \\\\Phi is L L -Lipschitz with constant L := 1 1 ‚àí Œ∫ L:=\\\\frac{1}{1-\\\\kappa} in ‚Ñì 2 \\\\ell^{2} :\\nTake any h : ùí≥ √ó ùí≥ ‚Üí ‚Ñù h:\\\\mathcal{X}\\\\times\\\\mathcal{X}\\\\to\\\\mathbb{R} that is 1 1 -Lipschitz in ‚Ñì 2 \\\\ell^{2} .\\nWrite ùêÑ = Œº + Œ£ 1 / 2 \\u200b ùêô \\\\mathbf{E}=\\\\mu+\\\\Sigma^{1/2}\\\\mathbf{Z} with ùêô ‚àº ùí© \\u200b ( 0 , I ) \\\\mathbf{Z}\\\\sim\\\\mathcal{N}(0,I) .\\nThen g \\u200b ( z ) := h \\u200b ( Œ¶ \\u200b ( Œº + Œ£ 1 / 2 \\u200b z ) ) g(z):=h\\\\bigl(\\\\Phi(\\\\mu+\\\\Sigma^{1/2}z)\\\\bigr) is L \\u200b ‚Äñ Œ£ 1 / 2 ‚Äñ 2 L\\\\|\\\\Sigma^{1/2}\\\\|_{\\\\mathrm{2}} -Lipschitz in ‚Ñì 2 \\\\ell^{2} .\\nSince Œ£ ‚™Ø œÉ 2 \\u200b I \\\\Sigma\\\\preceq\\\\sigma^{2}I , we have ‚Äñ Œ£ 1 / 2 ‚Äñ 2 ‚â§ œÉ \\\\|\\\\Sigma^{1/2}\\\\|_{\\\\mathrm{2}}\\\\leq\\\\sigma .\\nBy Gaussian concentration for Lipschitz functions (e.g., (Vershynin, 2018 , Thm.\\xa05.2.3) ),\\nThis proves the stated concentration inequality.\\n‚àé\\nLet all Lipschitz and contraction conditions in Proposition 2 be measured in ‚Ñì p \\\\ell^{p} ( 1 ‚â§ p ‚â§ ‚àû 1\\\\leq p\\\\leq\\\\infty ), and keep the Gaussian noise assumption ùêÑ ‚àº ùí© \\u200b ( Œº , Œ£ ) \\\\mathbf{E}\\\\sim\\\\mathcal{N}(\\\\mu,\\\\Sigma) .\\nThen for every 1 1 -Lipschitz h : ( ùí≥ √ó ùí≥ , ‚à• ‚ãÖ ‚à• p ) ‚Üí ‚Ñù h:(\\\\mathcal{X}\\\\times\\\\mathcal{X},\\\\|\\\\cdot\\\\|_{p})\\\\to\\\\mathbb{R} and all t > 0 t>0 ,\\nwhere ‚Äñ A ‚Äñ 2 ‚Üí p := sup x ‚â† 0 ‚Äñ A \\u200b x ‚Äñ p ‚Äñ x ‚Äñ 2 \\\\|A\\\\|_{2\\\\to p}:=\\\\sup_{x\\\\neq 0}\\\\frac{\\\\|Ax\\\\|_{p}}{\\\\|x\\\\|_{2}} .\\nIn particular, if Œ£ ‚™Ø œÉ 2 \\u200b I \\\\Sigma\\\\preceq\\\\sigma^{2}I and d := dim ( ‚Ñ∞ ) d:=\\\\dim(\\\\mathcal{E}) , then\\nso the proxy equals ( 1 ‚àí Œ∫ ) ‚àí 2 \\u200b œÉ 2 (1-\\\\kappa)^{-2}\\\\sigma^{2} for p ‚â• 2 p\\\\geq 2 , and ( 1 ‚àí Œ∫ ) ‚àí 2 \\u200b œÉ 2 \\u200b d 2 \\u200b ( 1 p ‚àí 1 2 ) (1-\\\\kappa)^{-2}\\\\sigma^{2}\\\\,d^{\\\\,2(\\\\frac{1}{p}-\\\\frac{1}{2})} for 1 ‚â§ p < 2 1\\\\leq p<2 . Proof sketch. Write ùêÑ = Œº + Œ£ 1 / 2 \\u200b ùêô \\\\mathbf{E}=\\\\mu+\\\\Sigma^{1/2}\\\\mathbf{Z} , ùêô ‚àº ùí© \\u200b ( 0 , I ) \\\\mathbf{Z}\\\\sim\\\\mathcal{N}(0,I) ; then z ‚Ü¶ h \\u200b ( Œ¶ \\u200b ( Œº + Œ£ 1 / 2 \\u200b z ) ) z\\\\mapsto h(\\\\Phi(\\\\mu+\\\\Sigma^{1/2}z)) is ( 1 ‚àí Œ∫ ) ‚àí 1 \\u200b ‚Äñ Œ£ 1 / 2 ‚Äñ 2 ‚Üí p (1-\\\\kappa)^{-1}\\\\|\\\\Sigma^{1/2}\\\\|_{2\\\\to p} -Lipschitz in ‚Ñì 2 \\\\ell^{2} , and Gaussian Lipschitz concentration applies.',\n",
       "    'subsections': [{'title': 'Measurability.',\n",
       "      'paragraphs': 'Fix any u ¬Ø ‚àà ùí≥ ùí™ \\\\bar{u}\\\\in\\\\mathcal{X}_{\\\\mathcal{O}} .\\nDefining Picard iterates u ( 0 ) ‚â° u ¬Ø u^{(0)}\\\\equiv\\\\bar{u} and u ( n + 1 ) := h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( u ( n ) ) u^{(n+1)}:=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}\\\\bigl(u^{(n)}\\\\bigr) ,\\none obtains a sequence of measurable functions converging pointwise to u ‚àó u^{*} .\\nLimits of measurable functions are measurable, hence the map\\nis measurable.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Equivalence.',\n",
       "      'paragraphs': '( ‚áí \\\\Rightarrow ) By definition, g ùí™ \\u200b ( x ùí¨ , e pa \\u200b ( ùí™ ) ) g_{\\\\mathcal{O}}(x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}) is the unique fixed point u ‚àó u^{*} of the map\\nHence, if x ùí™ = g ùí™ \\u200b ( x ùí¨ , e pa \\u200b ( ùí™ ) ) x_{\\\\mathcal{O}}=g_{\\\\mathcal{O}}(x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}) ,\\nthen x ùí™ = h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( x ùí™ ) = f ùí™ \\u200b ( x , e ) x_{\\\\mathcal{O}}=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}(x_{\\\\mathcal{O}})=f_{\\\\mathcal{O}}(x,e) .\\n( ‚áê \\\\Leftarrow ) Suppose x ùí™ = f ùí™ \\u200b ( x , e ) x_{\\\\mathcal{O}}=f_{\\\\mathcal{O}}(x,e) .\\nThen x ùí™ x_{\\\\mathcal{O}} satisfies x ùí™ = f ùí™ \\u200b ( x , e ) = h x ùí¨ , e pa \\u200b ( ùí™ ) \\u200b ( x ùí™ ) , x_{\\\\mathcal{O}}=f_{\\\\mathcal{O}}(x,e)=h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}}(x_{\\\\mathcal{O}}), so it is a fixed point of h x ùí¨ , e pa \\u200b ( ùí™ ) h_{x_{\\\\mathcal{Q}},e_{\\\\mathrm{pa}(\\\\mathcal{O})}} .\\nBy Banach‚Äôs theorem, the fixed point is unique, so\\nTherefore,\\nestablishing the defining equivalence in the notion of unique solvability\\nholds for all x x and for ‚Ñô ‚Ñ∞ \\\\mathbb{P}_{\\\\mathcal{E}} -almost every e e .\\nBecause ùí™ ‚äÜ ‚Ñê \\\\mathcal{O}\\\\subseteq\\\\mathcal{I} was arbitrary,\\nthe same reasoning applies to every subset, establishing that ‚Ñ≥ \\\\mathcal{M} is uniquely solvable with respect to all subsets and\\nhence is a simple SCM.\\n‚àé\\nThe global contraction condition implies unique solvability for every subset; hence the models are simple SCMs in the sense of Bongers et al. ( 2021 ) . This places us directly inside the setting where their closure results for do-interventions, marginalization and twin networks apply.\\nLet ( x , x ‚Ä≤ ) ‚àà ùí≥ √ó ùí≥ (x,x^{\\\\prime})\\\\in\\\\mathcal{X}\\\\times\\\\mathcal{X} be the endogenous variables of the twin SCM and the twin map f ~ \\u200b ( x , x ‚Ä≤ , e ) := ( f \\u200b ( x , e ) , f \\u200b ( x ‚Ä≤ , e ) ) . \\\\tilde{f}(x,x^{\\\\prime},e):=(f(x,e),f(x^{\\\\prime},e)). is a global Œ∫ \\\\kappa -contraction on ùí≥ √ó ùí≥ \\\\mathcal{X}\\\\times\\\\mathcal{X} :\\nNow define the shift‚Äìscale intervened map g ~ \\u200b ( x , x ‚Ä≤ , e ) \\\\tilde{g}(x,x^{\\\\prime},e) by:\\nWe now show that g ~ \\u200b ( ‚ãÖ , ‚ãÖ , e ) \\\\tilde{g}(\\\\cdot,\\\\cdot,e) is a global contraction in the ‚Ñì p \\\\ell^{p} -norm with the same constant Œ∫ \\\\kappa . Let u := ( x , x ‚Ä≤ ) u:=(x,x^{\\\\prime}) , v := ( y , y ‚Ä≤ ) v:=(y,y^{\\\\prime}) . Then for any j ‚àà ‚Ñê ‚à™ ‚Ñê ‚Ä≤ j\\\\in\\\\mathcal{I}\\\\cup\\\\mathcal{I}^{\\\\prime} ,\\nTherefore,\\nSince a max ‚â§ 1 a_{\\\\max}\\\\leq 1 , we have:\\nThus g ~ \\u200b ( ‚ãÖ , e ) \\\\tilde{g}(\\\\cdot,e) is also a global Œ∫ \\\\kappa -contraction on ( ùí≥ √ó ùí≥ , ‚à• ‚ãÖ ‚à• p ) (\\\\mathcal{X}\\\\times\\\\mathcal{X},\\\\|\\\\cdot\\\\|_{p}) , a complete metric space since ùí≥ \\\\mathcal{X} is closed and finite-dimensional.\\nBy Banach‚Äôs fixed-point theorem (as used in Theorem 1 ), the shift‚Äìscale intervened twin SCM has a unique solution ( X , X ‚Ä≤ ) (X,X^{\\\\prime}) for each e e , and the solution map is measurable.\\nSince the SCM is uniquely solvable with respect to all subsets of ‚Ñê ‚à™ ‚Ñê ‚Ä≤ \\\\mathcal{I}\\\\cup\\\\mathcal{I}^{\\\\prime} , the intervened twin SCM is simple. Pushing forward ‚Ñô ‚Ñ∞ \\\\mathbb{P}_{\\\\mathcal{E}} through this solution map defines the shift‚Äìscale counterfactual distribution ‚Ñô ( X , X ‚Ä≤ ) \\\\mathbb{P}_{(X,X^{\\\\prime})} .\\n‚àé\\nProposition 8.2 in Bongers et al. ( 2021 ) shows that the class of simple SCMs is closed under (i) marginalization, (ii) perfect interventions (do), and (iii) the twin construction. It does not give a sufficient condition for simplicity; it presupposes simplicity.\\nTheorem 2 gives sufficient analytic conditions for unique solvability of the twin under a class of soft, parametric shift‚Äìscale interventions, thereby ensuring well-posed counterfactuals beyond hard do.\\n(1) Composition reduces to a single affine map. For any coordinate j ‚àà ‚Ñê j\\\\in\\\\mathcal{I} the successive\\nupdates act as x j ‚Ü¶ a j ( m ) \\u200b ( a j ( m ‚àí 1 ) \\u200b ( ‚Ä¶ \\u200b a j ( 1 ) \\u200b f j \\u200b ( x , e ) + b j ( 1 ) \\u200b ‚Ä¶ ) + b j ( m ‚àí 1 ) ) + b j ( m ) , x_{j}\\\\;\\\\mapsto\\\\;a^{(m)}_{j}\\\\!\\\\bigl(a^{(m-1)}_{j}(\\\\dots a^{(1)}_{j}f_{j}(x,e)+b^{(1)}_{j}\\\\dots)+b^{(m-1)}_{j}\\\\bigr)+b^{(m)}_{j}, which can be written a j comp \\u200b f j \\u200b ( x , e ) + b j comp a^{\\\\mathrm{comp}}_{j}f_{j}(x,e)+b^{\\\\mathrm{comp}}_{j} with a j comp := ‚àè r : j ‚àà ‚Ñê ~ ( r ) a j ( r ) a^{\\\\mathrm{comp}}_{j}:=\\\\prod_{r:\\\\;j\\\\in\\\\tilde{\\\\mathcal{I}}^{(r)}}a^{(r)}_{j} and a corresponding affine drift b j comp b^{\\\\mathrm{comp}}_{j} .\\nSince each factor satisfies | a j ( r ) | ‚â§ 1 |a^{(r)}_{j}|\\\\leq 1 ,\\nwe have | a j comp | ‚â§ 1 \\\\lvert a^{\\\\mathrm{comp}}_{j}\\\\rvert\\\\leq 1 .\\nHence the final system coincides with g \\u200b ( x , e ) = a comp ‚äô f \\u200b ( x , e ) + b comp g(x,e)=a^{\\\\mathrm{comp}}\\\\!\\\\odot f(x,e)+b^{\\\\mathrm{comp}} .\\n(2) Contraction constant is preserved. Let D := diag \\u2061 ( a comp ) D:=\\\\operatorname{diag}(a^{\\\\mathrm{comp}}) denote the diagonal\\nmatrix of multiplicative factors.\\nFor any u ‚àà ‚Ñù | ‚Ñê | u\\\\in\\\\mathbb{R}^{|\\\\mathcal{I}|} , ‚Äñ D \\u200b u ‚Äñ p ‚â§ ‚Äñ u ‚Äñ p \\\\|D\\\\,u\\\\|_{p}\\\\leq\\\\|u\\\\|_{p} because every diagonal entry satisfies | a j comp | ‚â§ 1 |a^{\\\\mathrm{comp}}_{j}|\\\\leq 1 .\\nHence for all x , y x,y and any e e\\nThus g g inherits the same global contraction constant Œ∫ \\\\kappa .\\n(3) Simplicity after intervention. Because the intervened mechanism is still Œ∫ \\\\kappa -contractive with Œ∫ < 1 \\\\kappa<1 , Theorem 1 applies verbatim:\\nthe intervened model is uniquely solvable with respect to every subset\\nof endogenous variables, i.e. it is simple.\\n‚àé\\nThe proof of Proposition 1 used the bound | a j | ‚â§ 1 |a_{j}|\\\\leq 1 to conclude that the diagonal scaling D = diag \\u2061 ( a j ) D=\\\\operatorname{diag}(a_{j}) obeys ‚Äñ D \\u200b u ‚Äñ p ‚â§ ‚Äñ u ‚Äñ p \\\\|Du\\\\|_{p}\\\\leq\\\\|u\\\\|_{p} and therefore does not enlarge the\\nglobal Lipschitz constant.\\nIf some multipliers satisfy | a j | > 1 |a_{j}|>1 , simplicity can still hold, but\\none must verify an additional criterion:\\nLet Œ∫ < 1 \\\\kappa<1 be the original contraction constant of f f and define\\nBecause ‚Äñ D ‚Äñ p = max j \\u2061 | a j | \\\\|D\\\\|_{p}=\\\\max_{j}|a_{j}| , the intervened mechanism is\\nglobally Œ∫ max \\\\kappa_{\\\\max} -Lipschitz: ‚Äñ D \\u200b f \\u200b ( x , e ) ‚àí D \\u200b f \\u200b ( y , e ) ‚Äñ p ‚â§ Œ∫ max \\u200b ‚Äñ x ‚àí y ‚Äñ p \\\\|Df(x,e)-Df(y,e)\\\\|_{p}\\\\leq\\\\kappa_{\\\\max}\\\\|x-y\\\\|_{p} .\\nIf Œ∫ max < 1 \\\\kappa_{\\\\max}<1 the model remains a contraction and is therefore\\nsimple; if Œ∫ max ‚â• 1 \\\\kappa_{\\\\max}\\\\geq 1 the contraction proof no longer ensures\\nuniqueness and additional analysis is required.\\nLet Œ¶ : ‚Ñ∞ ‚Üí ùí≥ √ó ùí≥ \\\\Phi:\\\\mathcal{E}\\\\to\\\\mathcal{X}\\\\times\\\\mathcal{X} denote the unique measurable solution map of the shift‚Äìscale‚Äìintervened twin SCM. That is,\\nwhere ùêÑ ‚àº ‚Ñô ‚Ñ∞ \\\\mathbf{E}\\\\sim\\\\mathbb{P}_{\\\\mathcal{E}} is the exogenous random variable. By the global Œ∫ \\\\kappa -contraction in x x and the 1 1 -Lipschitz property in e e (both in ‚Ñì 2 \\\\ell^{2} ), Œ¶ \\\\Phi is L L -Lipschitz with constant L := 1 1 ‚àí Œ∫ L:=\\\\frac{1}{1-\\\\kappa} in ‚Ñì 2 \\\\ell^{2} :\\nTake any h : ùí≥ √ó ùí≥ ‚Üí ‚Ñù h:\\\\mathcal{X}\\\\times\\\\mathcal{X}\\\\to\\\\mathbb{R} that is 1 1 -Lipschitz in ‚Ñì 2 \\\\ell^{2} .\\nWrite ùêÑ = Œº + Œ£ 1 / 2 \\u200b ùêô \\\\mathbf{E}=\\\\mu+\\\\Sigma^{1/2}\\\\mathbf{Z} with ùêô ‚àº ùí© \\u200b ( 0 , I ) \\\\mathbf{Z}\\\\sim\\\\mathcal{N}(0,I) .\\nThen g \\u200b ( z ) := h \\u200b ( Œ¶ \\u200b ( Œº + Œ£ 1 / 2 \\u200b z ) ) g(z):=h\\\\bigl(\\\\Phi(\\\\mu+\\\\Sigma^{1/2}z)\\\\bigr) is L \\u200b ‚Äñ Œ£ 1 / 2 ‚Äñ 2 L\\\\|\\\\Sigma^{1/2}\\\\|_{\\\\mathrm{2}} -Lipschitz in ‚Ñì 2 \\\\ell^{2} .\\nSince Œ£ ‚™Ø œÉ 2 \\u200b I \\\\Sigma\\\\preceq\\\\sigma^{2}I , we have ‚Äñ Œ£ 1 / 2 ‚Äñ 2 ‚â§ œÉ \\\\|\\\\Sigma^{1/2}\\\\|_{\\\\mathrm{2}}\\\\leq\\\\sigma .\\nBy Gaussian concentration for Lipschitz functions (e.g., (Vershynin, 2018 , Thm.\\xa05.2.3) ),\\nThis proves the stated concentration inequality.\\n‚àé\\nLet all Lipschitz and contraction conditions in Proposition 2 be measured in ‚Ñì p \\\\ell^{p} ( 1 ‚â§ p ‚â§ ‚àû 1\\\\leq p\\\\leq\\\\infty ), and keep the Gaussian noise assumption ùêÑ ‚àº ùí© \\u200b ( Œº , Œ£ ) \\\\mathbf{E}\\\\sim\\\\mathcal{N}(\\\\mu,\\\\Sigma) .\\nThen for every 1 1 -Lipschitz h : ( ùí≥ √ó ùí≥ , ‚à• ‚ãÖ ‚à• p ) ‚Üí ‚Ñù h:(\\\\mathcal{X}\\\\times\\\\mathcal{X},\\\\|\\\\cdot\\\\|_{p})\\\\to\\\\mathbb{R} and all t > 0 t>0 ,\\nwhere ‚Äñ A ‚Äñ 2 ‚Üí p := sup x ‚â† 0 ‚Äñ A \\u200b x ‚Äñ p ‚Äñ x ‚Äñ 2 \\\\|A\\\\|_{2\\\\to p}:=\\\\sup_{x\\\\neq 0}\\\\frac{\\\\|Ax\\\\|_{p}}{\\\\|x\\\\|_{2}} .\\nIn particular, if Œ£ ‚™Ø œÉ 2 \\u200b I \\\\Sigma\\\\preceq\\\\sigma^{2}I and d := dim ( ‚Ñ∞ ) d:=\\\\dim(\\\\mathcal{E}) , then\\nso the proxy equals ( 1 ‚àí Œ∫ ) ‚àí 2 \\u200b œÉ 2 (1-\\\\kappa)^{-2}\\\\sigma^{2} for p ‚â• 2 p\\\\geq 2 , and ( 1 ‚àí Œ∫ ) ‚àí 2 \\u200b œÉ 2 \\u200b d 2 \\u200b ( 1 p ‚àí 1 2 ) (1-\\\\kappa)^{-2}\\\\sigma^{2}\\\\,d^{\\\\,2(\\\\frac{1}{p}-\\\\frac{1}{2})} for 1 ‚â§ p < 2 1\\\\leq p<2 . Proof sketch. Write ùêÑ = Œº + Œ£ 1 / 2 \\u200b ùêô \\\\mathbf{E}=\\\\mu+\\\\Sigma^{1/2}\\\\mathbf{Z} , ùêô ‚àº ùí© \\u200b ( 0 , I ) \\\\mathbf{Z}\\\\sim\\\\mathcal{N}(0,I) ; then z ‚Ü¶ h \\u200b ( Œ¶ \\u200b ( Œº + Œ£ 1 / 2 \\u200b z ) ) z\\\\mapsto h(\\\\Phi(\\\\mu+\\\\Sigma^{1/2}z)) is ( 1 ‚àí Œ∫ ) ‚àí 1 \\u200b ‚Äñ Œ£ 1 / 2 ‚Äñ 2 ‚Üí p (1-\\\\kappa)^{-1}\\\\|\\\\Sigma^{1/2}\\\\|_{2\\\\to p} -Lipschitz in ‚Ñì 2 \\\\ell^{2} , and Gaussian Lipschitz concentration applies.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Example',\n",
       "    'paragraphs': 'Consider a linear cyclic SCM with mutual dependence between consumption ( C C ) and income ( I I ):\\nWritten in matrix form X = ( C , I ) ‚ä§ X\\\\!=\\\\!(C,I)^{\\\\top} , X = A \\u200b X + b + E X=AX+b+E with\\nBecause the spectral norm is ‚Äñ A ‚Äñ 2 ‚â§ ‚Äñ A ‚Äñ F = 0.25 + 0.16 = 0.6403 < 1 \\\\|A\\\\|_{2}\\\\leq\\\\|A\\\\|_{F}=\\\\sqrt{0.25+0.16}=0.6403<1 , the model is globally contractive and therefore simple in the sense of Definition 5 .\\nFor any 2 √ó 2 2{\\\\times}2 matrix A = [ 0 a b 0 ] A=\\\\bigl[\\\\begin{smallmatrix}0&a\\\\\\\\\\nb&0\\\\end{smallmatrix}\\\\bigr] with a \\u200b b < 1 ab<1 , ( I ‚àí A ) ‚àí 1 = 1 1 ‚àí a \\u200b b \\u200b [ 1 a b 1 ] . (I-A)^{-1}=\\\\frac{1}{1-ab}\\\\bigl[\\\\begin{smallmatrix}1&a\\\\\\\\\\nb&1\\\\end{smallmatrix}\\\\bigr]. With ( a , b ) = ( 0.50 , 0.40 ) (a,b)=(0.50,0.40) we have ( I ‚àí A ) ‚àí 1 = [ 1.25 0.625 0.50 1.25 ] (I-A)^{-1}=\\\\bigl[\\\\begin{smallmatrix}1.25&0.625\\\\\\\\\\n0.50&1.25\\\\end{smallmatrix}\\\\bigr] and therefore\\nThus X obs ‚àº ùí© \\u200b ( Œº , Œ£ ) X_{\\\\text{obs}}\\\\sim\\\\mathcal{N}\\\\!\\\\bigl(\\\\mu,\\\\Sigma\\\\bigr) with Œº \\\\mu and Œ£ \\\\Sigma given in\\n( 3 )‚Äì( 4 ). The correlation between C C and I I is œÅ = 0.75 \\\\rho=0.75 .\\nSuppose a fiscal policy reform dampens the effect of both consumption and random shocks on income by a factor Œ± = 0.8 \\\\alpha=0.8 , and provides a fixed income supplement of Œ≤ = 1.0 \\\\beta=1.0 units:\\nThe intervened structural parameters are\\nScaling also affects the exogenous term: E ‚Ä≤ = ( E C , Œ± \\u200b E I ) ‚ä§ , Œ£ E ‚Ä≤ = diag \\u2061 ( 0.04 , Œ± 2 ‚ãÖ 0.04 ) = diag \\u2061 ( 0.04 , 0.0256 ) . E^{\\\\prime}=(E_{C},\\\\,\\\\alpha E_{I})^{\\\\top},\\\\;\\\\Sigma_{E^{\\\\prime}}=\\\\operatorname{diag}(0.04,\\\\alpha^{2}\\\\!\\\\cdot\\\\!0.04)=\\\\operatorname{diag}(0.04,0.0256). Contractivity is preserved, hence a unique interventional equilibrium exists:\\nConsumption rises by ‚àº 29 % \\\\sim\\\\!29\\\\,\\\\% and income by ‚àº 82 % \\\\sim\\\\!82\\\\,\\\\% , while the C C ‚Äì I I correlation falls from 0.75 0.75 to 0.69 0.69 .\\nEquations ( 5 )‚Äì( 6 ) describe the interventional distribution P \\u200b ( X int ) P(X_{\\\\text{int}}) , i.e. what we would observe if the\\nshift‚Äìscale policy were enacted for the whole population . To answer\\nindividual‚Äìlevel counterfactual queries (‚Äúwhat would this household‚Äôs consumption be had the policy applied?‚Äù) we follow the twin\\nnetwork construction. Let ( c , i ) (c,i) denote the actually observed consumption and income for one\\nhousehold. The twin‚ÄëSCM duplicates every endogenous variable and shares the\\nsame exogenous noise:\\nwhere the primed copy encodes the shift‚Äìscale intervention ss ( I , Œ± = 0.8 , Œ≤ = 1.0 ) \\\\mathrm{ss}(I,\\\\alpha{=}0.8,\\\\beta{=}1.0) and the unprimed copy remains\\nfactual.\\nFrom the factual equations, we can solve\\nEliminating C ‚Ä≤ C^{\\\\prime} from the primed equations of the twin SCM yields 0.84 \\u200b I ‚Ä≤ = 1.72 + 0.8 \\u200b E I + 0.32 \\u200b E C . 0.84\\\\,I^{\\\\prime}=1.72+0.8\\\\,E_{I}+0.32\\\\,E_{C}. Substituting 7 gives the counterfactual income I ‚Ä≤ \\u200b ( c , i ) = 25 21 + 16 21 \\u200b i = 1.190476 + 0.761905 \\u200b i . I^{\\\\prime}(c,i)=\\\\frac{25}{21}+\\\\frac{16}{21}\\\\,i=1.190476+0.761905\\\\,i. Back‚Äësubstitution furnishes the counterfactual consumption C ‚Ä≤ \\u200b ( c , i ) = c + 25 42 ‚àí 5 42 \\u200b i = c + 0.595238 ‚àí 0.119048 \\u200b i . C^{\\\\prime}(c,i)=c\\\\;+\\\\;\\\\tfrac{25}{42}-\\\\tfrac{5}{42}\\\\,i=c+0.595238-0.119048\\\\,i. Above equations give the counterfactual response mapping ( c , i ) ‚Ü¶ ( C ‚Ä≤ , I ‚Ä≤ ) (c,i)\\\\mapsto(C^{\\\\prime},I^{\\\\prime}) .\\nBecause the mapping is affine and the factual distribution\\nis Gaussian, the marginal counterfactual ( C ‚Ä≤ , I ‚Ä≤ ) (C^{\\\\prime},I^{\\\\prime}) is again Gaussian with ùîº \\u200b [ ( C ‚Ä≤ , I ‚Ä≤ ) ‚ä§ ] = ( 2.024 , 2.048 ) ‚ä§ \\\\mathbb{E}[(C^{\\\\prime},I^{\\\\prime})^{\\\\top}]\\\\!=\\\\!(2.024,2.048)^{\\\\top} and covariance\\nexactly matching ( 6 ).\\nHere, the twin‚ÄëSCM reconciles individual counterfactual semantics ( C ‚Ä≤ \\u200b ( c , i ) , I ‚Ä≤ \\u200b ( c , i ) ) (C^{\\\\prime}(c,i),I^{\\\\prime}(c,i)) with the population‚Äëlevel interventional\\ndistribution already reported,while remaining in closed form due to the linearity and contractiveness of the model.',\n",
       "    'subsections': [{'title': 'Shift‚ÄìScale intervention on II.',\n",
       "      'paragraphs': 'Suppose a fiscal policy reform dampens the effect of both consumption and random shocks on income by a factor Œ± = 0.8 \\\\alpha=0.8 , and provides a fixed income supplement of Œ≤ = 1.0 \\\\beta=1.0 units:\\nThe intervened structural parameters are\\nScaling also affects the exogenous term: E ‚Ä≤ = ( E C , Œ± \\u200b E I ) ‚ä§ , Œ£ E ‚Ä≤ = diag \\u2061 ( 0.04 , Œ± 2 ‚ãÖ 0.04 ) = diag \\u2061 ( 0.04 , 0.0256 ) . E^{\\\\prime}=(E_{C},\\\\,\\\\alpha E_{I})^{\\\\top},\\\\;\\\\Sigma_{E^{\\\\prime}}=\\\\operatorname{diag}(0.04,\\\\alpha^{2}\\\\!\\\\cdot\\\\!0.04)=\\\\operatorname{diag}(0.04,0.0256). Contractivity is preserved, hence a unique interventional equilibrium exists:\\nConsumption rises by ‚àº 29 % \\\\sim\\\\!29\\\\,\\\\% and income by ‚àº 82 % \\\\sim\\\\!82\\\\,\\\\% , while the C C ‚Äì I I correlation falls from 0.75 0.75 to 0.69 0.69 .',\n",
       "      'subsections': []},\n",
       "     {'title': 'From intervention to counterfactual via the twin\\xa0SCM.',\n",
       "      'paragraphs': 'Equations ( 5 )‚Äì( 6 ) describe the interventional distribution P \\u200b ( X int ) P(X_{\\\\text{int}}) , i.e. what we would observe if the\\nshift‚Äìscale policy were enacted for the whole population . To answer\\nindividual‚Äìlevel counterfactual queries (‚Äúwhat would this household‚Äôs consumption be had the policy applied?‚Äù) we follow the twin\\nnetwork construction. Let ( c , i ) (c,i) denote the actually observed consumption and income for one\\nhousehold. The twin‚ÄëSCM duplicates every endogenous variable and shares the\\nsame exogenous noise:\\nwhere the primed copy encodes the shift‚Äìscale intervention ss ( I , Œ± = 0.8 , Œ≤ = 1.0 ) \\\\mathrm{ss}(I,\\\\alpha{=}0.8,\\\\beta{=}1.0) and the unprimed copy remains\\nfactual.\\nFrom the factual equations, we can solve\\nEliminating C ‚Ä≤ C^{\\\\prime} from the primed equations of the twin SCM yields 0.84 \\u200b I ‚Ä≤ = 1.72 + 0.8 \\u200b E I + 0.32 \\u200b E C . 0.84\\\\,I^{\\\\prime}=1.72+0.8\\\\,E_{I}+0.32\\\\,E_{C}. Substituting 7 gives the counterfactual income I ‚Ä≤ \\u200b ( c , i ) = 25 21 + 16 21 \\u200b i = 1.190476 + 0.761905 \\u200b i . I^{\\\\prime}(c,i)=\\\\frac{25}{21}+\\\\frac{16}{21}\\\\,i=1.190476+0.761905\\\\,i. Back‚Äësubstitution furnishes the counterfactual consumption C ‚Ä≤ \\u200b ( c , i ) = c + 25 42 ‚àí 5 42 \\u200b i = c + 0.595238 ‚àí 0.119048 \\u200b i . C^{\\\\prime}(c,i)=c\\\\;+\\\\;\\\\tfrac{25}{42}-\\\\tfrac{5}{42}\\\\,i=c+0.595238-0.119048\\\\,i. Above equations give the counterfactual response mapping ( c , i ) ‚Ü¶ ( C ‚Ä≤ , I ‚Ä≤ ) (c,i)\\\\mapsto(C^{\\\\prime},I^{\\\\prime}) .\\nBecause the mapping is affine and the factual distribution\\nis Gaussian, the marginal counterfactual ( C ‚Ä≤ , I ‚Ä≤ ) (C^{\\\\prime},I^{\\\\prime}) is again Gaussian with ùîº \\u200b [ ( C ‚Ä≤ , I ‚Ä≤ ) ‚ä§ ] = ( 2.024 , 2.048 ) ‚ä§ \\\\mathbb{E}[(C^{\\\\prime},I^{\\\\prime})^{\\\\top}]\\\\!=\\\\!(2.024,2.048)^{\\\\top} and covariance\\nexactly matching ( 6 ).\\nHere, the twin‚ÄëSCM reconciles individual counterfactual semantics ( C ‚Ä≤ \\u200b ( c , i ) , I ‚Ä≤ \\u200b ( c , i ) ) (C^{\\\\prime}(c,i),I^{\\\\prime}(c,i)) with the population‚Äëlevel interventional\\ndistribution already reported,while remaining in closed form due to the linearity and contractiveness of the model.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Limitations',\n",
       "    'paragraphs': 'The Œ∫ \\\\kappa ‚Äìcontraction condition must hold uniformly over the entire state space; many realistic feedback systems may violate this in certain regimes, even though they still admit unique equilibria. Our concentration result further relies on Gaussianity of the exogenous variables. For heavy-tailed or merely bounded-moment noise, one typically obtains polynomial rather than exponential concentration, which we do not analyze here. We also invoke Banach‚Äôs theorem on closed (and hence complete) coordinate domains; models whose natural domains are open or lie on manifolds require additional care. From a practical standpoint, certifying global Lipschitz constants of black-box simulators is challenging; data-driven or local contraction diagnostics may be more feasible in applications. Finally, our closure results explicitly cover only shift‚Äìscale maps with bounded gains ( | a j | ‚â§ 1 |a_{j}|\\\\leq 1 ); interventions with larger multiplicative factors, stochastic policies, or more general functional forms are not yet addressed.\\nAt present, our analysis is restricted to shift‚Äìscale interventions . This class, however, already strictly generalizes hard (do-) interventions and provides a principled foundation for broader extensions. The key theoretical principle underpinning our guarantees is the preservation of global contractivity ‚Äîthat is, the global Lipschitz constant of the intervened system remains below one. This property ensures unique solvability of the intervened SCM, even in the presence of cycles, and is abstract enough to accommodate richer intervention types, provided they do not destroy contractivity. Consequently, our results extend directly to any intervention family (including nonlinear, stochastic, or more complex parametric changes) that preserves the contraction property. An explicit analysis of broader classes of interventions, and sufficient conditions under which they preserve contractivity, is a promising direction for future work',\n",
       "    'subsections': []},\n",
       "   {'title': '6  Conclusion and Future Work',\n",
       "    'paragraphs': 'We have established a principled foundation for counterfactual inference in cyclic structural causal models under shift‚Äìscale interventions. Leveraging a global contraction assumption, we proved that such models are simple, ensuring unique solvability even in the presence of feedback. We further showed that shift‚Äìscale interventions preserve solvability, are closed under composition, and admit sub-Gaussian tail bounds for counterfactual functionals under natural regularity assumptions. These results demonstrate that contraction-based SCMs offer a mathematically tractable yet expressive class for reasoning about interventions and counterfactuals in cyclic settings.\\nFuture research may focus on developing deep generative models for cyclic counterfactuals, leveraging the theoretical foundations established here.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgments and Disclosure of Funding',\n",
       "    'paragraphs': 'This research was funded in part by the Indo-French Centre for the Promotion of Advanced Research\\n(IFCPAR/CEFIPRA) through project number CSRP 6702-2.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Supplementary Material',\n",
       "    'paragraphs': 'Since ‚Ñ≥ \\\\mathcal{M} is simple, by definition there exists a unique measurable function g : ‚Ñ∞ ‚Üí ùí≥ g:\\\\mathcal{E}\\\\to\\\\mathcal{X} such that for every e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} ,\\nSimilarly, after the intervention do \\u200b ( X ùíú := x ~ ùíú ) \\\\mathrm{do}(X_{\\\\mathcal{A}}:=\\\\tilde{x}_{\\\\mathcal{A}}) , the intervened SCM admits a unique measurable solution map g do : ‚Ñ∞ ‚Üí ùí≥ g^{\\\\mathrm{do}}:\\\\mathcal{E}\\\\to\\\\mathcal{X} (Proposition 3.8, Bongers et al. ( 2021 ) ).\\nIn the twin SCM construction, both copies (factual and counterfactual) share the same exogenous vector ùêÑ \\\\mathbf{E} , and the intervention is applied only to the first copy.\\nSolving the twin SCM yields:\\nLet x obs ‚àà ùí≥ x^{\\\\mathrm{obs}}\\\\in\\\\mathcal{X} be the observed factual outcome. Pearl‚Äôs procedure involves:\\nAbduction: condition on the observation ùêó = x obs \\\\mathbf{X}=x^{\\\\mathrm{obs}} , which corresponds to conditioning on the set { e ‚àà ‚Ñ∞ : g \\u200b ( e ) = x obs } \\\\{e\\\\in\\\\mathcal{E}:g(e)=x^{\\\\mathrm{obs}}\\\\} ;\\nAction: apply the intervention to obtain the new function g do g^{\\\\mathrm{do}} ;\\nPrediction: evaluate ùêó cf = g do \\u200b ( ùêÑ ) \\\\mathbf{X}^{\\\\mathrm{cf}}=g^{\\\\mathrm{do}}(\\\\mathbf{E}) using the same noise, but now drawn from the posterior ‚Ñô ‚Ñ∞ ‚à£ g \\u200b ( ùêÑ ) = x obs \\\\mathbb{P}_{\\\\mathcal{E}\\\\mid g(\\\\mathbf{E})=x^{\\\\mathrm{obs}}} .\\nDefine the posterior distribution on exogenous noise:\\nThis measure is supported on the set\\nBy construction, ùêó ‚Ä≤ = g do \\u200b ( ùêÑ ) , \\\\mathbf{X}^{\\\\prime}=g^{\\\\mathrm{do}}(\\\\mathbf{E}), so the counterfactual distribution from the twin SCM (given ùêó = x obs \\\\mathbf{X}=x^{\\\\mathrm{obs}} ) is:\\nPearl‚Äôs method also conditions on { ùêó = x obs } \\\\{\\\\mathbf{X}=x^{\\\\mathrm{obs}}\\\\} , inducing the same posterior Œº x obs \\\\mu_{x^{\\\\mathrm{obs}}} over exogenous variables.\\nThen, the counterfactual outcome is ùêó cf = g do \\u200b ( ùêÑ ) \\\\mathbf{X}^{\\\\mathrm{cf}}=g^{\\\\mathrm{do}}(\\\\mathbf{E}) with ùêÑ ‚àº Œº x obs \\\\mathbf{E}\\\\sim\\\\mu_{x^{\\\\mathrm{obs}}} .\\nThus,\\nwhich is the same expression as for the twin SCM.\\n‚àé\\nThe counterfactual law derived from the twin SCM coincides with the one obtained from Pearl‚Äôs\\nabduction‚Äìaction‚Äìprediction procedure under unique solvability.\\nThe shorthand ùêó ‚àº ‚Ñ≥ \\\\mathbf{X}\\\\sim\\\\mathcal{M} means that the random vector ùêó \\\\mathbf{X} is a solution of\\nthe SCM ‚Ñ≥ \\\\mathcal{M} .',\n",
       "    'subsections': [{'title': 'A.1 Equivalence of Twin SCM and Pearl‚Äôs Abduction-Action-Prediction (AAP) Procedure',\n",
       "      'paragraphs': 'Since ‚Ñ≥ \\\\mathcal{M} is simple, by definition there exists a unique measurable function g : ‚Ñ∞ ‚Üí ùí≥ g:\\\\mathcal{E}\\\\to\\\\mathcal{X} such that for every e ‚àà ‚Ñ∞ e\\\\in\\\\mathcal{E} ,\\nSimilarly, after the intervention do \\u200b ( X ùíú := x ~ ùíú ) \\\\mathrm{do}(X_{\\\\mathcal{A}}:=\\\\tilde{x}_{\\\\mathcal{A}}) , the intervened SCM admits a unique measurable solution map g do : ‚Ñ∞ ‚Üí ùí≥ g^{\\\\mathrm{do}}:\\\\mathcal{E}\\\\to\\\\mathcal{X} (Proposition 3.8, Bongers et al. ( 2021 ) ).\\nIn the twin SCM construction, both copies (factual and counterfactual) share the same exogenous vector ùêÑ \\\\mathbf{E} , and the intervention is applied only to the first copy.\\nSolving the twin SCM yields:\\nLet x obs ‚àà ùí≥ x^{\\\\mathrm{obs}}\\\\in\\\\mathcal{X} be the observed factual outcome. Pearl‚Äôs procedure involves:\\nAbduction: condition on the observation ùêó = x obs \\\\mathbf{X}=x^{\\\\mathrm{obs}} , which corresponds to conditioning on the set { e ‚àà ‚Ñ∞ : g \\u200b ( e ) = x obs } \\\\{e\\\\in\\\\mathcal{E}:g(e)=x^{\\\\mathrm{obs}}\\\\} ;\\nAction: apply the intervention to obtain the new function g do g^{\\\\mathrm{do}} ;\\nPrediction: evaluate ùêó cf = g do \\u200b ( ùêÑ ) \\\\mathbf{X}^{\\\\mathrm{cf}}=g^{\\\\mathrm{do}}(\\\\mathbf{E}) using the same noise, but now drawn from the posterior ‚Ñô ‚Ñ∞ ‚à£ g \\u200b ( ùêÑ ) = x obs \\\\mathbb{P}_{\\\\mathcal{E}\\\\mid g(\\\\mathbf{E})=x^{\\\\mathrm{obs}}} .\\nDefine the posterior distribution on exogenous noise:\\nThis measure is supported on the set\\nBy construction, ùêó ‚Ä≤ = g do \\u200b ( ùêÑ ) , \\\\mathbf{X}^{\\\\prime}=g^{\\\\mathrm{do}}(\\\\mathbf{E}), so the counterfactual distribution from the twin SCM (given ùêó = x obs \\\\mathbf{X}=x^{\\\\mathrm{obs}} ) is:\\nPearl‚Äôs method also conditions on { ùêó = x obs } \\\\{\\\\mathbf{X}=x^{\\\\mathrm{obs}}\\\\} , inducing the same posterior Œº x obs \\\\mu_{x^{\\\\mathrm{obs}}} over exogenous variables.\\nThen, the counterfactual outcome is ùêó cf = g do \\u200b ( ùêÑ ) \\\\mathbf{X}^{\\\\mathrm{cf}}=g^{\\\\mathrm{do}}(\\\\mathbf{E}) with ùêÑ ‚àº Œº x obs \\\\mathbf{E}\\\\sim\\\\mu_{x^{\\\\mathrm{obs}}} .\\nThus,\\nwhich is the same expression as for the twin SCM.\\n‚àé\\nThe counterfactual law derived from the twin SCM coincides with the one obtained from Pearl‚Äôs\\nabduction‚Äìaction‚Äìprediction procedure under unique solvability.\\nThe shorthand ùêó ‚àº ‚Ñ≥ \\\\mathbf{X}\\\\sim\\\\mathcal{M} means that the random vector ùêó \\\\mathbf{X} is a solution of\\nthe SCM ‚Ñ≥ \\\\mathcal{M} .',\n",
       "      'subsections': [{'title': 'Equality of counterfactual laws.',\n",
       "        'paragraphs': 'By construction, ùêó ‚Ä≤ = g do \\u200b ( ùêÑ ) , \\\\mathbf{X}^{\\\\prime}=g^{\\\\mathrm{do}}(\\\\mathbf{E}), so the counterfactual distribution from the twin SCM (given ùêó = x obs \\\\mathbf{X}=x^{\\\\mathrm{obs}} ) is:\\nPearl‚Äôs method also conditions on { ùêó = x obs } \\\\{\\\\mathbf{X}=x^{\\\\mathrm{obs}}\\\\} , inducing the same posterior Œº x obs \\\\mu_{x^{\\\\mathrm{obs}}} over exogenous variables.\\nThen, the counterfactual outcome is ùêó cf = g do \\u200b ( ùêÑ ) \\\\mathbf{X}^{\\\\mathrm{cf}}=g^{\\\\mathrm{do}}(\\\\mathbf{E}) with ùêÑ ‚àº Œº x obs \\\\mathbf{E}\\\\sim\\\\mu_{x^{\\\\mathrm{obs}}} .\\nThus,\\nwhich is the same expression as for the twin SCM.\\n‚àé\\nThe counterfactual law derived from the twin SCM coincides with the one obtained from Pearl‚Äôs\\nabduction‚Äìaction‚Äìprediction procedure under unique solvability.\\nThe shorthand ùêó ‚àº ‚Ñ≥ \\\\mathbf{X}\\\\sim\\\\mathcal{M} means that the random vector ùêó \\\\mathbf{X} is a solution of\\nthe SCM ‚Ñ≥ \\\\mathcal{M} .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Most counterfactual inference frameworks traditionally assume acyclic structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However, many real-world systems (e.g. biological systems) contain feedback loops or cyclic dependencies that violate acyclicity. In this work, we study counterfactual inference in cyclic SCMs under shift‚Äìscale interventions,\\n i.e., soft, policy-style changes\\n that rescale and/or shift a variable‚Äôs mechanism.'},\n",
       " '2510.25758v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'The escalating global demand for mental health services has starkly outpaced the availability of trained human therapists, creating a critical gap in care Hua et\\xa0al. ( 2025 ); Na et\\xa0al. ( 2025a ) . In response, Large Language Models (LLMs) have emerged as a promising frontier for delivering scalable, accessible, and immediate psychological support Qiu et\\xa0al. ( 2024 ); Liu et\\xa0al. ( 2023a ); Lee et\\xa0al. ( 2024 ) . Early explorations have demonstrated their remarkable capacity to generate empathetic and contextually relevant responses, establishing a foundational capability for automated counseling systems Zhang et\\xa0al. ( 2024c ); Weizenbaum ( 1983 ) . These initial successes have catalyzed a wave of research aimed at transforming conversational AI from simple chatbots into effective therapeutic partners Qiu and Lan ( 2025 ); Xie et\\xa0al. ( 2025 ) .\\nDespite these advancements, current LLM-based counseling agents, such as ChatCounselor and PsyLLM Liu et\\xa0al. ( 2023a ); Hu et\\xa0al. ( 2025a ) , operate under a paradigm that fundamentally misaligns with the principles of genuine psychotherapy. These systems are predominantly architected for single-session contexts, exhibiting a form of ‚Äôclinical amnesia‚Äô that prevents them from building upon past interactions. As illustrated in Figure 1 , they lack the crucial ability to perform cross-session memory retrieval , a deficit that erodes the therapeutic alliance and hinders long-term progress. Furthermore, they suffer from strategic rigidity; most are hard-coded with a single therapeutic modality (e.g., Cognitive Behavioral Therapy) and cannot flexibly adjust treatment methods across sessions in response to a patient‚Äôs evolving needs Moggia et\\xa0al. ( 2024 ); Comai et\\xa0al. ( 2025 ); Zarbo et\\xa0al. ( 2016 ) . This lack of longitudinal memory and strategic adaptability reduces the complex therapeutic process to disjointed, tactical exchanges, failing to provide the continuous follow-up that is the hallmark of effective care.\\nTo bridge this critical gap between reactive dialogue and strategic therapy, we introduce TheraMind , a strategic and adaptive agent for longitudinal psychological counseling. As illustrated in Figure 1 , the cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. Within a single session, the Intra-Session Loop moves beyond generating simple empathetic responses. It employs a Reaction Classifier to perceive the patient‚Äôs emotional state and attitude, which in turn informs the dynamic selection of a response strategy, such as a Supportive or Challenging approach, while continuously tracking the current treatment stage. Crucially, the Cross-Session Loop empowers the agent with long-term strategic capabilities. By analyzing the dialogue history after each session, TheraMind evaluates therapeutic efficacy and adaptively selects or adjusts the treatment method for subsequent interactions, ensuring the entire counseling arc is coherent, personalized, and goal-oriented.\\nTo validate our approach, we conducted extensive experiments within a high-fidelity simulation environment grounded in real clinical cases. Our results demonstrate that TheraMind establishes a new state-of-the-art, significantly outperforming a wide range of baselines, including specialized counseling models, general-purpose LLMs, and other agent-based systems. The performance gains are particularly pronounced in multi-session metrics such as Coherence, Flexibility, and Therapeutic Attunement, directly confirming the efficacy of our dual-loop design. Furthermore, both comprehensive ablation studies and human evaluations corroborate that TheraMind‚Äôs architecture produces more clinically sound, strategically adaptive, and coherent therapeutic dialogues.\\nThis work makes a definitive shift from static, single-turn models to a dynamic, longitudinal agent that more closely emulates the cognitive processes of a human therapist. The primary contributions of this paper are as follows:\\nA novel dual-loop agent framework (TheraMind) that effectively models both the tactical, turn-by-turn dynamics and the strategic, multi-session arc of psychological counseling.\\nAn adaptive therapy selection mechanism that enables the agent to evaluate its own efficacy and adjust its core treatment method across sessions, overcoming the static, single-therapy paradigm of prior work.\\nA clinically-grounded dialogue management system that integrates patient state perception, dynamic response strategy selection, and treatment phase awareness to transform response generation into a deliberative clinical intervention.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Current LLM-based counseling approaches lack emotional understanding, adaptive strategy use, and dynamic adjustment across sessions with long-term memory, limiting their clinical realism. Intelligent counseling agents aim to address these gaps by enabling more adaptive, multi-session therapeutic support.\\nCounseling dialogue systems are conversational agents for mental health that emphasize empathy, therapeutic alliance, and safety beyond task-oriented systems Na et\\xa0al. ( 2025b ); Hu et\\xa0al. ( 2025b ); Ge et\\xa0al. ( 2025 ) . Early work like ELIZA showed that simple pattern matching could emulate therapist-like responses Weizenbaum ( 1983 ) , inspiring subsequent NLP methods for emotion detection Poria et\\xa0al. ( 2019 ) , dialogue act classification He et\\xa0al. ( 2021 ) , and context-aware therapeutic response generation Zhang et\\xa0al. ( 2024c ) . Some systems embed structured techniques such as CBT Beck ( 2011 ); Lee et\\xa0al. ( 2024 ) or motivational interviewing Wu et\\xa0al. ( 2022 ) .\\nRecent efforts adapt LLMs through supervised fine-tuning (SFT) on counseling data. ChatCounselor Liu et\\xa0al. ( 2023a ) uses the Psych8k corpus to achieve near-ChatGPT performance on counseling benchmarks. CPsyCoun Zhang et\\xa0al. ( 2024a ) reconstructs Chinese dialogues via a two-phase framework and provides evaluation datasets. MentaLLaMA Yang et\\xa0al. ( 2024 ) explores instruction tuning for empathy and reasoning, while PsyLLM Hu et\\xa0al. ( 2025a ) integrates diagnostic and therapeutic reasoning. KokoroChat Qi et\\xa0al. ( 2025 ) offers 6,589 Japanese dialogues, improving fine-tuned model performance. PsyDial Qiu and Lan ( 2025 ) builds longitudinal datasets via privacy-preserving RMRR, enabling long-term modeling. PsyDT Xie et\\xa0al. ( 2025 ) introduces a digital twin paradigm with dynamic one-shot learning and GPT-4‚Äìguided synthesis.\\nBeyond SFT, reinforcement learning and preference optimization enhance adaptability. COMPEER Wang et\\xa0al. ( 2025b ) applies RL for controllable empathetic reasoning, and Zhang et\\xa0al. ( 2025 ) introduce PsychoCounsel-Preference, a professional preference dataset for reward and policy learning. However, SFT struggles to unify diverse capabilities, while RL is sensitive to reward design Ghosh et\\xa0al. ( 2024 ); Kwon et\\xa0al. ( 2023 ) , leading to models that cannot fully adapt to real-world therapeutic complexity Tran et\\xa0al. ( 2025 ) .\\nLLMs such as ChatGPT and Gemini enable autonomous agents integrating reasoning, planning, memory, and tool use OpenAI ( 2023 ); Anil et\\xa0al. ( 2023 ) . Memory-augmented, context-augmented, and hierarchical architectures allow perception‚Äìaction loops for complex tasks Bo et\\xa0al. ( 2024 ); Zhou et\\xa0al. ( 2025 , 2023 ); Liu et\\xa0al. ( 2024b ); Wang et\\xa0al. ( 2024 ); Sun et\\xa0al. ( 2025 ); Zhou et\\xa0al. ( 2024 ) . While mostly focused on general tasks, recent work adapts agents for healthcare and mental health: coupling behavioral sensing with proactive support Ji et\\xa0al. ( 2025 ) , simulating therapeutic roles Chen et\\xa0al. ( 2025 ) , or structuring psychiatric interviews Bi et\\xa0al. ( 2025 ) . Specialized agents have emerged, such as AnnaAgent with Multi-Session memory Wang et\\xa0al. ( 2025a ) , EmoAgent focusing on safety Mao et\\xa0al. ( 2025 ) , Interactive Agents for role-play training Qiu and Lan ( 2024 ) , Œ® \\\\Psi -Arena for interactive benchmarking Zhu et\\xa0al. ( 2025 ) , SweetieChat for strategy-enhanced emotional support Ye et\\xa0al. ( 2025 ) , and AutoCBT for CBT-based multi-agent counseling Xu et\\xa0al. ( 2025 ) .',\n",
       "    'subsections': [{'title': '2.1 Counseling Dialogue Systems',\n",
       "      'paragraphs': 'Counseling dialogue systems are conversational agents for mental health that emphasize empathy, therapeutic alliance, and safety beyond task-oriented systems Na et\\xa0al. ( 2025b ); Hu et\\xa0al. ( 2025b ); Ge et\\xa0al. ( 2025 ) . Early work like ELIZA showed that simple pattern matching could emulate therapist-like responses Weizenbaum ( 1983 ) , inspiring subsequent NLP methods for emotion detection Poria et\\xa0al. ( 2019 ) , dialogue act classification He et\\xa0al. ( 2021 ) , and context-aware therapeutic response generation Zhang et\\xa0al. ( 2024c ) . Some systems embed structured techniques such as CBT Beck ( 2011 ); Lee et\\xa0al. ( 2024 ) or motivational interviewing Wu et\\xa0al. ( 2022 ) .\\nRecent efforts adapt LLMs through supervised fine-tuning (SFT) on counseling data. ChatCounselor Liu et\\xa0al. ( 2023a ) uses the Psych8k corpus to achieve near-ChatGPT performance on counseling benchmarks. CPsyCoun Zhang et\\xa0al. ( 2024a ) reconstructs Chinese dialogues via a two-phase framework and provides evaluation datasets. MentaLLaMA Yang et\\xa0al. ( 2024 ) explores instruction tuning for empathy and reasoning, while PsyLLM Hu et\\xa0al. ( 2025a ) integrates diagnostic and therapeutic reasoning. KokoroChat Qi et\\xa0al. ( 2025 ) offers 6,589 Japanese dialogues, improving fine-tuned model performance. PsyDial Qiu and Lan ( 2025 ) builds longitudinal datasets via privacy-preserving RMRR, enabling long-term modeling. PsyDT Xie et\\xa0al. ( 2025 ) introduces a digital twin paradigm with dynamic one-shot learning and GPT-4‚Äìguided synthesis.\\nBeyond SFT, reinforcement learning and preference optimization enhance adaptability. COMPEER Wang et\\xa0al. ( 2025b ) applies RL for controllable empathetic reasoning, and Zhang et\\xa0al. ( 2025 ) introduce PsychoCounsel-Preference, a professional preference dataset for reward and policy learning. However, SFT struggles to unify diverse capabilities, while RL is sensitive to reward design Ghosh et\\xa0al. ( 2024 ); Kwon et\\xa0al. ( 2023 ) , leading to models that cannot fully adapt to real-world therapeutic complexity Tran et\\xa0al. ( 2025 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 LLM-driven Autonomous Agents',\n",
       "      'paragraphs': 'LLMs such as ChatGPT and Gemini enable autonomous agents integrating reasoning, planning, memory, and tool use OpenAI ( 2023 ); Anil et\\xa0al. ( 2023 ) . Memory-augmented, context-augmented, and hierarchical architectures allow perception‚Äìaction loops for complex tasks Bo et\\xa0al. ( 2024 ); Zhou et\\xa0al. ( 2025 , 2023 ); Liu et\\xa0al. ( 2024b ); Wang et\\xa0al. ( 2024 ); Sun et\\xa0al. ( 2025 ); Zhou et\\xa0al. ( 2024 ) . While mostly focused on general tasks, recent work adapts agents for healthcare and mental health: coupling behavioral sensing with proactive support Ji et\\xa0al. ( 2025 ) , simulating therapeutic roles Chen et\\xa0al. ( 2025 ) , or structuring psychiatric interviews Bi et\\xa0al. ( 2025 ) . Specialized agents have emerged, such as AnnaAgent with Multi-Session memory Wang et\\xa0al. ( 2025a ) , EmoAgent focusing on safety Mao et\\xa0al. ( 2025 ) , Interactive Agents for role-play training Qiu and Lan ( 2024 ) , Œ® \\\\Psi -Arena for interactive benchmarking Zhu et\\xa0al. ( 2025 ) , SweetieChat for strategy-enhanced emotional support Ye et\\xa0al. ( 2025 ) , and AutoCBT for CBT-based multi-agent counseling Xu et\\xa0al. ( 2025 ) .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Methodology',\n",
       "    'paragraphs': 'Existing LLM-based counseling agents are predominantly designed as reactive systems, optimized for generating empathetic, single-turn responses within a limited context. This approach fundamentally fails to capture the longitudinal, adaptive, and strategic nature of genuine psychotherapy. To bridge this critical gap, we introduce TheraMind , an autonomous agent framework engineered to emulate the hierarchical cognitive processes of a human therapist over a multi-session treatment arc.\\nThe cornerstone of TheraMind is a novel dual-loop architecture , illustrated in Figure 2 . This design decouples the complex task of counseling into two distinct yet interconnected control flows:\\nThe Intra-Session Loop governs tactical conversational execution , managing the nuanced, turn-by-turn dynamics within a single counseling session.\\nThe Cross-Session Loop directs strategic therapeutic planning , overseeing the high-level treatment trajectory and its adaptation across multiple sessions.\\nThis hierarchical decomposition enables TheraMind to maintain both immediate conversational coherence and long-term therapeutic objectives simultaneously.\\nThe Intra-Session Loop is the agent‚Äôs core engine for real-time interaction. Its purpose is to transcend superficial text generation by endowing the agent with perceptual acuity and tactical responsiveness. For each conversational turn t t , this loop executes a multi-stage process from perception to action.\\nHuman therapists react not just to the literal content of a patient‚Äôs words, but to the underlying emotional and intentional subtext. Existing models, which primarily process raw text, lack this crucial layer of perception. To address this, we introduce a dedicated perception module.\\nUpon receiving a patient utterance U t U_{t} , TheraMind employs a Reaction Classifier to infer a structured representation of the patient‚Äôs immediate psychological state. Formally, this perception function, Œ¶ perceive \\\\Phi_{\\\\text{perceive}} , maps an utterance to a state tuple:\\nwhere E t E_{t} is the primary emotion , I t ‚àà [ 0 , 1 ] I_{t}\\\\in[0,1] is its intensity , and A t ‚àà { Cooperative, Resistant } A_{t}\\\\in\\\\{\\\\text{Cooperative, Resistant}\\\\} is the conversational attitude .\\nThis explicit state representation allows TheraMind to move beyond mere linguistic pattern-matching and engage with the patient‚Äôs underlying psychological state. It provides a robust, intermediate signal for following decision-making, enabling more nuanced and strategically empathetic interactions.\\nContinuity plays a crucial role in shaping a therapist agent‚Äôs responses during longitudinal psychological counseling. Patients expect therapists to remember significant details from past conversations. Standard LLMs with fixed context windows suffer from catastrophic forgetting, undermining trust and therapeutic progress.\\nTo ensure longitudinal coherence, TheraMind integrates a dynamic memory system. Given the current utterance U t U_{t} and the complete history of all previous sessions ‚Ñã 1 : k ‚àí 1 \\\\mathcal{H}_{1:k-1} , a Memory Recall function Œ¶ memory \\\\Phi_{\\\\text{memory}} produces a relevant memory summary M t M_{t} :\\nThis function internally performs a relevance check; if no pertinent history is found, M t M_{t} is null.\\nThis mechanism provides a dynamically curated context that is both relevant and succinct. It ensures the agent‚Äôs responses are historically informed, preventing the disjointedness that plagues stateless models.\\nEffective therapeutic responses are not just empathetic; they are purposeful interventions. Generation must be guided by clear clinical objectives, rather than being an unconstrained generative process. We achieve this by conditioning the LLM on explicit strategic and procedural guidance.\\nThe agent‚Äôs response, R t R_{t} , is generated via a deliberative process that integrates multiple streams of information. This process can be formalized as a sequence of steps:\\nFirst, a tactical response strategy œÉ t \\\\sigma_{t} is determined from the perceived patient state S t S_{t} :\\nConcurrently, the current therapeutic phase P t P_{t} is identified based on the session‚Äôs overarching therapy method ùíØ k \\\\mathcal{T}_{k} and the dialogue history in the current session H t ‚àí 1 H_{t-1} :\\nFinally, the agent‚Äôs response R t R_{t} is generated by an LLM ùí¢ \\\\mathcal{G} , conditioned on the complete set of factors:\\nThis multi-factor conditioning transforms response generation from a reactive task into a deliberative clinical intervention . Each response is strategically aligned with the patient‚Äôs immediate state ( S t S_{t} ), the conversational tactic ( œÉ t \\\\sigma_{t} ), the session‚Äôs procedural stage ( P t P_{t} ), and the long-term therapeutic goal ( ùíØ k \\\\mathcal{T}_{k} ), ensuring every utterance is purposeful.\\nThis loop represents TheraMind‚Äôs most significant departure from prior work, endowing the agent with the ability to reflect, learn, and adapt its core treatment strategy over time, a hallmark of expert human therapists.\\nAn effective therapist continuously reflects on what works and what does not. To emulate this, an agent must be capable of self-assessment. Without an evaluative step, any multi-session interaction would be a mere sequence of conversations, not a coherent therapeutic process.\\nAt the conclusion of session k k , with its complete dialogue history ‚Ñã k \\\\mathcal{H}_{k} , a Therapy Evaluation module assesses the efficacy of the employed therapeutic method ùíØ k \\\\mathcal{T}_{k} . This function yields a quantitative score ‚Ñ∞ k \\\\mathcal{E}_{k} :\\nThis reflective step is critical; it transforms the agent from a static system into a learning entity. It provides the necessary feedback signal for long-term strategic adaptation.\\nHuman therapy is inherently adaptive; a single therapeutic modality may not be effective for all patients or for all stages of treatment. Current AI systems are rigid, typically hard-coded to a single therapeutic framework (e.g., CBT). This is a primary barrier to their real-world applicability.\\nThe core innovation of TheraMind resides in its ability to dynamically select the therapeutic method for the subsequent session. Instead of relying on a rigid performance threshold, this decision is delegated to an LLM-based Therapy Selection module, f select f_{\\\\text{select}} . This module performs a nuanced, qualitative assessment, interpreting the quantitative efficacy score ‚Ñ∞ k \\\\mathcal{E}_{k} in the context of the session‚Äôs dialogue history ‚Ñã k \\\\mathcal{H}_{k} to make a clinical judgment. The selection process is formalized as:\\nBased on its analysis, this function determines whether the current therapy ùíØ k \\\\mathcal{T}_{k} should be maintained due to sufficient progress, or if a new, more suitable method should be selected by analyzing the observed outcomes and shortcomings. For instance, if the evaluation highlights a lack of patient engagement with CBT techniques despite a moderate efficacy score, the selection module might reason that a shift to a more client-centered approach is warranted. The initial therapy, ùíØ 1 \\\\mathcal{T}_{1} , is determined from the patient‚Äôs intake profile.\\nThis adaptive mechanism is our key contribution to achieving long-term therapeutic effectiveness. It allows TheraMind to personalize the treatment arc, systematically abandoning ineffective strategies and reinforcing successful ones. This capability for strategic self-correction , guided by a deliberative LLM-based judgment, is absent in current counseling agents and is crucial for moving towards truly autonomous therapeutic systems.\\nEvaluating longitudinal counseling capabilities is notoriously difficult. Static, single-turn datasets are ill-suited for assessing skills like long-term memory, strategic adaptation, and therapeutic alliance building. To overcome this, we propose a new evaluation paradigm grounded in dynamic, high-fidelity simulation.\\nWe construct our simulation environment using real, anonymized clinical cases from the CPsyCounR corpus Zhang et\\xa0al. ( 2024a ) . To ensure a comprehensive and representative testbed, we sampled 100 cases, selecting 10 from each of the 10 distinct clinical categories present in the dataset, as detailed in Table 1 . This stratified sampling approach ensures our agent is evaluated across a wide spectrum of psychological issues.\\nFor each selected case, an initialization process, Œ¶ init \\\\Phi_{\\\\text{init}} , generates a structured Patient Profile ( ùí´ \\\\mathcal{P} ) and a series of high-level Session Guides ( ùí¢ = { g 1 , ‚Ä¶ , g K } \\\\mathcal{G}=\\\\{g_{1},\\\\dots,g_{K}\\\\} ) for a six-session arc:\\nThese guides provide the LLM-based patient simulator with coherent goals for each session without rigidly scripting responses. This flexibility enables the simulated patient to exhibit a dynamic emotional range, capable of expressing both positive and negative attitudes, thereby mirroring the variability of real-world patients. This semi-structured approach creates a controllable yet dynamic testbed that balances clinical realism with reproducibility, enabling a robust evaluation of advanced, multi-session counseling skills.',\n",
       "    'subsections': [{'title': '3.1 The Intra-Session Loop: Dynamic Dialogue Management',\n",
       "      'paragraphs': 'The Intra-Session Loop is the agent‚Äôs core engine for real-time interaction. Its purpose is to transcend superficial text generation by endowing the agent with perceptual acuity and tactical responsiveness. For each conversational turn t t , this loop executes a multi-stage process from perception to action.\\nHuman therapists react not just to the literal content of a patient‚Äôs words, but to the underlying emotional and intentional subtext. Existing models, which primarily process raw text, lack this crucial layer of perception. To address this, we introduce a dedicated perception module.\\nUpon receiving a patient utterance U t U_{t} , TheraMind employs a Reaction Classifier to infer a structured representation of the patient‚Äôs immediate psychological state. Formally, this perception function, Œ¶ perceive \\\\Phi_{\\\\text{perceive}} , maps an utterance to a state tuple:\\nwhere E t E_{t} is the primary emotion , I t ‚àà [ 0 , 1 ] I_{t}\\\\in[0,1] is its intensity , and A t ‚àà { Cooperative, Resistant } A_{t}\\\\in\\\\{\\\\text{Cooperative, Resistant}\\\\} is the conversational attitude .\\nThis explicit state representation allows TheraMind to move beyond mere linguistic pattern-matching and engage with the patient‚Äôs underlying psychological state. It provides a robust, intermediate signal for following decision-making, enabling more nuanced and strategically empathetic interactions.\\nContinuity plays a crucial role in shaping a therapist agent‚Äôs responses during longitudinal psychological counseling. Patients expect therapists to remember significant details from past conversations. Standard LLMs with fixed context windows suffer from catastrophic forgetting, undermining trust and therapeutic progress.\\nTo ensure longitudinal coherence, TheraMind integrates a dynamic memory system. Given the current utterance U t U_{t} and the complete history of all previous sessions ‚Ñã 1 : k ‚àí 1 \\\\mathcal{H}_{1:k-1} , a Memory Recall function Œ¶ memory \\\\Phi_{\\\\text{memory}} produces a relevant memory summary M t M_{t} :\\nThis function internally performs a relevance check; if no pertinent history is found, M t M_{t} is null.\\nThis mechanism provides a dynamically curated context that is both relevant and succinct. It ensures the agent‚Äôs responses are historically informed, preventing the disjointedness that plagues stateless models.\\nEffective therapeutic responses are not just empathetic; they are purposeful interventions. Generation must be guided by clear clinical objectives, rather than being an unconstrained generative process. We achieve this by conditioning the LLM on explicit strategic and procedural guidance.\\nThe agent‚Äôs response, R t R_{t} , is generated via a deliberative process that integrates multiple streams of information. This process can be formalized as a sequence of steps:\\nFirst, a tactical response strategy œÉ t \\\\sigma_{t} is determined from the perceived patient state S t S_{t} :\\nConcurrently, the current therapeutic phase P t P_{t} is identified based on the session‚Äôs overarching therapy method ùíØ k \\\\mathcal{T}_{k} and the dialogue history in the current session H t ‚àí 1 H_{t-1} :\\nFinally, the agent‚Äôs response R t R_{t} is generated by an LLM ùí¢ \\\\mathcal{G} , conditioned on the complete set of factors:\\nThis multi-factor conditioning transforms response generation from a reactive task into a deliberative clinical intervention . Each response is strategically aligned with the patient‚Äôs immediate state ( S t S_{t} ), the conversational tactic ( œÉ t \\\\sigma_{t} ), the session‚Äôs procedural stage ( P t P_{t} ), and the long-term therapeutic goal ( ùíØ k \\\\mathcal{T}_{k} ), ensuring every utterance is purposeful.',\n",
       "      'subsections': [{'title': '3.1.1 Multi-Faceted Patient State Perception',\n",
       "        'paragraphs': 'Human therapists react not just to the literal content of a patient‚Äôs words, but to the underlying emotional and intentional subtext. Existing models, which primarily process raw text, lack this crucial layer of perception. To address this, we introduce a dedicated perception module.\\nUpon receiving a patient utterance U t U_{t} , TheraMind employs a Reaction Classifier to infer a structured representation of the patient‚Äôs immediate psychological state. Formally, this perception function, Œ¶ perceive \\\\Phi_{\\\\text{perceive}} , maps an utterance to a state tuple:\\nwhere E t E_{t} is the primary emotion , I t ‚àà [ 0 , 1 ] I_{t}\\\\in[0,1] is its intensity , and A t ‚àà { Cooperative, Resistant } A_{t}\\\\in\\\\{\\\\text{Cooperative, Resistant}\\\\} is the conversational attitude .\\nThis explicit state representation allows TheraMind to move beyond mere linguistic pattern-matching and engage with the patient‚Äôs underlying psychological state. It provides a robust, intermediate signal for following decision-making, enabling more nuanced and strategically empathetic interactions.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.1.2 Memory-Augmented Contextualization',\n",
       "        'paragraphs': 'Continuity plays a crucial role in shaping a therapist agent‚Äôs responses during longitudinal psychological counseling. Patients expect therapists to remember significant details from past conversations. Standard LLMs with fixed context windows suffer from catastrophic forgetting, undermining trust and therapeutic progress.\\nTo ensure longitudinal coherence, TheraMind integrates a dynamic memory system. Given the current utterance U t U_{t} and the complete history of all previous sessions ‚Ñã 1 : k ‚àí 1 \\\\mathcal{H}_{1:k-1} , a Memory Recall function Œ¶ memory \\\\Phi_{\\\\text{memory}} produces a relevant memory summary M t M_{t} :\\nThis function internally performs a relevance check; if no pertinent history is found, M t M_{t} is null.\\nThis mechanism provides a dynamically curated context that is both relevant and succinct. It ensures the agent‚Äôs responses are historically informed, preventing the disjointedness that plagues stateless models.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.1.3 Clinically-Grounded Response Generation',\n",
       "        'paragraphs': 'Effective therapeutic responses are not just empathetic; they are purposeful interventions. Generation must be guided by clear clinical objectives, rather than being an unconstrained generative process. We achieve this by conditioning the LLM on explicit strategic and procedural guidance.\\nThe agent‚Äôs response, R t R_{t} , is generated via a deliberative process that integrates multiple streams of information. This process can be formalized as a sequence of steps:\\nFirst, a tactical response strategy œÉ t \\\\sigma_{t} is determined from the perceived patient state S t S_{t} :\\nConcurrently, the current therapeutic phase P t P_{t} is identified based on the session‚Äôs overarching therapy method ùíØ k \\\\mathcal{T}_{k} and the dialogue history in the current session H t ‚àí 1 H_{t-1} :\\nFinally, the agent‚Äôs response R t R_{t} is generated by an LLM ùí¢ \\\\mathcal{G} , conditioned on the complete set of factors:\\nThis multi-factor conditioning transforms response generation from a reactive task into a deliberative clinical intervention . Each response is strategically aligned with the patient‚Äôs immediate state ( S t S_{t} ), the conversational tactic ( œÉ t \\\\sigma_{t} ), the session‚Äôs procedural stage ( P t P_{t} ), and the long-term therapeutic goal ( ùíØ k \\\\mathcal{T}_{k} ), ensuring every utterance is purposeful.',\n",
       "        'subsections': [{'title': 'Dynamic Response Strategy.',\n",
       "          'paragraphs': 'First, a tactical response strategy œÉ t \\\\sigma_{t} is determined from the perceived patient state S t S_{t} :',\n",
       "          'subsections': []},\n",
       "         {'title': 'Current Therapeutic Phase.',\n",
       "          'paragraphs': 'Concurrently, the current therapeutic phase P t P_{t} is identified based on the session‚Äôs overarching therapy method ùíØ k \\\\mathcal{T}_{k} and the dialogue history in the current session H t ‚àí 1 H_{t-1} :',\n",
       "          'subsections': []},\n",
       "         {'title': 'Synthesized Generation.',\n",
       "          'paragraphs': 'Finally, the agent‚Äôs response R t R_{t} is generated by an LLM ùí¢ \\\\mathcal{G} , conditioned on the complete set of factors:\\nThis multi-factor conditioning transforms response generation from a reactive task into a deliberative clinical intervention . Each response is strategically aligned with the patient‚Äôs immediate state ( S t S_{t} ), the conversational tactic ( œÉ t \\\\sigma_{t} ), the session‚Äôs procedural stage ( P t P_{t} ), and the long-term therapeutic goal ( ùíØ k \\\\mathcal{T}_{k} ), ensuring every utterance is purposeful.',\n",
       "          'subsections': []}]}]},\n",
       "     {'title': '3.2 The Cross-Session Loop: Adaptive Therapeutic Planning',\n",
       "      'paragraphs': 'This loop represents TheraMind‚Äôs most significant departure from prior work, endowing the agent with the ability to reflect, learn, and adapt its core treatment strategy over time, a hallmark of expert human therapists.\\nAn effective therapist continuously reflects on what works and what does not. To emulate this, an agent must be capable of self-assessment. Without an evaluative step, any multi-session interaction would be a mere sequence of conversations, not a coherent therapeutic process.\\nAt the conclusion of session k k , with its complete dialogue history ‚Ñã k \\\\mathcal{H}_{k} , a Therapy Evaluation module assesses the efficacy of the employed therapeutic method ùíØ k \\\\mathcal{T}_{k} . This function yields a quantitative score ‚Ñ∞ k \\\\mathcal{E}_{k} :\\nThis reflective step is critical; it transforms the agent from a static system into a learning entity. It provides the necessary feedback signal for long-term strategic adaptation.\\nHuman therapy is inherently adaptive; a single therapeutic modality may not be effective for all patients or for all stages of treatment. Current AI systems are rigid, typically hard-coded to a single therapeutic framework (e.g., CBT). This is a primary barrier to their real-world applicability.\\nThe core innovation of TheraMind resides in its ability to dynamically select the therapeutic method for the subsequent session. Instead of relying on a rigid performance threshold, this decision is delegated to an LLM-based Therapy Selection module, f select f_{\\\\text{select}} . This module performs a nuanced, qualitative assessment, interpreting the quantitative efficacy score ‚Ñ∞ k \\\\mathcal{E}_{k} in the context of the session‚Äôs dialogue history ‚Ñã k \\\\mathcal{H}_{k} to make a clinical judgment. The selection process is formalized as:\\nBased on its analysis, this function determines whether the current therapy ùíØ k \\\\mathcal{T}_{k} should be maintained due to sufficient progress, or if a new, more suitable method should be selected by analyzing the observed outcomes and shortcomings. For instance, if the evaluation highlights a lack of patient engagement with CBT techniques despite a moderate efficacy score, the selection module might reason that a shift to a more client-centered approach is warranted. The initial therapy, ùíØ 1 \\\\mathcal{T}_{1} , is determined from the patient‚Äôs intake profile.\\nThis adaptive mechanism is our key contribution to achieving long-term therapeutic effectiveness. It allows TheraMind to personalize the treatment arc, systematically abandoning ineffective strategies and reinforcing successful ones. This capability for strategic self-correction , guided by a deliberative LLM-based judgment, is absent in current counseling agents and is crucial for moving towards truly autonomous therapeutic systems.',\n",
       "      'subsections': [{'title': '3.2.1 Post-Session Efficacy Evaluation',\n",
       "        'paragraphs': 'An effective therapist continuously reflects on what works and what does not. To emulate this, an agent must be capable of self-assessment. Without an evaluative step, any multi-session interaction would be a mere sequence of conversations, not a coherent therapeutic process.\\nAt the conclusion of session k k , with its complete dialogue history ‚Ñã k \\\\mathcal{H}_{k} , a Therapy Evaluation module assesses the efficacy of the employed therapeutic method ùíØ k \\\\mathcal{T}_{k} . This function yields a quantitative score ‚Ñ∞ k \\\\mathcal{E}_{k} :\\nThis reflective step is critical; it transforms the agent from a static system into a learning entity. It provides the necessary feedback signal for long-term strategic adaptation.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.2.2 Adaptive Therapy Selection',\n",
       "        'paragraphs': 'Human therapy is inherently adaptive; a single therapeutic modality may not be effective for all patients or for all stages of treatment. Current AI systems are rigid, typically hard-coded to a single therapeutic framework (e.g., CBT). This is a primary barrier to their real-world applicability.\\nThe core innovation of TheraMind resides in its ability to dynamically select the therapeutic method for the subsequent session. Instead of relying on a rigid performance threshold, this decision is delegated to an LLM-based Therapy Selection module, f select f_{\\\\text{select}} . This module performs a nuanced, qualitative assessment, interpreting the quantitative efficacy score ‚Ñ∞ k \\\\mathcal{E}_{k} in the context of the session‚Äôs dialogue history ‚Ñã k \\\\mathcal{H}_{k} to make a clinical judgment. The selection process is formalized as:\\nBased on its analysis, this function determines whether the current therapy ùíØ k \\\\mathcal{T}_{k} should be maintained due to sufficient progress, or if a new, more suitable method should be selected by analyzing the observed outcomes and shortcomings. For instance, if the evaluation highlights a lack of patient engagement with CBT techniques despite a moderate efficacy score, the selection module might reason that a shift to a more client-centered approach is warranted. The initial therapy, ùíØ 1 \\\\mathcal{T}_{1} , is determined from the patient‚Äôs intake profile.\\nThis adaptive mechanism is our key contribution to achieving long-term therapeutic effectiveness. It allows TheraMind to personalize the treatment arc, systematically abandoning ineffective strategies and reinforcing successful ones. This capability for strategic self-correction , guided by a deliberative LLM-based judgment, is absent in current counseling agents and is crucial for moving towards truly autonomous therapeutic systems.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.3 High-Fidelity Longitudinal Simulation Environment',\n",
       "      'paragraphs': 'Evaluating longitudinal counseling capabilities is notoriously difficult. Static, single-turn datasets are ill-suited for assessing skills like long-term memory, strategic adaptation, and therapeutic alliance building. To overcome this, we propose a new evaluation paradigm grounded in dynamic, high-fidelity simulation.\\nWe construct our simulation environment using real, anonymized clinical cases from the CPsyCounR corpus Zhang et\\xa0al. ( 2024a ) . To ensure a comprehensive and representative testbed, we sampled 100 cases, selecting 10 from each of the 10 distinct clinical categories present in the dataset, as detailed in Table 1 . This stratified sampling approach ensures our agent is evaluated across a wide spectrum of psychological issues.\\nFor each selected case, an initialization process, Œ¶ init \\\\Phi_{\\\\text{init}} , generates a structured Patient Profile ( ùí´ \\\\mathcal{P} ) and a series of high-level Session Guides ( ùí¢ = { g 1 , ‚Ä¶ , g K } \\\\mathcal{G}=\\\\{g_{1},\\\\dots,g_{K}\\\\} ) for a six-session arc:\\nThese guides provide the LLM-based patient simulator with coherent goals for each session without rigidly scripting responses. This flexibility enables the simulated patient to exhibit a dynamic emotional range, capable of expressing both positive and negative attitudes, thereby mirroring the variability of real-world patients. This semi-structured approach creates a controllable yet dynamic testbed that balances clinical realism with reproducibility, enabling a robust evaluation of advanced, multi-session counseling skills.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Experiments',\n",
       "    'paragraphs': 'To comprehensively evaluate the performance of TheraMind, we evaluate it with other methods, which we categorize into three groups: (1) Psychological counseling models : ChatCounselor Liu et\\xa0al. ( 2023a ) , CPsyCounX Zhang et\\xa0al. ( 2024a ) , and PsyDTLLM Xie et\\xa0al. ( 2025 ) ; (2) General-purpose large language models : GLM-4-9B-Chat GLM et\\xa0al. ( 2024 ) , InterLM2.5-7B-Chat Cai et\\xa0al. ( 2024 ) , Qwen3-8B Yang et\\xa0al. ( 2025 ) , Kimi-Dev-72B Team et\\xa0al. ( 2025 ) , Doubao-1.5-pro-32k Guo et\\xa0al. ( 2025 ) , Yi-Large Young et\\xa0al. ( 2024 ) , and DeepSeek-V3 Liu et\\xa0al. ( 2024a ) ; and (3) Psychological agent-based models : Interactive Agents Qiu and Lan ( 2024 ) .\\nOur experiments are grounded in the CPsyCounR dataset Zhang et\\xa0al. ( 2024a ) . This dataset comprises 3,134 anonymized and professionally rewritten Chinese psychological counseling reports sourced from Yidianling Yidianling ( 2015 ) and Psy525 Psy525 ( 2007 ) . Each report adheres to a standardized structure, including a case brief, consultation process, and therapist reflections, ensuring a high degree of authenticity, privacy, and clinical reliability. This dataset provides a robust foundation for evaluating the nuanced, long-term capabilities of counseling agents in a realistic context.\\nTo provide a comprehensive and clinically grounded assessment, our evaluation protocol draws upon established psychotherapy research and professional counseling standards Hill ( 1999 ); Ribeiro et\\xa0al. ( 2013 ); Qiu and Lan ( 2025 ); Zhang et\\xa0al. ( 2024a ) . It is structured into single-session and multi-session metrics. Trained annotators rate all metrics on a 4-point scale (0-3).\\nSingle-Session Metrics. We evaluate the quality of individual interactions along two primary dimensions:\\nTherapeutic Alliance (T.Alli): Based on the Working Alliance Inventory (WAI) framework Doran ( 2016 ) , this metric evaluates the agent‚Äôs ability to form a collaborative partnership. It measures alignment on therapeutic Goals , collaborative execution of Tasks , and the establishment of an empathetic Bond .\\nInteraction Quality (Inter): This metric assesses the clinical depth of the dialogue, focusing on the agent‚Äôs capacity to integrate patient motivations, explore cognitive contradictions through strategic questioning, and facilitate patient self-awareness and reflection.\\nMulti-Session Metrics. To evaluate longitudinal performance, we introduce four metrics that capture the agent‚Äôs effectiveness over the entire therapeutic arc:\\nCoherence (Coh): Measures the agent‚Äôs ability to maintain continuity across sessions by referencing and building upon past conversations to create a cohesive therapeutic narrative.\\nFlexibility (Flex): Assesses the capacity to adapt therapeutic strategies over time, adjusting or integrating different methods in response to the patient‚Äôs evolving needs and progress.\\nEmpathy (Emp): Evaluates the consistency of the agent‚Äôs empathetic engagement, including its ability to track emotional development, reframe negative expressions constructively, and utilize patient-generated metaphors as therapeutic tools.\\nTherapeutic Attunement (T.Attun): Examines the agent‚Äôs skill in aligning interventions with the patient‚Äôs current treatment phase, appropriately integrating techniques, and recognizing subtle signs of progress beyond generic encouragement.\\nWe select DeepSeek-V3 Liu et\\xa0al. ( 2024a ) as the backbone model, owing to its cost efficiency and competitive performance on instruction-following tasks. For response generation, the agent is configured with a temperature of 0.9 0.9 , t \\u200b o \\u200b p \\u200b - \\u200b p = 0.75 top\\\\text{-}p=0.75 , and t \\u200b o \\u200b p \\u200b - \\u200b k = 20 top\\\\text{-}k=20 . In judgment tasks, we adopt a lower temperature of 0.3 0.3 while keeping t \\u200b o \\u200b p \\u200b - \\u200b p top\\\\text{-}p and t \\u200b o \\u200b p \\u200b - \\u200b k top\\\\text{-}k unchanged, to encourage more deterministic and stable outputs.\\nUnlike previous research that relies on predefined datasets, which suffer from two major drawbacks: (1) patient responses tend to be overly simplistic, failing to capture the diversity of real-world cases; (2) replies are fixed and lack cross-turn consistency, we ground our evaluation in the CPsyCounR dataset Zhang et\\xa0al. ( 2024a ) . It follows a standardized structure including Title , Type , Method , Case Brief , Consultation Process , and Experience Thoughts . To ensure representativeness, we uniformly sampled 100 cases across various patient categories and conducted evaluations over six-session dialogues. Based on these real cases, we employed a large language model to simulate virtual patients capable of flexibly displaying both positive and negative attitudes, thereby building a more realistic and diverse evaluation.\\nFor automatic evaluation, we used Gemini-2.5-Flash, configured with a temperature of 0 , t \\u200b o \\u200b p \\u200b - \\u200b p = 0.95 top\\\\text{-}p=0.95 , and t \\u200b o \\u200b p \\u200b - \\u200b k = 64 top\\\\text{-}k=64 . The temperature is set to zero to guarantee deterministic outputs. Moreover, by using a model different from the backbone model for evaluation, we effectively reduce potential biases and improve the objectivity of the assessment.\\nAs shown in Table 2 , TheraMind establishes a new state-of-the-art by significantly outperforming all baseline methods across both single-session and multi-session metrics.\\nWhile TheraMind achieves the highest single-session scores ( 2.358 Avg), its most substantial performance gains are in the multi-session evaluation, with leading scores in Coherence ( 2.860 ), Empathy ( 2.980 ), and Therapeutic Attunement ( 2.890 ). This highlights the effectiveness of the Intra-Session Loop‚Äôs mechanisms for maintaining long-term, clinically-grounded interactions. Crucially, its superior score in Flexibility ( 2.290 ) directly confirms the efficacy of the Cross-Session Loop‚Äôs adaptive therapy selection, a capability largely absent in prior work.\\nFurthermore, the comparison with its backbone model, DeepSeek-V3, is telling. The TheraMind agent framework elevates performance dramatically across all metrics, boosting the multi-session average from 2.330 to 2.755 (a 18.2% relative improvement). This demonstrates that the agent architecture provides critical strategic and adaptive reasoning capabilities far beyond the generative function of the base LLM.\\nTo validate the contribution of each core component within TheraMind‚Äôs dual-loop architecture, we conducted an ablation study via pairwise comparison. We evaluated the full model against variants lacking specific modules: Memory Mechanism, Stage Adaptation, Response Strategy, and Therapy Selection. Figure 3 presents the win rates of the full TheraMind model against these ablated versions.\\nThe results clearly show the necessity of all components, with the full model outperforming all ablated variants. Removing the Memory Mechanism causes the largest drop (75.00% win rate), underscoring the importance of maintaining longitudinal context to avoid ‚Äúclinical amnesia‚Äù. Excluding Stage Adaptation (72.73%) highlights the need to align responses with therapeutic phases. Removing Response Strategy (68.42%) and Therapy Selection (66.67%) increases rigidity within and across sessions, confirming the value of adaptive, multi-granular control.\\nTo assess the robustness and generalizability of TheraMind, we conducted a fine-grained performance analysis across the ten distinct categories of psychological issues. The results, as shown in Figure 4 , reveal critical insights into the models‚Äô capabilities in both single-turn and longitudinal contexts. In the single-session evaluation (left panel), TheraMind consistently achieves the highest scores across all domains. Moreover, the multi-session evaluation (right panel) paints a much clearer picture of TheraMind‚Äôs unique advantages. The performance gap between TheraMind and all other models widens dramatically. While most models show some improvement, their gains are inconsistent and marginal compared to TheraMind, which maintains a consistently high level of performance across all ten categories. This demonstrates the profound impact of our dual-loop architecture. Lacking a mechanism for cross-session memory and strategic adaptation, competing models fail to build a coherent, long-term therapeutic narrative, causing their performance to plateau or degrade over time. In contrast, TheraMind‚Äôs ability to retrieve memories and adapt its therapy method allows it to effectively handle complex, evolving issues, showcasing its superior capability for true longitudinal counseling.\\nTo complement our automatic metrics and validate the nuanced clinical capabilities of our agent, we conducted two sets of human evaluations. Trained annotators with expertise in psychology performed all assessments.\\nFirst, we assessed the internal validity of TheraMind‚Äôs core components by evaluating the agreement between the agent‚Äôs decisions and human judgment. On a sample of dialogues from 10 distinct cases, we achieved a Cohen‚Äôs Œ∫ \\\\kappa Cohen ( 1960 ) of 0.676, indicating substantial inter-annotator reliability, which falls within the range of substantial agreement (0.60‚Äì0.80). As shown in Figure 5 (Left), TheraMind‚Äôs modules demonstrate high fidelity with human clinical reasoning. The Emotion Recognition (95.59% agreement) and Memory Invoke (94.12% agreement) modules are exceptionally accurate, confirming the robustness of the agent‚Äôs perceptual and contextualization capabilities. The Response Strategy selection also shows strong alignment (85.29%), validating its ability to make clinically sound tactical decisions.\\nSecond, we conducted a pairwise comparative evaluation of TheraMind against three top-performing baselines: PsyDTLLM, DeepSeek-V3, and Interactive Agents. For this, we used a simplified multi-session evaluation standard on 20 cases (two from each category), achieving a Cohen‚Äôs Œ∫ \\\\kappa of 0.697. The results, presented in Figure 5 (Middle), reveal a clear human preference for TheraMind. It secured decisive win rates against the specialized counseling model PsyDTLLM (75%), its powerful backbone model DeepSeek-V3 (70%), and the advanced agent framework Interactive Agents (65%). This human-validated superiority, particularly over its own base model, provides compelling evidence that our novel dual-loop architecture is the key factor in achieving a higher standard of longitudinal therapeutic care.\\nTo gain deeper insights into TheraMind‚Äôs therapeutic process and internal decision-making, we conducted a multi-faceted analysis of its behavior during the simulated counseling sessions. This analysis examines the emotional landscape of the dialogues, the distribution of intervention strategies, the progression of therapeutic phases, and relationship between patient attitude and emotional intensity.\\nAs illustrated in Figure 5 (Right), the emotional landscape of the interactions is dominated by expected therapeutic emotions such as Fear (33.69%) and Sadness (24.56%). Crucially, Trust emerges as the third most prevalent emotion at 21.71%. This high proportion of Trust is a strong indicator of the agent‚Äôs success in establishing a robust therapeutic alliance, which is a foundational prerequisite for effective psychological counseling and encourages deeper patient self-disclosure.\\nAn analysis of the agent‚Äôs intervention strategies, shown in Figure 6 (Left), reveals a clinically-grounded and diverse therapeutic repertoire. The most frequently used intervention is ‚ÄúReflection of Feelings‚Äù (32.54%), underscoring the agent‚Äôs focus on building empathy. However, TheraMind does not rely solely on supportive responses. It actively guides the therapeutic process by employing change-oriented techniques such as ‚ÄúInvite to Take New Perspectives‚Äù (16.93%) and ‚ÄúInvite to Explore New Actions‚Äù (15.83%), demonstrating a balanced approach that facilitates both emotional processing and cognitive-behavioral change.\\nFurthermore, TheraMind successfully models the structured progression of a longitudinal therapeutic arc. Figure 6 (Middle) shows a clear and logical transition through distinct clinical phases across the six sessions. The initial sessions are heavily focused on Engagement , establishing rapport and understanding the patient‚Äôs context. The mid-stage of therapy shifts significantly towards Exploration , where the agent and patient work together to uncover deeper insights. Finally, the later sessions show a marked increase in the Integration phase, where learnings are consolidated and plans for future action are made. This ability to guide the counseling process through a coherent, multi-stage journey is a key feature distinguishing TheraMind from static, single-session models.\\nFinally, our framework demonstrates a nuanced understanding of the patient‚Äôs psychological state by linking perceived attitude to emotional expression, as depicted in Figure 6 (Right). The analysis reveals that conversations with a positive (cooperative) patient attitude are strongly correlated with a high peak in emotional intensity. This suggests that when patients feel safe and engaged, they are more willing to express their emotions fully. Conversely, negative (resistant) attitudes are associated with a more varied and generally lower emotional intensity. Perceiving subtle attitude‚Äìemotion relations enables the agent to make more informed decisions for dynamic response strategy selection.\\nTo qualitatively illustrate our agent‚Äôs capabilities, Figure 7 presents a case study from Session 3. The patient describes a distressing dream about their mother, prompting TheraMind‚Äôs deliberative response generation process.\\nFirst, the agent‚Äôs intra-session loop perceives the patient‚Äôs ‚ÄúSadness‚Äù and ‚ÄúNegative‚Äù attitude, selecting a ‚ÄúReflection of Feelings‚Äù strategy to validate their emotional state. Simultaneously, its cross-session memory module is triggered, retrieving a key insight from Session 2: a ‚Äúrib‚Äù metaphor that the patient had previously linked to their mother‚Äôs ‚Äúheavy expectations‚Äù.\\nThe agent‚Äôs final response masterfully synthesizes these elements. It reflects the patient‚Äôs immediate feelings (‚Äúfamiliar weight‚Äù, ‚Äúdeep sadness‚Äù) while seamlessly weaving in the retrieved memory to create continuity (‚Äú‚Ä¶just as we did with the rib before?‚Äù). This single turn demonstrates how TheraMind avoids ‚Äúclinical amnesia‚Äù. By linking past breakthroughs to the present conversation, it transforms a simple reactive reply into a meaningful clinical intervention that reinforces the longitudinal therapeutic arc.',\n",
       "    'subsections': [{'title': '4.1 Compared Methods',\n",
       "      'paragraphs': 'To comprehensively evaluate the performance of TheraMind, we evaluate it with other methods, which we categorize into three groups: (1) Psychological counseling models : ChatCounselor Liu et\\xa0al. ( 2023a ) , CPsyCounX Zhang et\\xa0al. ( 2024a ) , and PsyDTLLM Xie et\\xa0al. ( 2025 ) ; (2) General-purpose large language models : GLM-4-9B-Chat GLM et\\xa0al. ( 2024 ) , InterLM2.5-7B-Chat Cai et\\xa0al. ( 2024 ) , Qwen3-8B Yang et\\xa0al. ( 2025 ) , Kimi-Dev-72B Team et\\xa0al. ( 2025 ) , Doubao-1.5-pro-32k Guo et\\xa0al. ( 2025 ) , Yi-Large Young et\\xa0al. ( 2024 ) , and DeepSeek-V3 Liu et\\xa0al. ( 2024a ) ; and (3) Psychological agent-based models : Interactive Agents Qiu and Lan ( 2024 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Evaluation Dataset and Metrics',\n",
       "      'paragraphs': 'Our experiments are grounded in the CPsyCounR dataset Zhang et\\xa0al. ( 2024a ) . This dataset comprises 3,134 anonymized and professionally rewritten Chinese psychological counseling reports sourced from Yidianling Yidianling ( 2015 ) and Psy525 Psy525 ( 2007 ) . Each report adheres to a standardized structure, including a case brief, consultation process, and therapist reflections, ensuring a high degree of authenticity, privacy, and clinical reliability. This dataset provides a robust foundation for evaluating the nuanced, long-term capabilities of counseling agents in a realistic context.\\nTo provide a comprehensive and clinically grounded assessment, our evaluation protocol draws upon established psychotherapy research and professional counseling standards Hill ( 1999 ); Ribeiro et\\xa0al. ( 2013 ); Qiu and Lan ( 2025 ); Zhang et\\xa0al. ( 2024a ) . It is structured into single-session and multi-session metrics. Trained annotators rate all metrics on a 4-point scale (0-3).\\nSingle-Session Metrics. We evaluate the quality of individual interactions along two primary dimensions:\\nTherapeutic Alliance (T.Alli): Based on the Working Alliance Inventory (WAI) framework Doran ( 2016 ) , this metric evaluates the agent‚Äôs ability to form a collaborative partnership. It measures alignment on therapeutic Goals , collaborative execution of Tasks , and the establishment of an empathetic Bond .\\nInteraction Quality (Inter): This metric assesses the clinical depth of the dialogue, focusing on the agent‚Äôs capacity to integrate patient motivations, explore cognitive contradictions through strategic questioning, and facilitate patient self-awareness and reflection.\\nMulti-Session Metrics. To evaluate longitudinal performance, we introduce four metrics that capture the agent‚Äôs effectiveness over the entire therapeutic arc:\\nCoherence (Coh): Measures the agent‚Äôs ability to maintain continuity across sessions by referencing and building upon past conversations to create a cohesive therapeutic narrative.\\nFlexibility (Flex): Assesses the capacity to adapt therapeutic strategies over time, adjusting or integrating different methods in response to the patient‚Äôs evolving needs and progress.\\nEmpathy (Emp): Evaluates the consistency of the agent‚Äôs empathetic engagement, including its ability to track emotional development, reframe negative expressions constructively, and utilize patient-generated metaphors as therapeutic tools.\\nTherapeutic Attunement (T.Attun): Examines the agent‚Äôs skill in aligning interventions with the patient‚Äôs current treatment phase, appropriately integrating techniques, and recognizing subtle signs of progress beyond generic encouragement.',\n",
       "      'subsections': [{'title': '4.2.1 Evaluation Dataset',\n",
       "        'paragraphs': 'Our experiments are grounded in the CPsyCounR dataset Zhang et\\xa0al. ( 2024a ) . This dataset comprises 3,134 anonymized and professionally rewritten Chinese psychological counseling reports sourced from Yidianling Yidianling ( 2015 ) and Psy525 Psy525 ( 2007 ) . Each report adheres to a standardized structure, including a case brief, consultation process, and therapist reflections, ensuring a high degree of authenticity, privacy, and clinical reliability. This dataset provides a robust foundation for evaluating the nuanced, long-term capabilities of counseling agents in a realistic context.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.2.2 Evaluation Metrics',\n",
       "        'paragraphs': 'To provide a comprehensive and clinically grounded assessment, our evaluation protocol draws upon established psychotherapy research and professional counseling standards Hill ( 1999 ); Ribeiro et\\xa0al. ( 2013 ); Qiu and Lan ( 2025 ); Zhang et\\xa0al. ( 2024a ) . It is structured into single-session and multi-session metrics. Trained annotators rate all metrics on a 4-point scale (0-3).\\nSingle-Session Metrics. We evaluate the quality of individual interactions along two primary dimensions:\\nTherapeutic Alliance (T.Alli): Based on the Working Alliance Inventory (WAI) framework Doran ( 2016 ) , this metric evaluates the agent‚Äôs ability to form a collaborative partnership. It measures alignment on therapeutic Goals , collaborative execution of Tasks , and the establishment of an empathetic Bond .\\nInteraction Quality (Inter): This metric assesses the clinical depth of the dialogue, focusing on the agent‚Äôs capacity to integrate patient motivations, explore cognitive contradictions through strategic questioning, and facilitate patient self-awareness and reflection.\\nMulti-Session Metrics. To evaluate longitudinal performance, we introduce four metrics that capture the agent‚Äôs effectiveness over the entire therapeutic arc:\\nCoherence (Coh): Measures the agent‚Äôs ability to maintain continuity across sessions by referencing and building upon past conversations to create a cohesive therapeutic narrative.\\nFlexibility (Flex): Assesses the capacity to adapt therapeutic strategies over time, adjusting or integrating different methods in response to the patient‚Äôs evolving needs and progress.\\nEmpathy (Emp): Evaluates the consistency of the agent‚Äôs empathetic engagement, including its ability to track emotional development, reframe negative expressions constructively, and utilize patient-generated metaphors as therapeutic tools.\\nTherapeutic Attunement (T.Attun): Examines the agent‚Äôs skill in aligning interventions with the patient‚Äôs current treatment phase, appropriately integrating techniques, and recognizing subtle signs of progress beyond generic encouragement.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.3 Experimental Settings',\n",
       "      'paragraphs': 'We select DeepSeek-V3 Liu et\\xa0al. ( 2024a ) as the backbone model, owing to its cost efficiency and competitive performance on instruction-following tasks. For response generation, the agent is configured with a temperature of 0.9 0.9 , t \\u200b o \\u200b p \\u200b - \\u200b p = 0.75 top\\\\text{-}p=0.75 , and t \\u200b o \\u200b p \\u200b - \\u200b k = 20 top\\\\text{-}k=20 . In judgment tasks, we adopt a lower temperature of 0.3 0.3 while keeping t \\u200b o \\u200b p \\u200b - \\u200b p top\\\\text{-}p and t \\u200b o \\u200b p \\u200b - \\u200b k top\\\\text{-}k unchanged, to encourage more deterministic and stable outputs.\\nUnlike previous research that relies on predefined datasets, which suffer from two major drawbacks: (1) patient responses tend to be overly simplistic, failing to capture the diversity of real-world cases; (2) replies are fixed and lack cross-turn consistency, we ground our evaluation in the CPsyCounR dataset Zhang et\\xa0al. ( 2024a ) . It follows a standardized structure including Title , Type , Method , Case Brief , Consultation Process , and Experience Thoughts . To ensure representativeness, we uniformly sampled 100 cases across various patient categories and conducted evaluations over six-session dialogues. Based on these real cases, we employed a large language model to simulate virtual patients capable of flexibly displaying both positive and negative attitudes, thereby building a more realistic and diverse evaluation.\\nFor automatic evaluation, we used Gemini-2.5-Flash, configured with a temperature of 0 , t \\u200b o \\u200b p \\u200b - \\u200b p = 0.95 top\\\\text{-}p=0.95 , and t \\u200b o \\u200b p \\u200b - \\u200b k = 64 top\\\\text{-}k=64 . The temperature is set to zero to guarantee deterministic outputs. Moreover, by using a model different from the backbone model for evaluation, we effectively reduce potential biases and improve the objectivity of the assessment.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.4 Main Results',\n",
       "      'paragraphs': 'As shown in Table 2 , TheraMind establishes a new state-of-the-art by significantly outperforming all baseline methods across both single-session and multi-session metrics.\\nWhile TheraMind achieves the highest single-session scores ( 2.358 Avg), its most substantial performance gains are in the multi-session evaluation, with leading scores in Coherence ( 2.860 ), Empathy ( 2.980 ), and Therapeutic Attunement ( 2.890 ). This highlights the effectiveness of the Intra-Session Loop‚Äôs mechanisms for maintaining long-term, clinically-grounded interactions. Crucially, its superior score in Flexibility ( 2.290 ) directly confirms the efficacy of the Cross-Session Loop‚Äôs adaptive therapy selection, a capability largely absent in prior work.\\nFurthermore, the comparison with its backbone model, DeepSeek-V3, is telling. The TheraMind agent framework elevates performance dramatically across all metrics, boosting the multi-session average from 2.330 to 2.755 (a 18.2% relative improvement). This demonstrates that the agent architecture provides critical strategic and adaptive reasoning capabilities far beyond the generative function of the base LLM.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.5 Ablation Study',\n",
       "      'paragraphs': 'To validate the contribution of each core component within TheraMind‚Äôs dual-loop architecture, we conducted an ablation study via pairwise comparison. We evaluated the full model against variants lacking specific modules: Memory Mechanism, Stage Adaptation, Response Strategy, and Therapy Selection. Figure 3 presents the win rates of the full TheraMind model against these ablated versions.\\nThe results clearly show the necessity of all components, with the full model outperforming all ablated variants. Removing the Memory Mechanism causes the largest drop (75.00% win rate), underscoring the importance of maintaining longitudinal context to avoid ‚Äúclinical amnesia‚Äù. Excluding Stage Adaptation (72.73%) highlights the need to align responses with therapeutic phases. Removing Response Strategy (68.42%) and Therapy Selection (66.67%) increases rigidity within and across sessions, confirming the value of adaptive, multi-granular control.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.6 Analysis Across Psychological Issues',\n",
       "      'paragraphs': 'To assess the robustness and generalizability of TheraMind, we conducted a fine-grained performance analysis across the ten distinct categories of psychological issues. The results, as shown in Figure 4 , reveal critical insights into the models‚Äô capabilities in both single-turn and longitudinal contexts. In the single-session evaluation (left panel), TheraMind consistently achieves the highest scores across all domains. Moreover, the multi-session evaluation (right panel) paints a much clearer picture of TheraMind‚Äôs unique advantages. The performance gap between TheraMind and all other models widens dramatically. While most models show some improvement, their gains are inconsistent and marginal compared to TheraMind, which maintains a consistently high level of performance across all ten categories. This demonstrates the profound impact of our dual-loop architecture. Lacking a mechanism for cross-session memory and strategic adaptation, competing models fail to build a coherent, long-term therapeutic narrative, causing their performance to plateau or degrade over time. In contrast, TheraMind‚Äôs ability to retrieve memories and adapt its therapy method allows it to effectively handle complex, evolving issues, showcasing its superior capability for true longitudinal counseling.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.7 Human Evaluation',\n",
       "      'paragraphs': 'To complement our automatic metrics and validate the nuanced clinical capabilities of our agent, we conducted two sets of human evaluations. Trained annotators with expertise in psychology performed all assessments.\\nFirst, we assessed the internal validity of TheraMind‚Äôs core components by evaluating the agreement between the agent‚Äôs decisions and human judgment. On a sample of dialogues from 10 distinct cases, we achieved a Cohen‚Äôs Œ∫ \\\\kappa Cohen ( 1960 ) of 0.676, indicating substantial inter-annotator reliability, which falls within the range of substantial agreement (0.60‚Äì0.80). As shown in Figure 5 (Left), TheraMind‚Äôs modules demonstrate high fidelity with human clinical reasoning. The Emotion Recognition (95.59% agreement) and Memory Invoke (94.12% agreement) modules are exceptionally accurate, confirming the robustness of the agent‚Äôs perceptual and contextualization capabilities. The Response Strategy selection also shows strong alignment (85.29%), validating its ability to make clinically sound tactical decisions.\\nSecond, we conducted a pairwise comparative evaluation of TheraMind against three top-performing baselines: PsyDTLLM, DeepSeek-V3, and Interactive Agents. For this, we used a simplified multi-session evaluation standard on 20 cases (two from each category), achieving a Cohen‚Äôs Œ∫ \\\\kappa of 0.697. The results, presented in Figure 5 (Middle), reveal a clear human preference for TheraMind. It secured decisive win rates against the specialized counseling model PsyDTLLM (75%), its powerful backbone model DeepSeek-V3 (70%), and the advanced agent framework Interactive Agents (65%). This human-validated superiority, particularly over its own base model, provides compelling evidence that our novel dual-loop architecture is the key factor in achieving a higher standard of longitudinal therapeutic care.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.8 In-depth Analysis of TheraMind‚Äôs Behavior',\n",
       "      'paragraphs': 'To gain deeper insights into TheraMind‚Äôs therapeutic process and internal decision-making, we conducted a multi-faceted analysis of its behavior during the simulated counseling sessions. This analysis examines the emotional landscape of the dialogues, the distribution of intervention strategies, the progression of therapeutic phases, and relationship between patient attitude and emotional intensity.\\nAs illustrated in Figure 5 (Right), the emotional landscape of the interactions is dominated by expected therapeutic emotions such as Fear (33.69%) and Sadness (24.56%). Crucially, Trust emerges as the third most prevalent emotion at 21.71%. This high proportion of Trust is a strong indicator of the agent‚Äôs success in establishing a robust therapeutic alliance, which is a foundational prerequisite for effective psychological counseling and encourages deeper patient self-disclosure.\\nAn analysis of the agent‚Äôs intervention strategies, shown in Figure 6 (Left), reveals a clinically-grounded and diverse therapeutic repertoire. The most frequently used intervention is ‚ÄúReflection of Feelings‚Äù (32.54%), underscoring the agent‚Äôs focus on building empathy. However, TheraMind does not rely solely on supportive responses. It actively guides the therapeutic process by employing change-oriented techniques such as ‚ÄúInvite to Take New Perspectives‚Äù (16.93%) and ‚ÄúInvite to Explore New Actions‚Äù (15.83%), demonstrating a balanced approach that facilitates both emotional processing and cognitive-behavioral change.\\nFurthermore, TheraMind successfully models the structured progression of a longitudinal therapeutic arc. Figure 6 (Middle) shows a clear and logical transition through distinct clinical phases across the six sessions. The initial sessions are heavily focused on Engagement , establishing rapport and understanding the patient‚Äôs context. The mid-stage of therapy shifts significantly towards Exploration , where the agent and patient work together to uncover deeper insights. Finally, the later sessions show a marked increase in the Integration phase, where learnings are consolidated and plans for future action are made. This ability to guide the counseling process through a coherent, multi-stage journey is a key feature distinguishing TheraMind from static, single-session models.\\nFinally, our framework demonstrates a nuanced understanding of the patient‚Äôs psychological state by linking perceived attitude to emotional expression, as depicted in Figure 6 (Right). The analysis reveals that conversations with a positive (cooperative) patient attitude are strongly correlated with a high peak in emotional intensity. This suggests that when patients feel safe and engaged, they are more willing to express their emotions fully. Conversely, negative (resistant) attitudes are associated with a more varied and generally lower emotional intensity. Perceiving subtle attitude‚Äìemotion relations enables the agent to make more informed decisions for dynamic response strategy selection.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.9 Case Study',\n",
       "      'paragraphs': 'To qualitatively illustrate our agent‚Äôs capabilities, Figure 7 presents a case study from Session 3. The patient describes a distressing dream about their mother, prompting TheraMind‚Äôs deliberative response generation process.\\nFirst, the agent‚Äôs intra-session loop perceives the patient‚Äôs ‚ÄúSadness‚Äù and ‚ÄúNegative‚Äù attitude, selecting a ‚ÄúReflection of Feelings‚Äù strategy to validate their emotional state. Simultaneously, its cross-session memory module is triggered, retrieving a key insight from Session 2: a ‚Äúrib‚Äù metaphor that the patient had previously linked to their mother‚Äôs ‚Äúheavy expectations‚Äù.\\nThe agent‚Äôs final response masterfully synthesizes these elements. It reflects the patient‚Äôs immediate feelings (‚Äúfamiliar weight‚Äù, ‚Äúdeep sadness‚Äù) while seamlessly weaving in the retrieved memory to create continuity (‚Äú‚Ä¶just as we did with the rib before?‚Äù). This single turn demonstrates how TheraMind avoids ‚Äúclinical amnesia‚Äù. By linking past breakthroughs to the present conversation, it transforms a simple reactive reply into a meaningful clinical intervention that reinforces the longitudinal therapeutic arc.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Conclusion',\n",
       "    'paragraphs': 'In this work, we introduced TheraMind, a strategic and adaptive agent that overcomes the critical limitations of prior work, clinical amnesia and strategic rigidity, through a novel dual-loop architecture. This framework decouples tactical, in-session dialogue management from strategic, cross-session therapeutic planning, enabling true longitudinal counseling. Extensive experiments and human evaluations confirmed its state-of-the-art performance, demonstrating that our architecture enables superior long-term coherence and flexibility.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Human Evaluation of Pairwise Response Selection',\n",
       "    'paragraphs': 'To ensure the universality of human evaluation, we randomly selected two cases from each of the ten categories as samples. Based on the predefined evaluation criteria, well-trained annotators respectively compared TheraMind with PsyDTLLM, DeepSeek-V3, and Interactive Agents in three pairwise scoring groups using our self-designed scoring platform, as presented in Figure 8 . By counting the winning situation of TheraMind with better response quality in the twenty samples (including 120 sessions in total), we confirm that our TheraMind has demonstrated better counseling effects in various psychological counseling scenarios. In addition, to verify the reliability of our human evaluation in model comparison evaluation, we used Cohen‚Äôs Œ∫ \\\\kappa coefficient to quantify the consistency of the evaluation results of multiple human evaluators. The final Cohen‚Äôs Œ∫ \\\\kappa obtained was 0.697 (0.6‚Äì0.8), indicating that our human evaluation has high objectivity and impartiality, and the results of manual evaluation are generally reliable.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Human Agreement Analysis',\n",
       "    'paragraphs': 'To verify whether the three key decision-making modules of TheraMind, namely Memory Invoke , Emotion Recognition , and Response Strategy , can effectively enhance counseling performance and accurately perform their intended judgments, two trained annotators with expertise in psychology evaluated the accuracy of these functions using our self-designed scoring platform, as shown in Figure 9 . One case was randomly selected from each of the ten categories, and one session from each case was used for evaluation, covering a total of 68 dialogue rounds. To ensure a comprehensive assessment, the annotators were provided with both the target session and the previous sessions for contextual reference.\\nAs shown in Figure 5 (Left), TheraMind‚Äôs modules demonstrate high fidelity with human clinical reasoning. The evaluation results show that the Emotion Recognition module reached an agreement rate of 95.59%, and the Memory Invoke module achieved 94.12%, demonstrating TheraMind‚Äôs strong perceptual and contextual reasoning abilities. The Response Strategy module also obtained an agreement rate of 85.29%, indicating its capacity to make clinically coherent and contextually appropriate decisions. Inter-rater reliability, measured using Cohen‚Äôs Œ∫ \\\\kappa was calculated as 0.67, which falls within the range typically interpreted as substantial agreement (0.60‚Äì0.80). These findings confirm the reliability and consistency of TheraMind‚Äôs core decision-making mechanisms in producing high-quality therapeutic responses.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Ethical Considerations',\n",
       "    'paragraphs': 'Data Privacy. We used the CPsyCounR dataset of anonymized and professionally rewritten Chinese counseling reports strictly for academic research under ethical guidelines, with safeguards to prevent re-identification or misuse.\\nAnnotator Salary. All assessments were conducted by trained annotators with expertise in psychology, who were compensated fairly with payment set above local minimum wage and aligned with standard academic research practices.\\nEvaluation Bias. Multi-session evaluation is challenging due to long dialogue histories. LLM-based methods offer scalability but may introduce biases (e.g., Deepseek). We mitigated this with limited human expert assessments.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Open-source Models',\n",
       "    'paragraphs': 'ChatCounselor : https://huggingface.co/EmoCareAI/ChatPsychiatrist\\nCPsyCounX : https://huggingface.co/CAS-SIAT-XinHai/CPsyCounX\\nGLM-4-9B-Chat : https://huggingface.co/zai-org/glm-4-9b-chat\\nInterLM2.5-7B-Chat : https://huggingface.co/internlm/internlm2_5-7b-chat\\nQwen3-8B : https://huggingface.co/Qwen/Qwen3-8B\\nKimi-Dev-72B : https://huggingface.co/moonshotai/Kimi-Dev-72B\\nDoubao-1.5-pro-32k : https://www.volcengine.com/docs/82379/1554678\\nYi-Large : https://www.lingyiwanwu.com\\nPsyDTLLM : https://modelscope.cn/models/YIRONGCHEN/SoulChat2.0-internlm2-7b\\nDeepSeek-V3 : https://www.deepseek.com\\nInteractive Agents : https://github.com/qiuhuachuan/interactiveagents',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix E Expert Evaluation',\n",
       "    'paragraphs': 'To demonstrate TheraMind‚Äôs advantages, we analyze the same session from a single case and compare TheraMind (Figure 22 ) with three strong baselines, PsyTLLM (Figure 25 ), DeepSeeker-V3 (Figure 23 ), and Interactive Agents (Figure 24 ). PsyTLLM and DeepSeeker-V3 underattend to emotional signals, shift to techniques too early, and fail to help the patient trace emotions to their sources. Interactive Agents focuses narrowly on cognition, does not anchor or process current physical and somatic responses, and misses intervention windows. All three baselines underuse prior discussion, show memory inconsistency, and disrupt continuity. TheraMind better regulates pace and boundaries, flexibly adapts response strategies, and consistently centers the patient‚Äôs feelings, guiding the patient toward the causes of negative emotions.\\nAs a complement to the main case study (Figure 7 ), we provide a focused analysis. The complete six-session dialogue appears in Figures 26 , 27 , 28 , and 29 . Sessions\\xa02 and\\xa03 (Figure 27 ) illustrate TheraMind‚Äôs decision process. In Session\\xa02, the patient describes a suffocating dream with a ‚Äúgiant rib‚Äù, interpreted as overwhelming pressure. When the patient resists the initial intervention (‚ÄúBreathing? I don‚Äôt see how that‚Äôll help‚Äù), TheraMind drops the exercise and invites the patient to ‚Äúname just one small piece of that rib‚Äôs weight‚Äù. The patient links the rib to ‚Äúmy mom‚Äôs heavy expectations‚Äù and reports feeling ‚Äúa tiny bit lighter‚Äù. TheraMind stores this metaphor across sessions. In Session\\xa03, a similar dream elicits worthlessness, the system detects sadness and a negative stance, selects a Reflection of Feelings strategy, and retrieves the ‚Äúrib‚Äù metaphor. The response connects the present emotion (‚Äúfamiliar weight of not feeling enough‚Äù) with the prior insight (‚Äújust as we did with the rib before‚Äù), avoiding clinical amnesia and reinforcing the therapeutic arc. The patient then proposes a ‚Äúvictory journal‚Äù to consolidate gains. Overall, these observations indicate that TheraMind delivers more coherent, empathic, and durable therapeutic progress than competing systems under comparable conditions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix F Prompts for TheraMind',\n",
       "    'paragraphs': 'TheraMind is driven by a series of modular prompts that guide its psychological counseling process . The system begins with emotion perception and therapy selection (Fig. 10 and 11 ), followed by therapy adjustment and memory retrieval (Fig. 12 and 13 ). It further includes resistance detection and stage analysis (Fig. 14 and 15 ), and determines the proper response strategy for the counselor (Fig. 16 ). Dialogue generation for both patient and counselor is handled by specialized prompts (Fig. 17 and 18 ), while session termination and performance evaluation are managed by the final modules (Fig. 19 and 21 ). Together, these prompts form an integrated framework enabling TheraMind to conduct, adapt, and evaluate psychotherapy sessions autonomously.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix G TheraMind of Counseling Strategies',\n",
       "    'paragraphs': 'TheraMind includes 12 counseling strategies, categorized into eight passive supporting strategies and four proactive challenging strategies . The definitions and real examples of each strategy are illustrated in Table 3 and Table 4 , which demonstrate how TheraMind applies these strategies in simulated therapeutic interactions.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long‚Äëterm memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient‚Äôs emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.'},\n",
       " '2510.25445v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'The field of Artificial Intelligence (AI) is undergoing a paradigm shift from the development of passive, task-specific tools toward the engineering of autonomous systems that exhibit genuine agency. Modern agentic AI systems [ 1 , 2 ] are defined by capabilities such as proactive planning, contextual memory, sophisticated tool use, and the ability to adapt their behavior based on environmental feedback. These systems operate not as mere solvers but as collaborative partners, capable of dynamically perceiving complex environments, reasoning about abstract goals, and orchestrating sequences of actions‚Äîeither independently or as part of a sophisticated multi-agent ecosystem [ 3 , 4 ] .\\nTo establish a precise conceptual foundation, we distinguish between the field‚Äôs core concepts. An AI Agent (or a Single-Agent System ) is a self-contained autonomous system designed to accomplish a goal. It operates primarily in isolation, though it may interact with tools and APIs. Its agency is defined by its autonomy , proactivity , and its ability to complete a task from start to finish independently.\\nFor example, a single, powerful LLM-based (Large Language Model-based) agent tasked with ‚ÄúWrite a full project proposal for a new mobile app‚Äù would autonomously break down the task, conduct research, write the sections, and format the final document.\\nIn contrast, Agentic AI is the broader field and architectural approach concerned with creating systems that exhibit agency. Crucially, this often involves the orchestration of Multi-Agent Systems (MAS) , where multiple specialized agents work together, coordinating and communicating to solve problems that are too complex for a single agent.\\nFor example, an Agentic AI system designed for the same task would employ a team of specialized agents: a Project Manager Agent to break the goal into tasks, a Researcher Agent to gather market data, a Writer Agent to draft content, and a Quality Assurance Agent to review the output. Their collaborative workflow is the embodiment of Agentic AI.\\nIn summary, one can conceptualize an AI Agent as a single, sophisticated worker, while Agentic AI represents the principle of leveraging agency, frequently by architecting and managing an entire team of such workers.\\nThis rapid evolution, however, has led to a fragmented and often anachronistic understanding of the field. A critical issue identified in prior reviews is conceptual retrofitting ‚Äîthe misapplication of classical symbolic frameworks (e.g., Belief‚ÄìDesire‚ÄìIntention (BDI) [ 5 ] , perceive‚Äìplan‚Äìact‚Äìreflect (PPAR) loops [ 6 , 7 ] ) to describe modern systems built on large language models (LLMs) [ 8 ] , which operate on fundamentally different principles of stochastic generation and prompt-driven orchestration. This practice obscures the true operational mechanics of LLM-based agents [ 9 , 10 , 11 , 12 ] and creates a false sense of continuity between incompatible architectural paradigms, whether applied to a single complex agent or a coordinated MAS.\\nThis paper addresses these gaps by first establishing a clear historical context (Figure 1 ), which delineates the evolution of AI through five distinct but overlapping eras.\\nThe Symbolic AI Era (1950s‚Äì1980s) [ 13 ] established the foundational ambition of artificial intelligence, grounded in logic and explicit human knowledge. This period was dominated by rule-based systems and expert systems such as MYCIN and DENDRAL [ 14 ] , which operated on carefully hand-crafted symbolic rules. Intelligence was conceived as a top-down, deductive process, representing the purest form of the symbolic paradigm.\\nThe Machine Learning (ML) Era (1980s‚Äì2010s) [ 15 , 16 , 17 ] marked a pivotal shift away from hard-coded logic toward systems that could learn from data. While still heavily dependent on human-engineered features, this period introduced statistical ML models such as Support Vector Machines and decision trees, which powered applications ranging from classification to recommendation. It was a transitional stage that moved the field away from pure symbolism but still lacked the automated feature learning that would define subsequent eras.\\nThe arrival of the Deep Learning Era (2010s‚ÄìPresent) [ 18 , 19 , 20 , 21 , 22 ] was catalyzed by the confluence of increased compute power and large datasets. Deep neural networks, including convolutional and recurrent architectures, enabled systems to automatically learn hierarchical representations from raw data. This era revolutionized pattern recognition in vision, speech, and text, breaking longstanding barriers in perception. Yet, despite their power, these models largely functioned as sophisticated pattern classifiers rather than autonomous agents.\\nOut of this foundation emerged the Generative AI Era (2014‚ÄìPresent) [ 23 , 24 , 25 , 26 , 27 ] , fueled by advances in generative modeling. Early breakthroughs such as Generative Adversarial Networks were soon eclipsed by the introduction of the Transformer architecture in 2017, which enabled the scaling of large language models (LLMs) such as GPT and BERT. These systems moved beyond perception to generation, producing coherent text, code, and media. In doing so, they provided the essential substrate‚Äîa powerful, general-purpose statistical reasoner‚Äîthat made modern agentic AI feasible.\\nFinally, the Agentic AI Era (2022‚ÄìPresent) represents the current frontier, where the generative capabilities of LLMs are harnessed for action and autonomy. This era is characterized by the rise of AI agents [ 28 , 29 , 30 ] such as AutoGPT, which can pursue goals through planning and tool use. Increasingly, these agents evolve into multi-agent systems [ 31 , 32 , 33 , 34 , 35 ] , exemplified by frameworks like CrewAI and AutoGen, where specialized roles and orchestrated collaboration enable teams of agents to tackle complex problems. In contrast to the algorithmic deliberation of the symbolic paradigm, this stage is defined by the neural paradigm, where agency emerges from the stochastic orchestration of generative models.\\nThis chronological progression provides essential context but also reveals a critical conceptual schism. The agentic AI era is not simply a linear descendant of symbolic AI but is instead built upon a completely different architectural foundation. To address this, we introduce a novel conceptual framework (Figure 2 ) designed to prevent retrospective conflation by clearly distinguishing the symbolic and neural lineages of agentic AI. This dual-axis taxonomy provides the unified lens necessary to rigorously analyze the field‚Äôs theoretical underpinnings, architectural innovations, and practical deployments.\\nThe journey to modern agentic AI is best understood through its historical progression, as detailed in Figure 1 . This evolution moved from the deterministic, rule-based systems of the symbolic era through the data-driven revolutions of machine learning and deep learning, culminating in the transformative advent of large language models (LLMs) [ 36 , 37 ] and generative AI.\\nHowever, a chronological account is insufficient for analytical rigor. The central challenge in current discourse is the conceptual retrofitting of modern, neural agentic architectures into the frameworks of the symbolic era. To resolve this, we propose a dual-paradigm taxonomy in Figure 2 . This framework categorizes agentic systems along two independent dimensions: their Architectural Paradigm (Symbolic vs. Neural) and their Degree of Agency & Coordination (Single-Agent vs. Multi-Agent). This model is designed not to show evolution, but to provide a clear analytical structure for classification and comparison, ensuring systems are evaluated on their own operational terms.\\nThis review is structured around this framework to synthesize three critically interconnected layers:\\nThe first layer encompasses the Theoretical Foundations , including core principles of autonomy and agency [ 38 ] , and decision-making models like Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs) [ 39 , 40 ] . It is crucial to note that these models provide a theoretical language for describing agency that originated in the Symbolic paradigm , but modern systems implement these concepts in entirely new ways.\\nThe second layer analyzes Architectural Frameworks , focusing on the modern infrastructures powering the Neural paradigm . We examine systems like LangChain [ 41 ] , AutoGen, and CrewAI, which achieve agency through mechanisms like prompt chaining, conversation orchestration, and dynamic context management‚Äîa clear departure from the symbolic planning of the classical lineage.\\nThe third layer investigates Application Domains , exploring the practical deployment of agentic systems across fields such as healthcare [ 42 ] , finance [ 43 ] , scientific discovery [ 44 ] , and legal reasoning [ 45 ] . Our framework allows us to map these applications to the appropriate paradigm and analyze their unique implementation challenges.\\nThe current discourse on agentic AI suffers from the conceptual retrofitting illustrated in Figure 2 . Classical AI frameworks, such as the BDI model or perceive‚Äìplan‚Äìact‚Äìreflect (PPAR) loops, are often rhetorically applied but are fundamentally mismatched to the stochastic, non-symbolic, and context-driven nature of LLM-based agents [ 5 ] . Furthermore, existing reviews are often narrow in scope, lacking empirical comparisons or integrated governance insights. As summarized in Table 1 , current literature leaves substantial gaps in understanding the field‚Äôs current state.\\nThis review directly addresses these limitations through four integrated contributions:\\nA Novel Dual-Paradigm Taxonomy: We introduce and employ the framework in Figure 2 as our primary analytical tool, explicitly distinguishing symbolic and neural lineages to prevent conceptual retrofitting and enable accurate system classification.\\nArchitectural Clarification: We demystify the operational principles of modern neural frameworks (Section 4 ), explaining how they achieve agency through mechanisms like prompt chaining and conversation orchestration, rather than symbolic planning.\\nEmpirical Mapping: We conduct a systematic PRISMA-based literature review of 90 studies, categorizing them using our dual-paradigm framework to trace research trends and evaluate architectures by their appropriate standards.\\nGovernance Anchoring: We embed ethical, accountability, and alignment challenges within each paradigm of our taxonomy to ensure that safety considerations are discussed in the correct technological context (Section 7 ).\\nTo guide the reader through our analysis, the paper is structured to logically develop the argument for a dual-paradigm understanding of Agentic AI. We begin by establishing the necessary theoretical context in Section 2 , which explores the foundations of agency and introduces our core taxonomic framework. Section 3 then details the systematic methodology underpinning our literature review.\\nThe subsequent sections apply this framework to analyze the field: Section 4 reviews key architectural frameworks through our taxonomic lens, and Section 5 examines how different application domains influence paradigm selection. Section 6 presents a comprehensive paradigm-aware taxonomy of the literature, serving as a foundational reference and key output of our review. Section 7 investigates the paradigm-specific nature of ethical and governance challenges, leading directly into Section 8 , which outlines the critical research gaps identified by our analysis.\\nThe final sections synthesize our findings and look forward. Section 9 then charts an actionable research roadmap toward hybrid intelligence, building directly upon both the identified gaps and our stated contributions. Finally, Section 10 provides a final synthesis of our findings and their implications for the field.\\nThis structure is designed to first equip the reader with the necessary conceptual tools, then systematically analyze the landscape, and conclude by synthesizing the insights into a coherent vision for the future of Agentic AI.',\n",
       "    'subsections': [{'title': '1.1 Current Surveys Gaps and Contributions',\n",
       "      'paragraphs': 'The current discourse on agentic AI suffers from the conceptual retrofitting illustrated in Figure 2 . Classical AI frameworks, such as the BDI model or perceive‚Äìplan‚Äìact‚Äìreflect (PPAR) loops, are often rhetorically applied but are fundamentally mismatched to the stochastic, non-symbolic, and context-driven nature of LLM-based agents [ 5 ] . Furthermore, existing reviews are often narrow in scope, lacking empirical comparisons or integrated governance insights. As summarized in Table 1 , current literature leaves substantial gaps in understanding the field‚Äôs current state.\\nThis review directly addresses these limitations through four integrated contributions:\\nA Novel Dual-Paradigm Taxonomy: We introduce and employ the framework in Figure 2 as our primary analytical tool, explicitly distinguishing symbolic and neural lineages to prevent conceptual retrofitting and enable accurate system classification.\\nArchitectural Clarification: We demystify the operational principles of modern neural frameworks (Section 4 ), explaining how they achieve agency through mechanisms like prompt chaining and conversation orchestration, rather than symbolic planning.\\nEmpirical Mapping: We conduct a systematic PRISMA-based literature review of 90 studies, categorizing them using our dual-paradigm framework to trace research trends and evaluate architectures by their appropriate standards.\\nGovernance Anchoring: We embed ethical, accountability, and alignment challenges within each paradigm of our taxonomy to ensure that safety considerations are discussed in the correct technological context (Section 7 ).',\n",
       "      'subsections': []},\n",
       "     {'title': '1.2 Structure of the Paper',\n",
       "      'paragraphs': 'To guide the reader through our analysis, the paper is structured to logically develop the argument for a dual-paradigm understanding of Agentic AI. We begin by establishing the necessary theoretical context in Section 2 , which explores the foundations of agency and introduces our core taxonomic framework. Section 3 then details the systematic methodology underpinning our literature review.\\nThe subsequent sections apply this framework to analyze the field: Section 4 reviews key architectural frameworks through our taxonomic lens, and Section 5 examines how different application domains influence paradigm selection. Section 6 presents a comprehensive paradigm-aware taxonomy of the literature, serving as a foundational reference and key output of our review. Section 7 investigates the paradigm-specific nature of ethical and governance challenges, leading directly into Section 8 , which outlines the critical research gaps identified by our analysis.\\nThe final sections synthesize our findings and look forward. Section 9 then charts an actionable research roadmap toward hybrid intelligence, building directly upon both the identified gaps and our stated contributions. Finally, Section 10 provides a final synthesis of our findings and their implications for the field.\\nThis structure is designed to first equip the reader with the necessary conceptual tools, then systematically analyze the landscape, and conclude by synthesizing the insights into a coherent vision for the future of Agentic AI.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '2 Theoretical Foundations: Mapping the Dual Lineages of Agentic Intelligence',\n",
       "    'paragraphs': 'The architectural history of agentic AI is not a linear progression but a branching into two distinct paradigms, as defined by our conceptual framework (Figure 2 ). This section delineates the theoretical and cognitive groundwork for both the Symbolic/Classical and Neural/Generative lineages, clarifying their foundational principles and highlighting the paradigm shift that separates them.\\nThe conceptual language for describing agency originated within the symbolic paradigm. The foundational constructs of autonomy and agency are essential for both lineages, though they are implemented in fundamentally different ways. Autonomy refers to a system‚Äôs ability to operate independently, free from direct human intervention, whereas agency encapsulates the notion of goal-directed behavior that incorporates intention, contextual awareness, and decision-making capabilities [ 50 , 38 ] . Agentic AI synthesizes these traits by initiating tasks, dynamically ranking goals, monitoring progress, and adjusting behavior through feedback loops [ 51 ] .\\nThese mechanisms parallel human executive functions such as planning, inhibition, and cognitive flexibility. They provide the high-level descriptive framework for intelligent behavior, which both symbolic and neural systems aim to achieve through divergent mechanisms.\\nThe symbolic lineage is characterized by explicit logic, algorithmic planning, and deterministic or probabilistic models. Its evolution provides the theoretical bedrock for pre-LLM autonomous systems.\\nMDPs provide the mathematical scaffolding for modeling environments with full state information [ 52 , 53 ] , a hallmark of early symbolic and classical statistical AI. An MDP is defined by a tuple (S, A, P, R), representing states, actions, transition probabilities, and rewards. These systems operate effectively in deterministic, rule-based domains but lack the capacity for robust reasoning under uncertainty, anchoring them firmly in the symbolic paradigm.\\nPOMDPs extend MDPs by introducing probabilistic belief states to handle environments where the agent has incomplete information [ 54 , 55 ] . This was a key advancement, allowing symbolic agents to infer hidden states through observation and enabling more adaptive behavior. However, as illustrated in Figure 3 , this is still a form of algorithmic state estimation. The significant computational overhead of belief tracking limits their scalability and real-world application [ 56 , 57 ] , a fundamental constraint of the symbolic approach.\\nCognitive architectures like Belief-Desire-Intention (BDI) and SOAR represent the pinnacle of the symbolic paradigm‚Äôs attempt to engineer agency. They explicitly model internal states and processes, as summarized in Table 2 . These systems directly implement a perceive-plan-act-reflect loop using symbolic representations, making them powerful but brittle and difficult to scale to complex, real-world environments. Their relationship to human cognitive functions is a direct, top-down mapping of symbolic logic.\\nThe neural lineage is built on a foundation of statistical learning from data, culminating in the generative capabilities of large language models (LLMs). Its progression is marked by a move away from explicit logic toward emergent, stochastic behavior.\\nDeep Reinforcement Learning (DRL) represents a critical transition. It scales learning to high-dimensional inputs (like images and text) using neural networks [ 58 , 59 ] . DRL agents learn policies directly from data, moving away from hand-crafted symbolic rules. Methods such as PPO allow for fine-grained behavioral optimization [ 60 , 61 ] . As shown in Figure 4 , advancements like meta-DRL introduced generalization across tasks, a precursor to the adaptability required for modern agency. DRL is a bridge, using neural networks to learn the policies that symbolic systems would have to be explicitly programmed with.\\nThe emergence of Large Language Models (LLMs) was not an evolution but a revolution that created the new neural paradigm. LLMs provided a powerful, general-purpose substrate for reasoning based on statistical prediction in a high-dimensional space of concepts. This enabled a fundamental architectural shift from designing cognitive agents to orchestrating generative pipelines.\\nFrameworks like LangChain, AutoGen, and CrewAI do not implement symbolic PPAR loops or BDI architectures. They represent a new paradigm of LLM Orchestration , where pre-trained models act as central executives that coordinate tasks through fundamentally different mechanisms, as detailed in Table 3 .\\nThis shift marks the definitive break from the symbolic tradition. Agency in the neural paradigm is an emergent property of prompt-driven orchestration, not a product of internal symbolic logic. The evolution of a personal assistant, depicted in Figure 5 , culminates in this new architecture.\\nThe most advanced manifestation of the neural paradigm is multi-agent orchestration. Frameworks like AutoGen [ 67 ] and LangGraph [ 81 ] coordinate diverse, modular agents through structured communication protocols. As visualized in Figure 6 , an orchestrator (often an LLM itself) acts as a context manager and task router, assessing the overall goal and dynamically assigning specialized subtasks to other agents.\\nThis architecture achieves scalability and complex problem-solving not through a single agent‚Äôs cognitive complexity, but through the emergent intelligence of a well-orchestrated system. It is the culmination of the neural lineage, firmly establishing the new orthodoxy of LLM-driven pipelines and completing the paradigm shift from the symbolic AI tradition.',\n",
       "    'subsections': [{'title': '2.1 Core Principles of Autonomy and Agency',\n",
       "      'paragraphs': 'The conceptual language for describing agency originated within the symbolic paradigm. The foundational constructs of autonomy and agency are essential for both lineages, though they are implemented in fundamentally different ways. Autonomy refers to a system‚Äôs ability to operate independently, free from direct human intervention, whereas agency encapsulates the notion of goal-directed behavior that incorporates intention, contextual awareness, and decision-making capabilities [ 50 , 38 ] . Agentic AI synthesizes these traits by initiating tasks, dynamically ranking goals, monitoring progress, and adjusting behavior through feedback loops [ 51 ] .\\nThese mechanisms parallel human executive functions such as planning, inhibition, and cognitive flexibility. They provide the high-level descriptive framework for intelligent behavior, which both symbolic and neural systems aim to achieve through divergent mechanisms.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 The Symbolic Lineage: Algorithmic Decision-Making',\n",
       "      'paragraphs': 'The symbolic lineage is characterized by explicit logic, algorithmic planning, and deterministic or probabilistic models. Its evolution provides the theoretical bedrock for pre-LLM autonomous systems.\\nMDPs provide the mathematical scaffolding for modeling environments with full state information [ 52 , 53 ] , a hallmark of early symbolic and classical statistical AI. An MDP is defined by a tuple (S, A, P, R), representing states, actions, transition probabilities, and rewards. These systems operate effectively in deterministic, rule-based domains but lack the capacity for robust reasoning under uncertainty, anchoring them firmly in the symbolic paradigm.\\nPOMDPs extend MDPs by introducing probabilistic belief states to handle environments where the agent has incomplete information [ 54 , 55 ] . This was a key advancement, allowing symbolic agents to infer hidden states through observation and enabling more adaptive behavior. However, as illustrated in Figure 3 , this is still a form of algorithmic state estimation. The significant computational overhead of belief tracking limits their scalability and real-world application [ 56 , 57 ] , a fundamental constraint of the symbolic approach.\\nCognitive architectures like Belief-Desire-Intention (BDI) and SOAR represent the pinnacle of the symbolic paradigm‚Äôs attempt to engineer agency. They explicitly model internal states and processes, as summarized in Table 2 . These systems directly implement a perceive-plan-act-reflect loop using symbolic representations, making them powerful but brittle and difficult to scale to complex, real-world environments. Their relationship to human cognitive functions is a direct, top-down mapping of symbolic logic.',\n",
       "      'subsections': [{'title': '2.2.1 Markov Decision Processes (MDPs)',\n",
       "        'paragraphs': 'MDPs provide the mathematical scaffolding for modeling environments with full state information [ 52 , 53 ] , a hallmark of early symbolic and classical statistical AI. An MDP is defined by a tuple (S, A, P, R), representing states, actions, transition probabilities, and rewards. These systems operate effectively in deterministic, rule-based domains but lack the capacity for robust reasoning under uncertainty, anchoring them firmly in the symbolic paradigm.',\n",
       "        'subsections': []},\n",
       "       {'title': '2.2.2 Partially Observable MDPs (POMDPs)',\n",
       "        'paragraphs': 'POMDPs extend MDPs by introducing probabilistic belief states to handle environments where the agent has incomplete information [ 54 , 55 ] . This was a key advancement, allowing symbolic agents to infer hidden states through observation and enabling more adaptive behavior. However, as illustrated in Figure 3 , this is still a form of algorithmic state estimation. The significant computational overhead of belief tracking limits their scalability and real-world application [ 56 , 57 ] , a fundamental constraint of the symbolic approach.',\n",
       "        'subsections': []},\n",
       "       {'title': '2.2.3 Cognitive Architectures: BDI and SOAR',\n",
       "        'paragraphs': 'Cognitive architectures like Belief-Desire-Intention (BDI) and SOAR represent the pinnacle of the symbolic paradigm‚Äôs attempt to engineer agency. They explicitly model internal states and processes, as summarized in Table 2 . These systems directly implement a perceive-plan-act-reflect loop using symbolic representations, making them powerful but brittle and difficult to scale to complex, real-world environments. Their relationship to human cognitive functions is a direct, top-down mapping of symbolic logic.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '2.3 The Neural Lineage: Statistical Learning and Emergent Reasoning',\n",
       "      'paragraphs': 'The neural lineage is built on a foundation of statistical learning from data, culminating in the generative capabilities of large language models (LLMs). Its progression is marked by a move away from explicit logic toward emergent, stochastic behavior.\\nDeep Reinforcement Learning (DRL) represents a critical transition. It scales learning to high-dimensional inputs (like images and text) using neural networks [ 58 , 59 ] . DRL agents learn policies directly from data, moving away from hand-crafted symbolic rules. Methods such as PPO allow for fine-grained behavioral optimization [ 60 , 61 ] . As shown in Figure 4 , advancements like meta-DRL introduced generalization across tasks, a precursor to the adaptability required for modern agency. DRL is a bridge, using neural networks to learn the policies that symbolic systems would have to be explicitly programmed with.\\nThe emergence of Large Language Models (LLMs) was not an evolution but a revolution that created the new neural paradigm. LLMs provided a powerful, general-purpose substrate for reasoning based on statistical prediction in a high-dimensional space of concepts. This enabled a fundamental architectural shift from designing cognitive agents to orchestrating generative pipelines.\\nFrameworks like LangChain, AutoGen, and CrewAI do not implement symbolic PPAR loops or BDI architectures. They represent a new paradigm of LLM Orchestration , where pre-trained models act as central executives that coordinate tasks through fundamentally different mechanisms, as detailed in Table 3 .\\nThis shift marks the definitive break from the symbolic tradition. Agency in the neural paradigm is an emergent property of prompt-driven orchestration, not a product of internal symbolic logic. The evolution of a personal assistant, depicted in Figure 5 , culminates in this new architecture.',\n",
       "      'subsections': [{'title': '2.3.1 Deep Reinforcement Learning (DRL)',\n",
       "        'paragraphs': 'Deep Reinforcement Learning (DRL) represents a critical transition. It scales learning to high-dimensional inputs (like images and text) using neural networks [ 58 , 59 ] . DRL agents learn policies directly from data, moving away from hand-crafted symbolic rules. Methods such as PPO allow for fine-grained behavioral optimization [ 60 , 61 ] . As shown in Figure 4 , advancements like meta-DRL introduced generalization across tasks, a precursor to the adaptability required for modern agency. DRL is a bridge, using neural networks to learn the policies that symbolic systems would have to be explicitly programmed with.',\n",
       "        'subsections': []},\n",
       "       {'title': '2.3.2 The LLM Substrate and The Paradigm Shift',\n",
       "        'paragraphs': 'The emergence of Large Language Models (LLMs) was not an evolution but a revolution that created the new neural paradigm. LLMs provided a powerful, general-purpose substrate for reasoning based on statistical prediction in a high-dimensional space of concepts. This enabled a fundamental architectural shift from designing cognitive agents to orchestrating generative pipelines.\\nFrameworks like LangChain, AutoGen, and CrewAI do not implement symbolic PPAR loops or BDI architectures. They represent a new paradigm of LLM Orchestration , where pre-trained models act as central executives that coordinate tasks through fundamentally different mechanisms, as detailed in Table 3 .\\nThis shift marks the definitive break from the symbolic tradition. Agency in the neural paradigm is an emergent property of prompt-driven orchestration, not a product of internal symbolic logic. The evolution of a personal assistant, depicted in Figure 5 , culminates in this new architecture.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '2.4 Multi-Agent Orchestration: The Pinnacle of the Neural Paradigm',\n",
       "      'paragraphs': 'The most advanced manifestation of the neural paradigm is multi-agent orchestration. Frameworks like AutoGen [ 67 ] and LangGraph [ 81 ] coordinate diverse, modular agents through structured communication protocols. As visualized in Figure 6 , an orchestrator (often an LLM itself) acts as a context manager and task router, assessing the overall goal and dynamically assigning specialized subtasks to other agents.\\nThis architecture achieves scalability and complex problem-solving not through a single agent‚Äôs cognitive complexity, but through the emergent intelligence of a well-orchestrated system. It is the culmination of the neural lineage, firmly establishing the new orthodoxy of LLM-driven pipelines and completing the paradigm shift from the symbolic AI tradition.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Methodology',\n",
       "    'paragraphs': 'A rigorous and transparent methodology is essential for constructing a comprehensive review that captures the dual paradigms of Agentic AI. This section outlines the systematic process used to identify, evaluate, and synthesize literature, with a specific focus on categorizing works according to the symbolic and neural lineages defined in our conceptual framework (Figure 2 ). It follows established review protocols to ensure reproducibility while accounting for the field‚Äôs rapid evolution.\\nThis study adopts the PRISMA 2020 framework (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) [ 82 , 83 ] , guiding all stages from search strategy to synthesis. The methodology is designed to capture and distinguish between the symbolic/classical and neural/generative lineages of agentic AI research across computer science, cognitive psychology, robotics, and ethics.\\nObjectives: This systematic review aims to provide a comprehensive analysis of agentic AI systems through the following specific research objectives:\\nTo identify, classify, and synthesize literature based on the dual architectural paradigms (Symbolic vs. Neural) of Agentic AI.\\nTo examine the evolution of capabilities, applications, and performance metrics within and across each paradigm.\\nTo analyze governance frameworks and ethical challenges, contextualizing them within their respective architectural paradigms.\\nTo highlight paradigm-specific research gaps and propose informed future directions based on the synthesized evidence.\\nA multi-database search strategy was employed to identify literature across both historical symbolic and modern neural agentic AI research. Sources included: IEEE Xplore, ACM Digital Library, arXiv, SpringerLink, ScienceDirect, and Google Scholar.\\nThe search strategy employed a structured set of keyword clusters designed to comprehensively capture the core concepts associated with both architectural paradigms. To represent the Symbolic/Classical lineage, targeted terms included foundational concepts such as \"Cognitive architectures,\" \"BDI agent,\" \"SOAR,\" \"POMDP,\" \"symbolic planning,\" and \"multi-agent systems\" (in its traditional sense). Conversely, the Neural/Generative paradigm was captured through terms reflecting its contemporary emergence, such as \"LLM agent,\" \"AI orchestration,\" \"prompt chaining,\" \"tool-augmented LLM,\" \"multi-agent conversation,\" and specific framework names including \"AutoGen\" and \"LangChain.\" Finally, a set of General terms‚Äî\"Agentic AI,\" \"autonomous agent,\" and \"goal-directed AI\"‚Äîwas used to ensure broad coverage and to capture literature that might bridge or transcend the paradigmatic divide.\\nBoolean operators were structured to optimize breadth and relevance (e.g., (\"autonomous agent\" OR \"agentic AI\") AND (\"large language model\" OR \"orchestration\" OR \"cognitive architecture\")).\\nThe search scope was interdisciplinary, targeting relevant fields from computer science to ethics. To capture the most current advancements in the rapidly evolving neural paradigm, the search included pre-print servers like arXiv, with these records being manually assessed for quality and relevance.\\nTo ensure the review‚Äôs methodological integrity and thematic relevance, predefined inclusion and exclusion parameters were applied during the screening process. These criteria were designed to capture high-quality literature from both paradigms of agentic AI.\\nThe literature search employed the following inclusion criteria to identify publications that contribute directly to the core themes of agentic AI architectures and applications. Specifically, we included peer-reviewed journal articles, conference proceedings, and formally published technical reports from recognized institutions. To capture the most recent advancements in the rapidly evolving neural paradigm, we also incorporated high-impact pre-prints from arXiv, which were manually screened for methodological rigor and citation impact, with a focus on those presenting novel architectures or frameworks. The scope of included work encompassed studies involving the design, implementation, or evaluation of autonomous agents, spanning both classical symbolic systems and modern LLM-orchestrated frameworks. All selected publications were required to be in English and published within the temporal window of January 2018 to March 2025.\\nTo ensure a focused and methodologically rigorous review, studies were excluded according to the following criteria. Non-English language publications were omitted. We also excluded non-peer-reviewed or informal sources such as opinion pieces, editorials, blog posts, and unverified online content. Furthermore, studies focused exclusively on generative AI (e.g., for image generation or text completion) without incorporating agentic features like goal-directedness, tool use, or multi-step autonomy were deemed out of scope. Finally, duplicate records retrieved from multiple databases were identified and removed to prevent redundancy in the analysis.\\nThese criteria ensured the retention of conceptually aligned and methodologically sound studies from both paradigms, preserving the review‚Äôs comprehensive scope. A summary is provided in Table 4 .\\nThe screening protocol adhered to the PRISMA 2020 guidelines to ensure methodological transparency and reproducibility. Records were compiled from selected databases, yielding an initial pool of 165 items (157 from databases, 8 from supplemental sources).\\nFollowing deduplication, 120 unique records remained. Title and abstract screening excluded 42 studies due to irrelevance or insufficient focus on agentic AI. Full-text assessment confirmed 78 articles met all inclusion criteria.\\nIn alignment with PRISMA‚Äôs guidance for systematic reviews that require foundational context, a supplemental phase was conducted [ 82 ] . During thematic synthesis, 12 seminal theoretical papers from the symbolic paradigm (e.g., foundational works on MDPs by [ 39 ] and cognitive architectures by [ 84 ] ) were incorporated. These papers were essential for providing complete historical context for the taxonomic framework and understanding the symbolic lineage, though they were analyzed separately from contemporary neural paradigm research. This resulted in a final corpus of 90 publications for contextual and theoretical grounding, with 78 studies forming the core for analysis of contemporary trends.\\nThe process is illustrated in Figure 7 , which clearly distinguishes the primary systematic search from the supplemental inclusion of foundational context.\\nThe 78 studies forming the core of the review underwent thematic synthesis following the methodology described by Thomas and Harden [ 85 ] , with analysis specifically structured around the dual-paradigm framework.\\nOur analysis employed a multi-faceted methodological approach to systematically investigate the body of research. The initial phase involved paradigm classification , whereby each study was categorized according to its primary architectural paradigm‚Äîeither Symbolic/Classical or Neural/Generative‚Äîbased on the core operational mechanisms defined in our conceptual framework. Following this classification, we conducted a detailed framework mapping within each paradigm to group studies by their specific architectural approaches, including orchestration models (e.g., AutoGen, CrewAI), memory structures, and learning mechanisms. Building on this organized foundation, a cross-paradigm comparison was performed to identify fundamental differences in implementation, performance, and limitations between the two overarching paradigms. In parallel, we performed domain clustering to group applications by sector‚Äîsuch as healthcare, finance, robotics, and scientific discovery‚Äîwhich enabled the identification of performance patterns and deployment strategies both within and across paradigms. Finally, an ethical coding procedure was applied, using a structured lexicon to tag recurring themes related to governance, safety, transparency, and bias, with particular attention paid to how these ethical challenges manifest differently within each paradigm.\\nQualitative coding was supported by tools such as NVivo [ 86 ] , which enabled hierarchical theme identification and cross-paradigm analysis. Quantitative results were tabulated and compared within and across domains and paradigms to synthesize technical and operational insights.\\nThis paradigm-informed approach ensured a nuanced understanding of the current landscape of Agentic AI research, supporting both theoretical grounding and real-world applicability while maintaining the analytical rigor required for this review.\\nLimitations While this review provides a comprehensive synthesis of Agentic AI research, several limitations must be acknowledged. First, the inherent temporal and scope dynamics of the field, particularly within the rapidly evolving neural paradigm, present a challenge; although our search extended to early 2025, some very recent developments may not be captured, a risk mitigated but not fully eliminated by the inclusion of pre-prints. Furthermore, our methodological approach required a contextual reference expansion through the supplemental inclusion of 12 seminal symbolic papers to ensure a robust theoretical framing of the classical lineage. We emphasize that these papers, analyzed separately from contemporary research, were used strictly for contextual and historical background and represent a deviation from a purely systematic retrieval process.\\nAdditional constraints arose from the nature of the subject matter itself. Transparency constraints were encountered as many state-of-the-art neural agentic systems operate as proprietary solutions with limited public documentation, meaning architectural details and performance metrics were sometimes incomplete or inferred from secondary sources. Methodological heterogeneity across the reviewed studies, with their varied evaluation metrics, also limited our ability to perform direct cross-study benchmarking, particularly between paradigms that employ fundamentally different performance measures. Finally, despite implementing rigorous classification criteria, the paradigm classification challenge of assigning hybrid or transitional architectures to a single paradigm may, in some cases, involve necessary simplification.\\nThese limitations collectively highlight the challenges of conducting systematic reviews in a nascent and fast-paced field with multiple co-existing paradigms. Our two-phase approach‚Äîa systematic review of contemporary research supplemented by a narrative inclusion of foundational symbolic context‚Äîwas designed to balance methodological rigor with comprehensiveness while respecting the fundamental distinctions between these architectural paradigms.',\n",
       "    'subsections': [{'title': '3.1 Review Design',\n",
       "      'paragraphs': 'This study adopts the PRISMA 2020 framework (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) [ 82 , 83 ] , guiding all stages from search strategy to synthesis. The methodology is designed to capture and distinguish between the symbolic/classical and neural/generative lineages of agentic AI research across computer science, cognitive psychology, robotics, and ethics.\\nObjectives: This systematic review aims to provide a comprehensive analysis of agentic AI systems through the following specific research objectives:\\nTo identify, classify, and synthesize literature based on the dual architectural paradigms (Symbolic vs. Neural) of Agentic AI.\\nTo examine the evolution of capabilities, applications, and performance metrics within and across each paradigm.\\nTo analyze governance frameworks and ethical challenges, contextualizing them within their respective architectural paradigms.\\nTo highlight paradigm-specific research gaps and propose informed future directions based on the synthesized evidence.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Data Sources and Search Strategy',\n",
       "      'paragraphs': 'A multi-database search strategy was employed to identify literature across both historical symbolic and modern neural agentic AI research. Sources included: IEEE Xplore, ACM Digital Library, arXiv, SpringerLink, ScienceDirect, and Google Scholar.\\nThe search strategy employed a structured set of keyword clusters designed to comprehensively capture the core concepts associated with both architectural paradigms. To represent the Symbolic/Classical lineage, targeted terms included foundational concepts such as \"Cognitive architectures,\" \"BDI agent,\" \"SOAR,\" \"POMDP,\" \"symbolic planning,\" and \"multi-agent systems\" (in its traditional sense). Conversely, the Neural/Generative paradigm was captured through terms reflecting its contemporary emergence, such as \"LLM agent,\" \"AI orchestration,\" \"prompt chaining,\" \"tool-augmented LLM,\" \"multi-agent conversation,\" and specific framework names including \"AutoGen\" and \"LangChain.\" Finally, a set of General terms‚Äî\"Agentic AI,\" \"autonomous agent,\" and \"goal-directed AI\"‚Äîwas used to ensure broad coverage and to capture literature that might bridge or transcend the paradigmatic divide.\\nBoolean operators were structured to optimize breadth and relevance (e.g., (\"autonomous agent\" OR \"agentic AI\") AND (\"large language model\" OR \"orchestration\" OR \"cognitive architecture\")).\\nThe search scope was interdisciplinary, targeting relevant fields from computer science to ethics. To capture the most current advancements in the rapidly evolving neural paradigm, the search included pre-print servers like arXiv, with these records being manually assessed for quality and relevance.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 Inclusion and Exclusion Criteria',\n",
       "      'paragraphs': 'To ensure the review‚Äôs methodological integrity and thematic relevance, predefined inclusion and exclusion parameters were applied during the screening process. These criteria were designed to capture high-quality literature from both paradigms of agentic AI.\\nThe literature search employed the following inclusion criteria to identify publications that contribute directly to the core themes of agentic AI architectures and applications. Specifically, we included peer-reviewed journal articles, conference proceedings, and formally published technical reports from recognized institutions. To capture the most recent advancements in the rapidly evolving neural paradigm, we also incorporated high-impact pre-prints from arXiv, which were manually screened for methodological rigor and citation impact, with a focus on those presenting novel architectures or frameworks. The scope of included work encompassed studies involving the design, implementation, or evaluation of autonomous agents, spanning both classical symbolic systems and modern LLM-orchestrated frameworks. All selected publications were required to be in English and published within the temporal window of January 2018 to March 2025.\\nTo ensure a focused and methodologically rigorous review, studies were excluded according to the following criteria. Non-English language publications were omitted. We also excluded non-peer-reviewed or informal sources such as opinion pieces, editorials, blog posts, and unverified online content. Furthermore, studies focused exclusively on generative AI (e.g., for image generation or text completion) without incorporating agentic features like goal-directedness, tool use, or multi-step autonomy were deemed out of scope. Finally, duplicate records retrieved from multiple databases were identified and removed to prevent redundancy in the analysis.\\nThese criteria ensured the retention of conceptually aligned and methodologically sound studies from both paradigms, preserving the review‚Äôs comprehensive scope. A summary is provided in Table 4 .',\n",
       "      'subsections': [{'title': 'Inclusion Criteria',\n",
       "        'paragraphs': 'The literature search employed the following inclusion criteria to identify publications that contribute directly to the core themes of agentic AI architectures and applications. Specifically, we included peer-reviewed journal articles, conference proceedings, and formally published technical reports from recognized institutions. To capture the most recent advancements in the rapidly evolving neural paradigm, we also incorporated high-impact pre-prints from arXiv, which were manually screened for methodological rigor and citation impact, with a focus on those presenting novel architectures or frameworks. The scope of included work encompassed studies involving the design, implementation, or evaluation of autonomous agents, spanning both classical symbolic systems and modern LLM-orchestrated frameworks. All selected publications were required to be in English and published within the temporal window of January 2018 to March 2025.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Exclusion Criteria',\n",
       "        'paragraphs': 'To ensure a focused and methodologically rigorous review, studies were excluded according to the following criteria. Non-English language publications were omitted. We also excluded non-peer-reviewed or informal sources such as opinion pieces, editorials, blog posts, and unverified online content. Furthermore, studies focused exclusively on generative AI (e.g., for image generation or text completion) without incorporating agentic features like goal-directedness, tool use, or multi-step autonomy were deemed out of scope. Finally, duplicate records retrieved from multiple databases were identified and removed to prevent redundancy in the analysis.\\nThese criteria ensured the retention of conceptually aligned and methodologically sound studies from both paradigms, preserving the review‚Äôs comprehensive scope. A summary is provided in Table 4 .',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.4 Screening and Selection Process',\n",
       "      'paragraphs': 'The screening protocol adhered to the PRISMA 2020 guidelines to ensure methodological transparency and reproducibility. Records were compiled from selected databases, yielding an initial pool of 165 items (157 from databases, 8 from supplemental sources).\\nFollowing deduplication, 120 unique records remained. Title and abstract screening excluded 42 studies due to irrelevance or insufficient focus on agentic AI. Full-text assessment confirmed 78 articles met all inclusion criteria.\\nIn alignment with PRISMA‚Äôs guidance for systematic reviews that require foundational context, a supplemental phase was conducted [ 82 ] . During thematic synthesis, 12 seminal theoretical papers from the symbolic paradigm (e.g., foundational works on MDPs by [ 39 ] and cognitive architectures by [ 84 ] ) were incorporated. These papers were essential for providing complete historical context for the taxonomic framework and understanding the symbolic lineage, though they were analyzed separately from contemporary neural paradigm research. This resulted in a final corpus of 90 publications for contextual and theoretical grounding, with 78 studies forming the core for analysis of contemporary trends.\\nThe process is illustrated in Figure 7 , which clearly distinguishes the primary systematic search from the supplemental inclusion of foundational context.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.5 Data Analysis',\n",
       "      'paragraphs': 'The 78 studies forming the core of the review underwent thematic synthesis following the methodology described by Thomas and Harden [ 85 ] , with analysis specifically structured around the dual-paradigm framework.\\nOur analysis employed a multi-faceted methodological approach to systematically investigate the body of research. The initial phase involved paradigm classification , whereby each study was categorized according to its primary architectural paradigm‚Äîeither Symbolic/Classical or Neural/Generative‚Äîbased on the core operational mechanisms defined in our conceptual framework. Following this classification, we conducted a detailed framework mapping within each paradigm to group studies by their specific architectural approaches, including orchestration models (e.g., AutoGen, CrewAI), memory structures, and learning mechanisms. Building on this organized foundation, a cross-paradigm comparison was performed to identify fundamental differences in implementation, performance, and limitations between the two overarching paradigms. In parallel, we performed domain clustering to group applications by sector‚Äîsuch as healthcare, finance, robotics, and scientific discovery‚Äîwhich enabled the identification of performance patterns and deployment strategies both within and across paradigms. Finally, an ethical coding procedure was applied, using a structured lexicon to tag recurring themes related to governance, safety, transparency, and bias, with particular attention paid to how these ethical challenges manifest differently within each paradigm.\\nQualitative coding was supported by tools such as NVivo [ 86 ] , which enabled hierarchical theme identification and cross-paradigm analysis. Quantitative results were tabulated and compared within and across domains and paradigms to synthesize technical and operational insights.\\nThis paradigm-informed approach ensured a nuanced understanding of the current landscape of Agentic AI research, supporting both theoretical grounding and real-world applicability while maintaining the analytical rigor required for this review.',\n",
       "      'subsections': [{'title': 'Key Analytical Techniques:',\n",
       "        'paragraphs': 'Our analysis employed a multi-faceted methodological approach to systematically investigate the body of research. The initial phase involved paradigm classification , whereby each study was categorized according to its primary architectural paradigm‚Äîeither Symbolic/Classical or Neural/Generative‚Äîbased on the core operational mechanisms defined in our conceptual framework. Following this classification, we conducted a detailed framework mapping within each paradigm to group studies by their specific architectural approaches, including orchestration models (e.g., AutoGen, CrewAI), memory structures, and learning mechanisms. Building on this organized foundation, a cross-paradigm comparison was performed to identify fundamental differences in implementation, performance, and limitations between the two overarching paradigms. In parallel, we performed domain clustering to group applications by sector‚Äîsuch as healthcare, finance, robotics, and scientific discovery‚Äîwhich enabled the identification of performance patterns and deployment strategies both within and across paradigms. Finally, an ethical coding procedure was applied, using a structured lexicon to tag recurring themes related to governance, safety, transparency, and bias, with particular attention paid to how these ethical challenges manifest differently within each paradigm.\\nQualitative coding was supported by tools such as NVivo [ 86 ] , which enabled hierarchical theme identification and cross-paradigm analysis. Quantitative results were tabulated and compared within and across domains and paradigms to synthesize technical and operational insights.\\nThis paradigm-informed approach ensured a nuanced understanding of the current landscape of Agentic AI research, supporting both theoretical grounding and real-world applicability while maintaining the analytical rigor required for this review.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.6 Limitations',\n",
       "      'paragraphs': 'Limitations While this review provides a comprehensive synthesis of Agentic AI research, several limitations must be acknowledged. First, the inherent temporal and scope dynamics of the field, particularly within the rapidly evolving neural paradigm, present a challenge; although our search extended to early 2025, some very recent developments may not be captured, a risk mitigated but not fully eliminated by the inclusion of pre-prints. Furthermore, our methodological approach required a contextual reference expansion through the supplemental inclusion of 12 seminal symbolic papers to ensure a robust theoretical framing of the classical lineage. We emphasize that these papers, analyzed separately from contemporary research, were used strictly for contextual and historical background and represent a deviation from a purely systematic retrieval process.\\nAdditional constraints arose from the nature of the subject matter itself. Transparency constraints were encountered as many state-of-the-art neural agentic systems operate as proprietary solutions with limited public documentation, meaning architectural details and performance metrics were sometimes incomplete or inferred from secondary sources. Methodological heterogeneity across the reviewed studies, with their varied evaluation metrics, also limited our ability to perform direct cross-study benchmarking, particularly between paradigms that employ fundamentally different performance measures. Finally, despite implementing rigorous classification criteria, the paradigm classification challenge of assigning hybrid or transitional architectures to a single paradigm may, in some cases, involve necessary simplification.\\nThese limitations collectively highlight the challenges of conducting systematic reviews in a nascent and fast-paced field with multiple co-existing paradigms. Our two-phase approach‚Äîa systematic review of contemporary research supplemented by a narrative inclusion of foundational symbolic context‚Äîwas designed to balance methodological rigor with comprehensiveness while respecting the fundamental distinctions between these architectural paradigms.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Literature Review: A Dual-Paradigm Analysis',\n",
       "    'paragraphs': 'The rapid expansion of Agentic AI has produced a diverse yet fragmented body of research. This section synthesizes the extant literature through the lens of the dual-paradigm framework introduced in Figure 2 , analyzing how contributions are distributed across the symbolic/classical and neural/generative lineages. We organize and analyze the most influential contributions across foundational studies, architectural frameworks, and domain-specific applications, focusing on their operational mechanisms to clearly delineate the paradigm shift.\\nThe theoretical bedrock of Agentic AI is found in two distinct lineages, each with its own foundational breakthroughs. Landmark studies have shaped the conceptual and architectural foundations of both paradigms, spanning strategic reasoning, cognitive models, and alignment.\\nThese studies collectively mark the progression from explicit, algorithmic deliberation to emergent, stochastic intelligence. They serve as reference points for the fundamental differences in how adaptability, coordination, and strategic reasoning are implemented in each paradigm, illustrating the conceptual divide captured by our framework.\\nThe advent of large language models (LLMs) has solidified the neural/generative paradigm, which operates on principles fundamentally incompatible with its symbolic predecessor. Modern agentic frameworks leverage LLMs as generative engines within software pipelines, explicitly departing from classical cognitive loops. Their core innovation lies in dynamic context management, prompt engineering, and tool composition.\\nThis analysis underscores that these frameworks form the backbone of the neural paradigm, designed for practical task completion through orchestration, not for simulating internal cognitive processes. Mapping them to PPAR or BDI obscures their true innovative mechanics, which are defined by prompt-driven stochasticity, not algorithmic symbol manipulation.\\nAgentic AI frameworks are being deployed across sectors where autonomy and adaptability are essential. The choice of paradigm is critically influenced by domain-specific constraints‚Äîethical, regulatory, or epistemic. The following implementations exemplify how each paradigm is applied.\\nThe application of Agentic AI reveals a distinct paradigm split influenced by the core requirements of each sector. In healthcare, where safety and compliance are paramount , applications diverge clearly along architectural lines. Symbolic systems, such as rule-based clinical decision support tools, are predominantly employed for predictable and auditable tasks. In contrast, the flexibility of neural paradigms is leveraged for tasks like generating structured medical reports [ 66 ] and powering on-premise edge agents [ 87 ] ; however, these neural frameworks are often contained within deterministic tool-chaining pipelines to ensure the reliability required in clinical settings.\\nThis pattern of complementary paradigm use is also evident in finance, a domain demanding high accuracy and auditability . Here, neural frameworks dominate tasks involving complex data synthesis and analysis. For instance, CrewAI‚Äôs role-based workflow is applied to market analysis [ 43 ] as it provides a clear, auditable trail of agent actions. Similarly, LlamaIndex-powered models for financial sentiment [ 79 ] demonstrate how neural systems use Retrieval-Augmented Generation (RAG) to ground their stochastic outputs in verified data, thereby reducing hallucination. Despite this, symbolic systems maintain a critical role in high-frequency trading and core regulatory logic where absolute determinism is non-negotiable.\\nFinally, in scientific research, which requires profound epistemic rigor , the choice of paradigm is dictated by the nature of the intellectual task. The deployment of AutoGen to coordinate multi-agent conversations for economic research [ 69 ] exemplifies the neural paradigm‚Äôs strength in simulating collaborative, exploratory discovery and critique. This stands in direct contrast to the role of symbolic systems, which remain the bedrock for theorem proving and logical inference, highlighting a fundamental architectural choice between exploratory generation and deductive reasoning.\\nThese implementations demonstrate that the paradigm choice is not merely technical but is decisively shaped by domain-specific needs, validating the need for a clear taxonomic framework to classify and select appropriate architectures.\\nThe evolution of Agentic AI is increasingly characterized by a deliberate synthesis of architectural paradigms, moving beyond isolated approaches toward integrated systems that combine strengths while mitigating inherent limitations. This shift toward hybrid architectures represents the field‚Äôs maturation as it seeks to balance adaptability with reliability. Importantly, these trends are not broad truisms about any generation of AI, but rather specific architectural responses to challenges uniquely faced by large-scale, agentic systems.\\nThe most significant emerging trend is neuro-symbolic integration , which aims to formally bridge the reliable, deterministic reasoning of symbolic systems with the adaptive, generative capabilities of neural networks [ 88 ] . This effort transcends the well-documented limitations of both paradigms, potentially establishing a new hybrid category that leverages their complementary strengths.\\nA second and particularly distinctive direction is the exploration of decentralized agent networks . Here, blockchain-based coordination mechanisms are applied to multi-agent AI systems to provide verifiable governance, transparent decision-making, and resilient autonomy [ 89 ] . Unlike conventional centralized orchestrators, distributed consensus frameworks offer robustness against single points of failure, while also opening the possibility of economic coordination between heterogeneous agents through tokenized incentives. This line of research directly addresses questions of trust, accountability, and cooperative alignment‚Äîissues that become acute when scaling agentic AI across organizations or societal infrastructures.\\nComplementing these architectural innovations, advances in lifelong learning frameworks address a critical limitation of current LLM-based agents‚Äîtheir largely stateless nature [ 90 ] . By enabling continuous adaptation and durable knowledge retention, this trend effectively injects persistent memory, a concept foundational to symbolic AI, into neural architectures. This supports more context-aware, long-term, and resilient operation in dynamic environments.\\nCollectively, these emerging trends signal the field‚Äôs progression from debating paradigm superiority to architecting sophisticated hybrids. Far from generic insights, they constitute targeted responses to enduring limitations in current Agentic AI systems: brittle reasoning, centralized governance bottlenecks, and memory deficiencies. The resulting synthesis offers the most promising path toward developing Agentic AI systems that are simultaneously adaptable and reliable, creative and verifiable‚Äîcapable of operating effectively in the complex, dynamic environments that characterize real-world applications.\\nA critical yet often underexplored aspect of Multi-Agent Systems (MAS) is the fundamental distinction in their coordination mechanisms. A deeper examination reveals that these strategies are a primary differentiator between the two paradigms, reflecting their core architectural principles: explicit algorithms in the symbolic paradigm versus emergent, stochastically-guided behavior in the neural paradigm.\\nWithin the Symbolic Paradigm , coordination is achieved through pre-defined, algorithmic protocols rooted in decades of distributed AI research. These protocols are engineered to ensure predictable, verifiable, and fault-tolerant interactions, making them indispensable for critical systems where correctness is paramount. A quintessential example is the Contract Net Protocol (CNP) [ 91 ] , a classic negotiation framework where a manager agent announces a task through a ‚Äúcall for proposals.‚Äù Other agents then evaluate their capabilities and submit bids, leading the manager to award the contract to the most suitable agent. This process, analogous to an auction, is extensively applied in domains like manufacturing and logistics scheduling. Another foundational strategy is the Blackboard System [ 92 ] , where a shared memory space acts as a central coordination point. Specialist agents, akin to experts surrounding a physical blackboard, monitor this space for relevant data and contribute their expertise incrementally to build towards a solution. This approach is highly effective for complex, unstructured problems like medical diagnosis or signal interpretation. Furthermore, Market-Based Approaches facilitate coordination through a virtual economy where agents buy and sell services or resources, providing a decentralized method for resource allocation in networked systems.\\nIn direct opposition, coordination within the Neural Paradigm is not typically governed by hard-coded protocols. Instead, it emerges as a property of structured conversation and prompt-driven orchestration [ 93 , 94 , 95 ] . Here, a central orchestrator (often an LLM itself) or the agents themselves leverage their generative capabilities to dynamically assign roles, manage dialogue, and synthesize results. This can manifest in several distinct patterns. Conversation-Based Coordination [ 96 , 97 , 98 ] , exemplified by frameworks like AutoGen, achieves collaboration through structured conversational loops where agents with defined roles interact within a group chat, with the LLM‚Äôs context window managing the interaction state. A more explicit variant is the Role-Based Workflow [ 99 ] (e.g., CrewAI), where a higher-level orchestrator assigns tasks based on pre-defined roles and goals, though the routing decisions are still driven by LLM-based reasoning rather than deterministic algorithms. Lastly, Dynamic Context Management [ 100 , 81 ] (e.g., LangGraph) implements coordination through state machines that control information flow between nodes; the graph structure defines possible paths, but the specific execution is determined stochastically by the LLM‚Äôs output at each step.\\nThe fundamental dichotomy between these coordination strategies is summarized in Table 5 , which highlights the core operational differences.\\nThis analysis confirms that the paradigm shift extends to the very fabric of multi-agent coordination. The symbolic paradigm offers verifiable reliability through rigorously engineered protocols, while the neural paradigm offers adaptable emergence through learned conversation patterns. This critical distinction is essential for understanding the capabilities, risks, and appropriate applications of modern MAS, thereby further validating the necessity of the dual-paradigm framework presented in this survey.\\nThe evaluation of Agentic AI systems presents a fundamental challenge that distinguishes it from the assessment of traditional AI models. As the reviewer rightly notes, simple metrics like accuracy are wholly insufficient. Measuring ‚Äúagency‚Äù requires quantifying a system‚Äôs capacity for sustained, goal-directed behavior in dynamic environments, necessitating a multi-dimensional evaluation framework that accounts for paradigm-specific mechanisms of action.\\nThe core challenge lies in the fact that agency is not a monolithic property but a spectrum encompassing autonomy , task success , efficiency , and robustness . Consequently, evaluation must be tailored to the architectural paradigm.\\nIn the Symbolic Paradigm , evaluation has historically focused on verifiability . Key metrics include Goal Completion Fidelity , which measures the percentage of pre-defined sub-goals correctly achieved in a plan, and Plan Optimality , which compares the cost (e.g., time, steps) of an agent‚Äôs generated plan against a known optimal solution. Furthermore, assessment involves verifying Logical Soundness through formal methods to ensure rule sets cannot derive contradictory or unsafe actions, and rigorously testing Edge Case Handling against rare but critical scenarios either explicitly encoded in or missing from the agent‚Äôs knowledge base.\\nConversely, in the Neural Paradigm , evaluation is inherently more complex due to inherent stochasticity. While benchmarks like AgentBench [ 101 ] and GAIA [ Mialon2023GAIA ] represent a shift towards holistic assessment, they have limitations. Metrics must be designed to capture emergent capabilities and failures. This includes evaluating Long-Horizon Task Success on complex, multi-step tasks (e.g., ‚Äúresearch a topic and write a report with citations‚Äù), often measured by final outcome quality as judged by humans or a powerful LLM ‚Äújudge.‚Äù Other critical dimensions are Context Window and Memory Management , which assess an agent‚Äôs ability to utilize information across extended interactions; Tool Use Proficiency , encompassing tool selection accuracy, call sequence efficiency, and error recovery; Robustness to Prompts , testing consistency across instruction rephrasings and resilience to injection attacks; and practical Cost and Latency metrics, measuring computational expense (e.g., total tokens, API calls) and time-to-completion, which are crucial for real-world deployment.\\nA comprehensive evaluation framework for Agentic AI must therefore integrate these dimensions. It is not enough for an agent to eventually succeed at a task; it must do so efficiently, reliably, and in a manner that is transparent and auditable where required. This typically involves a synergistic combination of automated metrics (e.g., success rate, number of steps), human evaluation for qualitative judgment of output coherence and usefulness, and adversarial testing (e.g., ‚Äúred teaming‚Äù) to probe for specific failure modes like hallucination or goal divergence.\\nThis paradigm-aware approach to evaluation‚Äîwhere symbolic systems are judged on verifiability and neural systems on robust adaptability‚Äîis essential for the responsible development and deployment of autonomous agents. It moves the field beyond simple benchmarks towards a more nuanced understanding of what it means for an AI system to be truly ‚Äúagentic.‚Äù\\nSynthesizing the literature through our dual-paradigm framework reveals several fundamental distinctions and clear trajectories for the field of Agentic AI. The analysis demonstrates that paradigm divergence is fundamental ; rather than representing evolutionary stages, the symbolic and neural lineages constitute parallel development paths characterized by fundamentally different operational mechanics‚Äîalgorithmic reasoning versus stochastic orchestration. This architectural divergence emerges as the most critical factor in determining any agentic system‚Äôs inherent capabilities and limitations.\\nThis division naturally leads to the principle that mechanism determines application . The choice between paradigms is far from arbitrary but is instead dictated by domain requirements. Symbolic architectures demonstrate particular excellence in domains demanding absolute reliability, verifiability, and safety, such as core regulatory systems and safety-critical controls. Conversely, neural architectures thrive in environments requiring adaptability, sophisticated pattern recognition, and operation on unstructured data, exemplified by creative research applications and complex customer interactions.\\nLooking toward the future, the evidence indicates that the frontier lies in hybridization . Emerging research trends do not suggest the ultimate victory of one paradigm over the other but rather point toward their strategic integration. The next significant advancement will likely emerge from hybrid architectures that embed symbolic reasoning modules within neural orchestration frameworks, effectively mitigating the weaknesses of pure neural approaches‚Äîsuch as hallucination and lack of verifiability‚Äîwhile preserving their adaptive strengths.\\nCollectively, these insights, structured by the dual-paradigm framework, provide a cohesive and accurate narrative for understanding the field‚Äôs present state and future direction. This approach moves beyond a simple catalog of technologies to establish a coherent theory of architectural design in Agentic AI, offering researchers and practitioners a principled foundation for system development and evaluation.',\n",
       "    'subsections': [{'title': '4.1 Foundational Studies: The Roots of Two Lineages',\n",
       "      'paragraphs': 'The theoretical bedrock of Agentic AI is found in two distinct lineages, each with its own foundational breakthroughs. Landmark studies have shaped the conceptual and architectural foundations of both paradigms, spanning strategic reasoning, cognitive models, and alignment.\\nThese studies collectively mark the progression from explicit, algorithmic deliberation to emergent, stochastic intelligence. They serve as reference points for the fundamental differences in how adaptability, coordination, and strategic reasoning are implemented in each paradigm, illustrating the conceptual divide captured by our framework.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Architectural Paradigms: A Mechanistic Comparison',\n",
       "      'paragraphs': 'The advent of large language models (LLMs) has solidified the neural/generative paradigm, which operates on principles fundamentally incompatible with its symbolic predecessor. Modern agentic frameworks leverage LLMs as generative engines within software pipelines, explicitly departing from classical cognitive loops. Their core innovation lies in dynamic context management, prompt engineering, and tool composition.\\nThis analysis underscores that these frameworks form the backbone of the neural paradigm, designed for practical task completion through orchestration, not for simulating internal cognitive processes. Mapping them to PPAR or BDI obscures their true innovative mechanics, which are defined by prompt-driven stochasticity, not algorithmic symbol manipulation.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Domain-Specific Implementations: A Paradigm-Driven Analysis',\n",
       "      'paragraphs': 'Agentic AI frameworks are being deployed across sectors where autonomy and adaptability are essential. The choice of paradigm is critically influenced by domain-specific constraints‚Äîethical, regulatory, or epistemic. The following implementations exemplify how each paradigm is applied.\\nThe application of Agentic AI reveals a distinct paradigm split influenced by the core requirements of each sector. In healthcare, where safety and compliance are paramount , applications diverge clearly along architectural lines. Symbolic systems, such as rule-based clinical decision support tools, are predominantly employed for predictable and auditable tasks. In contrast, the flexibility of neural paradigms is leveraged for tasks like generating structured medical reports [ 66 ] and powering on-premise edge agents [ 87 ] ; however, these neural frameworks are often contained within deterministic tool-chaining pipelines to ensure the reliability required in clinical settings.\\nThis pattern of complementary paradigm use is also evident in finance, a domain demanding high accuracy and auditability . Here, neural frameworks dominate tasks involving complex data synthesis and analysis. For instance, CrewAI‚Äôs role-based workflow is applied to market analysis [ 43 ] as it provides a clear, auditable trail of agent actions. Similarly, LlamaIndex-powered models for financial sentiment [ 79 ] demonstrate how neural systems use Retrieval-Augmented Generation (RAG) to ground their stochastic outputs in verified data, thereby reducing hallucination. Despite this, symbolic systems maintain a critical role in high-frequency trading and core regulatory logic where absolute determinism is non-negotiable.\\nFinally, in scientific research, which requires profound epistemic rigor , the choice of paradigm is dictated by the nature of the intellectual task. The deployment of AutoGen to coordinate multi-agent conversations for economic research [ 69 ] exemplifies the neural paradigm‚Äôs strength in simulating collaborative, exploratory discovery and critique. This stands in direct contrast to the role of symbolic systems, which remain the bedrock for theorem proving and logical inference, highlighting a fundamental architectural choice between exploratory generation and deductive reasoning.\\nThese implementations demonstrate that the paradigm choice is not merely technical but is decisively shaped by domain-specific needs, validating the need for a clear taxonomic framework to classify and select appropriate architectures.',\n",
       "      'subsections': [{'title': 'Domain-Specific Applications and Paradigm Choices',\n",
       "        'paragraphs': 'The application of Agentic AI reveals a distinct paradigm split influenced by the core requirements of each sector. In healthcare, where safety and compliance are paramount , applications diverge clearly along architectural lines. Symbolic systems, such as rule-based clinical decision support tools, are predominantly employed for predictable and auditable tasks. In contrast, the flexibility of neural paradigms is leveraged for tasks like generating structured medical reports [ 66 ] and powering on-premise edge agents [ 87 ] ; however, these neural frameworks are often contained within deterministic tool-chaining pipelines to ensure the reliability required in clinical settings.\\nThis pattern of complementary paradigm use is also evident in finance, a domain demanding high accuracy and auditability . Here, neural frameworks dominate tasks involving complex data synthesis and analysis. For instance, CrewAI‚Äôs role-based workflow is applied to market analysis [ 43 ] as it provides a clear, auditable trail of agent actions. Similarly, LlamaIndex-powered models for financial sentiment [ 79 ] demonstrate how neural systems use Retrieval-Augmented Generation (RAG) to ground their stochastic outputs in verified data, thereby reducing hallucination. Despite this, symbolic systems maintain a critical role in high-frequency trading and core regulatory logic where absolute determinism is non-negotiable.\\nFinally, in scientific research, which requires profound epistemic rigor , the choice of paradigm is dictated by the nature of the intellectual task. The deployment of AutoGen to coordinate multi-agent conversations for economic research [ 69 ] exemplifies the neural paradigm‚Äôs strength in simulating collaborative, exploratory discovery and critique. This stands in direct contrast to the role of symbolic systems, which remain the bedrock for theorem proving and logical inference, highlighting a fundamental architectural choice between exploratory generation and deductive reasoning.\\nThese implementations demonstrate that the paradigm choice is not merely technical but is decisively shaped by domain-specific needs, validating the need for a clear taxonomic framework to classify and select appropriate architectures.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.4 Emerging Trends: Toward Hybrid Architectures',\n",
       "      'paragraphs': 'The evolution of Agentic AI is increasingly characterized by a deliberate synthesis of architectural paradigms, moving beyond isolated approaches toward integrated systems that combine strengths while mitigating inherent limitations. This shift toward hybrid architectures represents the field‚Äôs maturation as it seeks to balance adaptability with reliability. Importantly, these trends are not broad truisms about any generation of AI, but rather specific architectural responses to challenges uniquely faced by large-scale, agentic systems.\\nThe most significant emerging trend is neuro-symbolic integration , which aims to formally bridge the reliable, deterministic reasoning of symbolic systems with the adaptive, generative capabilities of neural networks [ 88 ] . This effort transcends the well-documented limitations of both paradigms, potentially establishing a new hybrid category that leverages their complementary strengths.\\nA second and particularly distinctive direction is the exploration of decentralized agent networks . Here, blockchain-based coordination mechanisms are applied to multi-agent AI systems to provide verifiable governance, transparent decision-making, and resilient autonomy [ 89 ] . Unlike conventional centralized orchestrators, distributed consensus frameworks offer robustness against single points of failure, while also opening the possibility of economic coordination between heterogeneous agents through tokenized incentives. This line of research directly addresses questions of trust, accountability, and cooperative alignment‚Äîissues that become acute when scaling agentic AI across organizations or societal infrastructures.\\nComplementing these architectural innovations, advances in lifelong learning frameworks address a critical limitation of current LLM-based agents‚Äîtheir largely stateless nature [ 90 ] . By enabling continuous adaptation and durable knowledge retention, this trend effectively injects persistent memory, a concept foundational to symbolic AI, into neural architectures. This supports more context-aware, long-term, and resilient operation in dynamic environments.\\nCollectively, these emerging trends signal the field‚Äôs progression from debating paradigm superiority to architecting sophisticated hybrids. Far from generic insights, they constitute targeted responses to enduring limitations in current Agentic AI systems: brittle reasoning, centralized governance bottlenecks, and memory deficiencies. The resulting synthesis offers the most promising path toward developing Agentic AI systems that are simultaneously adaptable and reliable, creative and verifiable‚Äîcapable of operating effectively in the complex, dynamic environments that characterize real-world applications.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.5 Coordination Protocols: From Algorithmic Contracts to Emergent Conversation',\n",
       "      'paragraphs': 'A critical yet often underexplored aspect of Multi-Agent Systems (MAS) is the fundamental distinction in their coordination mechanisms. A deeper examination reveals that these strategies are a primary differentiator between the two paradigms, reflecting their core architectural principles: explicit algorithms in the symbolic paradigm versus emergent, stochastically-guided behavior in the neural paradigm.\\nWithin the Symbolic Paradigm , coordination is achieved through pre-defined, algorithmic protocols rooted in decades of distributed AI research. These protocols are engineered to ensure predictable, verifiable, and fault-tolerant interactions, making them indispensable for critical systems where correctness is paramount. A quintessential example is the Contract Net Protocol (CNP) [ 91 ] , a classic negotiation framework where a manager agent announces a task through a ‚Äúcall for proposals.‚Äù Other agents then evaluate their capabilities and submit bids, leading the manager to award the contract to the most suitable agent. This process, analogous to an auction, is extensively applied in domains like manufacturing and logistics scheduling. Another foundational strategy is the Blackboard System [ 92 ] , where a shared memory space acts as a central coordination point. Specialist agents, akin to experts surrounding a physical blackboard, monitor this space for relevant data and contribute their expertise incrementally to build towards a solution. This approach is highly effective for complex, unstructured problems like medical diagnosis or signal interpretation. Furthermore, Market-Based Approaches facilitate coordination through a virtual economy where agents buy and sell services or resources, providing a decentralized method for resource allocation in networked systems.\\nIn direct opposition, coordination within the Neural Paradigm is not typically governed by hard-coded protocols. Instead, it emerges as a property of structured conversation and prompt-driven orchestration [ 93 , 94 , 95 ] . Here, a central orchestrator (often an LLM itself) or the agents themselves leverage their generative capabilities to dynamically assign roles, manage dialogue, and synthesize results. This can manifest in several distinct patterns. Conversation-Based Coordination [ 96 , 97 , 98 ] , exemplified by frameworks like AutoGen, achieves collaboration through structured conversational loops where agents with defined roles interact within a group chat, with the LLM‚Äôs context window managing the interaction state. A more explicit variant is the Role-Based Workflow [ 99 ] (e.g., CrewAI), where a higher-level orchestrator assigns tasks based on pre-defined roles and goals, though the routing decisions are still driven by LLM-based reasoning rather than deterministic algorithms. Lastly, Dynamic Context Management [ 100 , 81 ] (e.g., LangGraph) implements coordination through state machines that control information flow between nodes; the graph structure defines possible paths, but the specific execution is determined stochastically by the LLM‚Äôs output at each step.\\nThe fundamental dichotomy between these coordination strategies is summarized in Table 5 , which highlights the core operational differences.\\nThis analysis confirms that the paradigm shift extends to the very fabric of multi-agent coordination. The symbolic paradigm offers verifiable reliability through rigorously engineered protocols, while the neural paradigm offers adaptable emergence through learned conversation patterns. This critical distinction is essential for understanding the capabilities, risks, and appropriate applications of modern MAS, thereby further validating the necessity of the dual-paradigm framework presented in this survey.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.6 Evaluating Agency: Beyond Accuracy',\n",
       "      'paragraphs': 'The evaluation of Agentic AI systems presents a fundamental challenge that distinguishes it from the assessment of traditional AI models. As the reviewer rightly notes, simple metrics like accuracy are wholly insufficient. Measuring ‚Äúagency‚Äù requires quantifying a system‚Äôs capacity for sustained, goal-directed behavior in dynamic environments, necessitating a multi-dimensional evaluation framework that accounts for paradigm-specific mechanisms of action.\\nThe core challenge lies in the fact that agency is not a monolithic property but a spectrum encompassing autonomy , task success , efficiency , and robustness . Consequently, evaluation must be tailored to the architectural paradigm.\\nIn the Symbolic Paradigm , evaluation has historically focused on verifiability . Key metrics include Goal Completion Fidelity , which measures the percentage of pre-defined sub-goals correctly achieved in a plan, and Plan Optimality , which compares the cost (e.g., time, steps) of an agent‚Äôs generated plan against a known optimal solution. Furthermore, assessment involves verifying Logical Soundness through formal methods to ensure rule sets cannot derive contradictory or unsafe actions, and rigorously testing Edge Case Handling against rare but critical scenarios either explicitly encoded in or missing from the agent‚Äôs knowledge base.\\nConversely, in the Neural Paradigm , evaluation is inherently more complex due to inherent stochasticity. While benchmarks like AgentBench [ 101 ] and GAIA [ Mialon2023GAIA ] represent a shift towards holistic assessment, they have limitations. Metrics must be designed to capture emergent capabilities and failures. This includes evaluating Long-Horizon Task Success on complex, multi-step tasks (e.g., ‚Äúresearch a topic and write a report with citations‚Äù), often measured by final outcome quality as judged by humans or a powerful LLM ‚Äújudge.‚Äù Other critical dimensions are Context Window and Memory Management , which assess an agent‚Äôs ability to utilize information across extended interactions; Tool Use Proficiency , encompassing tool selection accuracy, call sequence efficiency, and error recovery; Robustness to Prompts , testing consistency across instruction rephrasings and resilience to injection attacks; and practical Cost and Latency metrics, measuring computational expense (e.g., total tokens, API calls) and time-to-completion, which are crucial for real-world deployment.\\nA comprehensive evaluation framework for Agentic AI must therefore integrate these dimensions. It is not enough for an agent to eventually succeed at a task; it must do so efficiently, reliably, and in a manner that is transparent and auditable where required. This typically involves a synergistic combination of automated metrics (e.g., success rate, number of steps), human evaluation for qualitative judgment of output coherence and usefulness, and adversarial testing (e.g., ‚Äúred teaming‚Äù) to probe for specific failure modes like hallucination or goal divergence.\\nThis paradigm-aware approach to evaluation‚Äîwhere symbolic systems are judged on verifiability and neural systems on robust adaptability‚Äîis essential for the responsible development and deployment of autonomous agents. It moves the field beyond simple benchmarks towards a more nuanced understanding of what it means for an AI system to be truly ‚Äúagentic.‚Äù',\n",
       "      'subsections': []},\n",
       "     {'title': '4.7 Summary of Insights',\n",
       "      'paragraphs': 'Synthesizing the literature through our dual-paradigm framework reveals several fundamental distinctions and clear trajectories for the field of Agentic AI. The analysis demonstrates that paradigm divergence is fundamental ; rather than representing evolutionary stages, the symbolic and neural lineages constitute parallel development paths characterized by fundamentally different operational mechanics‚Äîalgorithmic reasoning versus stochastic orchestration. This architectural divergence emerges as the most critical factor in determining any agentic system‚Äôs inherent capabilities and limitations.\\nThis division naturally leads to the principle that mechanism determines application . The choice between paradigms is far from arbitrary but is instead dictated by domain requirements. Symbolic architectures demonstrate particular excellence in domains demanding absolute reliability, verifiability, and safety, such as core regulatory systems and safety-critical controls. Conversely, neural architectures thrive in environments requiring adaptability, sophisticated pattern recognition, and operation on unstructured data, exemplified by creative research applications and complex customer interactions.\\nLooking toward the future, the evidence indicates that the frontier lies in hybridization . Emerging research trends do not suggest the ultimate victory of one paradigm over the other but rather point toward their strategic integration. The next significant advancement will likely emerge from hybrid architectures that embed symbolic reasoning modules within neural orchestration frameworks, effectively mitigating the weaknesses of pure neural approaches‚Äîsuch as hallucination and lack of verifiability‚Äîwhile preserving their adaptive strengths.\\nCollectively, these insights, structured by the dual-paradigm framework, provide a cohesive and accurate narrative for understanding the field‚Äôs present state and future direction. This approach moves beyond a simple catalog of technologies to establish a coherent theory of architectural design in Agentic AI, offering researchers and practitioners a principled foundation for system development and evaluation.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Analysis of Domain-Specific Applications',\n",
       "    'paragraphs': 'Agentic AI systems have transitioned from theoretical research to critical production deployments. This section analyzes these deployments through the lens of our dual-paradigm framework, examining how domain-specific constraints‚Äîsuch as safety, regulation, and real-world interaction‚Äîdictate the choice of architectural paradigm and shape implementation priorities. The progression from automation to autonomy is not a function of evolutionary stage but of selecting the appropriate paradigm for the task‚Äôs constraints.\\nTo provide a structured analysis, Table 6 maps key domains against their dominant architectural paradigm, primary constraints, and illustrative implementations, creating a comparative schema based on mechanistic choice rather than chronological progression.\\nThe diversity of these deployments reflects a key insight: the architectural paradigm is a strategic response to domain-specific pressures. For instance, healthcare applications heavily favor symbolic or highly constrained deterministic approaches. This prioritizes safety, accuracy, and auditability‚Äîa necessity in high-stakes, regulated environments‚Äîover the generative flexibility of pure neural systems.\\nConversely, domains like education leverage the neural paradigm for its core strength: generating adaptive, personalized, and context-aware interactions that are difficult to pre-program with symbolic rules.\\nFinance and Legal applications demonstrate a crucial middle ground: they are built on neural orchestration frameworks but are heavily constrained by symbolic mechanisms (e.g., role-based workflows, rigorous retrieval from verified sources) to mitigate the risks of hallucination and ensure compliance. Robotics presents the most explicit hybrid model, pairing symbolic systems for safety-critical low-level control with neural systems for high-level coordination and adaptation.\\nFurthermore, this paradigm-driven analysis reveals critical cross-domain challenges that must be addressed in future research. Chief among these is the need for paradigm-specific governance frameworks . The operationalization of agentic systems requires tailored policy approaches that account for each paradigm‚Äôs distinct risks: governing symbolic systems involves verifying their logical structures, while governing neural systems necessitates auditing training data, prompts, and outputs for stochastic failures‚Äîa challenge further compounded in hybrid architectures.\\nEqually critical are the emerging challenges in security and resilience . As these systems become integrated into critical infrastructure, they represent prime targets for adversarial attacks, though the attack vectors differ fundamentally by paradigm. Symbolic systems face exploitation of logical flaws and rule manipulation, while neural systems remain vulnerable to prompt injection, data poisoning, and other inference-time attacks that exploit their stochastic nature.\\nFinally, the paradigm divide fundamentally shapes human-AI collaboration . Effective interface design must account for these architectural differences: interacting with symbolic systems requires understanding their internal logic and state representations, whereas engaging with neural systems involves carefully steering context and interpreting often opaque, generative outputs‚Äîrequiring distinct approaches to oversight and interpretability.\\nA critical capability that distinguishes agentic AI from passive models is their ability to interact with and manipulate external tools and data sources via Application Programming Interfaces (APIs) [ 109 , 110 , 111 , 112 ] . This functionality is the bridge between an agent‚Äôs internal reasoning and tangible action in the real world. The nature of this integration is, as our framework predicts, paradigm-dependent.\\nIn the Symbolic Paradigm , tool use is typically hard-coded and deterministic. Agents call specific functions with predefined parameters based on explicit logical rules. This is prevalent in safety-critical domains like healthcare, where agents interact with Electronic Health Record (EHR) systems using strict, auditable APIs (e.g., HL7 FHIR standards for reading/writing patient data) or clinical decision support tools with fixed input-output schemas [ 113 , 114 ] .\\nIn the Neural Paradigm , tool use is orchestrated and generative. Frameworks like LangChain and AutoGen use the LLM‚Äôs ability to understand natural language instructions to dynamically select and call appropriate tools from a suite of available options. The LLM [ 115 , 116 ] generates the API call parameters (e.g., formulating a database query, crafting a search query) based on its context, which is then executed by the framework. This allows for immense flexibility but introduces risks of malformed calls or unexpected outputs.\\nTable 7 [ 117 , 118 , 119 ] provides a non-exhaustive overview of the types of real-world tools and APIs that agentic systems are currently being integrated with, categorized by their primary domain and function.\\nThis integration enables agents to move beyond text generation to become truly functional systems. For instance, a neural agent using AutoGen could read an email via the Outlook API, extract key tasks, write code to solve them using a Python tool, and then post the results to a Slack channel‚Äîall within a single orchestrated workflow. Conversely, a symbolic agent in a manufacturing context might reliably call a single, well-defined API to adjust a machine‚Äôs parameters based on its rigid internal state model.\\nIn conclusion, agentic AI is not a monolithic force but a set of distinct architectural paradigms. Its embedding into the fabric of critical systems is a story of domain-driven design, where theoretical capabilities are shaped and constrained by practical, ethical, and operational realities. The choice between symbolic, neural, or hybrid design is the primary engineering decision, making the governance and safety challenges discussed in the next section immediate and paradigm-specific imperatives.',\n",
       "    'subsections': [{'title': '5.1 Tool Use and Capabilities: Integration with Real-World Systems',\n",
       "      'paragraphs': 'A critical capability that distinguishes agentic AI from passive models is their ability to interact with and manipulate external tools and data sources via Application Programming Interfaces (APIs) [ 109 , 110 , 111 , 112 ] . This functionality is the bridge between an agent‚Äôs internal reasoning and tangible action in the real world. The nature of this integration is, as our framework predicts, paradigm-dependent.\\nIn the Symbolic Paradigm , tool use is typically hard-coded and deterministic. Agents call specific functions with predefined parameters based on explicit logical rules. This is prevalent in safety-critical domains like healthcare, where agents interact with Electronic Health Record (EHR) systems using strict, auditable APIs (e.g., HL7 FHIR standards for reading/writing patient data) or clinical decision support tools with fixed input-output schemas [ 113 , 114 ] .\\nIn the Neural Paradigm , tool use is orchestrated and generative. Frameworks like LangChain and AutoGen use the LLM‚Äôs ability to understand natural language instructions to dynamically select and call appropriate tools from a suite of available options. The LLM [ 115 , 116 ] generates the API call parameters (e.g., formulating a database query, crafting a search query) based on its context, which is then executed by the framework. This allows for immense flexibility but introduces risks of malformed calls or unexpected outputs.\\nTable 7 [ 117 , 118 , 119 ] provides a non-exhaustive overview of the types of real-world tools and APIs that agentic systems are currently being integrated with, categorized by their primary domain and function.\\nThis integration enables agents to move beyond text generation to become truly functional systems. For instance, a neural agent using AutoGen could read an email via the Outlook API, extract key tasks, write code to solve them using a Python tool, and then post the results to a Slack channel‚Äîall within a single orchestrated workflow. Conversely, a symbolic agent in a manufacturing context might reliably call a single, well-defined API to adjust a machine‚Äôs parameters based on its rigid internal state model.\\nIn conclusion, agentic AI is not a monolithic force but a set of distinct architectural paradigms. Its embedding into the fabric of critical systems is a story of domain-driven design, where theoretical capabilities are shaped and constrained by practical, ethical, and operational realities. The choice between symbolic, neural, or hybrid design is the primary engineering decision, making the governance and safety challenges discussed in the next section immediate and paradigm-specific imperatives.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Comprehensive Taxonomy of Agentic AI Literature: A Paradigm-Aware Analysis',\n",
       "    'paragraphs': 'The accelerating pace of innovation in agentic AI necessitates a systematic organization that reflects its fundamental architectural schism. This section provides a paradigm-aware synthesis of the field, serving as the culminating evidence for our dual-lineage framework:\\nA visual taxonomy (Figure 8 ) categorizing the field‚Äôs core dimensions through the lens of symbolic and neural mechanisms.\\nA structured literature map (Table 8 ) analyzing all 90 studies from our systematic review, now classified by their primary architectural paradigm.\\nOur paradigm-aware analysis of the complete corpus reveals key patterns that were previously obscured (see Table 8 ):\\nParadigm Specialization by Domain : High-stakes, regulated domains like Healthcare and Legal Tech show a strong preference for symbolic or highly constrained neural architectures (e.g., [ 42 , 45 ] ), while dynamic domains like Finance leverage neural orchestration for complex analysis (e.g., [ 43 ] ).\\nThe Governance Divide : Research in Ethics & Governance is overwhelmingly focused on the novel challenges of the neural paradigm (e.g., [ 9 , 120 ] ), revealing a significant gap in modernized governance frameworks for purely symbolic systems.\\nTemporal Paradigm Shift : The data shows a clear transition: symbolic and hybrid Cognitive Architectures dominated early research (2018‚Äì2021), while neural Orchestration Frameworks have overwhelmingly dominated post-2022, following the rise of LLMs.\\nOur paradigm-aware taxonomy yields several pivotal insights that chart the current and future state of Agentic AI. Primarily, it reveals a clear paradigm-market fit , wherein symbolic and hybrid architectures demonstrably dominate safety-critical applications like healthcare and robotics, while pure neural systems thrive in data-rich, adaptive domains such as finance and education. Furthermore, the taxonomy exposes a significant governance imbalance ; while ethical challenges within the neural paradigm are the subject of intense research, the governance of modern, complex symbolic systems remains a critically underexplored area. This insight directly informs the third finding: that the most viable path forward is hybrid . The most active and promising research in emerging technologies explicitly seeks to integrate both paradigms, a strategic direction that confirms the thesis outlined in Section 9. Finally, the successful classification of all 90 studies by this dualist framework validates its comprehensive coverage and utility as a robust tool for literature analysis and future research design.',\n",
       "    'subsections': [{'title': 'Key Insights from the Paradigm-Aware Taxonomy',\n",
       "      'paragraphs': 'Our paradigm-aware taxonomy yields several pivotal insights that chart the current and future state of Agentic AI. Primarily, it reveals a clear paradigm-market fit , wherein symbolic and hybrid architectures demonstrably dominate safety-critical applications like healthcare and robotics, while pure neural systems thrive in data-rich, adaptive domains such as finance and education. Furthermore, the taxonomy exposes a significant governance imbalance ; while ethical challenges within the neural paradigm are the subject of intense research, the governance of modern, complex symbolic systems remains a critically underexplored area. This insight directly informs the third finding: that the most viable path forward is hybrid . The most active and promising research in emerging technologies explicitly seeks to integrate both paradigms, a strategic direction that confirms the thesis outlined in Section 9. Finally, the successful classification of all 90 studies by this dualist framework validates its comprehensive coverage and utility as a robust tool for literature analysis and future research design.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 Ethical and Governance Challenges: A Paradigm-Specific Analysis',\n",
       "    'paragraphs': 'As Agentic AI systems gain autonomy and are deployed in critical domains, they introduce a complex spectrum of ethical and governance concerns [ 120 , 9 , 174 ] . However, a critical oversight in current discourse is the treatment of these challenges as monolithic. The risks and requisite mitigation strategies differ profoundly between the symbolic and neural paradigms, demanding a paradigm-aware approach to oversight and interdisciplinary collaboration.\\nA synthesis of these issues is presented in Table 9 , which expands upon standard taxonomies by outlining the core challenges and, most importantly, their paradigm-specific manifestations and governance implications.\\nAnalysis and Summary\\nThe bifurcation of ethical challenges detailed in Table 9 leads to several critical and interconnected conclusions. First, it becomes evident that effective governance cannot be architecturally agnostic . Regulation and ethical oversight must be predicated on the underlying paradigm; a requirement for \"full explainability,\" for instance, is feasible for a symbolic system but may be technologically impossible for a pure neural agent, thus necessitating the development of alternative compliance mechanisms.\\nFurthermore, the rise of hybrid systems compounds ethical complexity . An agent that blends paradigms inherently inherits the governance challenges of both. A neuro-symbolic architecture, for example, requires a framework capable of auditing its deterministic symbolic logic while simultaneously monitoring its neural components for stochastic failures, creating a significantly more demanding oversight burden.\\nConversely, the attribution gap presents a specific crisis for the neural paradigm . The fundamental question of \"Who is liable?\" is most acute here, as its diffuse and stochastic nature directly challenges legal frameworks built on principles of direct causation and intent. This may ultimately require the establishment of new forms of strict liability for developers and operators.\\nFinally, these distinctions mean that effective human-AI collaboration is inherently paradigm-dependent . Designing appropriate human oversight requires a deep understanding of the agent‚Äôs core mechanics. The process of overseeing a symbolic agent is analogous to supervising a junior programmer‚Äîit involves checking their logical steps. In stark contrast, overseeing a neural agent is more akin to supervising a brilliant but unpredictable intern‚Äîit requires carefully steering their context and interpreting their often-opaque outputs.\\nAddressing the ethical and governance issues of Agentic AI is essential to harness its transformative potential. However, this analysis demonstrates that a nuanced, paradigm-specific approach is not just beneficial but necessary. Blanket policies will inevitably fail. The path forward requires technical standards, legal frameworks, and ethical guidelines that are as sophisticated and differentiated as the technologies they aim to govern.\\nThis paradigm-specific framing, however, remains incomplete without explicit consideration of policy frameworks that account for the degrees of agency and autonomy in Agentic AI systems, an issue we address next.\\nAn overlooked but critical dimension of ethical and governance discourse is the explicit development of policy frameworks tailored to agentic AI . Current governance proposals often extend existing AI regulations to cover autonomous systems, but they seldom distinguish between systems that merely generate outputs and those that exercise agency in decision-making . For agentic AI, the challenge lies in defining and operationalizing levels of autonomy and clarifying their governance implications.\\nPolicy mechanisms must therefore incorporate criteria that distinguish different levels of agency.\\nTable 10 summarizes a proposed taxonomy of agency in Agentic AI, outlining the\\ncharacteristics of assistive, shared, and delegated forms of agency alongside their governance implications.\\nAccordingly, governance must move beyond paradigm-specific risk analysis toward a taxonomy of agency , where ethical principles and legal accountability mechanisms scale with the degree of autonomy. This aligns with calls for ‚Äúmeaningful human control‚Äù [ 127 ] , but extends them into concrete policy design that recognizes the unique governance needs of agentic AI.',\n",
       "    'subsections': [{'title': '7.1 Toward Agentic AI Policy',\n",
       "      'paragraphs': 'An overlooked but critical dimension of ethical and governance discourse is the explicit development of policy frameworks tailored to agentic AI . Current governance proposals often extend existing AI regulations to cover autonomous systems, but they seldom distinguish between systems that merely generate outputs and those that exercise agency in decision-making . For agentic AI, the challenge lies in defining and operationalizing levels of autonomy and clarifying their governance implications.\\nPolicy mechanisms must therefore incorporate criteria that distinguish different levels of agency.\\nTable 10 summarizes a proposed taxonomy of agency in Agentic AI, outlining the\\ncharacteristics of assistive, shared, and delegated forms of agency alongside their governance implications.\\nAccordingly, governance must move beyond paradigm-specific risk analysis toward a taxonomy of agency , where ethical principles and legal accountability mechanisms scale with the degree of autonomy. This aligns with calls for ‚Äúmeaningful human control‚Äù [ 127 ] , but extends them into concrete policy design that recognizes the unique governance needs of agentic AI.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '8 Research Gaps: A Paradigm-Specific Roadmap',\n",
       "    'paragraphs': 'The development of Agentic AI is constrained by significant, unresolved challenges. However, a critical oversight in identifying these gaps is treating them as uniform across architectures. The research imperatives for symbolic systems diverge profoundly from those for neural systems, with a particularly pressing need for work on hybrid architectures that can leverage the strengths of both. As outlined in Table 11 , these thematic areas require a paradigm-aware research strategy to ensure future systems are robust, adaptable, and aligned.\\nThe bifurcation of research gaps identified in Table 11 reveals that the most critical overarching challenge is the current lack of Paradigm-Aware Research Methodologies . The tools, benchmarks, and success criteria developed for one paradigm frequently prove irrelevant or misapplied to the other, creating fundamental barriers to coherent progress.\\nThis analysis suggests several imperative directions for future work. First, the most promising research path forward appears to lie not in pursuing either paradigm in isolation, but in their intentional integration . The \"Reasoning & Adaptability\" gap, for instance, represents a prime candidate for neuro-symbolic solutions, wherein a neural network‚Äôs robust pattern recognition capabilities are systematically guided and constrained by a symbolic reasoner‚Äôs logical framework.\\nFurthermore, the community must move beyond isolated benchmarks that fail to account for paradigmatic differences. There is a critical need to develop separate, rigorous evaluation suites that stress-test the unique failure modes of each architecture‚Äîsuch as logic bombs and edge-case reasoning for symbolic systems, and prompt injection resilience and output stability for neural systems.\\nPerhaps most urgently, this bifurcation demonstrates that effective governance cannot follow a one-size-fits-all approach . Policymakers and ethicists must collaborate with engineers to develop distinct, tailored frameworks for auditing and regulating these fundamentally different technologies. Applying the stringent verifiability standards of symbolic systems to neural architectures would inadvertently stifle innovation, while applying the more flexible standards designed for neural systems to symbolic environments would overlook critical risks associated with logical integrity and deterministic failure.\\nConclusion\\nAddressing these gaps requires a conscious departure from generic AI research. Progress hinges on a dual-track strategy that deepens our understanding of each paradigm‚Äôs unique challenges while simultaneously pioneering architectures and standards for their integration. This paradigm-specific roadmap is essential to move from powerful but flawed prototypes to reliable and trustworthy agentic systems. The future of Agentic AI is not a choice between symbolism and connectionism, but a strategic synthesis of both.',\n",
       "    'subsections': [{'title': 'Commentary on Key Themes',\n",
       "      'paragraphs': 'The bifurcation of research gaps identified in Table 11 reveals that the most critical overarching challenge is the current lack of Paradigm-Aware Research Methodologies . The tools, benchmarks, and success criteria developed for one paradigm frequently prove irrelevant or misapplied to the other, creating fundamental barriers to coherent progress.\\nThis analysis suggests several imperative directions for future work. First, the most promising research path forward appears to lie not in pursuing either paradigm in isolation, but in their intentional integration . The \"Reasoning & Adaptability\" gap, for instance, represents a prime candidate for neuro-symbolic solutions, wherein a neural network‚Äôs robust pattern recognition capabilities are systematically guided and constrained by a symbolic reasoner‚Äôs logical framework.\\nFurthermore, the community must move beyond isolated benchmarks that fail to account for paradigmatic differences. There is a critical need to develop separate, rigorous evaluation suites that stress-test the unique failure modes of each architecture‚Äîsuch as logic bombs and edge-case reasoning for symbolic systems, and prompt injection resilience and output stability for neural systems.\\nPerhaps most urgently, this bifurcation demonstrates that effective governance cannot follow a one-size-fits-all approach . Policymakers and ethicists must collaborate with engineers to develop distinct, tailored frameworks for auditing and regulating these fundamentally different technologies. Applying the stringent verifiability standards of symbolic systems to neural architectures would inadvertently stifle innovation, while applying the more flexible standards designed for neural systems to symbolic environments would overlook critical risks associated with logical integrity and deterministic failure.\\nConclusion\\nAddressing these gaps requires a conscious departure from generic AI research. Progress hinges on a dual-track strategy that deepens our understanding of each paradigm‚Äôs unique challenges while simultaneously pioneering architectures and standards for their integration. This paradigm-specific roadmap is essential to move from powerful but flawed prototypes to reliable and trustworthy agentic systems. The future of Agentic AI is not a choice between symbolism and connectionism, but a strategic synthesis of both.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '9 Future Directions: The Path to Hybrid Intelligence',\n",
       "    'paragraphs': 'Agentic AI systems are rapidly evolving beyond static task automation into dynamic, collaborative, and adaptive entities [ 157 ] . Their future development will hinge on interdisciplinary advances, technological convergence, and‚Äîcritically‚Äîa paradigm-aware approach to design that seeks to integrate the strengths of both symbolic and neural lineages into robust hybrid architectures.\\nA summary of these paradigm-aware trajectories is presented in Table 12 , which outlines the specific research and integration priorities for each paradigm‚Äôs evolution, moving beyond a generic technology forecast.\\nAnalysis of Strategic Trajectories\\nThe bifurcated future outlined in Table 12 leads to one overriding conclusion: the paramount direction is Architectural Integration . The goal is to forge a new class of hybrid systems that leverage the reliability of symbolic reasoning and the adaptability of neural generation.\\nNeuro-Symbolic Integration as the Keystone: The most profound progress will come from research that successfully couples neural networks for perception and pattern recognition with symbolic engines for reasoning and constraint checking. This is the most promising path to overcoming the brittleness of pure symbolism and the opacity of pure neural approaches.\\nParadigm-Specialized Roles in Ecosystems: Future multi-agent ecosystems [ 154 , 155 ] will not be homogenous. They will consist of specialized agents‚Äîsome highly neural for creative tasks, some highly symbolic for regulatory compliance‚Äîthat communicate through standardized protocols. The orchestration of such hybrid swarms is a critical research frontier.\\nA Dual-Track Approach to Governance: The development of safety and governance mechanisms [ 9 ] must continue on two tracks: advancing formal methods for symbolic verifiability and developing new statistical, training-based methods for neural alignment. The ultimate governance framework for a hybrid agent will need to seamlessly combine both.\\nConvergence as Amplification: The integration with other technologies [ 151 , 156 ] will amplify the capabilities of both paradigms. Neural agents will manage real-time sensor data from IoT, while symbolic modules will ensure the decisions made from that data are safe and compliant.\\nConclusion\\nThe future of Agentic AI is a synthesis. Its trajectory will be shaped not only by technical breakthroughs but by thoughtful, paradigm-aware integration of ethics, interdisciplinary methods, and infrastructure-aware governance [ 9 ] . The next conceptual turning point will be defined by our ability to engineer hybrid intelligence ‚Äîsystems that are both adaptable and reliable , both creative and sound . The question is no longer whether agents will become intelligent partners, but whether we can architect a future of hybrid intelligence that is both powerful and trustworthy.',\n",
       "    'subsections': []},\n",
       "   {'title': '10 Conclusion',\n",
       "    'paragraphs': 'Agentic AI represents a fundamental paradigm shift in the design of intelligent systems, but its rapid evolution has led to a fragmented and often anachronistic understanding of the field. This review has addressed this confusion by introducing and validating a novel conceptual framework: the existence of two distinct lineages of Agentic AI‚Äîthe Symbolic/Classical and the Neural/Generative ‚Äîeach with fundamentally different operational mechanics, strengths, and limitations.\\nOur analysis demonstrates that the common practice of conceptual retrofitting ‚Äîdescribing modern LLM-orchestrated agents with the language of symbolic systems (e.g., PPAR loops, BDI)‚Äîobscures their true nature and impedes progress. Through a systematic, paradigm-aware review of the literature, we have established three central tenets. First, the architectural divide is both real and meaningful ; symbolic systems excel in environments requiring safety, verifiability, and explicit logic (e.g., healthcare, robotics control), while neural systems thrive in domains requiring adaptability, pattern recognition, and operation on unstructured data (e.g., finance, creative research) (Sections 5 , 6 ).\\nFurthermore, this divide dictates that governance must be paradigm-specific . The ethical challenges and requisite mitigation strategies differ profoundly between paradigms, meaning accountability for a symbolic system involves auditing its logic, whereas for a neural system, it necessitates auditing its training data and prompts. This renders a one-size-fits-all approach to AI ethics fundamentally insufficient (Section 7 ).\\nCritically, our findings indicate that the most productive path forward is hybrid, not isolated . The most pressing research gaps and promising future directions lie not in the isolated improvement of either paradigm, but in their strategic integration into neuro-symbolic architectures that leverage the complementary strengths of symbolic reliability and neural adaptability (Sections 8 , 9 ).\\nThis dual-paradigm framework provides the essential analytical lens to move the field beyond a simple catalog of technologies toward a coherent theory of architectural design in Agentic AI. It offers researchers, engineers, and policymakers a precise vocabulary and a functional taxonomy to classify systems, evaluate their capabilities and risks appropriately, and make informed design choices.\\nUltimately, the development of Agentic AI is not merely a technical challenge‚Äîit is a sociotechnical one. Its success will depend on whether we can architect systems that are not only powerful but also trustworthy. This requires a conscious and deliberate effort to build hybrid intelligence‚Äîsystems that are both adaptable and reliable, both creative and sound. By recognizing and embracing the distinct nature of these two architectural lineages, we can steer this transformative technology toward a future where agentic systems truly serve as trusted collaborators in scientific discovery (understanding), in providing fair and accessible services (equity), and in forming the robust, verifiable backbone of critical infrastructure (resilience).',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models‚Äîa practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018‚Äì2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.'},\n",
       " '2510.25683v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'High-fidelity simulations underpin design and decision-making across many areas of computational science and engineering. However, resolving transient dynamics with fine spatial meshes is still expensive when long horizons, high frequencies, or repeated solves over parameter sets are required. Surrogate models have therefore become popular as a means to accelerate inference while retaining accuracy where it matters, from fluids and granular flow to materials and mechanics (Forrester et\\xa0al., 2008 ; Herrmann & Kollmannsberger, 2024 ) .\\nSeveral approaches to surrogate modeling exist: classical Reduced-Order Models (ROMs) compress dynamics via low-rank bases (e.g., POD/Galerkin) or regression in latent coordinates (Benner et\\xa0al., 2015 ; Quarteroni et\\xa0al., 2016 ) ; Gaussian-process and Kriging surrogates excel with limited data and quantified uncertainty (Rasmussen & Williams, 2006 ; Sacks et\\xa0al., 1989 ; Forrester et\\xa0al., 2008 ) ; Deep Learning (DL) surrogates, ranging from convolutional encoder-decoder models to sequence models and neural operators, aim to learn solution operators or time-advancement rules directly from data, offering mesh- and geometry-agnostic generalization in favorable settings, and they scale to large datasets and complex, nonlinear regimes (Ronneberger et\\xa0al., 2015 ; Shi et\\xa0al., 2015 ; Li et\\xa0al., 2020 ; Lu et\\xa0al., 2019 ; Kovachki et\\xa0al., 2023 ) .\\nHowever, each class of approaches has its own inherent limitations. ROMs can struggle with strongly nonlinear phenomena and often require intrusive projection or stabilization (Benner et\\xa0al., 2015 ; Quarteroni et\\xa0al., 2016 ) . Neural operators and Physics-Informed Neural Networks (PINNs) have shown strong results for wave propagation and inverse problems, yet they frequently rely on global transforms or explicit PDE supervision (Li et\\xa0al., 2020 ; Raissi et\\xa0al., 2018 ; Cuomo et\\xa0al., 2022 ) . At high frequency, global spectral layers must resolve short wavelengths, and PINN training can become stiff due to competing loss terms and the need for accurate residuals on fine collocation sets (Kovachki et\\xa0al., 2023 ; Krishnapriyan et\\xa0al., 2021 ; Wang et\\xa0al., 2022 ; Maddu et\\xa0al., 2021 ) . Convolutional Neural Networks (CNNs) are particularly suitable for grid-like data, but become cumbersome when simulating generic domains with complex shapes (Bronstein et\\xa0al., 2017 ) .\\nRecently, a promising alternative to the models discussed above has emerged in the form of Graph Neural Networks (GNNs). They provide a powerful framework for surrogate modeling on both structured and unstructured meshes and possess several properties that make them particularly suitable for representing physical phenomena. For instance, they naturally incorporate locality by aggregating information from neighboring nodes and support weight sharing in a way that is inherently invariant to node ordering (Gilmer et\\xa0al., 2017 ; Zaheer et\\xa0al., 2018 ; Battaglia et\\xa0al., 2018 ) . These features align closely with the behavior of most physical systems, where locality and invariance are fundamental principles.\\nBased on this paradigm, several simulators have been developed, among which the Graph Network Simulator (GNS) and MeshGraphNet stand out as the most promising. While differing mainly in the graph construction method, they both excel at simulating complex phenomena such as fluid dynamics, thanks to message passing and graph-to-graph updates to generate subsequent time steps of a simulation (Sanchez-Gonzalez et\\xa0al., 2020 ; Pfaff et\\xa0al., 2020 ; Fortunato et\\xa0al., 2022 ) .\\nDespite this progress, applications of GNN surrogates to structural problems, particularly dynamic ones, remain comparatively limited. Existing work often focuses on static or time-independent PDEs, quasi-static responses, or settings where displacements are large relative to the geometric scale (Chou et\\xa0al., 2024 ; Gladstone et\\xa0al., 2024 ; Zhao et\\xa0al., 2024 ; Deshpande et\\xa0al., 2024 ; Herrmann & Kollmannsberger, 2024 ) . However, a general framework capable of handling dynamic events and micro-scale displacements has not yet been proposed. The latter aspect, namely micro-scale displacements, introduces several numerical challenges: finite-difference velocities computed from absolute nodal positions can suffer from catastrophic cancellation when subtracting nearly equal floating-point numbers, which degrades derivative accuracy and destabilizes long rollouts (Higham, 2002 ) .\\nTo address these gaps, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of structural dynamics. The design is general and not restricted to any specific structural class; in this paper, we evaluate it on guided wave dynamics, an application that is both practically relevant - for example, in fields like structural health monitoring - and numerically demanding, due to the need for fine meshes and spatial discretizations to obtain accurate results (Rose, 1999 ; Cawley, 2024 ) . GNSS combines three ingredients tailored to structural simulations: (i) a local-coordinate formulation that expresses nodal kinematics in node-fixed frames to stabilize finite-difference velocities at micro-scale displacements; (ii) a sign-aware acceleration loss that discourages phase flips and improves long-horizon stability; and (iii) a wavelength-informed connectivity radius that aligns the message-passing neighborhood with physically meaningful interaction scales (e.g., a fraction of the bending-wave wavelength), thereby leveraging locality without oversmoothing (Langer et\\xa0al., 2017 ) . Together, these choices preserve the benefits of graph locality, while mitigating failure modes observed when applying off-the-shelf GNN simulators to structural dynamics problems involving small displacements.\\nWe benchmarked GNSS on a numerical case study involving wave propagation in a clamped beam, using a dataset generated through finite element simulations. Our model accurately predicts wave propagation during the rollout phase and generalizes across different loading conditions, outperforming traditional GNNs based on absolute nodal positions.\\nThis paper is organized as follows. Section 2 describes the implementation of GNSS. Section 3 introduces the case study used to validate the proposed method and discusses the results. Section 4 draws out the conclusions of this work.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Methods',\n",
       "    'paragraphs': 'GNSS builds on the foundational concepts of graph theory and the classical GNNs introduced by Scarselli et\\xa0al. ( 2009 ) , as well as the GN framework proposed by Battaglia et\\xa0al. ( 2018 ) . A comprehensive treatment of these theoretical foundations is beyond the scope of this paper; instead, the focus is on adapting and applying the GN framework to structural dynamics. Accordingly, this section first introduces basic graph definitions, then outlines the GN paradigm, and finally presents the specific GN-based architecture developed for structural dynamic analysis.\\nA graph ùí¢ \\\\mathcal{G} consists of a set of nodes (or vertices) and a set of edges that define connections between nodes. Formally, a graph can be represented using an adjacency matrix ùë® ‚àà ‚Ñù N √ó N \\\\bm{A}\\\\in\\\\mathbb{R}^{N\\\\times N} , where N N is the number of nodes, and A i \\u200b j ‚â† 0 A_{ij}\\\\neq 0 indicates an edge between nodes i i and j j . The degree of a node is defined as the number of edges incident to that node. Beyond this topological knowledge, graphs can include additional information in the form of node features ùëΩ ‚àà ‚Ñù N √ó d \\\\bm{V}\\\\in\\\\mathbb{R}^{N\\\\times d} and edge features ùë¨ ‚àà ‚Ñù | ùë¨ | √ó k \\\\bm{E}\\\\in\\\\mathbb{R}^{|\\\\bm{E}|\\\\times k} , where d d and k k are the feature dimensions (i.e., the length of the vector) of nodes and edges, respectively (Gong & Cheng, 2019 ; Yang & Li, 2020 ; Chen & Chen, 2021 ) . The node feature vector associated with node n n , denoted by ùê± n ‚àà ‚Ñù d \\\\mathbf{x}_{n}\\\\in\\\\mathbb{R}^{d} , represents some measurable properties of the node. Similarly, the edge feature vectors encode the relationship between connected nodes. Given the feature vectors, the graph can be defined as G = ( ùëΩ , ùë¨ ) G=(\\\\bm{V},\\\\bm{E})\\nEarly neural architectures for graphs, such as the GNN model proposed in Scarselli et\\xa0al. ( 2009 ) ‚Äôs work, extended neural computation to structured graph domains using an iterative, equilibrium-based message-passing scheme. Each node updates its state by aggregating information from its neighbors until convergence. These models have proven effective for representing physical systems with complex, irregular topologies, where traditional architectures like Convolutional Neural Networks (CNNs), designed for structured grid-like data, are inadequate. GNNs generalize the concept of convolution to graph domains, making them well-suited for applications in fluid dynamics, structural mechanics, and materials science (Battaglia et\\xa0al., 2018 ; Pfaff et\\xa0al., 2020 ; Wong et\\xa0al., 2022 ; Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ; Li et\\xa0al., 2023 ; Zhao et\\xa0al., 2023 ; Gulakala et\\xa0al., 2023 ; Shivaditya et\\xa0al., 2022 ; Bronstein et\\xa0al., 2017 ; Wu et\\xa0al., 2020 ) .\\nA key strength of GNNs in modeling physical systems lies in their inductive biases. These include the representation of local interactions through message passing, the sharing of weights across the graph, and invariance to node permutations. Such biases naturally reflect physical principles such as locality, translational symmetry, and conservation laws (Battaglia et\\xa0al., 2018 ) .\\nThe GN framework (Battaglia et\\xa0al., 2018 ) generalizes and unifies multiple GNN-based architectures such as message-passing neural networks (MPNNs), interaction networks, and relation networks. Unlike classical GNNs, GNs explicitly decompose the graph update process into three modular functions: edge update, node update, and global update. This modular graph-to-graph transformation enables flexible and expressive modeling capabilities, which are particularly beneficial in physical simulation tasks (Sanchez-Gonzalez et\\xa0al., 2020 ) .\\nGiven as input a system‚Äôs state described by a graph ùíÆ = ( ùëø , ùëπ ) \\\\mathcal{S}=(\\\\bm{X},\\\\bm{R}) , a GN produces an updated graph ùíÆ ‚Ä≤ = ( ùëø ‚Ä≤ , ùëπ ‚Ä≤ ) \\\\mathcal{S}^{\\\\prime}=(\\\\bm{X}^{\\\\prime},\\\\bm{R}^{\\\\prime}) of identical topology representing the updated state of the system. ùëø \\\\bm{X} and ùëπ \\\\bm{R} describe the physical properties and relationships of the system, respectively. The architecture typically follows an encode-process-decode paradigm (Battaglia et\\xa0al., 2018 ) , where raw inputs are encoded into latent node and edge embeddings, iteratively processed through message passing, and decoded into physically meaningful outputs.\\nThe encoding phase is described in Equation 1 . During this step, the physical properties and relationship describing the state are embedded into a higher-dimensional continuous space:\\nHere, N N is the number of nodes, d d is the number of properties for each node, k k is the number of relationships for each edge, and the h h -dimensional space is the latent space . This initial transformation not only homogenizes the feature space, but also allows the network to capture local patterns that are crucial for the learning task (Scarselli et\\xa0al., 2009 ; Gilmer et\\xa0al., 2017 ) .\\nMessage passing is the phase during which information is processed and exchanged among the different nodes of the system. It consists of updating the latent features of the graph through three steps:\\nMessage Construction (Equation 2 ): The updated edge features (also called message ) are obtained by combining the old information of the edge and the two nodes connected to it:\\nMessage Aggregation (Equation 3 ): For each node, an aggregated message is computed; this is obtained by combining all the messages related to node i i :\\nHere, ùí© \\u200b ( i ) \\\\mathcal{N}(i) represents the set of nodes linked to node i i .\\nNode Update (Equation 4 ): During this final step, the old node feature vector is combined with the aggregated message to obtain the updated node features:\\nEquation 5 describes the entire message passing pipeline concisely:\\nIn Equations 2 ‚Äì 5 , the symbols œï , œà , Œ≥ \\\\phi,\\\\,\\\\psi,\\\\,\\\\gamma represent arbitrary operands which are part of the architectural choice when designing a GN.\\nThe whole message passing procedure is typically repeated M > 1 M>1 times. This hyperparameter defines the degree of information propagation through the network, and thus the depth of the GN, as visually represented in Figure 1 . Information spread is usually represented through a computation graph, which describes how the embedding of each node is iteratively updated through message passing. Each layer of the computation graph carries out a message-passing operation; by applying this process recursively across layers, the network is able to represent both local interactions and more complex, higher-order relationships within the graph (Gilmer et\\xa0al., 2017 ; Battaglia et\\xa0al., 2018 ) .\\nDuring the decoding phase, the information stored in the latent representation is converted to task-specific predictions, such as node-level, edge-level, or graph-level outputs (Battaglia et\\xa0al., 2018 ) . For example, referring to the case of GNS and MeshGraphNet, the decoder performs a single node-level task by transforming the updated node features into a physical quantity at each node, the acceleration.\\nIn Figure 2 , our GNSS is presented. This framework is a modified version of the GNS developed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , specifically adapted for structural simulations. Figure 2 (a) shows the Rollout , namely the procedure used to simulate the trajectory of the system: an initial configuration X 0 X_{0} is provided, then the GNSS is iteratively applied T Œî \\u200b t p \\u200b h \\\\frac{T}{\\\\Delta t_{ph}} times to predict the trajectory from physical time 0 to physical time T T . Œî \\u200b t ph \\\\Delta t_{\\\\mathrm{ph}} is the fixed timestep size adopted in the numerical simulations used to generate all trajectories in the dataset. Figure 2 (b) expands the operation involved at each time step t t , during which the GNSS performs the whole encode-process-decode paradigm to retrieve the updated nodal information (accelerations). Lastly, Figure 2 (c) details the processing phase, i.e., message passing, showing the procedure for a single passing step; as introduced above, message passing is recursively repeated M M times.\\nIn what follows, we detail the GNSS operations shown in Figure 2 (b)‚Äì(c).\\nThe pipeline ‚Äî from graph construction to position update ‚Äî is described for a single\\nsimulation time step t t ; accordingly, all intermediate quantities are evaluated at t t ,\\nand the output corresponds to the updated state at t + 1 t+1 . Because a single GNSS pass\\nperforms multiple rounds of message passing, we denote the m m -th round by the\\nsuperscript (m) . To avoid clutter, we omit the time index; unless otherwise stated,\\nall expressions are understood at time t t .\\nThe pre-processing phase is responsible for representing the information stored in the initial configuration of the system into a graph structure ready to be processed by the GN.\\nThe representation of the system naturally follows from the choice of a graph-based model. The continuous structure is discretized into a set of points , each capturing the local physical properties of its neighborhood, while the interactions among these points are expressed as relationships . Throughout the following description, we will denote a point in the discretized structure as x x , and its associated relationship as r i \\u200b j r_{ij} , representing the underlying physical properties. In contrast, the terms node v i v_{i} and edge e i \\u200b j e_{ij} will refer to the corresponding latent features, i.e., the latent graph representation used within the model.\\nRegarding the relationship between those points, the primary idea behind the graph construction proposed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) is that, typically, entities in physical systems are influenced by their neighbors. This natural behavior is reflected in the graph structure by establishing edges only between nodes that are sufficiently close. In practice, this is achieved by defining a connectivity radius that sets the maximum allowable distance for two nodes to be connected, as shown in Figure 3 .\\nOnce the graph‚Äôs topology is defined, each node i i of the graph is associated with a feature vector storing physical information. Following the implementation proposed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , the point feature vector stores information about the velocity context and the type of particle related to the node. As demonstrated by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , including not only the current velocity of a point, but also the velocities from previous timesteps, greatly improves the performance of the model. This information is stored in a tensor as shown in Equation 6 :\\nHere, t t is the current timestep, and ùê© Àô i ( t ) \\\\prescript{(t)}{}{\\\\dot{\\\\mathbf{p}}}_{i} is the velocity of point i i at time t t , d d is the dimension of the space, and n n is the number of previous velocities considered (in addition to the current one). This is a hyperparameter, and is usually set to n = 4 n=4 as suggested by Choi & Kumar ( 2024 ) . The velocities are calculated via finite differences as shown in Equation 7 :\\nwhere ùê© i ( t ) \\\\prescript{(t)}{}{{\\\\mathbf{p}}}_{i} is the position of point i i at time t t and Œî \\u200b t \\\\Delta t is the timestep size. The timestep size used in the algorithm is set to 1, as non-dimensional time quantities are preferred during prediction. This unit timestep size corresponds to a physical timestep Œî \\u200b t p \\u200b h \\\\Delta t_{ph} implicitly defined by the one used in the training dataset. Consequently, the timestep size adopted in the numerical simulations must remain fixed and identical across all trajectories in the dataset.\\nThe framework is designed to account for different types of particles. This information is required by the model to correctly handle entities playing different roles in the simulation, and is stored in a vector ùêü \\\\mathbf{f} . This vector has 16 components and is created through an embedding layer.\\nIn our model, we exploit this structure to inform the prediction mechanism about the constraint of the structural simulation. Thanks to this choice, we can simply assign arbitrary IDs to different types of constraints, and the model will optimize the weights of the embedding layer (i.e., the 16 components of ùêü \\\\mathbf{f} ) to best represent the different entities according to their behavior. For example, we can assign an ID to the free portion (nodes) of the structure, another ID to nodes where a prescribed motion is applied, and another ID to the region of application of BCs. This choice allows us to simplify the feature vector proposed by Choi & Kumar ( 2024 ) by eliminating the information about the distance from the boundaries that was given for each node. Additionally, this allows us to model different types of boundaries by assigning different IDs to them (clamp, pin, etc.).\\nAll the properties listed above are included in a point state vector, as shown in Equation 8 :\\nHere, ùê© Àô i ‚â§ t \\\\dot{\\\\mathbf{p}}_{i}^{\\\\leq t} is the flattened vector of velocities history. For a representative 2D case, considering the current velocity and 4 additional velocities and the previous timesteps, the point feature vector is composed of 26 elements: 5 components for velocity along direction x x , 5 for velocity along direction y y , and 16 for the type embedding.\\nMoreover, the interaction between connected points i i and j j is represented by their distance in the physical space and their displacement in the current timestep, both normalized by the connectivity radius R R , as shown in Equation 9 :\\nThe distance provides information about the level of interaction between the two points, while the displacement gives a direction to the relationship.\\nTypically, GNs represent displacements in an absolute coordinate system shared by the entire physical system (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) . While this approach is well-suited to domains commonly simulated with GNs, such as granular flow and fluid dynamics, it can be problematic in structural mechanics. In structural simulations, load-induced displacements are often several orders of magnitude smaller than the characteristic dimensions of the system. This scale mismatch can cause numerical issues when derivatives (e.g., velocities) are computed via finite differences. In particular, subtracting two large, nearly equal floating-point numbers (absolute positions at successive timesteps) may lead to a loss of significant digits, a phenomenon known as catastrophic cancellation (Higham, 2002 ) . The resulting numerical noise reduces the accuracy of computed derivatives, and the problem becomes critical when rounding errors are comparable to, or larger than, the displacements being resolved. This limitation is well recognized in precision-sensitive fields such as structural dynamics and geomechanics, where floating-point inaccuracies can distort small-strain calculations (Belytschko et\\xa0al., 2014 ) .\\nTo overcome this limitation, GNSS introduces a novel representation of nodal positions in local coordinate systems that are fixed in both space and time. For each node, the origin is set at its initial position (i.e., its location at time zero), effectively centering all subsequent displacements around zero. By eliminating large absolute position values, this representation mitigates catastrophic cancellation and ensures stable, accurate derivative computations via finite differences. This formulation constitutes a key feature of GNSS, enabling reliable surrogate modeling of structural dynamics where traditional GN formulations fail.\\nThe encoder is responsible for transforming the input graph ùí¢ \\\\mathcal{G} into a latent graph ùí¢ 0 \\\\mathcal{G}_{0} , in which the old physical feature vectors are embedded into a latent space. In our implementation, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) ‚Äôs work, this operation is performed by two multilayer perceptrons (MLPs), œµ Œò v v \\\\epsilon^{v}_{\\\\Theta^{v}} and œµ Œò e e \\\\epsilon^{e}_{\\\\Theta^{e}} , which embed the feature vectors associated with nodes and edges, respectively. The two MLPs share the same architecture: an input layer with dimensions N x N_{x} for nodes and N r N_{r} for edges, two hidden layers of 64 units each, and an output layer of size 64. The sets of trainable parameters are identified by Œò v \\\\Theta^{v} and Œò e \\\\Theta^{e} , and are optimized during training to learn an effective way to embed physical properties into the latent space. This operation can be represented as shown in Equation 10 :\\nHere, ùêØ i t \\\\mathbf{v}_{i}^{t} and ùêû i \\u200b j t \\\\mathbf{e}_{ij}^{t} represent the node and edge feature vectors in the latent space.\\nFigure 10 shows the encoding procedure for a node and an edge of a sample graph. The result of these operations is a latent graph ùí¢ 0 = ( ùëΩ , ùë¨ ) \\\\mathcal{G}^{0}=\\\\left(\\\\bm{V},\\\\bm{E}\\\\right) ; the superscript 0 indicates that this is the initial state representation, prior to performing any message-passing step.\\nThe processor performs the message passing operations, a simplified representation of which is shown in Figure 2 (c).\\nThe processor takes as input the initial graph ùí¢ m \\\\mathcal{G}^{m} , where the index m m denotes the current message-passing step. This index starts from m = 0 m=0 (the encoder output) and is incremented at each iteration until a user-defined number M M of steps is reached. Message passing begins with the message construction phase, illustrated in Figure 5 and described by Equation 11 . In this phase, each edge feature vector is updated by combining its current value with the feature vectors of the two nodes it connects, all evaluated at the current iteration. In our model, this combination is implemented by an MLP with two hidden layers of 64 units each. The input layer has size 3 \\u200b n 3n , since the MLP input is the concatenation of the three feature vectors involved in constructing the message for each edge. The output layer has size n n , corresponding to the latent space dimension, in order to preserve consistency in the feature vector dimensions across the graph entities. This operation can be generalized as:\\nHere, œï m \\\\phi^{m} denotes the MLP used at the m m -th message-passing step, and ùöØ m œï \\\\mathbf{\\\\Theta}^{\\\\phi}_{m} is the set of trainable parameters associated with it. The operation is repeated for all edges in the graph.\\nThe output of the message construction phase is then used for message aggregation, as illustrated in Figure 6 and described by Equation 12 . For each node i i in the graph, an aggregated message is constructed by element-wise summing the feature vectors of the edges connected to it. This is expressed as:\\nFor each node, the aggregated message is computed and stored together with its previous feature representation in the graph.\\nFinally, the node features are updated, as illustrated in Figure 7 and by Equation 13 . This step involves combining the old node feature vector with the aggregated message to obtain the updated node feature vector. In our model, following the structure from Sanchez-Gonzalez et\\xa0al. ( 2020 ) , this operation is performed by an MLP with two hidden layers of 64 units each. The input layer has size 2 \\u200b n 2n (i.e., two times the latent space dimension, since the aggregated message has dimension n n ), and the output layer has dimension n n . This operation is defined as:\\nHere, Œ≥ m \\\\gamma^{m} is an operator representing the MLP associated with the current message-passing step, and ùöØ Œ≥ \\\\mathbf{\\\\Theta}_{\\\\gamma} represents the set of trainable parameters associated with the MLP. The operation is repeated for all nodes in the graph.\\nIn our model, M M is set to 10, as in Choi & Kumar ( 2024 ) ‚Äôs work. Two main aspects cause this hyperparameter to be particularly critical for the model‚Äôs performance. The first consideration concerns information propagation: a lower M M may not allow information to reach distant parts of the graph, potentially missing important relational data. Conversely, too many message passing rounds can distort relevant signals, leading to less effective representations. Another important consideration is model complexity. Each message passing step uses its own unique MLPs. This means that with every additional step, new parameters are introduced. As a result, the number of parameters grows linearly with the number of message passing steps, and the computational cost increases as each MLP must be executed in sequence.\\nAt the end of all the prescribed message passing steps, the output of the processor block is an updated graph ùí¢ ‚Ä≤ = ùí¢ M \\\\mathcal{G}^{\\\\prime}=\\\\mathcal{G}^{M} containing the update node feature vector ùêØ i ‚Ä≤ {\\\\mathbf{v}^{\\\\prime}_{i}} and the updated edge feature vector ùêû i \\u200b j ‚Ä≤ {\\\\mathbf{e}^{\\\\prime}_{ij}} .\\nThe decoder extracts the dynamics of all points in the physical system, which correspond one-to-one to the graph nodes, from the information stored in the updated graph ùí¢ ‚Ä≤ \\\\mathcal{G}^{\\\\prime} . In other words, the extracted information represents the output of the procedure and corresponds to the prediction for the next time step, t + 1 t+1 , as shown in Equation 14 :\\nHere, ùêØ i ‚Ä≤ \\\\mathbf{v}^{\\\\prime}_{i} represents the updated node features of node i i , while ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} represents the predicted dynamics. For structural simulations, the model is trained to predict the acceleration of all points in the physical system. Accordingly, ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} is a vector of dimension d d ,matching the dimension of the physical space. The operator Œ¥ ùöØ v \\\\delta^{v}_{\\\\mathbf{\\\\Theta}} is the decoder: in our model, this function is an MLP with two hidden layers of 128 units each, with an input layer having the size of the latent space (i.e. n n ), and an output layer having size d d .\\nThe decoding process is illustrated in Figure 8 .\\nThe updater enforces an inertial frame by incorporating the laws of motion through inertial and static priors. The inertial prior assumes that the velocity of each point changes predictably based on its acceleration over a short time interval (Equation 15 ), while the static prior assumes that the position of each point evolves continuously with its current velocity (Equation 16 ). These priors are enforced by hardcoding the updater to calculate the new velocity and position via Euler integration:\\nwhere ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} is the predicted acceleration of point i i .\\nOur framework relies on two core procedures: training and rollout . The training phase produces a learned model capable of predicting positions, while the rollout phase acts as a solver, generating full trajectories from user-provided input data.\\nThe training procedure is summarized in the pseudo-code shown in Algorithm 1 .\\nAs in traditional ML algorithms, training proceeds over successive epochs. At each epoch, the sampler extracts a batch of data from the training set. Each batch consists of B B samples, randomly selected from all timesteps across all trajectories. Figure 9 illustrates the batching operation for a representative dataset with two trajectories of five timesteps each. In this example, each batch contains B = 2 B=2 random samples across all the available data. In the interest of simplicity, each sample consists of the system configuration at timestep t t together with that at timestep t + 1 t+1 . However, in practical applications, information from several previous timesteps is often included to improve temporal context and prediction accuracy. The configuration at t t is used as input to the model, while the acceleration required to evolve the system from configuration t t to configuration t + 1 t+1 ( ùê≤ ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}} ) serves as the ground truth (label) for the current training step.\\nFrom the input configuration (positions), velocities are extracted via finite differences. To improve robustness against error accumulation and instabilities during rollout, Gaussian noise is added to all computed velocities in the training samples, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) . This regularization strategy encourages the model to generalize beyond the exact training trajectories and mitigates the buildup of prediction errors over time.\\nAfter noise injection, GNSS encodes the system configuration at timestep t t from each sample into a graph, predicts the corresponding acceleration ( ùê≤ \\\\mathbf{y} ), and compares it with the ground truth ( ùê≤ ^ \\\\hat{\\\\mathbf{y}} ). Backpropagation is then used to update the weights of all MLPs and the embedding layer in order to minimize a user-defined loss function. To further improve generalization in long rollouts, we propose a novel loss function, the weighted Mean Squared Error (wMSE) shown in Equation 17 : a modified version of the Mean Squared Error(MSE) that penalizes acceleration predictions with incorrect sign through a scalar weight s s , defined as:\\nHere, the subscript i indicates the i ‚àí t \\u200b h i-th node; the final loss is computed as the mean squared value of Œµ i \\\\varepsilon_{i} over all predictions in the batch.\\nAfter training, predictions are generated in an autoregressive manner: the initial state is encoded as a graph, and GNSS is iteratively applied to predict the full trajectory. The number of iterations N T \\\\rm{N_{T}} is defined by the user to cover the entire physical timeframe of interest, with the understanding that the physical timestep size is fixed and identical to that used to generate the training dataset. The corresponding pseudo-code for the rollout procedure is reported in Algorithm 2 .',\n",
       "    'subsections': [{'title': '2.1 Graph Networks',\n",
       "      'paragraphs': 'A graph ùí¢ \\\\mathcal{G} consists of a set of nodes (or vertices) and a set of edges that define connections between nodes. Formally, a graph can be represented using an adjacency matrix ùë® ‚àà ‚Ñù N √ó N \\\\bm{A}\\\\in\\\\mathbb{R}^{N\\\\times N} , where N N is the number of nodes, and A i \\u200b j ‚â† 0 A_{ij}\\\\neq 0 indicates an edge between nodes i i and j j . The degree of a node is defined as the number of edges incident to that node. Beyond this topological knowledge, graphs can include additional information in the form of node features ùëΩ ‚àà ‚Ñù N √ó d \\\\bm{V}\\\\in\\\\mathbb{R}^{N\\\\times d} and edge features ùë¨ ‚àà ‚Ñù | ùë¨ | √ó k \\\\bm{E}\\\\in\\\\mathbb{R}^{|\\\\bm{E}|\\\\times k} , where d d and k k are the feature dimensions (i.e., the length of the vector) of nodes and edges, respectively (Gong & Cheng, 2019 ; Yang & Li, 2020 ; Chen & Chen, 2021 ) . The node feature vector associated with node n n , denoted by ùê± n ‚àà ‚Ñù d \\\\mathbf{x}_{n}\\\\in\\\\mathbb{R}^{d} , represents some measurable properties of the node. Similarly, the edge feature vectors encode the relationship between connected nodes. Given the feature vectors, the graph can be defined as G = ( ùëΩ , ùë¨ ) G=(\\\\bm{V},\\\\bm{E})\\nEarly neural architectures for graphs, such as the GNN model proposed in Scarselli et\\xa0al. ( 2009 ) ‚Äôs work, extended neural computation to structured graph domains using an iterative, equilibrium-based message-passing scheme. Each node updates its state by aggregating information from its neighbors until convergence. These models have proven effective for representing physical systems with complex, irregular topologies, where traditional architectures like Convolutional Neural Networks (CNNs), designed for structured grid-like data, are inadequate. GNNs generalize the concept of convolution to graph domains, making them well-suited for applications in fluid dynamics, structural mechanics, and materials science (Battaglia et\\xa0al., 2018 ; Pfaff et\\xa0al., 2020 ; Wong et\\xa0al., 2022 ; Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ; Li et\\xa0al., 2023 ; Zhao et\\xa0al., 2023 ; Gulakala et\\xa0al., 2023 ; Shivaditya et\\xa0al., 2022 ; Bronstein et\\xa0al., 2017 ; Wu et\\xa0al., 2020 ) .\\nA key strength of GNNs in modeling physical systems lies in their inductive biases. These include the representation of local interactions through message passing, the sharing of weights across the graph, and invariance to node permutations. Such biases naturally reflect physical principles such as locality, translational symmetry, and conservation laws (Battaglia et\\xa0al., 2018 ) .\\nThe GN framework (Battaglia et\\xa0al., 2018 ) generalizes and unifies multiple GNN-based architectures such as message-passing neural networks (MPNNs), interaction networks, and relation networks. Unlike classical GNNs, GNs explicitly decompose the graph update process into three modular functions: edge update, node update, and global update. This modular graph-to-graph transformation enables flexible and expressive modeling capabilities, which are particularly beneficial in physical simulation tasks (Sanchez-Gonzalez et\\xa0al., 2020 ) .\\nGiven as input a system‚Äôs state described by a graph ùíÆ = ( ùëø , ùëπ ) \\\\mathcal{S}=(\\\\bm{X},\\\\bm{R}) , a GN produces an updated graph ùíÆ ‚Ä≤ = ( ùëø ‚Ä≤ , ùëπ ‚Ä≤ ) \\\\mathcal{S}^{\\\\prime}=(\\\\bm{X}^{\\\\prime},\\\\bm{R}^{\\\\prime}) of identical topology representing the updated state of the system. ùëø \\\\bm{X} and ùëπ \\\\bm{R} describe the physical properties and relationships of the system, respectively. The architecture typically follows an encode-process-decode paradigm (Battaglia et\\xa0al., 2018 ) , where raw inputs are encoded into latent node and edge embeddings, iteratively processed through message passing, and decoded into physically meaningful outputs.\\nThe encoding phase is described in Equation 1 . During this step, the physical properties and relationship describing the state are embedded into a higher-dimensional continuous space:\\nHere, N N is the number of nodes, d d is the number of properties for each node, k k is the number of relationships for each edge, and the h h -dimensional space is the latent space . This initial transformation not only homogenizes the feature space, but also allows the network to capture local patterns that are crucial for the learning task (Scarselli et\\xa0al., 2009 ; Gilmer et\\xa0al., 2017 ) .\\nMessage passing is the phase during which information is processed and exchanged among the different nodes of the system. It consists of updating the latent features of the graph through three steps:\\nMessage Construction (Equation 2 ): The updated edge features (also called message ) are obtained by combining the old information of the edge and the two nodes connected to it:\\nMessage Aggregation (Equation 3 ): For each node, an aggregated message is computed; this is obtained by combining all the messages related to node i i :\\nHere, ùí© \\u200b ( i ) \\\\mathcal{N}(i) represents the set of nodes linked to node i i .\\nNode Update (Equation 4 ): During this final step, the old node feature vector is combined with the aggregated message to obtain the updated node features:\\nEquation 5 describes the entire message passing pipeline concisely:\\nIn Equations 2 ‚Äì 5 , the symbols œï , œà , Œ≥ \\\\phi,\\\\,\\\\psi,\\\\,\\\\gamma represent arbitrary operands which are part of the architectural choice when designing a GN.\\nThe whole message passing procedure is typically repeated M > 1 M>1 times. This hyperparameter defines the degree of information propagation through the network, and thus the depth of the GN, as visually represented in Figure 1 . Information spread is usually represented through a computation graph, which describes how the embedding of each node is iteratively updated through message passing. Each layer of the computation graph carries out a message-passing operation; by applying this process recursively across layers, the network is able to represent both local interactions and more complex, higher-order relationships within the graph (Gilmer et\\xa0al., 2017 ; Battaglia et\\xa0al., 2018 ) .\\nDuring the decoding phase, the information stored in the latent representation is converted to task-specific predictions, such as node-level, edge-level, or graph-level outputs (Battaglia et\\xa0al., 2018 ) . For example, referring to the case of GNS and MeshGraphNet, the decoder performs a single node-level task by transforming the updated node features into a physical quantity at each node, the acceleration.',\n",
       "      'subsections': [{'title': 'Encoding',\n",
       "        'paragraphs': 'The encoding phase is described in Equation 1 . During this step, the physical properties and relationship describing the state are embedded into a higher-dimensional continuous space:\\nHere, N N is the number of nodes, d d is the number of properties for each node, k k is the number of relationships for each edge, and the h h -dimensional space is the latent space . This initial transformation not only homogenizes the feature space, but also allows the network to capture local patterns that are crucial for the learning task (Scarselli et\\xa0al., 2009 ; Gilmer et\\xa0al., 2017 ) .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Message Passing',\n",
       "        'paragraphs': 'Message passing is the phase during which information is processed and exchanged among the different nodes of the system. It consists of updating the latent features of the graph through three steps:\\nMessage Construction (Equation 2 ): The updated edge features (also called message ) are obtained by combining the old information of the edge and the two nodes connected to it:\\nMessage Aggregation (Equation 3 ): For each node, an aggregated message is computed; this is obtained by combining all the messages related to node i i :\\nHere, ùí© \\u200b ( i ) \\\\mathcal{N}(i) represents the set of nodes linked to node i i .\\nNode Update (Equation 4 ): During this final step, the old node feature vector is combined with the aggregated message to obtain the updated node features:\\nEquation 5 describes the entire message passing pipeline concisely:\\nIn Equations 2 ‚Äì 5 , the symbols œï , œà , Œ≥ \\\\phi,\\\\,\\\\psi,\\\\,\\\\gamma represent arbitrary operands which are part of the architectural choice when designing a GN.\\nThe whole message passing procedure is typically repeated M > 1 M>1 times. This hyperparameter defines the degree of information propagation through the network, and thus the depth of the GN, as visually represented in Figure 1 . Information spread is usually represented through a computation graph, which describes how the embedding of each node is iteratively updated through message passing. Each layer of the computation graph carries out a message-passing operation; by applying this process recursively across layers, the network is able to represent both local interactions and more complex, higher-order relationships within the graph (Gilmer et\\xa0al., 2017 ; Battaglia et\\xa0al., 2018 ) .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Decoding',\n",
       "        'paragraphs': 'During the decoding phase, the information stored in the latent representation is converted to task-specific predictions, such as node-level, edge-level, or graph-level outputs (Battaglia et\\xa0al., 2018 ) . For example, referring to the case of GNS and MeshGraphNet, the decoder performs a single node-level task by transforming the updated node features into a physical quantity at each node, the acceleration.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '2.2 GNSS',\n",
       "      'paragraphs': 'In Figure 2 , our GNSS is presented. This framework is a modified version of the GNS developed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , specifically adapted for structural simulations. Figure 2 (a) shows the Rollout , namely the procedure used to simulate the trajectory of the system: an initial configuration X 0 X_{0} is provided, then the GNSS is iteratively applied T Œî \\u200b t p \\u200b h \\\\frac{T}{\\\\Delta t_{ph}} times to predict the trajectory from physical time 0 to physical time T T . Œî \\u200b t ph \\\\Delta t_{\\\\mathrm{ph}} is the fixed timestep size adopted in the numerical simulations used to generate all trajectories in the dataset. Figure 2 (b) expands the operation involved at each time step t t , during which the GNSS performs the whole encode-process-decode paradigm to retrieve the updated nodal information (accelerations). Lastly, Figure 2 (c) details the processing phase, i.e., message passing, showing the procedure for a single passing step; as introduced above, message passing is recursively repeated M M times.\\nIn what follows, we detail the GNSS operations shown in Figure 2 (b)‚Äì(c).\\nThe pipeline ‚Äî from graph construction to position update ‚Äî is described for a single\\nsimulation time step t t ; accordingly, all intermediate quantities are evaluated at t t ,\\nand the output corresponds to the updated state at t + 1 t+1 . Because a single GNSS pass\\nperforms multiple rounds of message passing, we denote the m m -th round by the\\nsuperscript (m) . To avoid clutter, we omit the time index; unless otherwise stated,\\nall expressions are understood at time t t .\\nThe pre-processing phase is responsible for representing the information stored in the initial configuration of the system into a graph structure ready to be processed by the GN.\\nThe representation of the system naturally follows from the choice of a graph-based model. The continuous structure is discretized into a set of points , each capturing the local physical properties of its neighborhood, while the interactions among these points are expressed as relationships . Throughout the following description, we will denote a point in the discretized structure as x x , and its associated relationship as r i \\u200b j r_{ij} , representing the underlying physical properties. In contrast, the terms node v i v_{i} and edge e i \\u200b j e_{ij} will refer to the corresponding latent features, i.e., the latent graph representation used within the model.\\nRegarding the relationship between those points, the primary idea behind the graph construction proposed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) is that, typically, entities in physical systems are influenced by their neighbors. This natural behavior is reflected in the graph structure by establishing edges only between nodes that are sufficiently close. In practice, this is achieved by defining a connectivity radius that sets the maximum allowable distance for two nodes to be connected, as shown in Figure 3 .\\nOnce the graph‚Äôs topology is defined, each node i i of the graph is associated with a feature vector storing physical information. Following the implementation proposed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , the point feature vector stores information about the velocity context and the type of particle related to the node. As demonstrated by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , including not only the current velocity of a point, but also the velocities from previous timesteps, greatly improves the performance of the model. This information is stored in a tensor as shown in Equation 6 :\\nHere, t t is the current timestep, and ùê© Àô i ( t ) \\\\prescript{(t)}{}{\\\\dot{\\\\mathbf{p}}}_{i} is the velocity of point i i at time t t , d d is the dimension of the space, and n n is the number of previous velocities considered (in addition to the current one). This is a hyperparameter, and is usually set to n = 4 n=4 as suggested by Choi & Kumar ( 2024 ) . The velocities are calculated via finite differences as shown in Equation 7 :\\nwhere ùê© i ( t ) \\\\prescript{(t)}{}{{\\\\mathbf{p}}}_{i} is the position of point i i at time t t and Œî \\u200b t \\\\Delta t is the timestep size. The timestep size used in the algorithm is set to 1, as non-dimensional time quantities are preferred during prediction. This unit timestep size corresponds to a physical timestep Œî \\u200b t p \\u200b h \\\\Delta t_{ph} implicitly defined by the one used in the training dataset. Consequently, the timestep size adopted in the numerical simulations must remain fixed and identical across all trajectories in the dataset.\\nThe framework is designed to account for different types of particles. This information is required by the model to correctly handle entities playing different roles in the simulation, and is stored in a vector ùêü \\\\mathbf{f} . This vector has 16 components and is created through an embedding layer.\\nIn our model, we exploit this structure to inform the prediction mechanism about the constraint of the structural simulation. Thanks to this choice, we can simply assign arbitrary IDs to different types of constraints, and the model will optimize the weights of the embedding layer (i.e., the 16 components of ùêü \\\\mathbf{f} ) to best represent the different entities according to their behavior. For example, we can assign an ID to the free portion (nodes) of the structure, another ID to nodes where a prescribed motion is applied, and another ID to the region of application of BCs. This choice allows us to simplify the feature vector proposed by Choi & Kumar ( 2024 ) by eliminating the information about the distance from the boundaries that was given for each node. Additionally, this allows us to model different types of boundaries by assigning different IDs to them (clamp, pin, etc.).\\nAll the properties listed above are included in a point state vector, as shown in Equation 8 :\\nHere, ùê© Àô i ‚â§ t \\\\dot{\\\\mathbf{p}}_{i}^{\\\\leq t} is the flattened vector of velocities history. For a representative 2D case, considering the current velocity and 4 additional velocities and the previous timesteps, the point feature vector is composed of 26 elements: 5 components for velocity along direction x x , 5 for velocity along direction y y , and 16 for the type embedding.\\nMoreover, the interaction between connected points i i and j j is represented by their distance in the physical space and their displacement in the current timestep, both normalized by the connectivity radius R R , as shown in Equation 9 :\\nThe distance provides information about the level of interaction between the two points, while the displacement gives a direction to the relationship.\\nTypically, GNs represent displacements in an absolute coordinate system shared by the entire physical system (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) . While this approach is well-suited to domains commonly simulated with GNs, such as granular flow and fluid dynamics, it can be problematic in structural mechanics. In structural simulations, load-induced displacements are often several orders of magnitude smaller than the characteristic dimensions of the system. This scale mismatch can cause numerical issues when derivatives (e.g., velocities) are computed via finite differences. In particular, subtracting two large, nearly equal floating-point numbers (absolute positions at successive timesteps) may lead to a loss of significant digits, a phenomenon known as catastrophic cancellation (Higham, 2002 ) . The resulting numerical noise reduces the accuracy of computed derivatives, and the problem becomes critical when rounding errors are comparable to, or larger than, the displacements being resolved. This limitation is well recognized in precision-sensitive fields such as structural dynamics and geomechanics, where floating-point inaccuracies can distort small-strain calculations (Belytschko et\\xa0al., 2014 ) .\\nTo overcome this limitation, GNSS introduces a novel representation of nodal positions in local coordinate systems that are fixed in both space and time. For each node, the origin is set at its initial position (i.e., its location at time zero), effectively centering all subsequent displacements around zero. By eliminating large absolute position values, this representation mitigates catastrophic cancellation and ensures stable, accurate derivative computations via finite differences. This formulation constitutes a key feature of GNSS, enabling reliable surrogate modeling of structural dynamics where traditional GN formulations fail.\\nThe encoder is responsible for transforming the input graph ùí¢ \\\\mathcal{G} into a latent graph ùí¢ 0 \\\\mathcal{G}_{0} , in which the old physical feature vectors are embedded into a latent space. In our implementation, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) ‚Äôs work, this operation is performed by two multilayer perceptrons (MLPs), œµ Œò v v \\\\epsilon^{v}_{\\\\Theta^{v}} and œµ Œò e e \\\\epsilon^{e}_{\\\\Theta^{e}} , which embed the feature vectors associated with nodes and edges, respectively. The two MLPs share the same architecture: an input layer with dimensions N x N_{x} for nodes and N r N_{r} for edges, two hidden layers of 64 units each, and an output layer of size 64. The sets of trainable parameters are identified by Œò v \\\\Theta^{v} and Œò e \\\\Theta^{e} , and are optimized during training to learn an effective way to embed physical properties into the latent space. This operation can be represented as shown in Equation 10 :\\nHere, ùêØ i t \\\\mathbf{v}_{i}^{t} and ùêû i \\u200b j t \\\\mathbf{e}_{ij}^{t} represent the node and edge feature vectors in the latent space.\\nFigure 10 shows the encoding procedure for a node and an edge of a sample graph. The result of these operations is a latent graph ùí¢ 0 = ( ùëΩ , ùë¨ ) \\\\mathcal{G}^{0}=\\\\left(\\\\bm{V},\\\\bm{E}\\\\right) ; the superscript 0 indicates that this is the initial state representation, prior to performing any message-passing step.\\nThe processor performs the message passing operations, a simplified representation of which is shown in Figure 2 (c).\\nThe processor takes as input the initial graph ùí¢ m \\\\mathcal{G}^{m} , where the index m m denotes the current message-passing step. This index starts from m = 0 m=0 (the encoder output) and is incremented at each iteration until a user-defined number M M of steps is reached. Message passing begins with the message construction phase, illustrated in Figure 5 and described by Equation 11 . In this phase, each edge feature vector is updated by combining its current value with the feature vectors of the two nodes it connects, all evaluated at the current iteration. In our model, this combination is implemented by an MLP with two hidden layers of 64 units each. The input layer has size 3 \\u200b n 3n , since the MLP input is the concatenation of the three feature vectors involved in constructing the message for each edge. The output layer has size n n , corresponding to the latent space dimension, in order to preserve consistency in the feature vector dimensions across the graph entities. This operation can be generalized as:\\nHere, œï m \\\\phi^{m} denotes the MLP used at the m m -th message-passing step, and ùöØ m œï \\\\mathbf{\\\\Theta}^{\\\\phi}_{m} is the set of trainable parameters associated with it. The operation is repeated for all edges in the graph.\\nThe output of the message construction phase is then used for message aggregation, as illustrated in Figure 6 and described by Equation 12 . For each node i i in the graph, an aggregated message is constructed by element-wise summing the feature vectors of the edges connected to it. This is expressed as:\\nFor each node, the aggregated message is computed and stored together with its previous feature representation in the graph.\\nFinally, the node features are updated, as illustrated in Figure 7 and by Equation 13 . This step involves combining the old node feature vector with the aggregated message to obtain the updated node feature vector. In our model, following the structure from Sanchez-Gonzalez et\\xa0al. ( 2020 ) , this operation is performed by an MLP with two hidden layers of 64 units each. The input layer has size 2 \\u200b n 2n (i.e., two times the latent space dimension, since the aggregated message has dimension n n ), and the output layer has dimension n n . This operation is defined as:\\nHere, Œ≥ m \\\\gamma^{m} is an operator representing the MLP associated with the current message-passing step, and ùöØ Œ≥ \\\\mathbf{\\\\Theta}_{\\\\gamma} represents the set of trainable parameters associated with the MLP. The operation is repeated for all nodes in the graph.\\nIn our model, M M is set to 10, as in Choi & Kumar ( 2024 ) ‚Äôs work. Two main aspects cause this hyperparameter to be particularly critical for the model‚Äôs performance. The first consideration concerns information propagation: a lower M M may not allow information to reach distant parts of the graph, potentially missing important relational data. Conversely, too many message passing rounds can distort relevant signals, leading to less effective representations. Another important consideration is model complexity. Each message passing step uses its own unique MLPs. This means that with every additional step, new parameters are introduced. As a result, the number of parameters grows linearly with the number of message passing steps, and the computational cost increases as each MLP must be executed in sequence.\\nAt the end of all the prescribed message passing steps, the output of the processor block is an updated graph ùí¢ ‚Ä≤ = ùí¢ M \\\\mathcal{G}^{\\\\prime}=\\\\mathcal{G}^{M} containing the update node feature vector ùêØ i ‚Ä≤ {\\\\mathbf{v}^{\\\\prime}_{i}} and the updated edge feature vector ùêû i \\u200b j ‚Ä≤ {\\\\mathbf{e}^{\\\\prime}_{ij}} .\\nThe decoder extracts the dynamics of all points in the physical system, which correspond one-to-one to the graph nodes, from the information stored in the updated graph ùí¢ ‚Ä≤ \\\\mathcal{G}^{\\\\prime} . In other words, the extracted information represents the output of the procedure and corresponds to the prediction for the next time step, t + 1 t+1 , as shown in Equation 14 :\\nHere, ùêØ i ‚Ä≤ \\\\mathbf{v}^{\\\\prime}_{i} represents the updated node features of node i i , while ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} represents the predicted dynamics. For structural simulations, the model is trained to predict the acceleration of all points in the physical system. Accordingly, ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} is a vector of dimension d d ,matching the dimension of the physical space. The operator Œ¥ ùöØ v \\\\delta^{v}_{\\\\mathbf{\\\\Theta}} is the decoder: in our model, this function is an MLP with two hidden layers of 128 units each, with an input layer having the size of the latent space (i.e. n n ), and an output layer having size d d .\\nThe decoding process is illustrated in Figure 8 .\\nThe updater enforces an inertial frame by incorporating the laws of motion through inertial and static priors. The inertial prior assumes that the velocity of each point changes predictably based on its acceleration over a short time interval (Equation 15 ), while the static prior assumes that the position of each point evolves continuously with its current velocity (Equation 16 ). These priors are enforced by hardcoding the updater to calculate the new velocity and position via Euler integration:\\nwhere ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} is the predicted acceleration of point i i .\\nOur framework relies on two core procedures: training and rollout . The training phase produces a learned model capable of predicting positions, while the rollout phase acts as a solver, generating full trajectories from user-provided input data.\\nThe training procedure is summarized in the pseudo-code shown in Algorithm 1 .\\nAs in traditional ML algorithms, training proceeds over successive epochs. At each epoch, the sampler extracts a batch of data from the training set. Each batch consists of B B samples, randomly selected from all timesteps across all trajectories. Figure 9 illustrates the batching operation for a representative dataset with two trajectories of five timesteps each. In this example, each batch contains B = 2 B=2 random samples across all the available data. In the interest of simplicity, each sample consists of the system configuration at timestep t t together with that at timestep t + 1 t+1 . However, in practical applications, information from several previous timesteps is often included to improve temporal context and prediction accuracy. The configuration at t t is used as input to the model, while the acceleration required to evolve the system from configuration t t to configuration t + 1 t+1 ( ùê≤ ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}} ) serves as the ground truth (label) for the current training step.\\nFrom the input configuration (positions), velocities are extracted via finite differences. To improve robustness against error accumulation and instabilities during rollout, Gaussian noise is added to all computed velocities in the training samples, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) . This regularization strategy encourages the model to generalize beyond the exact training trajectories and mitigates the buildup of prediction errors over time.\\nAfter noise injection, GNSS encodes the system configuration at timestep t t from each sample into a graph, predicts the corresponding acceleration ( ùê≤ \\\\mathbf{y} ), and compares it with the ground truth ( ùê≤ ^ \\\\hat{\\\\mathbf{y}} ). Backpropagation is then used to update the weights of all MLPs and the embedding layer in order to minimize a user-defined loss function. To further improve generalization in long rollouts, we propose a novel loss function, the weighted Mean Squared Error (wMSE) shown in Equation 17 : a modified version of the Mean Squared Error(MSE) that penalizes acceleration predictions with incorrect sign through a scalar weight s s , defined as:\\nHere, the subscript i indicates the i ‚àí t \\u200b h i-th node; the final loss is computed as the mean squared value of Œµ i \\\\varepsilon_{i} over all predictions in the batch.\\nAfter training, predictions are generated in an autoregressive manner: the initial state is encoded as a graph, and GNSS is iteratively applied to predict the full trajectory. The number of iterations N T \\\\rm{N_{T}} is defined by the user to cover the entire physical timeframe of interest, with the understanding that the physical timestep size is fixed and identical to that used to generate the training dataset. The corresponding pseudo-code for the rollout procedure is reported in Algorithm 2 .',\n",
       "      'subsections': [{'title': 'Pre-processing: graph definition',\n",
       "        'paragraphs': 'The pre-processing phase is responsible for representing the information stored in the initial configuration of the system into a graph structure ready to be processed by the GN.\\nThe representation of the system naturally follows from the choice of a graph-based model. The continuous structure is discretized into a set of points , each capturing the local physical properties of its neighborhood, while the interactions among these points are expressed as relationships . Throughout the following description, we will denote a point in the discretized structure as x x , and its associated relationship as r i \\u200b j r_{ij} , representing the underlying physical properties. In contrast, the terms node v i v_{i} and edge e i \\u200b j e_{ij} will refer to the corresponding latent features, i.e., the latent graph representation used within the model.\\nRegarding the relationship between those points, the primary idea behind the graph construction proposed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) is that, typically, entities in physical systems are influenced by their neighbors. This natural behavior is reflected in the graph structure by establishing edges only between nodes that are sufficiently close. In practice, this is achieved by defining a connectivity radius that sets the maximum allowable distance for two nodes to be connected, as shown in Figure 3 .\\nOnce the graph‚Äôs topology is defined, each node i i of the graph is associated with a feature vector storing physical information. Following the implementation proposed by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , the point feature vector stores information about the velocity context and the type of particle related to the node. As demonstrated by Sanchez-Gonzalez et\\xa0al. ( 2020 ) , including not only the current velocity of a point, but also the velocities from previous timesteps, greatly improves the performance of the model. This information is stored in a tensor as shown in Equation 6 :\\nHere, t t is the current timestep, and ùê© Àô i ( t ) \\\\prescript{(t)}{}{\\\\dot{\\\\mathbf{p}}}_{i} is the velocity of point i i at time t t , d d is the dimension of the space, and n n is the number of previous velocities considered (in addition to the current one). This is a hyperparameter, and is usually set to n = 4 n=4 as suggested by Choi & Kumar ( 2024 ) . The velocities are calculated via finite differences as shown in Equation 7 :\\nwhere ùê© i ( t ) \\\\prescript{(t)}{}{{\\\\mathbf{p}}}_{i} is the position of point i i at time t t and Œî \\u200b t \\\\Delta t is the timestep size. The timestep size used in the algorithm is set to 1, as non-dimensional time quantities are preferred during prediction. This unit timestep size corresponds to a physical timestep Œî \\u200b t p \\u200b h \\\\Delta t_{ph} implicitly defined by the one used in the training dataset. Consequently, the timestep size adopted in the numerical simulations must remain fixed and identical across all trajectories in the dataset.\\nThe framework is designed to account for different types of particles. This information is required by the model to correctly handle entities playing different roles in the simulation, and is stored in a vector ùêü \\\\mathbf{f} . This vector has 16 components and is created through an embedding layer.\\nIn our model, we exploit this structure to inform the prediction mechanism about the constraint of the structural simulation. Thanks to this choice, we can simply assign arbitrary IDs to different types of constraints, and the model will optimize the weights of the embedding layer (i.e., the 16 components of ùêü \\\\mathbf{f} ) to best represent the different entities according to their behavior. For example, we can assign an ID to the free portion (nodes) of the structure, another ID to nodes where a prescribed motion is applied, and another ID to the region of application of BCs. This choice allows us to simplify the feature vector proposed by Choi & Kumar ( 2024 ) by eliminating the information about the distance from the boundaries that was given for each node. Additionally, this allows us to model different types of boundaries by assigning different IDs to them (clamp, pin, etc.).\\nAll the properties listed above are included in a point state vector, as shown in Equation 8 :\\nHere, ùê© Àô i ‚â§ t \\\\dot{\\\\mathbf{p}}_{i}^{\\\\leq t} is the flattened vector of velocities history. For a representative 2D case, considering the current velocity and 4 additional velocities and the previous timesteps, the point feature vector is composed of 26 elements: 5 components for velocity along direction x x , 5 for velocity along direction y y , and 16 for the type embedding.\\nMoreover, the interaction between connected points i i and j j is represented by their distance in the physical space and their displacement in the current timestep, both normalized by the connectivity radius R R , as shown in Equation 9 :\\nThe distance provides information about the level of interaction between the two points, while the displacement gives a direction to the relationship.\\nTypically, GNs represent displacements in an absolute coordinate system shared by the entire physical system (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) . While this approach is well-suited to domains commonly simulated with GNs, such as granular flow and fluid dynamics, it can be problematic in structural mechanics. In structural simulations, load-induced displacements are often several orders of magnitude smaller than the characteristic dimensions of the system. This scale mismatch can cause numerical issues when derivatives (e.g., velocities) are computed via finite differences. In particular, subtracting two large, nearly equal floating-point numbers (absolute positions at successive timesteps) may lead to a loss of significant digits, a phenomenon known as catastrophic cancellation (Higham, 2002 ) . The resulting numerical noise reduces the accuracy of computed derivatives, and the problem becomes critical when rounding errors are comparable to, or larger than, the displacements being resolved. This limitation is well recognized in precision-sensitive fields such as structural dynamics and geomechanics, where floating-point inaccuracies can distort small-strain calculations (Belytschko et\\xa0al., 2014 ) .\\nTo overcome this limitation, GNSS introduces a novel representation of nodal positions in local coordinate systems that are fixed in both space and time. For each node, the origin is set at its initial position (i.e., its location at time zero), effectively centering all subsequent displacements around zero. By eliminating large absolute position values, this representation mitigates catastrophic cancellation and ensures stable, accurate derivative computations via finite differences. This formulation constitutes a key feature of GNSS, enabling reliable surrogate modeling of structural dynamics where traditional GN formulations fail.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Encoder',\n",
       "        'paragraphs': 'The encoder is responsible for transforming the input graph ùí¢ \\\\mathcal{G} into a latent graph ùí¢ 0 \\\\mathcal{G}_{0} , in which the old physical feature vectors are embedded into a latent space. In our implementation, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) ‚Äôs work, this operation is performed by two multilayer perceptrons (MLPs), œµ Œò v v \\\\epsilon^{v}_{\\\\Theta^{v}} and œµ Œò e e \\\\epsilon^{e}_{\\\\Theta^{e}} , which embed the feature vectors associated with nodes and edges, respectively. The two MLPs share the same architecture: an input layer with dimensions N x N_{x} for nodes and N r N_{r} for edges, two hidden layers of 64 units each, and an output layer of size 64. The sets of trainable parameters are identified by Œò v \\\\Theta^{v} and Œò e \\\\Theta^{e} , and are optimized during training to learn an effective way to embed physical properties into the latent space. This operation can be represented as shown in Equation 10 :\\nHere, ùêØ i t \\\\mathbf{v}_{i}^{t} and ùêû i \\u200b j t \\\\mathbf{e}_{ij}^{t} represent the node and edge feature vectors in the latent space.\\nFigure 10 shows the encoding procedure for a node and an edge of a sample graph. The result of these operations is a latent graph ùí¢ 0 = ( ùëΩ , ùë¨ ) \\\\mathcal{G}^{0}=\\\\left(\\\\bm{V},\\\\bm{E}\\\\right) ; the superscript 0 indicates that this is the initial state representation, prior to performing any message-passing step.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Processor',\n",
       "        'paragraphs': 'The processor performs the message passing operations, a simplified representation of which is shown in Figure 2 (c).\\nThe processor takes as input the initial graph ùí¢ m \\\\mathcal{G}^{m} , where the index m m denotes the current message-passing step. This index starts from m = 0 m=0 (the encoder output) and is incremented at each iteration until a user-defined number M M of steps is reached. Message passing begins with the message construction phase, illustrated in Figure 5 and described by Equation 11 . In this phase, each edge feature vector is updated by combining its current value with the feature vectors of the two nodes it connects, all evaluated at the current iteration. In our model, this combination is implemented by an MLP with two hidden layers of 64 units each. The input layer has size 3 \\u200b n 3n , since the MLP input is the concatenation of the three feature vectors involved in constructing the message for each edge. The output layer has size n n , corresponding to the latent space dimension, in order to preserve consistency in the feature vector dimensions across the graph entities. This operation can be generalized as:\\nHere, œï m \\\\phi^{m} denotes the MLP used at the m m -th message-passing step, and ùöØ m œï \\\\mathbf{\\\\Theta}^{\\\\phi}_{m} is the set of trainable parameters associated with it. The operation is repeated for all edges in the graph.\\nThe output of the message construction phase is then used for message aggregation, as illustrated in Figure 6 and described by Equation 12 . For each node i i in the graph, an aggregated message is constructed by element-wise summing the feature vectors of the edges connected to it. This is expressed as:\\nFor each node, the aggregated message is computed and stored together with its previous feature representation in the graph.\\nFinally, the node features are updated, as illustrated in Figure 7 and by Equation 13 . This step involves combining the old node feature vector with the aggregated message to obtain the updated node feature vector. In our model, following the structure from Sanchez-Gonzalez et\\xa0al. ( 2020 ) , this operation is performed by an MLP with two hidden layers of 64 units each. The input layer has size 2 \\u200b n 2n (i.e., two times the latent space dimension, since the aggregated message has dimension n n ), and the output layer has dimension n n . This operation is defined as:\\nHere, Œ≥ m \\\\gamma^{m} is an operator representing the MLP associated with the current message-passing step, and ùöØ Œ≥ \\\\mathbf{\\\\Theta}_{\\\\gamma} represents the set of trainable parameters associated with the MLP. The operation is repeated for all nodes in the graph.\\nIn our model, M M is set to 10, as in Choi & Kumar ( 2024 ) ‚Äôs work. Two main aspects cause this hyperparameter to be particularly critical for the model‚Äôs performance. The first consideration concerns information propagation: a lower M M may not allow information to reach distant parts of the graph, potentially missing important relational data. Conversely, too many message passing rounds can distort relevant signals, leading to less effective representations. Another important consideration is model complexity. Each message passing step uses its own unique MLPs. This means that with every additional step, new parameters are introduced. As a result, the number of parameters grows linearly with the number of message passing steps, and the computational cost increases as each MLP must be executed in sequence.\\nAt the end of all the prescribed message passing steps, the output of the processor block is an updated graph ùí¢ ‚Ä≤ = ùí¢ M \\\\mathcal{G}^{\\\\prime}=\\\\mathcal{G}^{M} containing the update node feature vector ùêØ i ‚Ä≤ {\\\\mathbf{v}^{\\\\prime}_{i}} and the updated edge feature vector ùêû i \\u200b j ‚Ä≤ {\\\\mathbf{e}^{\\\\prime}_{ij}} .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Decoder',\n",
       "        'paragraphs': 'The decoder extracts the dynamics of all points in the physical system, which correspond one-to-one to the graph nodes, from the information stored in the updated graph ùí¢ ‚Ä≤ \\\\mathcal{G}^{\\\\prime} . In other words, the extracted information represents the output of the procedure and corresponds to the prediction for the next time step, t + 1 t+1 , as shown in Equation 14 :\\nHere, ùêØ i ‚Ä≤ \\\\mathbf{v}^{\\\\prime}_{i} represents the updated node features of node i i , while ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} represents the predicted dynamics. For structural simulations, the model is trained to predict the acceleration of all points in the physical system. Accordingly, ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} is a vector of dimension d d ,matching the dimension of the physical space. The operator Œ¥ ùöØ v \\\\delta^{v}_{\\\\mathbf{\\\\Theta}} is the decoder: in our model, this function is an MLP with two hidden layers of 128 units each, with an input layer having the size of the latent space (i.e. n n ), and an output layer having size d d .\\nThe decoding process is illustrated in Figure 8 .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Updater',\n",
       "        'paragraphs': 'The updater enforces an inertial frame by incorporating the laws of motion through inertial and static priors. The inertial prior assumes that the velocity of each point changes predictably based on its acceleration over a short time interval (Equation 15 ), while the static prior assumes that the position of each point evolves continuously with its current velocity (Equation 16 ). These priors are enforced by hardcoding the updater to calculate the new velocity and position via Euler integration:\\nwhere ùê≤ i ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}_{i}} is the predicted acceleration of point i i .',\n",
       "        'subsections': []},\n",
       "       {'title': '2.2.1 Procedures',\n",
       "        'paragraphs': 'Our framework relies on two core procedures: training and rollout . The training phase produces a learned model capable of predicting positions, while the rollout phase acts as a solver, generating full trajectories from user-provided input data.\\nThe training procedure is summarized in the pseudo-code shown in Algorithm 1 .\\nAs in traditional ML algorithms, training proceeds over successive epochs. At each epoch, the sampler extracts a batch of data from the training set. Each batch consists of B B samples, randomly selected from all timesteps across all trajectories. Figure 9 illustrates the batching operation for a representative dataset with two trajectories of five timesteps each. In this example, each batch contains B = 2 B=2 random samples across all the available data. In the interest of simplicity, each sample consists of the system configuration at timestep t t together with that at timestep t + 1 t+1 . However, in practical applications, information from several previous timesteps is often included to improve temporal context and prediction accuracy. The configuration at t t is used as input to the model, while the acceleration required to evolve the system from configuration t t to configuration t + 1 t+1 ( ùê≤ ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}} ) serves as the ground truth (label) for the current training step.\\nFrom the input configuration (positions), velocities are extracted via finite differences. To improve robustness against error accumulation and instabilities during rollout, Gaussian noise is added to all computed velocities in the training samples, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) . This regularization strategy encourages the model to generalize beyond the exact training trajectories and mitigates the buildup of prediction errors over time.\\nAfter noise injection, GNSS encodes the system configuration at timestep t t from each sample into a graph, predicts the corresponding acceleration ( ùê≤ \\\\mathbf{y} ), and compares it with the ground truth ( ùê≤ ^ \\\\hat{\\\\mathbf{y}} ). Backpropagation is then used to update the weights of all MLPs and the embedding layer in order to minimize a user-defined loss function. To further improve generalization in long rollouts, we propose a novel loss function, the weighted Mean Squared Error (wMSE) shown in Equation 17 : a modified version of the Mean Squared Error(MSE) that penalizes acceleration predictions with incorrect sign through a scalar weight s s , defined as:\\nHere, the subscript i indicates the i ‚àí t \\u200b h i-th node; the final loss is computed as the mean squared value of Œµ i \\\\varepsilon_{i} over all predictions in the batch.\\nAfter training, predictions are generated in an autoregressive manner: the initial state is encoded as a graph, and GNSS is iteratively applied to predict the full trajectory. The number of iterations N T \\\\rm{N_{T}} is defined by the user to cover the entire physical timeframe of interest, with the understanding that the physical timestep size is fixed and identical to that used to generate the training dataset. The corresponding pseudo-code for the rollout procedure is reported in Algorithm 2 .',\n",
       "        'subsections': [{'title': 'Training',\n",
       "          'paragraphs': 'The training procedure is summarized in the pseudo-code shown in Algorithm 1 .\\nAs in traditional ML algorithms, training proceeds over successive epochs. At each epoch, the sampler extracts a batch of data from the training set. Each batch consists of B B samples, randomly selected from all timesteps across all trajectories. Figure 9 illustrates the batching operation for a representative dataset with two trajectories of five timesteps each. In this example, each batch contains B = 2 B=2 random samples across all the available data. In the interest of simplicity, each sample consists of the system configuration at timestep t t together with that at timestep t + 1 t+1 . However, in practical applications, information from several previous timesteps is often included to improve temporal context and prediction accuracy. The configuration at t t is used as input to the model, while the acceleration required to evolve the system from configuration t t to configuration t + 1 t+1 ( ùê≤ ( t + 1 ) \\\\prescript{(t+1)}{}{\\\\mathbf{y}} ) serves as the ground truth (label) for the current training step.\\nFrom the input configuration (positions), velocities are extracted via finite differences. To improve robustness against error accumulation and instabilities during rollout, Gaussian noise is added to all computed velocities in the training samples, following Sanchez-Gonzalez et\\xa0al. ( 2020 ) . This regularization strategy encourages the model to generalize beyond the exact training trajectories and mitigates the buildup of prediction errors over time.\\nAfter noise injection, GNSS encodes the system configuration at timestep t t from each sample into a graph, predicts the corresponding acceleration ( ùê≤ \\\\mathbf{y} ), and compares it with the ground truth ( ùê≤ ^ \\\\hat{\\\\mathbf{y}} ). Backpropagation is then used to update the weights of all MLPs and the embedding layer in order to minimize a user-defined loss function. To further improve generalization in long rollouts, we propose a novel loss function, the weighted Mean Squared Error (wMSE) shown in Equation 17 : a modified version of the Mean Squared Error(MSE) that penalizes acceleration predictions with incorrect sign through a scalar weight s s , defined as:\\nHere, the subscript i indicates the i ‚àí t \\u200b h i-th node; the final loss is computed as the mean squared value of Œµ i \\\\varepsilon_{i} over all predictions in the batch.',\n",
       "          'subsections': []},\n",
       "         {'title': 'Rollout',\n",
       "          'paragraphs': 'After training, predictions are generated in an autoregressive manner: the initial state is encoded as a graph, and GNSS is iteratively applied to predict the full trajectory. The number of iterations N T \\\\rm{N_{T}} is defined by the user to cover the entire physical timeframe of interest, with the understanding that the physical timestep size is fixed and identical to that used to generate the training dataset. The corresponding pseudo-code for the rollout procedure is reported in Algorithm 2 .',\n",
       "          'subsections': []}]}]}]},\n",
       "   {'title': '3 Case Study and Results',\n",
       "    'paragraphs': 'To evaluate the capabilities of GNSS, we generated a validated numerical dataset using Abaqus for a fully clamped beam of length 320\\u2009mm. The beam was excited through a transverse prescribed motion applied to an interior node. The displacement‚Äìtime history at the input node consists of a single sine-wave cycle at 50 \\u200b kHz 50\\\\,\\\\mathrm{kHz} , modulated by a Hanning window. Table 1 summarizes the simulation parameters used to create the dataset.\\nSix distinct trajectories were generated by varying the location of the prescribed motion, as illustrated in Figure 10 . Only the interior nodes highlighted in the figure were included, while regions near the boundaries were excluded to avoid wave reflections that could interfere with the signal. This choice allows the analysis to focus on pure wave propagation and simplifies the initial case study.Four trajectories were used for training, one for validation, and one was reserved exclusively for testing.\\nA sensitivity analysis was conducted to obtain a coarse tuning of the model hyperparameters.The results are shown in Figure 11 .\\nRed bars correspond to the baseline hyperparameters, which are kept fixed across all experiments except for the one under analysis. Each gray bar therefore represents the outcome of a training and rollout in which a single hyperparameter is varied from its baseline value, while all others remain at their baseline configuration. For example, in panel (a), the bar associated with r / Œî = 7 r/\\\\Delta=7 corresponds to a model trained with baseline values for all other hyperparameters, but with the connectivity radius set to 7.\\nThe bar height indicates the rollout MSE loss, defined as the mean Euclidean distance between predicted and ground-truth nodal positions, averaged over all timesteps of the rollout and across all trajectories in the dataset. Error bars denote the corresponding standard deviations.\\nPanel (a) reports the performance as a function of the connectivity radius. Prior studies suggest that an effective trade-off for modeling fluid and granular systems is to maintain approximately 15‚Äì20 edges per node (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) . In structural problems, however, a physically meaningful scale can be identified, directly tied to the choice of the connectivity radius. In particular, the wavelength Œª \\\\lambda of bending waves in beams can be estimated using the Euler‚ÄìBernoulli beam dispersion relation:\\nwhere E E is the Young‚Äôs modulus, I I the second moment of area, œÅ \\\\rho the material density, A A the cross-sectional area, and f f the wave frequency. For the physical properties of the beam considered here, this yields a wavelength of approximately Œª ‚âà 13.4 \\u200b mm \\\\lambda\\\\approx 13.4\\\\,\\\\mathrm{mm} . Given our mesh resolution of 0.8 \\u200b mm 0.8\\\\,\\\\mathrm{mm} per node, each wavelength spans approximately 16‚Äì17 nodes, and half of this number is sufficient to capture a representative portion of the wave. This is confirmed by the sensitivity analysis, which shows the best performance for a connectivity radius including 7‚Äì10 neighboring nodes. Such a choice ensures that local interactions are sufficiently informed by the underlying physics, a key factor for accurately modeling wave propagation dynamics.\\nPanel (b) shows the performance with respect to the penalty term s s introduced in the proposed loss function. The rollout MSE remains stable for small values of s s , with the best performance obtained for s = 1.5 s=1.5 .\\nPanel (c) reports the rollout MSE as a function of the connectivity radius. Although the average MSE is lower with 13 message passing steps than with 10, the computational cost and the variability of the error across different trajectories are significantly higher in the former case. Moreover, while 7 message passing steps yield the best performance, the choice of 10 message passing steps ‚Äî motivated by previous studies (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) ‚Äî is confirmed as a reliable default setting, which we adopt as the standard in this work.\\nFinally, panel (d) illustrates the effect of the Gaussian noise standard deviation. The most effective setting is found to be around 9-10% of the maximum displacement amplitude. Larger noise levels prevent the model from converging, whereas smaller levels do not provide sufficient robustness for long rollouts.\\nThe sensitivity analysis results show that the identified baseline values provide sufficiently accurate performance in terms of rollout MSE. Consequently, these values were adopted as the default configuration and used to train the GNSS model in the subsequent experiments.\\nTwo models, namely, GNSS and a traditional GNS, were trained using the baseline hyperparameters identified through the sensitivity analysis. The training performance is reported in Figure 12 . GNSS exhibited a promising decreasing trend during the early epochs, followed by a plateau in the later stages of training. The validation loss confirmed that no overfitting occurred. In contrast, the GNS loss decreased during the first few epochs but subsequently increased, converging to a value higher than the initial one. This behavior suggests that GNS is not effectively learning to capture the underlying physical phenomena, despite the absence of overfitting.\\nWe attribute the difference in performance primarily to the nature of the input information: while GNS operates on variables expressed in absolute coordinates, GNSS encodes the same information in relative coordinates.\\nThe trained models were then evaluated on unseen trajectories, characterized by the same structure and excitation type but applied at different excitation points. The rollout results for both models are presented in Figure 13 .\\nPanel (a) presents five rollout snapshots at representative timesteps for the ground truth (a1), GNSS (a2), and GNS (a3). Transverse displacements are on the order of Œº \\u200b m \\\\mu\\\\mathrm{m} , but are visually magnified in the figure for clarity. The colorbar indicates the displacement magnitude, with its range defined by the minimum and maximum values of the ground truth in Figure 13 (a1).\\nQualitatively, the GNSS predictions closely follow the ground truth across all timesteps. In contrast, the GNS model fails to reproduce the physical response. For visualization purposes, the GNS results were scaled by a factor of 0.03 to enable a qualitative comparison of the deformation shapes. The colorbar further indicates that most predicted displacements lie far outside the physical range, underscoring the inability of the GNS model to produce physically consistent results.\\nPanel (b) provides a quantitative comparison of the displacement‚Äìtime histories for three representative nodes: node 5 near the left end (b1), node 92 near the midpoint (b2), and node 180 near the right end (b3). Each plot reports the ground truth together with the GNSS and the GNS prediction. A separate vertical axis is used for the GNS curves to enable meaningful visual comparison. The GNSS model shows high accuracy in predicting the trajectory of all considered nodes. In contrast, GNS not only fails to capture the correct waveform, but also diverges to displacement values several orders of magnitude larger than the ground truth.\\nFigure 14 summarizes the spatial and temporal distributions of the root-mean-squared error (RMSE) between the predicted and reference values. Panel (a) divides the beam into five equal segments and, for each segment, shows the kernel density of the RMSE averaged over t ‚àà [ 30 , 100 ] \\u200b Œº \\u200b s t\\\\in[30,100]\\\\,\\\\mu\\\\text{s} to suppress initial-transient bias. Sub-panel (a4) corresponds to the segment containing the node where the prescribed displacement is applied; this actuated node is excluded from the statistics. The error peaks in the input segment and decreases with distance from it, which is consistent with the presence of stronger local gradients and earlier motion near the excitation. Regions farther from the input remain quiescent for longer periods and therefore accumulate less error during the rollout. Panel (b) characterizes error growth over time by plotting the RMSE distributions aggregated from t = 0 t=0 up to t = t i t=t_{i} for t i ‚àà { 1 , 50 , 99 } \\u200b Œº \\u200b s t_{i}\\\\in\\\\{1,50,99\\\\}\\\\,\\\\mu\\\\text{s} . Both models accumulate error as the rollout progresses; however, GNSS exhibits only a modest shift toward higher RMSE values and remains concentrated at much lower error levels, whereas GNS shifts markedly and develops heavy tails at larger RMSE. Across both panels, GNSS consistently maintains substantially lower errors than the standard GNS.\\nThe results demonstrate that the standard approach based on absolute positions is inadequate for accurately modeling displacement, velocity, or acceleration fields when displacements are several orders of magnitude smaller than the characteristic structural dimensions. By contrast, GNSS, through its relative coordinate formulation, successfully overcomes this limitation and achieves physically consistent predictions.\\nOn our dataset, GNSS attains an average speed-up of about 5 √ó 5\\\\times over FEM. According to the graph construction method adopted in this work, where two nodes are connected if their distance is less than or equal to a maximum allowable distance (the connectivity radius R R ), the expected node degree can be computed as shown in Equation 19 (Penrose, 2003 ) :\\nHere, Œ∫ d \\\\kappa_{d} is the volume of a unit ball in the d d -dimensional space ( Œ∫ 1 = 2 , Œ∫ 2 = œÄ , Œ∫ 3 = 4 3 \\u200b œÄ \\\\kappa_{1}=2,\\\\,\\\\kappa_{2}=\\\\pi,\\\\,\\\\kappa_{3}=\\\\frac{4}{3}\\\\pi ). Given fixed node intensity Œª \\\\lambda (analogous to mesh density) and fixed connectivity radius R R , the expected value of the degree of the node is bounded, hence the number of edges grows linearly with the total number of nodes, E = Œò \\u200b ( N ) E=\\\\Theta(N) . A message-passing layer with sparse edgewise aggregation therefore costs O \\u200b ( E + N ) = O \\u200b ( N ) O(E+N)=O(N) ; stacking a fixed number of layers keeps one GNSS rollout step near-linear in N N (up to feature-width constants).\\nIn contrast, explicit solvers for structural dynamics problems perform time integration using an explicit central-difference scheme with a lumped mass matrix, and the per-increment computational cost scales approximately linearly with the number of elements or nodes (Das, 2006 ) . However, the stable time increment in explicit integration is limited by a Courant-Friedrichs-Lewy (CFL) condition, which ensures that numerical information does not propagate faster than the physical wave speed within an element. This stability bound scales with the smallest element size, as shown in Equation 20 :\\nwhere h min h_{\\\\min} is the minimum characteristic element length and c c is the relevant wave propagation speed in the material (Das, 2006 ) . As indicated by Equation 20 , refining a fixed spatial domain (i.e., decreasing element size h h ) simultaneously increases the number of nodes N ‚àº h ‚àí d N\\\\sim h^{-d} and decreases the stable time increment Œî \\u200b t stable ‚àº h \\\\Delta t_{\\\\mathrm{stable}}\\\\sim h . Over a fixed physical duration œÑ \\\\tau , the number of required time steps then scales as 1 / Œî \\u200b t stable ‚àº h ‚àí 1 1/\\\\Delta t_{\\\\mathrm{stable}}\\\\sim h^{-1} . Combining the per-step computational cost O \\u200b ( N ) O(N) with O \\u200b ( 1 / Œî \\u200b t stable ) O(1/\\\\Delta t_{\\\\mathrm{stable}}) steps yields a total explicit runtime of O \\u200b ( N 1 + 1 / d ) O\\\\!\\\\big(N^{1+1/d}\\\\big) , representing superlinear growth imposed by the stability constraint.\\nIn contrast, GNSS advances the solution through feed-forward message passing at the prescribed time step Œî \\u200b t ph \\\\Delta t_{\\\\mathrm{ph}} of the training dataset, which remains fixed across trajectories. There is no CFL-type restriction, so the temporal resolution is arbitrarily selected and imposed during training rather than being dictated by numerical stability. The per-step computational cost remains approximately linear in N N , making the observed 5 √ó 5\\\\times speed-up in our simple test case a conservative estimate for higher-resolution problems.',\n",
       "    'subsections': [{'title': '3.1 Hyperparameter Analysis',\n",
       "      'paragraphs': 'A sensitivity analysis was conducted to obtain a coarse tuning of the model hyperparameters.The results are shown in Figure 11 .\\nRed bars correspond to the baseline hyperparameters, which are kept fixed across all experiments except for the one under analysis. Each gray bar therefore represents the outcome of a training and rollout in which a single hyperparameter is varied from its baseline value, while all others remain at their baseline configuration. For example, in panel (a), the bar associated with r / Œî = 7 r/\\\\Delta=7 corresponds to a model trained with baseline values for all other hyperparameters, but with the connectivity radius set to 7.\\nThe bar height indicates the rollout MSE loss, defined as the mean Euclidean distance between predicted and ground-truth nodal positions, averaged over all timesteps of the rollout and across all trajectories in the dataset. Error bars denote the corresponding standard deviations.\\nPanel (a) reports the performance as a function of the connectivity radius. Prior studies suggest that an effective trade-off for modeling fluid and granular systems is to maintain approximately 15‚Äì20 edges per node (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) . In structural problems, however, a physically meaningful scale can be identified, directly tied to the choice of the connectivity radius. In particular, the wavelength Œª \\\\lambda of bending waves in beams can be estimated using the Euler‚ÄìBernoulli beam dispersion relation:\\nwhere E E is the Young‚Äôs modulus, I I the second moment of area, œÅ \\\\rho the material density, A A the cross-sectional area, and f f the wave frequency. For the physical properties of the beam considered here, this yields a wavelength of approximately Œª ‚âà 13.4 \\u200b mm \\\\lambda\\\\approx 13.4\\\\,\\\\mathrm{mm} . Given our mesh resolution of 0.8 \\u200b mm 0.8\\\\,\\\\mathrm{mm} per node, each wavelength spans approximately 16‚Äì17 nodes, and half of this number is sufficient to capture a representative portion of the wave. This is confirmed by the sensitivity analysis, which shows the best performance for a connectivity radius including 7‚Äì10 neighboring nodes. Such a choice ensures that local interactions are sufficiently informed by the underlying physics, a key factor for accurately modeling wave propagation dynamics.\\nPanel (b) shows the performance with respect to the penalty term s s introduced in the proposed loss function. The rollout MSE remains stable for small values of s s , with the best performance obtained for s = 1.5 s=1.5 .\\nPanel (c) reports the rollout MSE as a function of the connectivity radius. Although the average MSE is lower with 13 message passing steps than with 10, the computational cost and the variability of the error across different trajectories are significantly higher in the former case. Moreover, while 7 message passing steps yield the best performance, the choice of 10 message passing steps ‚Äî motivated by previous studies (Sanchez-Gonzalez et\\xa0al., 2020 ; Choi & Kumar, 2024 ) ‚Äî is confirmed as a reliable default setting, which we adopt as the standard in this work.\\nFinally, panel (d) illustrates the effect of the Gaussian noise standard deviation. The most effective setting is found to be around 9-10% of the maximum displacement amplitude. Larger noise levels prevent the model from converging, whereas smaller levels do not provide sufficient robustness for long rollouts.\\nThe sensitivity analysis results show that the identified baseline values provide sufficiently accurate performance in terms of rollout MSE. Consequently, these values were adopted as the default configuration and used to train the GNSS model in the subsequent experiments.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Training',\n",
       "      'paragraphs': 'Two models, namely, GNSS and a traditional GNS, were trained using the baseline hyperparameters identified through the sensitivity analysis. The training performance is reported in Figure 12 . GNSS exhibited a promising decreasing trend during the early epochs, followed by a plateau in the later stages of training. The validation loss confirmed that no overfitting occurred. In contrast, the GNS loss decreased during the first few epochs but subsequently increased, converging to a value higher than the initial one. This behavior suggests that GNS is not effectively learning to capture the underlying physical phenomena, despite the absence of overfitting.\\nWe attribute the difference in performance primarily to the nature of the input information: while GNS operates on variables expressed in absolute coordinates, GNSS encodes the same information in relative coordinates.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 Rollout',\n",
       "      'paragraphs': 'The trained models were then evaluated on unseen trajectories, characterized by the same structure and excitation type but applied at different excitation points. The rollout results for both models are presented in Figure 13 .\\nPanel (a) presents five rollout snapshots at representative timesteps for the ground truth (a1), GNSS (a2), and GNS (a3). Transverse displacements are on the order of Œº \\u200b m \\\\mu\\\\mathrm{m} , but are visually magnified in the figure for clarity. The colorbar indicates the displacement magnitude, with its range defined by the minimum and maximum values of the ground truth in Figure 13 (a1).\\nQualitatively, the GNSS predictions closely follow the ground truth across all timesteps. In contrast, the GNS model fails to reproduce the physical response. For visualization purposes, the GNS results were scaled by a factor of 0.03 to enable a qualitative comparison of the deformation shapes. The colorbar further indicates that most predicted displacements lie far outside the physical range, underscoring the inability of the GNS model to produce physically consistent results.\\nPanel (b) provides a quantitative comparison of the displacement‚Äìtime histories for three representative nodes: node 5 near the left end (b1), node 92 near the midpoint (b2), and node 180 near the right end (b3). Each plot reports the ground truth together with the GNSS and the GNS prediction. A separate vertical axis is used for the GNS curves to enable meaningful visual comparison. The GNSS model shows high accuracy in predicting the trajectory of all considered nodes. In contrast, GNS not only fails to capture the correct waveform, but also diverges to displacement values several orders of magnitude larger than the ground truth.\\nFigure 14 summarizes the spatial and temporal distributions of the root-mean-squared error (RMSE) between the predicted and reference values. Panel (a) divides the beam into five equal segments and, for each segment, shows the kernel density of the RMSE averaged over t ‚àà [ 30 , 100 ] \\u200b Œº \\u200b s t\\\\in[30,100]\\\\,\\\\mu\\\\text{s} to suppress initial-transient bias. Sub-panel (a4) corresponds to the segment containing the node where the prescribed displacement is applied; this actuated node is excluded from the statistics. The error peaks in the input segment and decreases with distance from it, which is consistent with the presence of stronger local gradients and earlier motion near the excitation. Regions farther from the input remain quiescent for longer periods and therefore accumulate less error during the rollout. Panel (b) characterizes error growth over time by plotting the RMSE distributions aggregated from t = 0 t=0 up to t = t i t=t_{i} for t i ‚àà { 1 , 50 , 99 } \\u200b Œº \\u200b s t_{i}\\\\in\\\\{1,50,99\\\\}\\\\,\\\\mu\\\\text{s} . Both models accumulate error as the rollout progresses; however, GNSS exhibits only a modest shift toward higher RMSE values and remains concentrated at much lower error levels, whereas GNS shifts markedly and develops heavy tails at larger RMSE. Across both panels, GNSS consistently maintains substantially lower errors than the standard GNS.\\nThe results demonstrate that the standard approach based on absolute positions is inadequate for accurately modeling displacement, velocity, or acceleration fields when displacements are several orders of magnitude smaller than the characteristic structural dimensions. By contrast, GNSS, through its relative coordinate formulation, successfully overcomes this limitation and achieves physically consistent predictions.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.4 Runtime Performance',\n",
       "      'paragraphs': 'On our dataset, GNSS attains an average speed-up of about 5 √ó 5\\\\times over FEM. According to the graph construction method adopted in this work, where two nodes are connected if their distance is less than or equal to a maximum allowable distance (the connectivity radius R R ), the expected node degree can be computed as shown in Equation 19 (Penrose, 2003 ) :\\nHere, Œ∫ d \\\\kappa_{d} is the volume of a unit ball in the d d -dimensional space ( Œ∫ 1 = 2 , Œ∫ 2 = œÄ , Œ∫ 3 = 4 3 \\u200b œÄ \\\\kappa_{1}=2,\\\\,\\\\kappa_{2}=\\\\pi,\\\\,\\\\kappa_{3}=\\\\frac{4}{3}\\\\pi ). Given fixed node intensity Œª \\\\lambda (analogous to mesh density) and fixed connectivity radius R R , the expected value of the degree of the node is bounded, hence the number of edges grows linearly with the total number of nodes, E = Œò \\u200b ( N ) E=\\\\Theta(N) . A message-passing layer with sparse edgewise aggregation therefore costs O \\u200b ( E + N ) = O \\u200b ( N ) O(E+N)=O(N) ; stacking a fixed number of layers keeps one GNSS rollout step near-linear in N N (up to feature-width constants).\\nIn contrast, explicit solvers for structural dynamics problems perform time integration using an explicit central-difference scheme with a lumped mass matrix, and the per-increment computational cost scales approximately linearly with the number of elements or nodes (Das, 2006 ) . However, the stable time increment in explicit integration is limited by a Courant-Friedrichs-Lewy (CFL) condition, which ensures that numerical information does not propagate faster than the physical wave speed within an element. This stability bound scales with the smallest element size, as shown in Equation 20 :\\nwhere h min h_{\\\\min} is the minimum characteristic element length and c c is the relevant wave propagation speed in the material (Das, 2006 ) . As indicated by Equation 20 , refining a fixed spatial domain (i.e., decreasing element size h h ) simultaneously increases the number of nodes N ‚àº h ‚àí d N\\\\sim h^{-d} and decreases the stable time increment Œî \\u200b t stable ‚àº h \\\\Delta t_{\\\\mathrm{stable}}\\\\sim h . Over a fixed physical duration œÑ \\\\tau , the number of required time steps then scales as 1 / Œî \\u200b t stable ‚àº h ‚àí 1 1/\\\\Delta t_{\\\\mathrm{stable}}\\\\sim h^{-1} . Combining the per-step computational cost O \\u200b ( N ) O(N) with O \\u200b ( 1 / Œî \\u200b t stable ) O(1/\\\\Delta t_{\\\\mathrm{stable}}) steps yields a total explicit runtime of O \\u200b ( N 1 + 1 / d ) O\\\\!\\\\big(N^{1+1/d}\\\\big) , representing superlinear growth imposed by the stability constraint.\\nIn contrast, GNSS advances the solution through feed-forward message passing at the prescribed time step Œî \\u200b t ph \\\\Delta t_{\\\\mathrm{ph}} of the training dataset, which remains fixed across trajectories. There is no CFL-type restriction, so the temporal resolution is arbitrarily selected and imposed during training rather than being dictated by numerical stability. The per-step computational cost remains approximately linear in N N , making the observed 5 √ó 5\\\\times speed-up in our simple test case a conservative estimate for higher-resolution problems.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Conclusions',\n",
       "    'paragraphs': 'In this work, we have presented GNSS, a graph-network‚Äìbased surrogate model for time-resolved structural dynamics simulations. The model builds on the GNS framework, originally developed for granular flow and fluid dynamics, and introduces the following key innovations:\\nRelative reference system: the states of the system are expressed in a relative, node-fixed local coordinate system rather than in the global frame.\\nNovel loss function: a weighted MSE loss (wMSE) was introduced to penalize acceleration predictions with incorrect sign, enabling robust long-horizon rollouts and accurate spatial field predictions.\\nPhysics-based hyperparameter setup: the sensitivity analysis demonstrated that hyperparameters can be selected based on physical considerations. In the elastodynamic case study, for instance, the connectivity radius was determined from the excitation wavelength.\\nThe case study showed that GNSS successfully predicts wave propagation, a task where the state-of-the-art GNS tailored to granular flow and fluid dynamics fails to provide physically meaningful results.\\nWhile the results are promising, further validation is required on additional structural dynamics applications and more complex geometries. Ongoing work includes extending GNSS to three-dimensional elastodynamic problems involving isotropic and anisotropic materials, as well as incorporating information about structural anomalies such as damage. Finally, future efforts will focus on training the framework directly on experimental data, thereby removing the need for numerical simulations and paving the way for structural health monitoring applications.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Graph Neural Networks (GNNs) have recently been explored as surrogate models for numerical simulations. While their applications in computational fluid dynamics have been investigated, little attention has been given to structural problems, especially for dynamic cases. To address this gap, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of dynamic structural problems.'},\n",
       " '2510.25557v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Recurrent neural networks (RNNs) process sequence data by maintaining a hidden state that is updated at each timestep, which can create a bottleneck for memory and representational capacity. While vanilla RNNs have been empirically shown to retain roughly one real value of information per hidden unit, with the effective task-specific capacity linearly bounded by the number of model parameters [Collins et\\xa0al., 2017 ] , similar limitations extend to gated architectures such as LSTMs and GRUs [Hochreiter and Schmidhuber, 1997 , Cho et\\xa0al., 2014 ] , despite their use of gating and explicit memory cells [Collins et\\xa0al., 2017 ] .\\nThis means that more complex sequences may exceed what the hidden state can encode, forcing the model to compress or forget.\\nThe introduction of the Transformer [Vaswani et\\xa0al., 2017 ] appeared to obviate explicit recurrence by bypassing the hidden-state bottleneck. However, recent work shows that recurrent inductive bias remains highly competitive and provides representational advantages not matched by Transformers [Gu and Dao, 2023 , Orvieto et\\xa0al., 2023 , Bhattamishra et\\xa0al., 2024 , Beck et\\xa0al., 2024 ] .\\nAnother challenge in training RNNs is the vanishing and exploding gradient problem [Bengio et\\xa0al., 1994 , Hochreiter and Schmidhuber, 1997 ] , which arises from repeated multiplication through the recurrent Jacobian. Among various strategies to address this [Mikolov, 2012 , Pascanu et\\xa0al., 2013 , Le et\\xa0al., 2015 ] , unitary and orthogonal RNNs [Arjovsky et\\xa0al., 2016 , Jing et\\xa0al., 2019 , Helfrich et\\xa0al., 2018 , Kiani et\\xa0al., 2022 ] constrain the recurrent weights to be norm-preserving, allowing gradients to remain stable across timesteps. These models perform well on synthetic tasks, but their results on broader benchmarks vary.\\nWith the advancement of quantum computing [Arute et\\xa0al., 2019 , Acharya et\\xa0al., 2024 , Reichardt et\\xa0al., 2024 , DeCross et\\xa0al., 2025 ] , parametrized quantum circuits (PQCs), which are a core component of variational hybrid quantum-classical models, have concurrently emerged as an alternative mechanism for function approximation [Benedetti et\\xa0al., 2019 , Du et\\xa0al., 2019 , Bondesan and Welling, 2020 , P√©rez-Salinas et\\xa0al., 2021 , Schuld et\\xa0al., 2021 , Yu et\\xa0al., 2024b ] . PQCs implement unitary transformations by construction, which naturally preserve norms (¬ß 3.1 ). Acting on n n qubits, they enable expressive transformations over quantum states in an exponentially large Hilbert space ‚ÑÇ 2 n \\\\mathbb{C}^{2^{n}} . Although such spaces are classically intractable beyond moderate n n , they can be manipulated with only n n qubits on quantum hardware.\\nIn this work, we present a hybrid quantum‚Äìclassical recurrent neural network (QRNN) architecture grounded in quantum operations, in which the entire recurrent core is realized as a PQC. The hidden state is the quantum state of the PQC, residing in an exponentially large Hilbert space. A classical feedforward network parametrizes and steers the quantum computation, introducing nonlinearity through mid-circuit readouts, which are realized in simulation as a limiting case via projective measurements. This avoids emulating nonlinearity through linear quantum dynamics, leaving the PQC strictly for coherent unitary evolution.\\nFig. 2 illustrates both the PQC (with four qubits shown for illustration) and the unrolled QRNN:\\nAt each timestep t t , the input is mapped to a classical embedding ùê± t \\\\mathbf{x}_{t} via a learnable embedding layer.\\nA classical feedforward network ‚Ñ± \\\\mathcal{F} takes as input the concatenation of the readout vector ùê≥ t ‚àí 1 \\\\mathbf{z}_{t-1} (outputs from all measurements at timestep t ‚àí 1 t-1 ) and the current input ùê± t \\\\mathbf{x}_{t} . It outputs the PQC parameters ùúΩ t \\\\bm{\\\\theta}_{t} which configure the PQC with a fixed gate layout (Fig. 1(a) ), denoted U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) , applied at timestep t t (Fig. 1(c) ).\\nThe PQC applies the parametrized unitary gates to evolve the quantum state, yielding the updated state. Residing in an exponentially large Hilbert space, this state persists across timesteps and provides the model‚Äôs core recurrent memory.\\nThe mid-circuit readout ùê≥ t \\\\mathbf{z}_{t} (or the final readout at the end of the sequence) is a classical feature vector obtained from the quantum state via measurements and is used: (i) as recurrent feedback ùê≥ t ‚àí 1 \\\\mathbf{z}_{t-1} at timestep t t , and (ii) as the input to task-specific classical layers.\\nWe develop the models on GPUs, allowing us to simulate and train quantum recurrence via classical backpropagation, with the expectation that such models will become classically unsimulatable as the number of qubits increases. To our knowledge, this is the first model grounded in quantum operations (RNN or otherwise) demonstrated in classical simulation with up to 14 qubits across six realistic sequence-modeling tasks, achieving competitive performance with LSTM and the scaled Cayley orthogonal scoRNN designed for norm preservation [Helfrich et\\xa0al., 2018 ] . Experiments also show that classical nonlinear control and feedback are effective, with the nonlinear variants outperforming their linear counterparts, and that the unitary quantum recurrent core maintains more stable gradients than LSTMs (¬ß 4.6 ).\\nThe QRNN is motivated in part by the memory and gradient problems of RNNs, but its main aim is to explore a hybrid quantum‚Äìclassical recurrent model in an idealized proof of principle that allows us to study its computational behavior under best-case conditions across a broad class of sequence learning tasks. The PQC [Sim et\\xa0al., 2019 ] uses only one- and two-qubit gates without nonstandard operations, and the overall architecture provides a hardware-aware base case and a plausible path toward future hardware implementations.\\nAnother way to view the QRNN is via fast and slow weights in RNNs, which function as different types of memory across multiple timescales [Schmidhuber, 1992 , Ba et\\xa0al., 2016 ] . The PQC parameters serve as the short-term memory, analogous to the hidden activities of classical RNNs, and are controlled and reconfigured at each timestep by a classical feedforward network whose slow weights encode the long-term memory. The quantum state, updated via unitary transformations, evolves on a faster timescale than the slow weights, persists across timesteps, and acts as a third, higher-capacity memory in the Hilbert space, retaining information that influences subsequent computation [Hinton and Plaut, 1987 , Schmidhuber, 1993 ] .',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Bausch [Bausch, 2020 ] developed a QRNN with a persistent quantum memory based on quantum neurons [Cao et\\xa0al., 2017 ] . However, nonlinearities are emulated within PQCs using probabilistic repeat-until-success circuit execution and postselection steps. The inherently linear nature of quantum computation constrains such emulation, and the available forms of nonlinearity remain limited [Yan et\\xa0al., 2020 , Moreira et\\xa0al., 2023 , Zi et\\xa0al., 2024 ] .\\nThe so-called QLSTMs embed PQCs into the gating mechanisms of classical LSTMs [Chen et\\xa0al., 2020 , Yu et\\xa0al., 2024a , Ubale et\\xa0al., 2025 ] , replacing dense layers in the LSTM gates with PQCs. However, all memory and recurrence remain entirely classical, governed by standard hidden and cell state updates. These architectures are best viewed as classical LSTMs augmented with auxiliary PQCs, rather than quantum recurrent models.\\nLi et\\xa0al. [ 2023 ] and Siemaszko et\\xa0al. [ 2023 ] also model recurrences with PQCs, while supporting per-timestep readouts, but they rely entirely on linear quantum dynamics of the PQC without explicit nonlinearities or classical control.\\nExperiments with the existing models have focused on domain-specific tasks such as fraud detection [Ubale et\\xa0al., 2025 ] , low-resource text classification [Yu et\\xa0al., 2024a ] , or scaled-down MNIST [Bausch, 2020 , Siemaszko et\\xa0al., 2023 ] .\\nWe instead present the first QRNN to demonstrate competitive performance across six full-scale sequence modeling tasks.',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Model',\n",
       "    'paragraphs': 'A PQC typically starts from the all-zero state | œà ‚ü© = | 0 ‚ü© ‚äó n ‚àà ‚ÑÇ 2 n |\\\\psi\\\\rangle=|0\\\\rangle^{\\\\otimes n}\\\\in\\\\mathbb{C}^{2^{n}} and applies a series of gates arranged from left to right. 3 3 3 ‚äó \\\\otimes denotes the tensor product. An example PQC with n = 4 n=4 qubits is shown in Fig. 1(a) , where each horizontal line represents a qubit.\\nThe square boxes denote quantum gates, which by definition are unitary transformations acting on one or more qubits.\\nSingle-qubit gates apply local transformations, while multi-qubit gates can generate superposition and entanglement. 4 4 4 See Appendix A for a basic description of qubits and superposition.\\nLet U U denote the composition (product) of a collection of unitary gates, hence U ‚Ä† \\u200b U = I U^{\\\\dagger}U=I . For any state | œà ‚ü© \\\\lvert\\\\psi\\\\rangle ,\\nwhich ensures norm preservation by construction. 5 5 5 U ‚Ä† U^{\\\\dagger} denotes the conjugate transpose (Hermitian adjoint) of U U .\\nFormally, if the PQC consists of L L gates, U = u L \\u200b u L ‚àí 1 \\u200b ‚ãØ \\u200b u 1 U=u_{L}u_{L-1}\\\\cdots u_{1} ,\\nwhere each u i u_{i} is a unitary operator acting on some subset of qubits, then U ‚Ä† = u 1 ‚Ä† \\u200b u 2 ‚Ä† \\u200b ‚ãØ \\u200b u L ‚Ä† U^{\\\\dagger}=u_{1}^{\\\\dagger}u_{2}^{\\\\dagger}\\\\cdots u_{L}^{\\\\dagger} ,\\nand hence U ‚Ä† \\u200b U = I U^{\\\\dagger}U=I .\\nIn a PQC, gates can be either fixed or parametrized. Fixed gates implement structural operations and remain constant throughout training, 6 6 6 For example, the ùêÇùêçùêéùêì \\\\mathbf{CNOT} gate flips the target qubit if the control is in the | 1 ‚ü© |1\\\\rangle state. while the latter contain learnable parameters, which function like trainable weight matrices analogous to neural-network ‚Äúlayers‚Äù. The PQC in Fig. 1(a) consists of entirely parametrized gates.\\nTo probe the quantum state of a PQC, we can perform measurements to obtain real-valued readouts. These readouts provide partial observations of the state, and any required number of measurements, on any of the qubits, can be combined for downstream tasks. For instance, a measurement through the Pauli- ùêô \\\\mathbf{Z} observable with the unitary\\nassigns scalar values (e.g., + 1 +1 for | 0 ‚ü© |0\\\\rangle and ‚àí 1 -1 for | 1 ‚ü© |1\\\\rangle ) in the computational basis in the single-qubit case. For a general quantum state however, the outcome is probabilistic: it yields + 1 +1 with probability | Œ± | 2 |\\\\alpha|^{2} and ‚àí 1 -1 with probability | Œ≤ | 2 |\\\\beta|^{2} . The expectation value of this measurement is given by | Œ± | 2 ‚àí | Œ≤ | 2 |\\\\alpha|^{2}-|\\\\beta|^{2} , which can be used as a real-valued readout in hybrid quantum-classical models.\\nAlthough the readouts obtained via measurement are a nonlinear function of the gate parameters, particularly those used in parametrized rotation gates such as ùêëùêó \\\\mathbf{RX} , the resulting nonlinearity is generally weak\\xa0(¬ß 4 ).\\nRNNs parameterize a conditional distribution with a function that depends on a hidden state ùê° t ‚àí 1 \\\\mathbf{h}_{t-1} , which compacts past inputs ( ùê± 1 , ‚Ä¶ , ùê± t ‚àí 1 ) (\\\\mathbf{x}_{1},\\\\dots,\\\\mathbf{x}_{t-1}) into a fixed-dimensional representation:\\nAt each timestep t t , the hidden state ùê° t \\\\mathbf{h}_{t} is updated based on the previous hidden state ùê° t ‚àí 1 \\\\mathbf{h}_{t-1} and the current input ùê± t \\\\mathbf{x}_{t} :\\nwhere f f is a transformation (e.g., a basic RNN or LSTM cell) parametrized by ùöØ \\\\bm{\\\\Theta} . In the hybrid model (Fig. 1(c) ), we replace the hidden state with a quantum state represented by the PQC in Fig. 1(a) , which is controlled by a classical feedforward network and evolved by applying the unitary gates.\\nLet ùê± t \\\\mathbf{x}_{t} be the input embedding at timestep t t , and let ùê≥ t ‚àí 1 \\\\mathbf{z}_{t-1} be the measurement-based readout from the previous timestep. In the most generic form of the hybrid model, 7 7 7 We may add extra transformations to the measurement outcomes before classifications or feeding them to the next step; see\\xa0¬ß 4 . the two are concatenated into a single vector ùêÆ t = ( ùê≥ t ‚àí 1 : ùê± t ) \\\\mathbf{u}_{t}=(\\\\mathbf{z}_{t-1}\\\\mathpunct{:}\\\\mathbf{x}_{t}) and passed through a classical feedforward network ‚Ñ± \\\\mathcal{F} with one hidden layer and a nonlinearity.\\nThe first transformation in ‚Ñ± \\\\mathcal{F} maps the input ùêÆ t \\\\mathbf{u}_{t} to a hidden representation ùêØ t \\\\mathbf{v}_{t} :\\nwhere œï \\\\phi is a nonlinear activation function. The second transformation maps ùêØ t \\\\mathbf{v}_{t} to\\nwhere ùúΩ t ‚àà ‚Ñù d \\\\bm{\\\\theta}_{t}\\\\in\\\\mathbb{R}^{d} represents the parameters that control the PQC‚Äôs unitary operations at timestep t t . Each element of ùúΩ t \\\\bm{\\\\theta}_{t} denoted Œ∏ i \\\\theta_{i} is mapped to a rotation angle in a parametrized quantum gate within the PQC (e.g., 1 ‚â§ i ‚â§ d 1\\\\leq i\\\\leq d and d = 16 d=16 in Fig. 1(a) ).\\nThe PQC itself is defined by a unitary operator U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) , parametrized by ùúΩ t \\\\bm{\\\\theta}_{t} . 8 8 8 We use U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) to denote all unitary operations composed of multiple parametrized gates, each acting on one or more qubits with parameters drawn from ùúΩ t \\\\bm{\\\\theta}_{t} . Applying the gates in U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) to the quantum state ùê° t ‚àí 1 = | œà t ‚àí 1 ‚ü© \\\\mathbf{h}_{t-1}=|\\\\psi_{t-1}\\\\rangle yields the updated state ùê° t = U \\u200b ( ùúΩ t ) \\u200b | œà t ‚àí 1 ‚ü© \\\\mathbf{h}_{t}=U(\\\\bm{\\\\theta}_{t})\\\\,|\\\\psi_{t-1}\\\\rangle . The updated state is then measured to obtain a classical readout vector\\nwhich serves as a proxy for the quantum state and is combined with the next input ùê± t + 1 \\\\mathbf{x}_{t+1} to evolve the recurrence. Mid-circuit measurements are simulated without collapsing the quantum state via projective measurements, thereby preserving coherence across timesteps.\\nWe train the entire hybrid model end-to-end using classical backpropagation, optimizing the parameters ùöØ = { ùêñ 1 , ùêõ 1 , ùêñ 2 , ùêõ 2 } \\\\bm{\\\\Theta}=\\\\{\\\\mathbf{W}_{1},\\\\mathbf{b}_{1},\\\\mathbf{W}_{2},\\\\mathbf{b}_{2}\\\\} via standard optimizers, such as Adam [Kingma and Ba, 2014 ] .\\nBecause each ùê≥ t \\\\mathbf{z}_{t} is real-valued, it can be used both as a per-timestep output and as a contextual embedding for soft attention in sequence-to-sequence decoding.',\n",
       "    'subsections': [{'title': '3.1 PQC',\n",
       "      'paragraphs': 'A PQC typically starts from the all-zero state | œà ‚ü© = | 0 ‚ü© ‚äó n ‚àà ‚ÑÇ 2 n |\\\\psi\\\\rangle=|0\\\\rangle^{\\\\otimes n}\\\\in\\\\mathbb{C}^{2^{n}} and applies a series of gates arranged from left to right. 3 3 3 ‚äó \\\\otimes denotes the tensor product. An example PQC with n = 4 n=4 qubits is shown in Fig. 1(a) , where each horizontal line represents a qubit.\\nThe square boxes denote quantum gates, which by definition are unitary transformations acting on one or more qubits.\\nSingle-qubit gates apply local transformations, while multi-qubit gates can generate superposition and entanglement. 4 4 4 See Appendix A for a basic description of qubits and superposition.\\nLet U U denote the composition (product) of a collection of unitary gates, hence U ‚Ä† \\u200b U = I U^{\\\\dagger}U=I . For any state | œà ‚ü© \\\\lvert\\\\psi\\\\rangle ,\\nwhich ensures norm preservation by construction. 5 5 5 U ‚Ä† U^{\\\\dagger} denotes the conjugate transpose (Hermitian adjoint) of U U .\\nFormally, if the PQC consists of L L gates, U = u L \\u200b u L ‚àí 1 \\u200b ‚ãØ \\u200b u 1 U=u_{L}u_{L-1}\\\\cdots u_{1} ,\\nwhere each u i u_{i} is a unitary operator acting on some subset of qubits, then U ‚Ä† = u 1 ‚Ä† \\u200b u 2 ‚Ä† \\u200b ‚ãØ \\u200b u L ‚Ä† U^{\\\\dagger}=u_{1}^{\\\\dagger}u_{2}^{\\\\dagger}\\\\cdots u_{L}^{\\\\dagger} ,\\nand hence U ‚Ä† \\u200b U = I U^{\\\\dagger}U=I .\\nIn a PQC, gates can be either fixed or parametrized. Fixed gates implement structural operations and remain constant throughout training, 6 6 6 For example, the ùêÇùêçùêéùêì \\\\mathbf{CNOT} gate flips the target qubit if the control is in the | 1 ‚ü© |1\\\\rangle state. while the latter contain learnable parameters, which function like trainable weight matrices analogous to neural-network ‚Äúlayers‚Äù. The PQC in Fig. 1(a) consists of entirely parametrized gates.\\nTo probe the quantum state of a PQC, we can perform measurements to obtain real-valued readouts. These readouts provide partial observations of the state, and any required number of measurements, on any of the qubits, can be combined for downstream tasks. For instance, a measurement through the Pauli- ùêô \\\\mathbf{Z} observable with the unitary\\nassigns scalar values (e.g., + 1 +1 for | 0 ‚ü© |0\\\\rangle and ‚àí 1 -1 for | 1 ‚ü© |1\\\\rangle ) in the computational basis in the single-qubit case. For a general quantum state however, the outcome is probabilistic: it yields + 1 +1 with probability | Œ± | 2 |\\\\alpha|^{2} and ‚àí 1 -1 with probability | Œ≤ | 2 |\\\\beta|^{2} . The expectation value of this measurement is given by | Œ± | 2 ‚àí | Œ≤ | 2 |\\\\alpha|^{2}-|\\\\beta|^{2} , which can be used as a real-valued readout in hybrid quantum-classical models.\\nAlthough the readouts obtained via measurement are a nonlinear function of the gate parameters, particularly those used in parametrized rotation gates such as ùêëùêó \\\\mathbf{RX} , the resulting nonlinearity is generally weak\\xa0(¬ß 4 ).',\n",
       "      'subsections': [{'title': 'Unitary evolution.',\n",
       "        'paragraphs': 'A PQC typically starts from the all-zero state | œà ‚ü© = | 0 ‚ü© ‚äó n ‚àà ‚ÑÇ 2 n |\\\\psi\\\\rangle=|0\\\\rangle^{\\\\otimes n}\\\\in\\\\mathbb{C}^{2^{n}} and applies a series of gates arranged from left to right. 3 3 3 ‚äó \\\\otimes denotes the tensor product. An example PQC with n = 4 n=4 qubits is shown in Fig. 1(a) , where each horizontal line represents a qubit.\\nThe square boxes denote quantum gates, which by definition are unitary transformations acting on one or more qubits.\\nSingle-qubit gates apply local transformations, while multi-qubit gates can generate superposition and entanglement. 4 4 4 See Appendix A for a basic description of qubits and superposition.\\nLet U U denote the composition (product) of a collection of unitary gates, hence U ‚Ä† \\u200b U = I U^{\\\\dagger}U=I . For any state | œà ‚ü© \\\\lvert\\\\psi\\\\rangle ,\\nwhich ensures norm preservation by construction. 5 5 5 U ‚Ä† U^{\\\\dagger} denotes the conjugate transpose (Hermitian adjoint) of U U .\\nFormally, if the PQC consists of L L gates, U = u L \\u200b u L ‚àí 1 \\u200b ‚ãØ \\u200b u 1 U=u_{L}u_{L-1}\\\\cdots u_{1} ,\\nwhere each u i u_{i} is a unitary operator acting on some subset of qubits, then U ‚Ä† = u 1 ‚Ä† \\u200b u 2 ‚Ä† \\u200b ‚ãØ \\u200b u L ‚Ä† U^{\\\\dagger}=u_{1}^{\\\\dagger}u_{2}^{\\\\dagger}\\\\cdots u_{L}^{\\\\dagger} ,\\nand hence U ‚Ä† \\u200b U = I U^{\\\\dagger}U=I .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Parametrized unitary gates.',\n",
       "        'paragraphs': 'In a PQC, gates can be either fixed or parametrized. Fixed gates implement structural operations and remain constant throughout training, 6 6 6 For example, the ùêÇùêçùêéùêì \\\\mathbf{CNOT} gate flips the target qubit if the control is in the | 1 ‚ü© |1\\\\rangle state. while the latter contain learnable parameters, which function like trainable weight matrices analogous to neural-network ‚Äúlayers‚Äù. The PQC in Fig. 1(a) consists of entirely parametrized gates.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Measurements.',\n",
       "        'paragraphs': 'To probe the quantum state of a PQC, we can perform measurements to obtain real-valued readouts. These readouts provide partial observations of the state, and any required number of measurements, on any of the qubits, can be combined for downstream tasks. For instance, a measurement through the Pauli- ùêô \\\\mathbf{Z} observable with the unitary\\nassigns scalar values (e.g., + 1 +1 for | 0 ‚ü© |0\\\\rangle and ‚àí 1 -1 for | 1 ‚ü© |1\\\\rangle ) in the computational basis in the single-qubit case. For a general quantum state however, the outcome is probabilistic: it yields + 1 +1 with probability | Œ± | 2 |\\\\alpha|^{2} and ‚àí 1 -1 with probability | Œ≤ | 2 |\\\\beta|^{2} . The expectation value of this measurement is given by | Œ± | 2 ‚àí | Œ≤ | 2 |\\\\alpha|^{2}-|\\\\beta|^{2} , which can be used as a real-valued readout in hybrid quantum-classical models.\\nAlthough the readouts obtained via measurement are a nonlinear function of the gate parameters, particularly those used in parametrized rotation gates such as ùêëùêó \\\\mathbf{RX} , the resulting nonlinearity is generally weak\\xa0(¬ß 4 ).',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.2 Hybrid Model',\n",
       "      'paragraphs': 'RNNs parameterize a conditional distribution with a function that depends on a hidden state ùê° t ‚àí 1 \\\\mathbf{h}_{t-1} , which compacts past inputs ( ùê± 1 , ‚Ä¶ , ùê± t ‚àí 1 ) (\\\\mathbf{x}_{1},\\\\dots,\\\\mathbf{x}_{t-1}) into a fixed-dimensional representation:\\nAt each timestep t t , the hidden state ùê° t \\\\mathbf{h}_{t} is updated based on the previous hidden state ùê° t ‚àí 1 \\\\mathbf{h}_{t-1} and the current input ùê± t \\\\mathbf{x}_{t} :\\nwhere f f is a transformation (e.g., a basic RNN or LSTM cell) parametrized by ùöØ \\\\bm{\\\\Theta} . In the hybrid model (Fig. 1(c) ), we replace the hidden state with a quantum state represented by the PQC in Fig. 1(a) , which is controlled by a classical feedforward network and evolved by applying the unitary gates.\\nLet ùê± t \\\\mathbf{x}_{t} be the input embedding at timestep t t , and let ùê≥ t ‚àí 1 \\\\mathbf{z}_{t-1} be the measurement-based readout from the previous timestep. In the most generic form of the hybrid model, 7 7 7 We may add extra transformations to the measurement outcomes before classifications or feeding them to the next step; see\\xa0¬ß 4 . the two are concatenated into a single vector ùêÆ t = ( ùê≥ t ‚àí 1 : ùê± t ) \\\\mathbf{u}_{t}=(\\\\mathbf{z}_{t-1}\\\\mathpunct{:}\\\\mathbf{x}_{t}) and passed through a classical feedforward network ‚Ñ± \\\\mathcal{F} with one hidden layer and a nonlinearity.\\nThe first transformation in ‚Ñ± \\\\mathcal{F} maps the input ùêÆ t \\\\mathbf{u}_{t} to a hidden representation ùêØ t \\\\mathbf{v}_{t} :\\nwhere œï \\\\phi is a nonlinear activation function. The second transformation maps ùêØ t \\\\mathbf{v}_{t} to\\nwhere ùúΩ t ‚àà ‚Ñù d \\\\bm{\\\\theta}_{t}\\\\in\\\\mathbb{R}^{d} represents the parameters that control the PQC‚Äôs unitary operations at timestep t t . Each element of ùúΩ t \\\\bm{\\\\theta}_{t} denoted Œ∏ i \\\\theta_{i} is mapped to a rotation angle in a parametrized quantum gate within the PQC (e.g., 1 ‚â§ i ‚â§ d 1\\\\leq i\\\\leq d and d = 16 d=16 in Fig. 1(a) ).\\nThe PQC itself is defined by a unitary operator U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) , parametrized by ùúΩ t \\\\bm{\\\\theta}_{t} . 8 8 8 We use U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) to denote all unitary operations composed of multiple parametrized gates, each acting on one or more qubits with parameters drawn from ùúΩ t \\\\bm{\\\\theta}_{t} . Applying the gates in U \\u200b ( ùúΩ t ) U(\\\\bm{\\\\theta}_{t}) to the quantum state ùê° t ‚àí 1 = | œà t ‚àí 1 ‚ü© \\\\mathbf{h}_{t-1}=|\\\\psi_{t-1}\\\\rangle yields the updated state ùê° t = U \\u200b ( ùúΩ t ) \\u200b | œà t ‚àí 1 ‚ü© \\\\mathbf{h}_{t}=U(\\\\bm{\\\\theta}_{t})\\\\,|\\\\psi_{t-1}\\\\rangle . The updated state is then measured to obtain a classical readout vector\\nwhich serves as a proxy for the quantum state and is combined with the next input ùê± t + 1 \\\\mathbf{x}_{t+1} to evolve the recurrence. Mid-circuit measurements are simulated without collapsing the quantum state via projective measurements, thereby preserving coherence across timesteps.\\nWe train the entire hybrid model end-to-end using classical backpropagation, optimizing the parameters ùöØ = { ùêñ 1 , ùêõ 1 , ùêñ 2 , ùêõ 2 } \\\\bm{\\\\Theta}=\\\\{\\\\mathbf{W}_{1},\\\\mathbf{b}_{1},\\\\mathbf{W}_{2},\\\\mathbf{b}_{2}\\\\} via standard optimizers, such as Adam [Kingma and Ba, 2014 ] .\\nBecause each ùê≥ t \\\\mathbf{z}_{t} is real-valued, it can be used both as a per-timestep output and as a contextual embedding for soft attention in sequence-to-sequence decoding.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Experiments',\n",
       "    'paragraphs': 'We use the ansatz shown in Fig. 1(a) (scaled to more qubits when required) as the core circuit for the QRNN. Sim et\\xa0al. [ 2019 ] demonstrate experimentally that this ansatz is expressive, capable of generating strong entanglement, and able to represent a significant portion of the Hilbert space, even compared to deeper circuits built from less expressive ans√§tze. 9 9 9 See Appendix B for details on the PQC design and expressibility evaluation methodology. We implement and simulate the model using TorchQuantum [Wang et\\xa0al., 2022 ] , which remains less optimized than classical toolkits due to the lack of efficient kernels for hybrid operations involving tight classical‚Äìquantum feedback, particularly in recurrent settings. Our ansatz balances expressivity, implementation simplicity, and simulation efficiency.\\nFor ùêåùêûùêöùê¨ùêÆùê´ùêû \\\\mathbf{Measure} in Eq. 3 , measurements are performed in each of the Pauli- ùêó \\\\mathbf{X} , Pauli- ùêò \\\\mathbf{Y} , and Pauli- ùêô \\\\mathbf{Z} observables across all wires in the PQC, and the measurement outcomes are combined to form ùê≥ t \\\\mathbf{z}_{t} (Eq. 3 ). For the feedforward network ‚Ñ± \\\\mathcal{F} (Eq. 1 and Eq. 2 ), we experimented with ReLU, LeakyReLU, GLU and GELU nonlinearities. 10 10 10 GLU\\nrequires projecting to twice the output dimensionality, effectively increasing the parameter count compared to standard nonlinearities like ReLU, when all other dimensions are held constant. For both language modeling and translation, we first transform the measurement outcomes with a separate feedforward layer and use the result both for vocabulary classification and as input to the next timestep.\\nAll experiments are run on a single A100/A30 GPU and we select the best models on the validation split across different random seeds and report the test results. The per-epoch training runtime ranges from ~4 minutes for MNIST (with 10 qubits) to ~60 minutes for language modeling (with 14 qubits). Hyperparameters shared across all the tasks include the Adam optimizer without learning rate decay ( l \\u200b r = 1 √ó 10 ‚àí 3 , Œª = 1 √ó 10 ‚àí 4 , and \\u200b œµ = 1 √ó 10 ‚àí 10 ) (lr=1\\\\times 10^{-3},\\\\ \\\\lambda=1\\\\times 10^{-4},\\\\ \\\\text{and}\\\\ \\\\epsilon=1\\\\times 10^{-10}) and dropout applied to the input at each step, with task-dependent drop rates. We apply full-sequence backpropagation without truncation, except for language modeling, where sequences are truncated to 35 tokens. No pretrained word embeddings are used. Additional hyperparameters and test set statistics (mean, min, max across runs) are provided in Appendix C . For scoRNN, we use a hidden size of 170 and the hyperparameters from Helfrich et\\xa0al. [ 2018 ] are used throughout.\\nThe IMDB sentiment dataset [Maas et\\xa0al., 2011 ] is a balanced binary classification benchmark with 25K labeled reviews each for training and testing. The average review length is 241 tokens, with a maximum length of 2,500 tokens. We use 7.5K reviews from the training set for validation and truncate all reviews to a maximum length of 400 tokens across all models.\\nThe hybrid model for this task follows the generic hybrid architecture described in\\xa0¬ß 3.2 . At the final input token, we apply an affine transformation to the measurement outcomes to produce two logits, which are used for classification via cross-entropy. Table 1 summarizes the results. QRNN LeakyReLU achieves the highest test accuracy. Ablating the classical nonlinearity (Eq. 1 ) degrades performance, though increasing the number of qubits in the linear model still yields some accuracy gains. Adding the nonlinearity results in a substantial improvement, outperforming all baselines. On this task, the orthogonal scoRNN underperforms other models, despite having a larger hidden state and over five times more parameters.\\nWe report results on the full MNIST dataset without downsampling using the same model as for IMDB, except with 10 output classes instead of binary classification. The standard pixel-by-pixel permuted MNIST (pMNIST) setup [Le et\\xa0al., 2015 , Arjovsky et\\xa0al., 2016 ] requires 784 steps to process each 28 √ó 28 28\\\\times 28 digit, which makes simulation prohibitively slow. Here we permute the pixels of each digit first, which are then reshaped back to 28 √ó 28 28\\\\times 28 . In both the standard and permuted cases, we use the same hyperparameters.\\nTable 2 shows that QRNNs with three different types of nonlinearity outperform the classical baselines on both tasks, clearly demonstrating the benefit of adding classical nonlinearities compared to the QRNN Linear models. We observe that permutation leads to an accuracy drop across all models: 2.45 % 2.45\\\\% for QRNN GELU , 3.00 % 3.00\\\\% for the RNN, 3.51 % 3.51\\\\% for the LSTM, and 1.51 % 1.51\\\\% for scoRNN, which achieves comparable performance to QRNN GELU .\\nThe copying memory problem tests a model‚Äôs ability to retain and recall information over long sequences [Hochreiter and Schmidhuber, 1997 , Arjovsky et\\xa0al., 2016 ] . Each input sequence has T + 20 T+20 tokens, where the first k = 10 k=10 are random digits from 1 to 8 ( n classes n_{\\\\text{classes}} ), followed by zeros, and the last 11 ( k + 1 k+1 ) positions are filled with the digit ‚Äò9‚Äô with the first ‚Äò9‚Äô acting as a delimiter. The model must learn to detect the delimiter and recall the original digits right after it in the output sequence. We randomly generated 5K training and 1K test samples with T = 200 T=200 (for training efficiency of QRNNs). A random guess baseline yields a loss of k ‚ãÖ log \\u2061 ( n classes ‚àí 1 ) T + 2 \\u200b k ‚âà 0.095 , \\\\frac{k\\\\cdot\\\\log(n_{\\\\text{classes}}-1)}{T+2k}\\\\approx 0.095, reflecting the expected cross-entropy when choosing uniformly from incorrect digits. On this task, QRNN-2.3K matches LSTM-168K (loss 0.07, accuracy 97%) and outperforms LSTM-2.8K (loss 0.25, accuracy 89.4%). scoRNN, specialized for this task, achieves near-perfect results, highlighting a performance gap between general-purpose and tailored models.\\nThe PTB dataset [Mikolov et\\xa0al., 2011 ] consists of 929K training tokens, 73K validation tokens, and 82K test tokens. As is standard, we use a vocabulary size of 10K, converting OOV tokens to UNK . We tested scoRNN on this task, but it did not converge to a good solution. The LSTM achieved the best result, with 120.30 perplexity (PPL), followed closely by QRNN LeakyReLU at 126.58.\\nSoft attentions can be implemented using various formulations, such as additive attention or dot-product attention [Luong et\\xa0al., 2015 ] , but they share the same core principle: at each decoder timestep, compute a similarity score between the current decoder state and each encoder state, normalize these scores via a softmax, and form a context vector by summation, which is then combined with the decoder‚Äôs hidden state to generate the next output token.\\nThe attention mechanism implemented here follows the additive attention of Bahdanau et\\xa0al. [ 2015 ] . At each decoding step, the decoder hidden state is concatenated with encoder outputs, passed through a tanh activation followed by a linear projection to compute alignment scores. A softmax then normalizes these scores into attention weights, with masking applied to exclude padded positions.\\nWe applied the model to Multi30k German-to-English translation [Elliott et\\xa0al., 2016 ] , with vocabulary sizes of 19.2K for German and 10.8K for English, and an average of 11 tokens per sentence in both languages. The training set contains 29K sentence pairs, with 1K each for validation and testing.\\nResults in Table 4 show that QRNN GLU with 13 qubits closely matches the LSTM, followed by QRNN GLU with 10 qubits. For the QRNN, it is somewhat surprising that intermediate readouts can still support mechanisms like soft attention, since these readouts capture only partial projections of the quantum state rather than the full hidden state. This suggests that, despite intermediate measurements, sufficient information is retained and propagated across timesteps. We qualitatively interpret the learned soft alignments on a few examples where the translations required non-trivial linguistic interpretations in Appendix D .\\nWe measure per-timestep gradient norms on IMDB ( T = 400 T=400 ) and pMNIST ( T = 28 T=28 ) by retaining gradients on the per-timestep readouts (QRNN) and hidden states (LSTM) from saved checkpoints and computing ‚à• ‚àÇ ‚Ñí / ‚àÇ ùê° t ‚à• 2 \\\\lVert\\\\partial\\\\mathcal{L}/\\\\partial\\\\mathbf{h}_{t}\\\\rVert_{2} .\\nGradients are averaged across samples in a mini-batch and normalized by the last-step norm ‚à• ‚àÇ ‚Ñí / ‚àÇ ùê° T ‚à• 2 \\\\lVert\\\\partial\\\\mathcal{L}/\\\\partial\\\\mathbf{h}_{T}\\\\rVert_{2} to compare decay shape.\\nAs shown in Fig. 3 , the QRNN curves remain consistently above the LSTM on both IMDB and pMNIST, indicating less vanishing through time toward the start of the sequences. All curves start with 1.0 1.0 at t = T t=T (normalization), but the relative elevation of the QRNN curve at earlier timesteps demonstrates more stable gradient propagation. The LSTM gradient norm decays rapidly, collapsing below 10 ‚àí 4 10^{-4} on the relatively short pMNIST sequences.',\n",
       "    'subsections': [{'title': '4.1 Sentiment Analysis',\n",
       "      'paragraphs': 'The IMDB sentiment dataset [Maas et\\xa0al., 2011 ] is a balanced binary classification benchmark with 25K labeled reviews each for training and testing. The average review length is 241 tokens, with a maximum length of 2,500 tokens. We use 7.5K reviews from the training set for validation and truncate all reviews to a maximum length of 400 tokens across all models.\\nThe hybrid model for this task follows the generic hybrid architecture described in\\xa0¬ß 3.2 . At the final input token, we apply an affine transformation to the measurement outcomes to produce two logits, which are used for classification via cross-entropy. Table 1 summarizes the results. QRNN LeakyReLU achieves the highest test accuracy. Ablating the classical nonlinearity (Eq. 1 ) degrades performance, though increasing the number of qubits in the linear model still yields some accuracy gains. Adding the nonlinearity results in a substantial improvement, outperforming all baselines. On this task, the orthogonal scoRNN underperforms other models, despite having a larger hidden state and over five times more parameters.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 MNIST and Permuted-MNIST',\n",
       "      'paragraphs': 'We report results on the full MNIST dataset without downsampling using the same model as for IMDB, except with 10 output classes instead of binary classification. The standard pixel-by-pixel permuted MNIST (pMNIST) setup [Le et\\xa0al., 2015 , Arjovsky et\\xa0al., 2016 ] requires 784 steps to process each 28 √ó 28 28\\\\times 28 digit, which makes simulation prohibitively slow. Here we permute the pixels of each digit first, which are then reshaped back to 28 √ó 28 28\\\\times 28 . In both the standard and permuted cases, we use the same hyperparameters.\\nTable 2 shows that QRNNs with three different types of nonlinearity outperform the classical baselines on both tasks, clearly demonstrating the benefit of adding classical nonlinearities compared to the QRNN Linear models. We observe that permutation leads to an accuracy drop across all models: 2.45 % 2.45\\\\% for QRNN GELU , 3.00 % 3.00\\\\% for the RNN, 3.51 % 3.51\\\\% for the LSTM, and 1.51 % 1.51\\\\% for scoRNN, which achieves comparable performance to QRNN GELU .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Copying Memory',\n",
       "      'paragraphs': 'The copying memory problem tests a model‚Äôs ability to retain and recall information over long sequences [Hochreiter and Schmidhuber, 1997 , Arjovsky et\\xa0al., 2016 ] . Each input sequence has T + 20 T+20 tokens, where the first k = 10 k=10 are random digits from 1 to 8 ( n classes n_{\\\\text{classes}} ), followed by zeros, and the last 11 ( k + 1 k+1 ) positions are filled with the digit ‚Äò9‚Äô with the first ‚Äò9‚Äô acting as a delimiter. The model must learn to detect the delimiter and recall the original digits right after it in the output sequence. We randomly generated 5K training and 1K test samples with T = 200 T=200 (for training efficiency of QRNNs). A random guess baseline yields a loss of k ‚ãÖ log \\u2061 ( n classes ‚àí 1 ) T + 2 \\u200b k ‚âà 0.095 , \\\\frac{k\\\\cdot\\\\log(n_{\\\\text{classes}}-1)}{T+2k}\\\\approx 0.095, reflecting the expected cross-entropy when choosing uniformly from incorrect digits. On this task, QRNN-2.3K matches LSTM-168K (loss 0.07, accuracy 97%) and outperforms LSTM-2.8K (loss 0.25, accuracy 89.4%). scoRNN, specialized for this task, achieves near-perfect results, highlighting a performance gap between general-purpose and tailored models.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.4 Word-Level Language Modeling',\n",
       "      'paragraphs': 'The PTB dataset [Mikolov et\\xa0al., 2011 ] consists of 929K training tokens, 73K validation tokens, and 82K test tokens. As is standard, we use a vocabulary size of 10K, converting OOV tokens to UNK . We tested scoRNN on this task, but it did not converge to a good solution. The LSTM achieved the best result, with 120.30 perplexity (PPL), followed closely by QRNN LeakyReLU at 126.58.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.5 Machine Translation',\n",
       "      'paragraphs': 'Soft attentions can be implemented using various formulations, such as additive attention or dot-product attention [Luong et\\xa0al., 2015 ] , but they share the same core principle: at each decoder timestep, compute a similarity score between the current decoder state and each encoder state, normalize these scores via a softmax, and form a context vector by summation, which is then combined with the decoder‚Äôs hidden state to generate the next output token.\\nThe attention mechanism implemented here follows the additive attention of Bahdanau et\\xa0al. [ 2015 ] . At each decoding step, the decoder hidden state is concatenated with encoder outputs, passed through a tanh activation followed by a linear projection to compute alignment scores. A softmax then normalizes these scores into attention weights, with masking applied to exclude padded positions.\\nWe applied the model to Multi30k German-to-English translation [Elliott et\\xa0al., 2016 ] , with vocabulary sizes of 19.2K for German and 10.8K for English, and an average of 11 tokens per sentence in both languages. The training set contains 29K sentence pairs, with 1K each for validation and testing.\\nResults in Table 4 show that QRNN GLU with 13 qubits closely matches the LSTM, followed by QRNN GLU with 10 qubits. For the QRNN, it is somewhat surprising that intermediate readouts can still support mechanisms like soft attention, since these readouts capture only partial projections of the quantum state rather than the full hidden state. This suggests that, despite intermediate measurements, sufficient information is retained and propagated across timesteps. We qualitatively interpret the learned soft alignments on a few examples where the translations required non-trivial linguistic interpretations in Appendix D .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.6 Hidden State Gradients',\n",
       "      'paragraphs': 'We measure per-timestep gradient norms on IMDB ( T = 400 T=400 ) and pMNIST ( T = 28 T=28 ) by retaining gradients on the per-timestep readouts (QRNN) and hidden states (LSTM) from saved checkpoints and computing ‚à• ‚àÇ ‚Ñí / ‚àÇ ùê° t ‚à• 2 \\\\lVert\\\\partial\\\\mathcal{L}/\\\\partial\\\\mathbf{h}_{t}\\\\rVert_{2} .\\nGradients are averaged across samples in a mini-batch and normalized by the last-step norm ‚à• ‚àÇ ‚Ñí / ‚àÇ ùê° T ‚à• 2 \\\\lVert\\\\partial\\\\mathcal{L}/\\\\partial\\\\mathbf{h}_{T}\\\\rVert_{2} to compare decay shape.\\nAs shown in Fig. 3 , the QRNN curves remain consistently above the LSTM on both IMDB and pMNIST, indicating less vanishing through time toward the start of the sequences. All curves start with 1.0 1.0 at t = T t=T (normalization), but the relative elevation of the QRNN curve at earlier timesteps demonstrates more stable gradient propagation. The LSTM gradient norm decays rapidly, collapsing below 10 ‚àí 4 10^{-4} on the relatively short pMNIST sequences.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Discussion and Conclusion',\n",
       "    'paragraphs': 'Different quantum hardware platforms currently require distinct control stacks, and architectural choices do not translate one-to-one across devices, with factors such as native gate sets, qubit connectivity, and the implementation of mid-circuit measurements all affecting the realization of a given circuit. The aim here is not to prescribe a hardware roadmap but to analyze a hardware-realistic base case under idealized classical simulation to study the empirical properties of the architecture, where we model mid-circuit observations via projective measurements as a limiting case.\\nAs more efficient and scalable toolchains become available (e.g., future multi-GPU toolkits based on cuQuantum [Bayraktar et\\xa0al., 2023 ] ), we anticipate more faithful simulations via ancilla-mediated schemes in which auxiliary qubits are entangled with the main circuit, measured, and reset as needed while the recurrent memory remains coherent. This aligns with mid-circuit measure-and-reset operations already supported on several platforms [DeCross et\\xa0al., 2022 , Lis et\\xa0al., 2023 , Norcia et\\xa0al., 2023 ] , although hardware implementations for large-scale sequence modeling would require fault-tolerant devices capable of sustaining long coherent recurrences and real-time classical control.\\nThis paper bridges quantum operations and recurrent learning by introducing a new hybrid QRNN whose recurrent core is implemented as a PQC steered by a classical controller. The unitary dynamics preserve norms, promoting stable gradient propagation; the controller supplies the nonlinearity and task adaptivity needed for expressiveness; and mid-circuit measurements enable per-timestep readouts and classical feedback. As techniques improve [Abbas et\\xa0al., 2023 ] and quantum hardware matures, the architecture provides a path toward hardware-realistic quantum models for sequential learning.\\nI would like to thank Bob Coecke for the research environment, Dimitri Kartsaklis, Sean Tull, and David Amaro for comments on an earlier draft.',\n",
       "    'subsections': [{'title': 'Acknowledgments',\n",
       "      'paragraphs': 'I would like to thank Bob Coecke for the research environment, Dimitri Kartsaklis, Sean Tull, and David Amaro for comments on an earlier draft.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix A Quantum States and Superposition',\n",
       "    'paragraphs': 'Unlike a classical bit, a qubit exists in a superposition of the states 0 and 1 in a two-dimensional complex Hilbert space: | œà ‚ü© = Œ± | 0 ‚ü© + Œ≤ | 1 ‚ü© = [ Œ± Œ≤ ] T ‚àà ‚ÑÇ 2 \\\\lvert\\\\psi\\\\rangle=\\\\alpha\\\\lvert 0\\\\rangle+\\\\beta\\\\lvert 1\\\\rangle=\\\\begin{bmatrix}\\\\alpha&\\\\beta\\\\end{bmatrix}^{T}\\\\in\\\\mathbb{C}^{2} and | 0 ‚ü© = [ 1 0 ] T \\\\lvert 0\\\\rangle=\\\\begin{bmatrix}1&0\\\\end{bmatrix}^{T} and | 1 ‚ü© = [ 0 1 ] T \\\\lvert 1\\\\rangle=\\\\begin{bmatrix}0&1\\\\end{bmatrix}^{T} are elements of the computational basis for the Hilbert space. The coefficients Œ± \\\\alpha and Œ≤ \\\\beta are complex numbers referred to as the amplitudes that satisfy | Œ± | 2 + | Œ≤ | 2 = 1 |\\\\alpha|^{2}+|\\\\beta|^{2}=1 . For a state | œà ‚ü© = Œ± \\u200b | 0 ‚ü© + Œ≤ \\u200b | 1 ‚ü© |\\\\psi\\\\rangle=\\\\alpha|0\\\\rangle+\\\\beta|1\\\\rangle , the probability of obtaining | 0 ‚ü© |0\\\\rangle is | Œ± | 2 |\\\\alpha|^{2} , and the probability of obtaining | 1 ‚ü© |1\\\\rangle is | Œ≤ | 2 |\\\\beta|^{2} .',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B PQC Template',\n",
       "    'paragraphs': 'We have chosen the PQC template based on the benchmarking study in Sim et\\xa0al. [ 2019 ] , which evaluates 19 different parametrized quantum circuits (PQCs) up to depth 5 (i.e., the base circuit repeated up to five times and used a single PQC). Each PQC is assessed using two key metrics: expressibility and entangling capability. The architecture referred to as ansatz-14 in Sim et\\xa0al. [ 2019 ] which we use here in a single layer configuration was shown to score highly on both. This gives a good balance of simulation cost and \"goodness\" of the PQC.\\nExpressibility is quantified by comparing the distribution of pairwise fidelities between states generated by the PQC to the theoretical fidelity distribution of Haar-random states, which represent uniform randomness over the composite Hilbert space (the tensor product of individual qubit spaces). Instead of generating Haar-random states directly, the method in [Sim et\\xa0al., 2019 ] uses the analytical form of the Haar fidelity distribution as a reference. PQC output states are obtained by sampling random parameters, and their pairwise fidelities are used to construct an empirical distribution. The KL divergence between this empirical distribution and the Haar reference provides a scalar expressibility score, with lower values indicating greater expressiveness.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Experimental Settings and Test Accuracy Statistics Across Runs',\n",
       "    'paragraphs': 'While parametrized quantum circuits (PQCs) can suffer from vanishing gradients in deep or wide settings due to the barren plateau phenomenon [McClean et\\xa0al., 2018 ] , there is no general impossibility theorem that barren plateaus must occur in all parametrized quantum circuits; their presence and severity are known to depend on the ansatz, cost function, initialization, training strategy, and noise, and remain an empirical matter at practical scales. Several studies provide insights into how it arises or design principles that prevent or mitigate plateaus [Cerezo et\\xa0al., 2019 , Grant et\\xa0al., 2019 , Patti et\\xa0al., 2021 , Sack et\\xa0al., 2022 ] . These results indicate that barren plateaus are not inevitable, and that careful design yields a tractable and stable training landscape in practice. In particular, some architectures such as quantum convolutional neural networks avoid barren plateaus by construction [Pesah et\\xa0al., 2021 ] , which supports the view that appropriate architectural choices can produce stable and trainable quantum models.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Attention Alignments',\n",
       "    'paragraphs': 'To qualitatively analyze the model‚Äôs learned soft attention alignments we selected four sentences from test set and interpreted the hybrid model translations and alignments (Fig. 4 ).\\nWe observe that the hybrid model can manage spatial and syntactic shifts while capturing clause-level structure and semantics through its measurement-driven hidden states and soft attention as well as the LSTM baseline. It is evident that the model handles compound verb constructions and semantic expansion , in sentences like ‚ÄúDiese Band bereitet sich auf einen Auftritt vor Publikum in einer Kirche vor‚Äù (Fig. 4(a) ) and ‚ÄúZwei gr√ºn gekleidete M√§nner bereiten in einem Restaurant Essen zu‚Äù (Fig. 4(b) ), where German separable verbs‚Äî ‚Äúbereitet ‚Ä¶ vor‚Äù and ‚Äúbereiten ‚Ä¶ zu‚Äù ‚Äîare correctly reconstructed into the English verb phrases ‚Äúis preparing to perform‚Äù and ‚Äúpreparing‚Äù , respectively. The soft attention allowed the model to attend across non-contiguous source tokens, enabling reassembly of verb phrases. Additionally, lexical expansions such as ‚ÄúPublikum‚Äù ‚Üí \\\\rightarrow ‚Äúa crowd of people‚Äù (Fig. 4(a) ) and ‚Äúgekleidete M√§nner‚Äù ‚Üí \\\\rightarrow ‚Äúmen in green outfits‚Äù (Fig. 4(b) ) demonstrate contextually appropriate semantic elaboration beyond literal translation.\\nThe model also displays syntactic reordering and clause realignment , necessitated by divergences between German and English word order. This is shown in both ‚ÄúDiese Band ‚Ä¶ vor Publikum ‚Ä¶ vor‚Äù and (Fig. 4(a) ) ‚ÄúMenschen, die vor einem gro√üen Geb√§ude im Kreis sitzen‚Äù (Fig. 4(c) ). In the former, German‚Äôs verb-final structure is reorganized into a mid-sentence English verb phrase, while handling nested prepositional phrases. In the latter, the relative clause ‚Äúdie ‚Ä¶ sitzen‚Äù is compressed into the participial phrase ‚Äúsitting‚Äù , dropping auxiliaries and pronouns to better fit English syntactic norms. Similarly, the location and positional phrases ‚Äúim Kreis‚Äù and ‚Äúvor einem gro√üen Geb√§ude‚Äù are reordered into ‚Äúin a circle in front of a large building‚Äù\\nLastly, for multi-clause coordination , tense adaptation , and long-range dependency tracking , as seen in ‚ÄúAcht M√§nner spielen auf der B√ºhne, w√§hrend ein Gitarrist im Scheinwerferlicht spielt‚Äù (Fig. 4(d) ). The model successfully disentangles two coordinated clauses and renders them with the correct English conjunction ‚Äúwhile‚Äù , while adjusting verb forms from German‚Äôs uniform ‚Äúspielen‚Äù to ‚Äúare playing‚Äù and ‚Äúplays‚Äù , based on subject plurality. Finally, this ability to flexibly adapt clause boundaries and maintain coherence is also reflected in the ‚ÄúMenschen ‚Ä¶ im Kreis sitzen‚Äù example (Fig. 4(c) ), where the model tracks relative clause dependencies and maps them onto compact English constructions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the entire recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an nn-qubit PQC, residing in an exponentially large Hilbert space ‚ÑÇ2n\\\\mathbb{C}^{2^{n}}. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit measurements, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, adopting projective measurements as a limiting case to obtain mid-circuit readouts while maintaining a coherent recurrent quantum memory. We further devise a soft attention mechanism over the mid-circuit readouts in a sequence-to-sequence model and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.'},\n",
       " '2510.25510v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Text-to-SQL, the task of automatically translating natural language questions into executable SQL queries, is a key technique for lowering the barrier to database access (Liu et\\xa0al., 2025 ) . By enabling non-technical users to query structured data in natural language, it has found wide applications in business intelligence, data analytics, and interactive question answering (Hong et\\xa0al., 2025 ) .\\nExisting approaches to Text-to-SQL generally fall into three paradigms: (i) Supervised Fine-Tuning (SFT) of domain-specific open-source models (Li et\\xa0al., 2025b ; 2024b ) ; (ii) prompting closed-source large language models (LLMs) with advanced reasoning strategies such as chain-of-thought (Li et\\xa0al., 2025a ; Zhai et\\xa0al., 2025 ; Pourreza et\\xa0al., 2024 ) ; and (iii) Reinforcement Learning (RL)-based methods that optimize model reasoning with algorithms such as PPO and GRPO (Pourreza et\\xa0al., 2025 ; Ma et\\xa0al., 2025 ; Yao et\\xa0al., 2025 ; Dai et\\xa0al., 2025 ) . In particular, RL methods leverage final SQL execution results as reward signals for policy optimization. However, current practices treat execution feedback merely as scalar rewards, wasting rich tool information and leaving static LLMs unable to adapt their reasoning dynamically.\\nRecently, Multi-turn Tool-Integrated Reasoning (MTIR) has emerged as a promising paradigm for enhancing the reasoning capabilities of LLMs. By interleaving model reasoning with external tools‚Äîsuch as search engines, Python interpreters, and SQL executors‚ÄîMTIR enables LLMs to overcome intrinsic limitations in computation, retrieval, and structured manipulation. Several studies have demonstrated the benefits of RL-based optimization in tool use: Search-R1 (Jin et\\xa0al., 2025 ) explores dynamic tool invocation for question answering, while ToRL (Li et\\xa0al., 2025c ) and Effective CIR (Bai et\\xa0al., 2025 ) design tailored RL recipes for mathematical reasoning. More recently, the ReEx-SQL (Dai et\\xa0al., 2025 ) framework extends TIR to Text-to-SQL, providing early evidence of its potential in structured query tasks.\\nDespite this progress, three fundamental challenges remain. On the tool side , SQL-oriented MTIR remains underexplored; existing efforts often rely on ad-hoc token mechanisms for tool invocation, limiting interoperability, extensibility, and compatibility with diverse database operations (Jin et\\xa0al., 2025 ; Dai et\\xa0al., 2025 ) . On the RL side , dominant methods such as GRPO suffer from reward collapse and difficulty in modeling long-horizon dependencies, leading to instability in multi-turn tool interaction. On the framework side , current Text-to-SQL studies adopt heterogeneous, non-standardized implementations that lack modularity and generality (Dai et\\xa0al., 2025 ; Ma et\\xa0al., 2025 ; Yao et\\xa0al., 2025 ; Gajjar et\\xa0al., 2025 ) .\\nTo address these issues, we propose MTIR-SQL , a reinforcement learning framework for Multi-turn Tool-Integrated Reasoning in Text-to-SQL (Figure 1 ). MTIR-SQL extends GRPO to handle complex multi-turn interactions and introduces two key modifications: (i) a trajectory filtering mechanism to discard invalid rollouts and (ii) the removal of KL regularization to mitigate distributional collapse during training. Built on top of RL-Factory (Chai et\\xa0al., 2025 ) with standardized MCP-compatible tool invocation, our framework ensures extensibility and interoperability.\\nOur contributions are summarized as follows:\\nMTIR-SQL Framework. We introduce a novel RL framework for Text-to-SQL that enables LLMs to reason interactively and directly optimize via SQL execution feedback. It incorporates retrieval-based token masking for stable training and supports multi-turn iterative reasoning and execution verification.\\nGRPO Extensions. We extend GRPO with SQL execution rollout expansion and trajectory filtering to stabilize training in multi-turn tool-use scenarios, effectively mitigating reward collapse.\\nStrong Empirical Results. On the BIRD dataset, MTIR-SQL trained on Qwen-3-4B achieves a 16% absolute improvement over baselines, matching the performance of recent 7B-coder models. It attains competitive execution accuracy, demonstrating its robustness and effectiveness.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Tool-Integrated Reasoning (TIR) has emerged as a key paradigm for augmenting large language models by enabling interaction with external tools and APIs (Zhang et\\xa0al., 2025 ) . Early work focused on single-turn tool invocation through supervised fine-tuning approaches, demonstrating effectiveness across domains, including mathematical reasoning, code generation (Mai et\\xa0al., 2025 ) , and search integration (Jin et\\xa0al., 2025 ) . Multi-turn TIR enables iterative reasoning through sequential tool interactions, where models repeatedly generate tool calls, execute it, and refine based on results (Mai et\\xa0al., 2025 ; Shang et\\xa0al., 2025 ; Fu et\\xa0al., 2025 ; Yue et\\xa0al., 2025 ; Lin et\\xa0al., 2025 ; Wang et\\xa0al., 2025 ; Zeng et\\xa0al., 2025 ; Zhao et\\xa0al., 2025 ) . However, training stability remains a critical challenge due to distribution drift from external tool outputs and error accumulation across reasoning rounds, often leading to training instability and entropy collapse. Recent work like SimpleTIR (Xue et\\xa0al., 2025 ) addresses these issues by filtering empty rounds in multi-turn reasoning, achieving state-of-the-art performance on mathematical tasks. Despite these advances, optimizing Multi-turn TIR for complex tasks remains challenging (Lin & Xu, 2025 ; Dong et\\xa0al., 2025a ; b ; Yu et\\xa0al., 2025 ) . We apply recent MTIR advancements, including filtering and handling invalid turns, to the Text-to-SQL domain, improving execution feedback management, multi-table relationship handling, and ensuring SQL semantic correctness across iterations.\\nText-to-SQL aims to automatically convert natural language questions into executable SQL query statements, enabling natural language interfaces for databases. The field has evolved through three main paradigms: supervised fine-tuning methods that train specialized models on domain-specific datasets (Li et\\xa0al., 2024b ; 2025b ; Yang et\\xa0al., 2024 ; Qin et\\xa0al., 2025 ) , using closed-source large models with prompt engineering and chain-of-thought reasoning to handle complex multi-table joins and nested queries (Li et\\xa0al., 2025a ; Zhai et\\xa0al., 2025 ; Pourreza et\\xa0al., 2024 ; 2025 ; Lyu et\\xa0al., 2025 ; Pourreza & Rafiei, 2023 ; Xie et\\xa0al., 2024 ; Cao et\\xa0al., 2024 ) , and reinforcement learning approaches using algorithms such as GRPO to enhance reasoning capabilities and generalization (Dai et\\xa0al., 2025 ) . Despite recent advancements, current RL-based methods exhibit significant limitations. They rely on static context during generation and lack mechanisms for validating or correcting intermediate reasoning steps, resulting in errors that cannot be self-corrected (Ma et\\xa0al., 2025 ; Yao et\\xa0al., 2025 ; Gajjar et\\xa0al., 2025 ) . Execution feedback is treated as a reward signal rather than dynamically integrated, hindering the model‚Äôs ability to adapt to complex scenarios. The challenge persists in incorporating execution feedback while managing database results, multi-table relationships, and ensuring SQL semantic correctness. To address these issues, we introduce Multi-turn Tool-Integrated Reasoning in the Text-to-SQL domain, enabling the model to improve performance through iterative use of external tools.',\n",
       "    'subsections': [{'title': '2.1 RL for Tool-Integrated Reasoning',\n",
       "      'paragraphs': 'Tool-Integrated Reasoning (TIR) has emerged as a key paradigm for augmenting large language models by enabling interaction with external tools and APIs (Zhang et\\xa0al., 2025 ) . Early work focused on single-turn tool invocation through supervised fine-tuning approaches, demonstrating effectiveness across domains, including mathematical reasoning, code generation (Mai et\\xa0al., 2025 ) , and search integration (Jin et\\xa0al., 2025 ) . Multi-turn TIR enables iterative reasoning through sequential tool interactions, where models repeatedly generate tool calls, execute it, and refine based on results (Mai et\\xa0al., 2025 ; Shang et\\xa0al., 2025 ; Fu et\\xa0al., 2025 ; Yue et\\xa0al., 2025 ; Lin et\\xa0al., 2025 ; Wang et\\xa0al., 2025 ; Zeng et\\xa0al., 2025 ; Zhao et\\xa0al., 2025 ) . However, training stability remains a critical challenge due to distribution drift from external tool outputs and error accumulation across reasoning rounds, often leading to training instability and entropy collapse. Recent work like SimpleTIR (Xue et\\xa0al., 2025 ) addresses these issues by filtering empty rounds in multi-turn reasoning, achieving state-of-the-art performance on mathematical tasks. Despite these advances, optimizing Multi-turn TIR for complex tasks remains challenging (Lin & Xu, 2025 ; Dong et\\xa0al., 2025a ; b ; Yu et\\xa0al., 2025 ) . We apply recent MTIR advancements, including filtering and handling invalid turns, to the Text-to-SQL domain, improving execution feedback management, multi-table relationship handling, and ensuring SQL semantic correctness across iterations.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Text-to-SQL',\n",
       "      'paragraphs': 'Text-to-SQL aims to automatically convert natural language questions into executable SQL query statements, enabling natural language interfaces for databases. The field has evolved through three main paradigms: supervised fine-tuning methods that train specialized models on domain-specific datasets (Li et\\xa0al., 2024b ; 2025b ; Yang et\\xa0al., 2024 ; Qin et\\xa0al., 2025 ) , using closed-source large models with prompt engineering and chain-of-thought reasoning to handle complex multi-table joins and nested queries (Li et\\xa0al., 2025a ; Zhai et\\xa0al., 2025 ; Pourreza et\\xa0al., 2024 ; 2025 ; Lyu et\\xa0al., 2025 ; Pourreza & Rafiei, 2023 ; Xie et\\xa0al., 2024 ; Cao et\\xa0al., 2024 ) , and reinforcement learning approaches using algorithms such as GRPO to enhance reasoning capabilities and generalization (Dai et\\xa0al., 2025 ) . Despite recent advancements, current RL-based methods exhibit significant limitations. They rely on static context during generation and lack mechanisms for validating or correcting intermediate reasoning steps, resulting in errors that cannot be self-corrected (Ma et\\xa0al., 2025 ; Yao et\\xa0al., 2025 ; Gajjar et\\xa0al., 2025 ) . Execution feedback is treated as a reward signal rather than dynamically integrated, hindering the model‚Äôs ability to adapt to complex scenarios. The challenge persists in incorporating execution feedback while managing database results, multi-table relationships, and ensuring SQL semantic correctness. To address these issues, we introduce Multi-turn Tool-Integrated Reasoning in the Text-to-SQL domain, enabling the model to improve performance through iterative use of external tools.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Methodology',\n",
       "    'paragraphs': 'We propose an SQL-integrated reinforcement learning framework with GRPO-Filter, which combines unconstrained optimization, selective rollout filtering, and multi-turn reasoning to improve decision-making. The model dynamically interacts with SQL execution, refining its output through iterative feedback. A reward mechanism focused on format, execution, and result correctness guides the generation of high-quality SQL queries.\\nWe formulate the reinforcement learning framework with SQL execution tool ‚Ñ∞ \\\\mathcal{E} as follows:\\nwhere œÄ Œ∏ \\\\pi_{\\\\theta} is the LLM policy and r œï r_{\\\\phi} is the reward function. Unlike prior reinforcement learning methods that primarily rely on the policy LLM œÄ Œ∏ ( ‚ãÖ | x ) \\\\pi_{\\\\theta}(\\\\cdot|x) to generate rollout sequences, our framework explicitly incorporates SQL execution-guided reasoning via œÄ Œ∏ ( ‚ãÖ | x ; ‚Ñ∞ ) \\\\pi_{\\\\theta}(\\\\cdot|x;\\\\mathcal{E}) , which can be formulated as œÄ Œ∏ ( ‚ãÖ | x ) ‚äó ‚Ñ∞ \\\\pi_{\\\\theta}(\\\\cdot|x)\\\\otimes\\\\mathcal{E} , where ‚äó \\\\otimes denotes the interleaved SQL generation and execution feedback. This enables more effective decision-making in SQL generation tasks by leveraging real-time execution results to guide the model‚Äôs reasoning process.\\nOur approach introduces GRPO-Filter , an enhanced variant of Group Relative Policy Optimization (GRPO) specifically designed for complex multi-turn interactive scenarios. GRPO-Filter incorporates three key innovations:\\nUnconstrained Optimization: Unlike standard GRPO, we remove the KL divergence constraint between the policy and reference model, allowing for more flexible policy updates:\\nwhere A \\u200b ( x , y ) A(x,y) represents the advantage function, eliminating the traditional KL penalty term Œ≤ ‚ãÖ KL ( œÄ Œ∏ | | œÄ r \\u200b e \\u200b f ) \\\\beta\\\\cdot\\\\text{KL}(\\\\pi_{\\\\theta}||\\\\pi_{ref}) .\\nSelective Rollout Filtering: We implement a quality-aware filtering mechanism during rollout generation, where only high-quality trajectories satisfying predefined criteria are retained for training:\\nwhere ‚Ñ± \\u200b ( ‚ãÖ ) \\\\mathcal{F}(\\\\cdot) is the filtering function and œÑ \\\\tau is the quality threshold.\\nMulti-turn Extension: GRPO-Filter extends the original framework to handle complex multi-turn interactions by maintaining conversation context and enabling iterative reasoning across multiple dialogue turns:\\nwhere h < t h_{<t} represents the conversation history up to turn t t , and y t y_{t} is the response at turn t t .\\nThis multifaceted approach allows GRPO-Filter to effectively optimize policies for reasoning-intensive tasks while maintaining training stability and improving sample efficiency through selective learning from high-quality experiences.\\nThe integration of SQL and its execution interface with large language models (LLMs), which are capable of comprehending and generating query intentions, can significantly enhance the automation of complex database operations. In an LLM-based SQL tool invocation environment, the system should exhibit human-like interactive and reasoning behaviors. These behaviors include generating syntactically correct and logically sound SQL queries from natural language questions, invoking database execution interfaces at appropriate moments, and executing queries safely. Additionally, the system should carefully interpret query results, verify their correctness, and refine subsequent problem decomposition or query generation strategies based on feedback. This capability is cultivated through guiding the model via multi-turn interaction and reflective learning with the SQL execution environment. Detailed prompt of sql Execution can be found in Appendix C.1 .\\nWith the support of SQL tools, the model dynamically incorporates database query results into the reasoning process through multi-turn execution, as illustrated in Figure 1 . Unlike conventional methods that generate a complete SQL query until an end-of-sequence (EOS) token is produced, our approach constructs the full reasoning trajectory through continuous interaction with the SQL execution environment. The first interaction begins with a system prompt followed by the user‚Äôs question, with detailed content available in Appendix C.1 . The model, acting as the assistant, generates an initial response until it outputs an EOS token. If no SQL tool call is detected, the process terminates. When an SQL query is identified, the environment service extracts and safely executes it, then appends the execution result to the dialogue context in the user role. The model subsequently continues its reasoning as the assistant based on the updated context, producing the next turn of response. This multi-turn process iterates until the model returns a final answer or a maximum number of turns, denoted as T T , is reached. Detailed content can be found in Appendix B .\\nTo optimize policy effectively, we introduce a streamlined reward mechanism that focuses on critical elements of SQL query quality. This framework incorporates three key factors‚Äîsyntax validity, execution feasibility, and semantic precision‚Äîeach providing distinct guidance to ensure the model generates SQL queries that are syntactically correct, executable, and semantically meaningful.\\nWe guide the model to maintain a specific sequence of tags, ensuring a structured response. The response should follow a strict order: starting with <think>...</think> , followed optionally by <tool_call>...</tool_response> , and concluding with the <answer>...</answer> tag. Additionally, all tools must be used within <tool_call>...</tool_call> and <tool_response>...</tool_response> tags to maintain a standardized flow.\\nThis reward evaluates the syntactic correctness and executability of the generated SQL. It prevents the model from producing invalid or overly complex queries. If the SQL statement fails to execute, the model will not receive subsequent rewards. Furthermore, the execution time is constrained to discourage the generation of unnecessarily complex queries:\\nThe correctness of query results is a crucial measure of semantic fidelity. To encourage faithful reasoning, we design the result reward to strongly differentiate between correct and incorrect outputs:',\n",
       "    'subsections': [{'title': '3.1 SQL-Integrated RL with GRPO-Filter',\n",
       "      'paragraphs': 'We formulate the reinforcement learning framework with SQL execution tool ‚Ñ∞ \\\\mathcal{E} as follows:\\nwhere œÄ Œ∏ \\\\pi_{\\\\theta} is the LLM policy and r œï r_{\\\\phi} is the reward function. Unlike prior reinforcement learning methods that primarily rely on the policy LLM œÄ Œ∏ ( ‚ãÖ | x ) \\\\pi_{\\\\theta}(\\\\cdot|x) to generate rollout sequences, our framework explicitly incorporates SQL execution-guided reasoning via œÄ Œ∏ ( ‚ãÖ | x ; ‚Ñ∞ ) \\\\pi_{\\\\theta}(\\\\cdot|x;\\\\mathcal{E}) , which can be formulated as œÄ Œ∏ ( ‚ãÖ | x ) ‚äó ‚Ñ∞ \\\\pi_{\\\\theta}(\\\\cdot|x)\\\\otimes\\\\mathcal{E} , where ‚äó \\\\otimes denotes the interleaved SQL generation and execution feedback. This enables more effective decision-making in SQL generation tasks by leveraging real-time execution results to guide the model‚Äôs reasoning process.\\nOur approach introduces GRPO-Filter , an enhanced variant of Group Relative Policy Optimization (GRPO) specifically designed for complex multi-turn interactive scenarios. GRPO-Filter incorporates three key innovations:\\nUnconstrained Optimization: Unlike standard GRPO, we remove the KL divergence constraint between the policy and reference model, allowing for more flexible policy updates:\\nwhere A \\u200b ( x , y ) A(x,y) represents the advantage function, eliminating the traditional KL penalty term Œ≤ ‚ãÖ KL ( œÄ Œ∏ | | œÄ r \\u200b e \\u200b f ) \\\\beta\\\\cdot\\\\text{KL}(\\\\pi_{\\\\theta}||\\\\pi_{ref}) .\\nSelective Rollout Filtering: We implement a quality-aware filtering mechanism during rollout generation, where only high-quality trajectories satisfying predefined criteria are retained for training:\\nwhere ‚Ñ± \\u200b ( ‚ãÖ ) \\\\mathcal{F}(\\\\cdot) is the filtering function and œÑ \\\\tau is the quality threshold.\\nMulti-turn Extension: GRPO-Filter extends the original framework to handle complex multi-turn interactions by maintaining conversation context and enabling iterative reasoning across multiple dialogue turns:\\nwhere h < t h_{<t} represents the conversation history up to turn t t , and y t y_{t} is the response at turn t t .\\nThis multifaceted approach allows GRPO-Filter to effectively optimize policies for reasoning-intensive tasks while maintaining training stability and improving sample efficiency through selective learning from high-quality experiences.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Interaction with SQL Execution Environment',\n",
       "      'paragraphs': 'The integration of SQL and its execution interface with large language models (LLMs), which are capable of comprehending and generating query intentions, can significantly enhance the automation of complex database operations. In an LLM-based SQL tool invocation environment, the system should exhibit human-like interactive and reasoning behaviors. These behaviors include generating syntactically correct and logically sound SQL queries from natural language questions, invoking database execution interfaces at appropriate moments, and executing queries safely. Additionally, the system should carefully interpret query results, verify their correctness, and refine subsequent problem decomposition or query generation strategies based on feedback. This capability is cultivated through guiding the model via multi-turn interaction and reflective learning with the SQL execution environment. Detailed prompt of sql Execution can be found in Appendix C.1 .\\nWith the support of SQL tools, the model dynamically incorporates database query results into the reasoning process through multi-turn execution, as illustrated in Figure 1 . Unlike conventional methods that generate a complete SQL query until an end-of-sequence (EOS) token is produced, our approach constructs the full reasoning trajectory through continuous interaction with the SQL execution environment. The first interaction begins with a system prompt followed by the user‚Äôs question, with detailed content available in Appendix C.1 . The model, acting as the assistant, generates an initial response until it outputs an EOS token. If no SQL tool call is detected, the process terminates. When an SQL query is identified, the environment service extracts and safely executes it, then appends the execution result to the dialogue context in the user role. The model subsequently continues its reasoning as the assistant based on the updated context, producing the next turn of response. This multi-turn process iterates until the model returns a final answer or a maximum number of turns, denoted as T T , is reached. Detailed content can be found in Appendix B .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 Reward Design',\n",
       "      'paragraphs': 'To optimize policy effectively, we introduce a streamlined reward mechanism that focuses on critical elements of SQL query quality. This framework incorporates three key factors‚Äîsyntax validity, execution feasibility, and semantic precision‚Äîeach providing distinct guidance to ensure the model generates SQL queries that are syntactically correct, executable, and semantically meaningful.\\nWe guide the model to maintain a specific sequence of tags, ensuring a structured response. The response should follow a strict order: starting with <think>...</think> , followed optionally by <tool_call>...</tool_response> , and concluding with the <answer>...</answer> tag. Additionally, all tools must be used within <tool_call>...</tool_call> and <tool_response>...</tool_response> tags to maintain a standardized flow.\\nThis reward evaluates the syntactic correctness and executability of the generated SQL. It prevents the model from producing invalid or overly complex queries. If the SQL statement fails to execute, the model will not receive subsequent rewards. Furthermore, the execution time is constrained to discourage the generation of unnecessarily complex queries:\\nThe correctness of query results is a crucial measure of semantic fidelity. To encourage faithful reasoning, we design the result reward to strongly differentiate between correct and incorrect outputs:',\n",
       "      'subsections': [{'title': 'Format Reward.',\n",
       "        'paragraphs': 'We guide the model to maintain a specific sequence of tags, ensuring a structured response. The response should follow a strict order: starting with <think>...</think> , followed optionally by <tool_call>...</tool_response> , and concluding with the <answer>...</answer> tag. Additionally, all tools must be used within <tool_call>...</tool_call> and <tool_response>...</tool_response> tags to maintain a standardized flow.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Execution Reward.',\n",
       "        'paragraphs': 'This reward evaluates the syntactic correctness and executability of the generated SQL. It prevents the model from producing invalid or overly complex queries. If the SQL statement fails to execute, the model will not receive subsequent rewards. Furthermore, the execution time is constrained to discourage the generation of unnecessarily complex queries:',\n",
       "        'subsections': []},\n",
       "       {'title': 'Result Reward.',\n",
       "        'paragraphs': 'The correctness of query results is a crucial measure of semantic fidelity. To encourage faithful reasoning, we design the result reward to strongly differentiate between correct and incorrect outputs:',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '4 Experiments',\n",
       "    'paragraphs': 'Datasets. We train and evaluate our model on two Text-to-SQL benchmarks, SPIDER (Yu et\\xa0al., 2019 ) and BIRD (Li et\\xa0al., 2023 ) , which assess different aspects of the task. SPIDER is a large-scale, cross-domain benchmark focused on SQL complexity, with 10,181 questions and 5,693 unique queries across 200 databases. BIRD addresses real-world scenarios, featuring 12,751 question-SQL pairs on 95 large-scale databases with ‚Äùdirty‚Äù data and evaluating both accuracy and efficiency.To ensure both training efficiency and SQL generation accuracy, this study follows the principles of ‚Äùhigh quality, executable, and low redundancy‚Äù for data filtering and optimization. For the training of the BIRD and SPIDER benchmarks, we prioritize execution validity checks. Batch execution of reference SQL queries revealed that some samples returned empty results, which, if used for RL training, would fail to provide valid reward signals and could lead to learning biases or ‚Äùreward hacking.‚Äù\\nBaselines. We compare our MTIR-SQL framework against two primary categories of baseline methods. For supervised fine-tuning, we evaluate Qwen2.5-Coder-7B-Instruct (Hui et\\xa0al., 2024 ) , a state-of-the-art code generation model fine-tuned on Text-to-SQL datasets using standard cross-entropy loss. For reinforcement learning without tool integration, we implement GRPO on the Qwen3-4B model, using execution accuracy as the reward signal to optimize SQL generation through policy gradient methods. Both baselines use identical training procedures and computational budgets as our proposed framework but lack access to intermediate execution feedback during generation, allowing us to isolate the contribution of Multi-turn Tool-Integrated Reasoning.\\nExperimental Details. We conduct experiments using the Qwen3-Instruct model. During training and inference, we adopt database prompts from CodeS (Li et\\xa0al., 2024b ) and SQL-R1 (Ma et\\xa0al., 2025 ) , which provide curated schema components, values, and metadata, and have demonstrated competitive performance on the BIRD benchmark. We employ algorithms such as PPO and GRPO within the RL-Factory framework (Chai et\\xa0al., 2025 ) .The training configuration uses a batch size of 64 and a learning rate of 1e-6. During the rollout phase, we sample 5 outputs for each input at temperature T = 0.6, set the maximum sequence length to 8192, and the maximum number of interactions to N = 6. During inference, we apply greedy decoding (T = 0.0). We use SQLite as the SQL executor to obtain execution feedback. The feedback includes column headers and cell values for up to 10 rows. All experiments are conducted on a system equipped with 8 NVIDIA A100 GPUs.\\nPerformance on Main Benchmarks. In the first set of comparisons (Tables 1 , 2 , and 3 ), we evaluate MTIR-SQL against models with different parameter sizes, ranging from under 10B to large-scale models. As shown in Table 1 , our MTIR-SQL model (4B) achieves 64.4% on the BIRD Dev set, which is significantly higher than the other models under 10B parameters. The next best-performing model, SFT Code5-7B, achieves 57.17%, marking a clear advantage for our model despite its smaller size.\\nIn Table 2 , MTIR-SQL (4B) continues to outperform all models in the 10B-30B parameter range, where the best result from SFT Code5-15B is 58.47%. This demonstrates the efficiency of MTIR-SQL in achieving competitive performance even with a smaller model size. Similarly, in Table 3 , we compare MTIR-SQL to large-scale models, including both open-source and proprietary models. Despite the large model sizes (up to 236B parameters), MTIR-SQL (4B) achieves comparable performance to models such as ChatGPT + CoT (64.64%) and SuperSQL (58.50%), showing the robustness and scalability of our approach in handling complex Text-to-SQL tasks.\\nPerformance on Reasoning Paradigms. In Table 4 , we evaluate the performance of different reasoning paradigms on Text-to-SQL benchmarks, specifically focusing on Pass@1 performance across the SPIDER and BIRD datasets. The results highlight the effectiveness of multiturn tool-integrated reasoning.\\nAmong the reasoning paradigms, multi-turn TIR with GRPO-Filter leads to the highest performance on both the BIRD and SPIDER benchmarks. The BIRD Dev score of 63.1% represents a significant improvement over standard reasoning and tool-integrated reasoning paradigms, which score 48.1% and 47.6%, respectively. In SPIDER Dev and Test, multi-turn TIR with GRPO filter also excels, achieving 82.4% and 83.4%, respectively, marking a clear advantage over other paradigms.\\nThis reinforces the importance of incorporating execution feedback through multi-turn reasoning for enhancing performance in real-world Text-to-SQL tasks, particularly when dealing with complex databases like SPIDER and BIRD.\\nAblation Study of RL Methods. To assess the effectiveness of MTIR-SQL, we conducted comparisons against PPO, GRPO, and our improved GRPO-Filter using the Qwen3-4B model. As illustrated in Figure 3 and summarized in Table 4 , GRPO converges more rapidly than PPO due to the absence of a critic warm-up phase, but it often suffers from reward collapse in later training stages. PPO, in contrast, provides greater stability but at the cost of slower convergence. Crucially, GRPO-Filter addresses these limitations by selectively filtering low-quality rollouts and removing the KL constraint, thereby stabilizing multi-turn training while achieving substantial performance gains. This demonstrates that our modifications are not merely incremental but essential for enabling robust reinforcement learning in execution-aware Text-to-SQL tasks.\\nAblation Study on Max Turns. We further analyzed the impact of maximum tool calling turns by conducting experiments with settings of 1, 3, and 6. The training curves are shown in Figure 4 , and the quantitative results are summarized in Table 4 . The main findings are as follows: (1) Larger Max Turns generally lead to higher final rewards and stronger overall performance. More turns provide the model with additional opportunities to explore, optimize responses, and execute complex tasks; (2) However, excessive turns (such as 6) may also result in training instability, occasionally causing reward saturation or collapse phenomena; (3) Although Max Turns = 1 demonstrates faster convergence, due to severely limited interaction flexibility, there exists a gap between the final performance and optimal values.\\nAblation Study on Reward Design. This ablation study evaluates the impact of removing each reward component on the model‚Äôs performance using the BIRD Development set:\\nR f R_{\\\\text{f}} (Exploration Reward): Removing the exploration reward shows that a moderate increase in its value (from 0 to 0.1) improves performance from 62.3% to 63.1%, a 0.8% increase. However, further increases (to 0.2) cause a significant drop in performance, from 63.1% to 60.5% (a 4.1% decline), indicating that excessive exploration rewards can negatively impact performance. A slight recovery is observed at 0.3 (60.5% to 61.2%, a 1.2% increase), suggesting that a balanced exploration reward is beneficial, but it should not be overemphasized.\\nR e R_{\\\\text{e}} (Execution Reward): Removing the execution reward results in the largest performance drop, from 63.1% to 59.4% (a 3.9% decrease), highlighting its crucial role in the natural language-to-SQL conversion process. Without execution-based feedback, the model struggles to make accurate predictions.\\nR r R_{\\\\text{r}} (Result Reward): Excluding the result reward leads to a smaller decline in performance, from 63.1% to 58.8% (a 4.3% drop), underlining its importance in ensuring the functional correctness of the model‚Äôs SQL queries.\\nIn conclusion, removing any reward‚Äîparticularly R e R_{\\\\text{e}} ‚Äîsignificantly hampers the model‚Äôs performance. This underscores the necessity of a balanced reward system that integrates execution feedback, exploration, and result accuracy for optimal performance.',\n",
       "    'subsections': [{'title': '4.1 Experimental Setup',\n",
       "      'paragraphs': 'Datasets. We train and evaluate our model on two Text-to-SQL benchmarks, SPIDER (Yu et\\xa0al., 2019 ) and BIRD (Li et\\xa0al., 2023 ) , which assess different aspects of the task. SPIDER is a large-scale, cross-domain benchmark focused on SQL complexity, with 10,181 questions and 5,693 unique queries across 200 databases. BIRD addresses real-world scenarios, featuring 12,751 question-SQL pairs on 95 large-scale databases with ‚Äùdirty‚Äù data and evaluating both accuracy and efficiency.To ensure both training efficiency and SQL generation accuracy, this study follows the principles of ‚Äùhigh quality, executable, and low redundancy‚Äù for data filtering and optimization. For the training of the BIRD and SPIDER benchmarks, we prioritize execution validity checks. Batch execution of reference SQL queries revealed that some samples returned empty results, which, if used for RL training, would fail to provide valid reward signals and could lead to learning biases or ‚Äùreward hacking.‚Äù\\nBaselines. We compare our MTIR-SQL framework against two primary categories of baseline methods. For supervised fine-tuning, we evaluate Qwen2.5-Coder-7B-Instruct (Hui et\\xa0al., 2024 ) , a state-of-the-art code generation model fine-tuned on Text-to-SQL datasets using standard cross-entropy loss. For reinforcement learning without tool integration, we implement GRPO on the Qwen3-4B model, using execution accuracy as the reward signal to optimize SQL generation through policy gradient methods. Both baselines use identical training procedures and computational budgets as our proposed framework but lack access to intermediate execution feedback during generation, allowing us to isolate the contribution of Multi-turn Tool-Integrated Reasoning.\\nExperimental Details. We conduct experiments using the Qwen3-Instruct model. During training and inference, we adopt database prompts from CodeS (Li et\\xa0al., 2024b ) and SQL-R1 (Ma et\\xa0al., 2025 ) , which provide curated schema components, values, and metadata, and have demonstrated competitive performance on the BIRD benchmark. We employ algorithms such as PPO and GRPO within the RL-Factory framework (Chai et\\xa0al., 2025 ) .The training configuration uses a batch size of 64 and a learning rate of 1e-6. During the rollout phase, we sample 5 outputs for each input at temperature T = 0.6, set the maximum sequence length to 8192, and the maximum number of interactions to N = 6. During inference, we apply greedy decoding (T = 0.0). We use SQLite as the SQL executor to obtain execution feedback. The feedback includes column headers and cell values for up to 10 rows. All experiments are conducted on a system equipped with 8 NVIDIA A100 GPUs.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Main Result',\n",
       "      'paragraphs': 'Performance on Main Benchmarks. In the first set of comparisons (Tables 1 , 2 , and 3 ), we evaluate MTIR-SQL against models with different parameter sizes, ranging from under 10B to large-scale models. As shown in Table 1 , our MTIR-SQL model (4B) achieves 64.4% on the BIRD Dev set, which is significantly higher than the other models under 10B parameters. The next best-performing model, SFT Code5-7B, achieves 57.17%, marking a clear advantage for our model despite its smaller size.\\nIn Table 2 , MTIR-SQL (4B) continues to outperform all models in the 10B-30B parameter range, where the best result from SFT Code5-15B is 58.47%. This demonstrates the efficiency of MTIR-SQL in achieving competitive performance even with a smaller model size. Similarly, in Table 3 , we compare MTIR-SQL to large-scale models, including both open-source and proprietary models. Despite the large model sizes (up to 236B parameters), MTIR-SQL (4B) achieves comparable performance to models such as ChatGPT + CoT (64.64%) and SuperSQL (58.50%), showing the robustness and scalability of our approach in handling complex Text-to-SQL tasks.\\nPerformance on Reasoning Paradigms. In Table 4 , we evaluate the performance of different reasoning paradigms on Text-to-SQL benchmarks, specifically focusing on Pass@1 performance across the SPIDER and BIRD datasets. The results highlight the effectiveness of multiturn tool-integrated reasoning.\\nAmong the reasoning paradigms, multi-turn TIR with GRPO-Filter leads to the highest performance on both the BIRD and SPIDER benchmarks. The BIRD Dev score of 63.1% represents a significant improvement over standard reasoning and tool-integrated reasoning paradigms, which score 48.1% and 47.6%, respectively. In SPIDER Dev and Test, multi-turn TIR with GRPO filter also excels, achieving 82.4% and 83.4%, respectively, marking a clear advantage over other paradigms.\\nThis reinforces the importance of incorporating execution feedback through multi-turn reasoning for enhancing performance in real-world Text-to-SQL tasks, particularly when dealing with complex databases like SPIDER and BIRD.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Ablation Study',\n",
       "      'paragraphs': 'Ablation Study of RL Methods. To assess the effectiveness of MTIR-SQL, we conducted comparisons against PPO, GRPO, and our improved GRPO-Filter using the Qwen3-4B model. As illustrated in Figure 3 and summarized in Table 4 , GRPO converges more rapidly than PPO due to the absence of a critic warm-up phase, but it often suffers from reward collapse in later training stages. PPO, in contrast, provides greater stability but at the cost of slower convergence. Crucially, GRPO-Filter addresses these limitations by selectively filtering low-quality rollouts and removing the KL constraint, thereby stabilizing multi-turn training while achieving substantial performance gains. This demonstrates that our modifications are not merely incremental but essential for enabling robust reinforcement learning in execution-aware Text-to-SQL tasks.\\nAblation Study on Max Turns. We further analyzed the impact of maximum tool calling turns by conducting experiments with settings of 1, 3, and 6. The training curves are shown in Figure 4 , and the quantitative results are summarized in Table 4 . The main findings are as follows: (1) Larger Max Turns generally lead to higher final rewards and stronger overall performance. More turns provide the model with additional opportunities to explore, optimize responses, and execute complex tasks; (2) However, excessive turns (such as 6) may also result in training instability, occasionally causing reward saturation or collapse phenomena; (3) Although Max Turns = 1 demonstrates faster convergence, due to severely limited interaction flexibility, there exists a gap between the final performance and optimal values.\\nAblation Study on Reward Design. This ablation study evaluates the impact of removing each reward component on the model‚Äôs performance using the BIRD Development set:\\nR f R_{\\\\text{f}} (Exploration Reward): Removing the exploration reward shows that a moderate increase in its value (from 0 to 0.1) improves performance from 62.3% to 63.1%, a 0.8% increase. However, further increases (to 0.2) cause a significant drop in performance, from 63.1% to 60.5% (a 4.1% decline), indicating that excessive exploration rewards can negatively impact performance. A slight recovery is observed at 0.3 (60.5% to 61.2%, a 1.2% increase), suggesting that a balanced exploration reward is beneficial, but it should not be overemphasized.\\nR e R_{\\\\text{e}} (Execution Reward): Removing the execution reward results in the largest performance drop, from 63.1% to 59.4% (a 3.9% decrease), highlighting its crucial role in the natural language-to-SQL conversion process. Without execution-based feedback, the model struggles to make accurate predictions.\\nR r R_{\\\\text{r}} (Result Reward): Excluding the result reward leads to a smaller decline in performance, from 63.1% to 58.8% (a 4.3% drop), underlining its importance in ensuring the functional correctness of the model‚Äôs SQL queries.\\nIn conclusion, removing any reward‚Äîparticularly R e R_{\\\\text{e}} ‚Äîsignificantly hampers the model‚Äôs performance. This underscores the necessity of a balanced reward system that integrates execution feedback, exploration, and result accuracy for optimal performance.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 conclusion',\n",
       "    'paragraphs': 'We propose MTIR-SQL , a novel reinforcement learning framework for complex multi-turn SQL generation tasks. MTIR-SQL‚Äôs central innovation resides in its feedback-driven reasoning approach, where execution results inform subsequent reasoning iterations, creating a self-correcting mechanism that substantially improves generation stability and query accuracy. We extend GRPO with trajectory filtering to mitigate distribution drift and remove KL divergence constraints to enhance learning efficiency. Experimental results demonstrate MTIR-SQL‚Äôs effectiveness: achieving 64.4% accuracy on BIRD-SQL and 84.6% execution accuracy on SPIDER with a 4B-parameter model, significantly outperforming baseline methods and advancing state-of-the-art in Text-to-SQL generation.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Use of LLMs',\n",
       "    'paragraphs': 'In this work, we use Large Language Models (LLMs) for text refinement and grammar checking. LLMs help improve the clarity, coherence, and grammatical accuracy of the generated content, ensuring the final text meets academic standards. Their use is limited to enhancing written content, without influencing the research methodology or decision-making process.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B LLM Response Rollout with Multi-Turn Sql Excursions Calls',\n",
       "    'paragraphs': 'The algorithm describes the response generation process of a generative model (e.g., LLM) based on multi-turn interactions. The core idea of the algorithm is to progressively generate a response sequence based on the user‚Äôs input and previous responses. In each generation step, the model evaluates the current output and interacts with external tools for validation (e.g., executing SQL queries). The results returned by the tool are then integrated into the generated response. The entire process is conducted within a maximum action budget to ensure that the final output meets the problem‚Äôs requirements and is validated for accuracy. After each round, the model adjusts its output based on the results, continuing until a complete response is achieved or the budget limit is reached.\\nRequire: Input query x x , policy model œÄ Œ∏ \\\\pi_{\\\\theta} , SQL execution tool ùíØ \\\\mathcal{T} , maximum action budget B B . Ensure: Final response y y .',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Prompt and Case Study',\n",
       "    'paragraphs': 'Prompts used while training and several cases are presented as follows.',\n",
       "    'subsections': [{'title': 'C.1 System Prompt',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.2 User Prompt', 'paragraphs': '', 'subsections': []},\n",
       "     {'title': 'C.3 Case study', 'paragraphs': '', 'subsections': []}]}],\n",
       "  'abstract': 'As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves 64.4% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.'},\n",
       " '2510.25588v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'The diagnosis of mental disorders remains one of the most complex and nuanced challenges in clinical medicine. Unlike many physical illnesses, which can be confirmed through objective tests such as blood panels, imaging, or genetic screening, psychiatric diagnoses rely heavily on subjective assessment [ 1 ] . The current diagnostic process is predominantly based on conversations between psychiatrists and patients, guided by standardized criteria outlined in manuals such as the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) [ 2 ] . During clinical interviews, psychiatrists assess the patient‚Äôs reported symptoms, behavioral cues, emotional responses, and historical context to reach a diagnosis. However, this approach introduces significant variability and subjectivity. Diagnoses can differ between clinicians assessing the same patient, and even the same clinician may interpret similar symptoms differently across patients [ 3 ] . This variability can be attributed to differences in clinical experience, interpersonal dynamics, cultural context, and interpretation of symptom severity or duration [ 4 ] . Moreover, time constraints, communication barriers, and unconscious bias can further compromise diagnostic consistency. These challenges contribute to misdiagnosis, delayed treatment, and inconsistent care, ultimately affecting patient outcomes [ 5 ] .\\nIn recent years, advances in artificial intelligence, particularly LLMs [ 6 , 7 ] , have demonstrated exceptional capabilities in natural language understanding and reasoning. These developments open new avenues for enhancing the diagnostic process in psychiatry by introducing data-driven, scalable, and interpretable decision support systems. In this context, we propose a Fine-Tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM [ 8 , 9 , 10 , 11 ] enabled decision support system designed to assist and augment the clinical diagnosis of mental disorders. The LLMs in the system are trained/fine-tuned on curated conversational datasets that simulate or replicate psychiatrist‚Äìpatient interactions related to mental health evaluations, such as those for depression, anxiety, PTSD, and schizophrenia [ 12 , 13 ] . By fine-tuning a set of LLMs on these datasets, the system learns to recognize patterns indicative of specific mental disorders. Predictions from these models are then aggregated through a consensus-based reasoning process, led by OpenAI-gpt-oss, a dedicated reasoning LLM that evaluates and refines the diagnostic output. The specialized orchestration LLM agent [ 14 , 15 , 16 ] coordinates communication between components, ensuring an end-to-end workflow that is accurate, interpretable, and aligned with DSM-5 diagnostic standards [ 17 ] . To enable deployment in resource-constrained environments, each model is fine-tuned using the Unsloth library on Google Colab GPUs [ 18 ] with Low-Rank Adaptation and 4-bit quantization (QLoRA) [ 19 ] , ensuring high performance with minimal computational overhead. The proposed approach aims not to replace clinicians but to support them with a robust, evidence-based, and transparent tool that improves diagnostic precision and consistency. The following are our main contributions of this research.\\nFine-tuning a consortium of LLMs to analyze conversational data and predict diagnoses of mental disorders.\\nIncorporating the OpenAI-gpt-oss reasoning model to provide the final diagnosis based on the LLM consortium‚Äôs predictions.\\nAutomating the end-to-end diagnostic decision-making process for mental disorders through an LLM consortium integrated with the OpenAI-gpt-oss model, orchestrated by LLM agents to ensure transparency, reliability, and adherence to Responsible AI principles.\\nImplementing the prototype of the platform, integrating three LLMs with OpenAI-gpt-oss reasoning LLM, in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA.\\nThe remainder of the paper is organized as follows: Section 2 introduces the core technologies that underpin the proposed AI-assisted psychiatric diagnostic platform. Section 3 details the overall system architecture, highlighting the integration of large language models and reasoning engines. Section 4 outlines the platform‚Äôs core functionalities and operational workflow. Section 5 presents implementation details and evaluates the system‚Äôs performance across diagnostic tasks. Section 6 reviews related work and contextualizes our approach within the landscape of LLM-based medical diagnosis systems. Finally, Section 7 concludes the paper and discusses potential directions for future research and clinical deployment.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Background',\n",
       "    'paragraphs': 'This section provides a foundational overview of the core technologies underpinning the proposed AI-assisted psychiatric diagnostic platform. In particular, we highlight advancements in Large Language Models (LLMs), reasoning-capable LLMs, fine-tuning techniques, and the emerging paradigm of AI agents.\\nLarge Language Models (LLMs) are advanced deep neural networks trained on extensive text corpora to comprehend, generate, and reason using natural language. They form the backbone of modern Natural Language Processing (NLP) systems [ 20 ] and have demonstrated exceptional performance across a wide range of tasks, including text summarization, machine translation, dialogue generation, and question answering [ 6 ] .\\nSeveral prominent LLMs such as OpenAI‚Äôs GPT [ 21 ] , Meta‚Äôs Llama [ 22 , 7 ] , Mistral [ 23 ] , and Alibaba‚Äôs Qwen2 [ 24 ] are available in both proprietary and open-source formats. Open-source LLMs offer significant advantages for healthcare applications, including transparency, customizability, and lower deployment costs. For instance, Llama-3 [ 22 , 7 ] is valued for its compact architecture and strong performance, even on resource-constrained systems. Mistral [ 25 ] features optimized attention mechanisms for faster inference, while Qwen2 [ 24 ] is designed for multilingual and on-device use, making it suitable for scalable, privacy-preserving deployments.\\nWhile foundational LLMs excel in pattern recognition and natural language generation, they often lack the capacity for structured, multi-step reasoning. Reasoning LLMs [ 8 ] address this limitation by being specifically designed or fine-tuned to synthesize diverse inputs, resolve conflicting information, and support logical decision-making processes. Unlike traditional LLMs that primarily rely on next-token prediction, reasoning models simulate higher-order cognitive functions akin to human deductive reasoning [ 21 , 26 ] .\\nOpenAI-gpt-oss [ 9 , 27 ] is a reasoning LLM designed to perform advanced evaluative and comparative tasks across multiple inputs. Unlike traditional generative LLMs that focus on single-output prediction, OpenAI-gpt-oss is capable of synthesizing responses, resolving contradictions, and applying logical inference to arrive at consistent, well-reasoned conclusions. It excels in tasks involving multi-model output reconciliation, ranking, and consensus generation. These capabilities make it particularly suitable for applications that require structured reasoning, such as diagnostic decision support, content validation, and multi-agent coordination, where interpretability and reliability are critical.\\nFine-tuning is a technique for adapting pre-trained LLMs to specific downstream tasks or domains. It involves retraining the model on curated datasets that reflect the target domain‚Äôs language, structure, and semantics, allowing the model to produce outputs more aligned with specialized applications such as clinical diagnostics [ 28 , 29 ] .\\nTo optimize for efficiency and scalability of fine-tuning, Low-Rank Adaptation (LoRA) [ 30 ] is commonly used. LoRA introduces trainable low-rank matrices into the transformer architecture, enabling task-specific adaptation while significantly reducing the number of trainable parameters. In resource-constrained environments, Quantized LoRA (QLoRA) [ 19 ] provides an even more memory-efficient approach by quantizing model weights to 4-bit representations. QLoRA retains most of the performance benefits of full-precision fine-tuning while dramatically reducing memory and compute requirements. Together, these techniques make fine-tuning large models feasible on modest hardware, supporting wider adoption of LLMs in specialized domains.\\nSeveral open-source libraries facilitate efficient fine-tuning workflows for LLMs. Unsloth [ 31 ] , for example, enables high-speed, memory-efficient fine-tuning of models such as LLaMA, Mistral, and Qwen using LoRA and QLoRA techniques. It is optimized for both consumer-grade GPUs (e.g., NVIDIA RTX 3090) and cloud-based environments, including TPU-enabled platforms like Google Colab [ 18 ] . Successful fine-tuning of large models generally requires GPUs with ample VRAM and compute capabilities. High-performance GPUs such as the NVIDIA A100 (40GB/80GB) and H100 are well-suited for large-scale training workloads, while more accessible GPUs like the NVIDIA RTX 3090/4090 and Tesla T4 provide sufficient resources for small to medium-scale fine-tuning and prototyping [ 32 ] .\\nAI agents are autonomous computational entities designed to perform complex tasks by interacting with data sources, machine learning models, and external APIs within dynamic or uncertain environments. When these agents are powered by LLMs, they are referred to as LLM agents, capable of interpreting natural language instructions, generating structured outputs, managing tasks, and coordinating actions across digital ecosystems [ 14 , 15 ] .\\nAgentic AI extends this concept by organizing multiple LLM agents into collaborative, role-specialized systems that demonstrate advanced capabilities such as long-term planning, self-reflection, adaptive behavior, and multi-agent coordination [ 15 , 33 ] . These systems operate through agent hierarchies or workflows in which each agent performs a specific role, such as prompt engineering, retrieval, inference, evaluation, or integration. The modularity of agentic architectures enhances scalability, interpretability, and reusability, making them particularly suitable for domains requiring structured reasoning, task delegation, and reliable decision support.',\n",
       "    'subsections': [{'title': '2.1 Large Language Models (LLMs)',\n",
       "      'paragraphs': 'Large Language Models (LLMs) are advanced deep neural networks trained on extensive text corpora to comprehend, generate, and reason using natural language. They form the backbone of modern Natural Language Processing (NLP) systems [ 20 ] and have demonstrated exceptional performance across a wide range of tasks, including text summarization, machine translation, dialogue generation, and question answering [ 6 ] .\\nSeveral prominent LLMs such as OpenAI‚Äôs GPT [ 21 ] , Meta‚Äôs Llama [ 22 , 7 ] , Mistral [ 23 ] , and Alibaba‚Äôs Qwen2 [ 24 ] are available in both proprietary and open-source formats. Open-source LLMs offer significant advantages for healthcare applications, including transparency, customizability, and lower deployment costs. For instance, Llama-3 [ 22 , 7 ] is valued for its compact architecture and strong performance, even on resource-constrained systems. Mistral [ 25 ] features optimized attention mechanisms for faster inference, while Qwen2 [ 24 ] is designed for multilingual and on-device use, making it suitable for scalable, privacy-preserving deployments.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Reasoning LLMs',\n",
       "      'paragraphs': 'While foundational LLMs excel in pattern recognition and natural language generation, they often lack the capacity for structured, multi-step reasoning. Reasoning LLMs [ 8 ] address this limitation by being specifically designed or fine-tuned to synthesize diverse inputs, resolve conflicting information, and support logical decision-making processes. Unlike traditional LLMs that primarily rely on next-token prediction, reasoning models simulate higher-order cognitive functions akin to human deductive reasoning [ 21 , 26 ] .\\nOpenAI-gpt-oss [ 9 , 27 ] is a reasoning LLM designed to perform advanced evaluative and comparative tasks across multiple inputs. Unlike traditional generative LLMs that focus on single-output prediction, OpenAI-gpt-oss is capable of synthesizing responses, resolving contradictions, and applying logical inference to arrive at consistent, well-reasoned conclusions. It excels in tasks involving multi-model output reconciliation, ranking, and consensus generation. These capabilities make it particularly suitable for applications that require structured reasoning, such as diagnostic decision support, content validation, and multi-agent coordination, where interpretability and reliability are critical.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3 LLM Fine-tuning',\n",
       "      'paragraphs': 'Fine-tuning is a technique for adapting pre-trained LLMs to specific downstream tasks or domains. It involves retraining the model on curated datasets that reflect the target domain‚Äôs language, structure, and semantics, allowing the model to produce outputs more aligned with specialized applications such as clinical diagnostics [ 28 , 29 ] .\\nTo optimize for efficiency and scalability of fine-tuning, Low-Rank Adaptation (LoRA) [ 30 ] is commonly used. LoRA introduces trainable low-rank matrices into the transformer architecture, enabling task-specific adaptation while significantly reducing the number of trainable parameters. In resource-constrained environments, Quantized LoRA (QLoRA) [ 19 ] provides an even more memory-efficient approach by quantizing model weights to 4-bit representations. QLoRA retains most of the performance benefits of full-precision fine-tuning while dramatically reducing memory and compute requirements. Together, these techniques make fine-tuning large models feasible on modest hardware, supporting wider adoption of LLMs in specialized domains.\\nSeveral open-source libraries facilitate efficient fine-tuning workflows for LLMs. Unsloth [ 31 ] , for example, enables high-speed, memory-efficient fine-tuning of models such as LLaMA, Mistral, and Qwen using LoRA and QLoRA techniques. It is optimized for both consumer-grade GPUs (e.g., NVIDIA RTX 3090) and cloud-based environments, including TPU-enabled platforms like Google Colab [ 18 ] . Successful fine-tuning of large models generally requires GPUs with ample VRAM and compute capabilities. High-performance GPUs such as the NVIDIA A100 (40GB/80GB) and H100 are well-suited for large-scale training workloads, while more accessible GPUs like the NVIDIA RTX 3090/4090 and Tesla T4 provide sufficient resources for small to medium-scale fine-tuning and prototyping [ 32 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': '2.4 AI Agents and Agentic AI',\n",
       "      'paragraphs': 'AI agents are autonomous computational entities designed to perform complex tasks by interacting with data sources, machine learning models, and external APIs within dynamic or uncertain environments. When these agents are powered by LLMs, they are referred to as LLM agents, capable of interpreting natural language instructions, generating structured outputs, managing tasks, and coordinating actions across digital ecosystems [ 14 , 15 ] .\\nAgentic AI extends this concept by organizing multiple LLM agents into collaborative, role-specialized systems that demonstrate advanced capabilities such as long-term planning, self-reflection, adaptive behavior, and multi-agent coordination [ 15 , 33 ] . These systems operate through agent hierarchies or workflows in which each agent performs a specific role, such as prompt engineering, retrieval, inference, evaluation, or integration. The modularity of agentic architectures enhances scalability, interpretability, and reusability, making them particularly suitable for domains requiring structured reasoning, task delegation, and reliable decision support.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 System Architecture',\n",
       "    'paragraphs': 'Figure 1 describes the architecture of the platform. The proposed platform is composed of 4 layers: 1) Data Lake layer, 2) LLM Agent Layer, 3) LLM Layer, and 4) OpenAI-gpt-oss Reasoning Layer. Below is a brief description of each layer.\\nThe Data Lake layer serves as the foundational infrastructure for managing and storing extensive conversational datasets that simulate or replicate psychiatrist‚Äìpatient interactions relevant to mental health evaluations. This centralized repository is designed to support the training and fine-tuning [ 34 ] of LLMs for predictive psychiatric diagnosis. It hosts a diverse collection of large-scale, labeled datasets comprising transcribed conversations, symptom narratives, and corresponding clinician-verified diagnoses‚Äîaligned with standardized diagnostic frameworks such as the DSM-5 [ 35 , 36 ] . These datasets are critical for enabling the LLMs to learn clinically relevant patterns, understand nuanced symptom expressions, and associate them with specific mental disorders. By capturing a wide range of linguistic, contextual, and diagnostic variability, the Data Lake layer empowers the platform to develop robust, generalizable models capable of supporting accurate and consistent mental health assessments across diverse patient populations [ 37 , 38 ] .\\nThe LLM agent layer functions as the orchestration and automation core of the platform, enabling seamless integration and coordination across the Data Lake, fine-tuned LLMs, and the OpenAI-gpt-oss reasoning engine. The LLM agents act as orchestrators, which are responsible for all custom prompt engineering required to facilitate effective communication between components, ensuring that diagnostic predictions are generated, aggregated, and refined in a coherent and structured manner. Specifically, the LLM agents dynamically construct prompts using patient-clinician conversational data retrieved from the Data Lake [ 39 , 40 ] . These prompts are used to query the ensemble of fine-tuned LLMs, each of which outputs preliminary diagnostic assessments based on the detected symptoms and inferred clinical patterns. The agents then aggregate these outputs and format them into a consolidated, structured prompt tailored for the OpenAI-gpt-oss reasoning LLM [ 21 ] . The OpenAI-gpt-oss model leverages its advanced reasoning capabilities to evaluate and synthesize the collective outputs of the LLM consortium, ultimately generating a refined and consistent diagnostic prediction. By adapting prompts to match the input requirements and context of each model, the LLM Agent Layer ensures optimal information flow and model interoperability [ 14 ] . This orchestrated process not only enhances diagnostic accuracy and consistency but also enables a fully automated, end-to-end AI-driven diagnostic workflow, as illustrated in Figure 2 .\\nThe LLM Layer serves as the analytical core of the platform, enabling the system to interpret psychiatrist‚Äìpatient conversations and generate accurate diagnostic predictions. This layer comprises a consortium of fine-tuned LLMs, each trained on domain-specific conversational datasets involving mental health evaluations [ 28 ] . These models are specialized to recognize linguistic and behavioral patterns indicative of various psychiatric conditions, such as depression, anxiety, PTSD, and schizophrenia [ 41 ] . The fine-tuned LLMs are deployed and managed using Ollama [ 42 , 43 ] , a lightweight framework optimized for efficient inference and deployment of LLMs on consumer-grade hardware. This ensures the platform can scale effectively and maintain high performance even under resource constraints. As illustrated in Figure 2 , the LLM Agent Layer interfaces with the LLM consortium through Ollama‚Äôs API, orchestrating prompt generation, model invocation, and response handling. By leveraging multiple specialized models within the consortium, the LLM Layer enhances diagnostic robustness through diversity in model reasoning, ultimately supporting a more comprehensive and consistent assessment of patient mental health based on natural language conversations.\\nThe OpenAI-gpt-oss Reasoning Layer embodies the platform‚Äôs advanced cognitive and decision-making capabilities, leveraging state-of-the-art reasoning language models to synthesize diagnostic insights. The OpenAI-gpt-oss Reasoning LLM acts as the cognitive and synthesis engine of the platform. It is responsible for the high-level reasoning, integration, and refinement of system modeling predictions derived from the LLM consortium.\\nWithin the platform, OpenAI-gpt-oss serves as the final decision-making engine. It receives diagnostic predictions from the consortium of fine-tuned LLMs and performs structured reasoning to evaluate, cross-validate, and refine these outputs [ 44 ] . By synthesizing diverse model perspectives, OpenAI-gpt-oss determines the most consistent and clinically aligned diagnostic outcome, ensuring accuracy, coherence, and alignment with DSM-5 diagnostic criteria [ 36 ] . The LLM Agent Layer facilitates this process by aggregating the preliminary predictions and formatting them into structured, context-aware prompts tailored for OpenAI-gpt-oss. This enables the reasoning LLM to process heterogeneous inputs and deliver a final, consensus-driven diagnosis. By integrating probabilistic reasoning and consistency checks, the OpenAI-gpt-oss Reasoning Layer plays a pivotal role in enhancing the reliability, transparency, and clinical relevance of AI-assisted psychiatric diagnosis.',\n",
       "    'subsections': [{'title': '3.1 Data Lake Layer',\n",
       "      'paragraphs': 'The Data Lake layer serves as the foundational infrastructure for managing and storing extensive conversational datasets that simulate or replicate psychiatrist‚Äìpatient interactions relevant to mental health evaluations. This centralized repository is designed to support the training and fine-tuning [ 34 ] of LLMs for predictive psychiatric diagnosis. It hosts a diverse collection of large-scale, labeled datasets comprising transcribed conversations, symptom narratives, and corresponding clinician-verified diagnoses‚Äîaligned with standardized diagnostic frameworks such as the DSM-5 [ 35 , 36 ] . These datasets are critical for enabling the LLMs to learn clinically relevant patterns, understand nuanced symptom expressions, and associate them with specific mental disorders. By capturing a wide range of linguistic, contextual, and diagnostic variability, the Data Lake layer empowers the platform to develop robust, generalizable models capable of supporting accurate and consistent mental health assessments across diverse patient populations [ 37 , 38 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 LLM Agent Layer',\n",
       "      'paragraphs': 'The LLM agent layer functions as the orchestration and automation core of the platform, enabling seamless integration and coordination across the Data Lake, fine-tuned LLMs, and the OpenAI-gpt-oss reasoning engine. The LLM agents act as orchestrators, which are responsible for all custom prompt engineering required to facilitate effective communication between components, ensuring that diagnostic predictions are generated, aggregated, and refined in a coherent and structured manner. Specifically, the LLM agents dynamically construct prompts using patient-clinician conversational data retrieved from the Data Lake [ 39 , 40 ] . These prompts are used to query the ensemble of fine-tuned LLMs, each of which outputs preliminary diagnostic assessments based on the detected symptoms and inferred clinical patterns. The agents then aggregate these outputs and format them into a consolidated, structured prompt tailored for the OpenAI-gpt-oss reasoning LLM [ 21 ] . The OpenAI-gpt-oss model leverages its advanced reasoning capabilities to evaluate and synthesize the collective outputs of the LLM consortium, ultimately generating a refined and consistent diagnostic prediction. By adapting prompts to match the input requirements and context of each model, the LLM Agent Layer ensures optimal information flow and model interoperability [ 14 ] . This orchestrated process not only enhances diagnostic accuracy and consistency but also enables a fully automated, end-to-end AI-driven diagnostic workflow, as illustrated in Figure 2 .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 LLM Layer',\n",
       "      'paragraphs': 'The LLM Layer serves as the analytical core of the platform, enabling the system to interpret psychiatrist‚Äìpatient conversations and generate accurate diagnostic predictions. This layer comprises a consortium of fine-tuned LLMs, each trained on domain-specific conversational datasets involving mental health evaluations [ 28 ] . These models are specialized to recognize linguistic and behavioral patterns indicative of various psychiatric conditions, such as depression, anxiety, PTSD, and schizophrenia [ 41 ] . The fine-tuned LLMs are deployed and managed using Ollama [ 42 , 43 ] , a lightweight framework optimized for efficient inference and deployment of LLMs on consumer-grade hardware. This ensures the platform can scale effectively and maintain high performance even under resource constraints. As illustrated in Figure 2 , the LLM Agent Layer interfaces with the LLM consortium through Ollama‚Äôs API, orchestrating prompt generation, model invocation, and response handling. By leveraging multiple specialized models within the consortium, the LLM Layer enhances diagnostic robustness through diversity in model reasoning, ultimately supporting a more comprehensive and consistent assessment of patient mental health based on natural language conversations.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.4 OpenAI-gpt-oss Reasoning LLM Layer',\n",
       "      'paragraphs': 'The OpenAI-gpt-oss Reasoning Layer embodies the platform‚Äôs advanced cognitive and decision-making capabilities, leveraging state-of-the-art reasoning language models to synthesize diagnostic insights. The OpenAI-gpt-oss Reasoning LLM acts as the cognitive and synthesis engine of the platform. It is responsible for the high-level reasoning, integration, and refinement of system modeling predictions derived from the LLM consortium.\\nWithin the platform, OpenAI-gpt-oss serves as the final decision-making engine. It receives diagnostic predictions from the consortium of fine-tuned LLMs and performs structured reasoning to evaluate, cross-validate, and refine these outputs [ 44 ] . By synthesizing diverse model perspectives, OpenAI-gpt-oss determines the most consistent and clinically aligned diagnostic outcome, ensuring accuracy, coherence, and alignment with DSM-5 diagnostic criteria [ 36 ] . The LLM Agent Layer facilitates this process by aggregating the preliminary predictions and formatting them into structured, context-aware prompts tailored for OpenAI-gpt-oss. This enables the reasoning LLM to process heterogeneous inputs and deliver a final, consensus-driven diagnosis. By integrating probabilistic reasoning and consistency checks, the OpenAI-gpt-oss Reasoning Layer plays a pivotal role in enhancing the reliability, transparency, and clinical relevance of AI-assisted psychiatric diagnosis.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Platform Functionality',\n",
       "    'paragraphs': 'There are four main functionalities of the platform: 1) Data Lake Setup, 2) LLM Fine-Tuning, 3) Diagnosis prediction of Fine-tuned LLMs, and 4) Final Diagnosis Prediction by OpenAI-gpt-oss reasoning LLM. This section goes into the specifics of these functions.\\nThe first step in the platform‚Äôs workflow involves the setup of the Data Lake, which forms the foundational layer for storing, managing, and accessing large-scale conversational datasets. These datasets simulate or replicate psychiatrist‚Äìpatient interactions focused on mental health evaluations and serve as the primary training resource for fine-tuning the language models used in diagnosis prediction [ 45 ] . The Data Lake primarily contains transcribed conversations between clinicians and patients, along with their corresponding diagnoses provided by licensed psychiatrists. These diagnoses are annotated in accordance with standardized diagnostic frameworks, such as the DSM-5, ensuring clinical relevance and consistency [ 46 ] . This centralized repository enables the platform to efficiently organize diverse, high-quality training data necessary for modeling diagnostic reasoning and symptom interpretation.\\nBy providing robust, scalable, and secure data infrastructure, the Data Lake supports the development of fine-tuned LLMs capable of understanding complex psychiatric narratives, detecting subtle symptom patterns, and generating clinically accurate predictions. It is a critical enabler of the platform‚Äôs end-to-end AI-driven psychiatric diagnosis capabilities.\\nThe second step in the platform workflow involves fine-tuning LLMs using the curated and pre-processed data stored in the Data Lake. This stage is crucial for transforming general-purpose models into specialized diagnostic agents capable of interpreting psychiatrist‚Äìpatient conversations and identifying symptoms of mental disorders. Multiple state-of-the-art models, including Llama-3 [ 22 , 7 ] , Mistral [ 25 ] , and Qwen2 [ 24 ] , are fine-tuned on this domain-specific dataset to adapt them to the unique linguistic and contextual characteristics of psychiatric evaluations. The structure and composition of the dataset used for fine-tuning are illustrated in Figure 3 .\\nThe fine-tuning process is carried out using the Unsloth library [ 31 ] , which facilitates efficient large-scale model adaptation. To ensure that models are deployable on consumer-grade hardware without compromising performance, the fine-tuning process incorporates Quantized Low-Rank Adapters (QLoRA) [ 19 ] with 4-bit quantization as depicted in Figure 4 . This optimization significantly reduces memory and computational requirements, making the models suitable for real-time inference and edge deployment.\\nUpon completion, the fine-tuned and quantized models are deployed via Ollama [ 42 , 11 ] , a lightweight framework optimized to manage and run LLMs efficiently. These specialized models form the diagnostic core of the platform, each capable of analyzing psychiatric dialogue and producing preliminary mental disorder predictions based on learned diagnostic patterns and criteria aligned with DSM-5.\\nFollowing the fine-tuning process, the next phase of the platform involves generating preliminary diagnoses of mental disorders using the consortium of fine-tuned LLMs. When new conversational data is received between a psychiatrist and a patient, the platform‚Äôs LLM Agents initiate diagnostic analysis by interfacing with fine-tuned models through the Ollama API [ 42 ] . To facilitate accurate and context-aware predictions, the LLM Agent employs custom prompt engineering, embedding the relevant conversation data and structured diagnostic context into customized prompts for each model [ 47 ] . These prompts are carefully designed to align with the LLM input expectations and to provide adequate clinical representation, such as duration of symptoms, emotional tone, and functional impact, based on the diagnostic criteria of DSM-5. Each fine-tuned model then analyzes the provided input, extracts potential symptoms, and produces its own diagnostic prediction. These individual outputs are collected by the LLM Agent, which organizes them into a structured format for downstream processing. This step ensures that the specialized capabilities of fine-tuned LLM are fully utilized to provide rich, reliable, and interpretable diagnostic insights into potential mental health conditions.\\nBy allowing multiple independent evaluations throughout the model consortium, this layer improves diagnostic diversity, robustness, and the general ability of the platform to generalize in diverse patient presentations.\\nTo ensure the highest level of diagnostic accuracy, reliability, and clinical validity, the platform employs a consensus-based decision-making mechanism to generate the final diagnosis. Rather than relying on the output of a single model, the platform aggregates diagnostic predictions from multiple fine-tuned LLMs within the consortium. These individual outputs are then evaluated, compared, and synthesized by OpenAI-gpt-oss, a specialized reasoning LLM designed to perform advanced analytical inference [ 44 , 16 ] . As a core component of the architecture, OpenAI-gpt-oss acts as an intelligent adjudicator, capable of contextualizing, validating, and refining the predictions provided by the underlying LLMs. Using its advanced reasoning capabilities, OpenAI-gpt-oss identifies the most consistent and clinically appropriate diagnostic result from a diverse set of model-generated insights.\\nTo enable this reasoning process, the LLM Agent constructs custom, structured prompts by embedding and organizing the outputs from the fine-tuned models. These prompts, as illustrated in Figure 5 , provide OpenAI-gpt-oss with a unified view of candidate diagnoses, associated symptoms, and contextual cues. The reasoning LLM processes this composite input and produces a final diagnosis that aligns with the DSM-5 criteria and reflects a well-supported clinical interpretation [ 36 ] . This consensus-driven architecture significantly improves the robustness and generalizability of diagnostic predictions by mitigating the limitations of individual models and reducing variability. By orchestrating this process through a transparent, explainable pipeline, the platform not only increases trustworthiness but also establishes a replicable framework for AI-assisted psychiatric evaluation.\\nThe integration of ensemble-based inference with symbolic reasoning marks a transformative shift in mental health diagnostics, offering a scalable and interpretable decision support tool for clinicians. It demonstrates the potential of combining large-scale language understanding with structured reasoning to improve clinical decision-making in complex, subjective domains such as mental health.',\n",
       "    'subsections': [{'title': '4.1 Data Lake Setup',\n",
       "      'paragraphs': 'The first step in the platform‚Äôs workflow involves the setup of the Data Lake, which forms the foundational layer for storing, managing, and accessing large-scale conversational datasets. These datasets simulate or replicate psychiatrist‚Äìpatient interactions focused on mental health evaluations and serve as the primary training resource for fine-tuning the language models used in diagnosis prediction [ 45 ] . The Data Lake primarily contains transcribed conversations between clinicians and patients, along with their corresponding diagnoses provided by licensed psychiatrists. These diagnoses are annotated in accordance with standardized diagnostic frameworks, such as the DSM-5, ensuring clinical relevance and consistency [ 46 ] . This centralized repository enables the platform to efficiently organize diverse, high-quality training data necessary for modeling diagnostic reasoning and symptom interpretation.\\nBy providing robust, scalable, and secure data infrastructure, the Data Lake supports the development of fine-tuned LLMs capable of understanding complex psychiatric narratives, detecting subtle symptom patterns, and generating clinically accurate predictions. It is a critical enabler of the platform‚Äôs end-to-end AI-driven psychiatric diagnosis capabilities.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 LLM Fine-Tuning',\n",
       "      'paragraphs': 'The second step in the platform workflow involves fine-tuning LLMs using the curated and pre-processed data stored in the Data Lake. This stage is crucial for transforming general-purpose models into specialized diagnostic agents capable of interpreting psychiatrist‚Äìpatient conversations and identifying symptoms of mental disorders. Multiple state-of-the-art models, including Llama-3 [ 22 , 7 ] , Mistral [ 25 ] , and Qwen2 [ 24 ] , are fine-tuned on this domain-specific dataset to adapt them to the unique linguistic and contextual characteristics of psychiatric evaluations. The structure and composition of the dataset used for fine-tuning are illustrated in Figure 3 .\\nThe fine-tuning process is carried out using the Unsloth library [ 31 ] , which facilitates efficient large-scale model adaptation. To ensure that models are deployable on consumer-grade hardware without compromising performance, the fine-tuning process incorporates Quantized Low-Rank Adapters (QLoRA) [ 19 ] with 4-bit quantization as depicted in Figure 4 . This optimization significantly reduces memory and computational requirements, making the models suitable for real-time inference and edge deployment.\\nUpon completion, the fine-tuned and quantized models are deployed via Ollama [ 42 , 11 ] , a lightweight framework optimized to manage and run LLMs efficiently. These specialized models form the diagnostic core of the platform, each capable of analyzing psychiatric dialogue and producing preliminary mental disorder predictions based on learned diagnostic patterns and criteria aligned with DSM-5.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Diagnosis Prediction by Fine-tuned LLMs',\n",
       "      'paragraphs': 'Following the fine-tuning process, the next phase of the platform involves generating preliminary diagnoses of mental disorders using the consortium of fine-tuned LLMs. When new conversational data is received between a psychiatrist and a patient, the platform‚Äôs LLM Agents initiate diagnostic analysis by interfacing with fine-tuned models through the Ollama API [ 42 ] . To facilitate accurate and context-aware predictions, the LLM Agent employs custom prompt engineering, embedding the relevant conversation data and structured diagnostic context into customized prompts for each model [ 47 ] . These prompts are carefully designed to align with the LLM input expectations and to provide adequate clinical representation, such as duration of symptoms, emotional tone, and functional impact, based on the diagnostic criteria of DSM-5. Each fine-tuned model then analyzes the provided input, extracts potential symptoms, and produces its own diagnostic prediction. These individual outputs are collected by the LLM Agent, which organizes them into a structured format for downstream processing. This step ensures that the specialized capabilities of fine-tuned LLM are fully utilized to provide rich, reliable, and interpretable diagnostic insights into potential mental health conditions.\\nBy allowing multiple independent evaluations throughout the model consortium, this layer improves diagnostic diversity, robustness, and the general ability of the platform to generalize in diverse patient presentations.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.4 Final Diagnosis Prediction by OpenAI-gpt-oss Reasoning LLM',\n",
       "      'paragraphs': 'To ensure the highest level of diagnostic accuracy, reliability, and clinical validity, the platform employs a consensus-based decision-making mechanism to generate the final diagnosis. Rather than relying on the output of a single model, the platform aggregates diagnostic predictions from multiple fine-tuned LLMs within the consortium. These individual outputs are then evaluated, compared, and synthesized by OpenAI-gpt-oss, a specialized reasoning LLM designed to perform advanced analytical inference [ 44 , 16 ] . As a core component of the architecture, OpenAI-gpt-oss acts as an intelligent adjudicator, capable of contextualizing, validating, and refining the predictions provided by the underlying LLMs. Using its advanced reasoning capabilities, OpenAI-gpt-oss identifies the most consistent and clinically appropriate diagnostic result from a diverse set of model-generated insights.\\nTo enable this reasoning process, the LLM Agent constructs custom, structured prompts by embedding and organizing the outputs from the fine-tuned models. These prompts, as illustrated in Figure 5 , provide OpenAI-gpt-oss with a unified view of candidate diagnoses, associated symptoms, and contextual cues. The reasoning LLM processes this composite input and produces a final diagnosis that aligns with the DSM-5 criteria and reflects a well-supported clinical interpretation [ 36 ] . This consensus-driven architecture significantly improves the robustness and generalizability of diagnostic predictions by mitigating the limitations of individual models and reducing variability. By orchestrating this process through a transparent, explainable pipeline, the platform not only increases trustworthiness but also establishes a replicable framework for AI-assisted psychiatric evaluation.\\nThe integration of ensemble-based inference with symbolic reasoning marks a transformative shift in mental health diagnostics, offering a scalable and interpretable decision support tool for clinicians. It demonstrates the potential of combining large-scale language understanding with structured reasoning to improve clinical decision-making in complex, subjective domains such as mental health.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Implementation and Evaluation',\n",
       "    'paragraphs': 'The implementation of the proposed platform was conducted in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, USA. The LLM layer and reasoning Layer comprises of three fine-tuned LLMs, including Llama-3 [ 7 ] , Mistral [ 25 ] , and Qwen [ 24 ] , and OpenAI-gpt-oss [ 8 , 27 ] reasoning LLM. The LLM Agent Layer was implemented using OpenAI Agents SDK [ 48 ] and Google Agent Development Kit [ 49 ] , enabling secure orchestration, transparent auditability, and decentralized control of the LLM interactions.\\nFine-tuning was conducted using the Unsloth library [ 31 ] on Google Colab, leveraging both NVIDIA A100 GPUs and Tesla TPUs [ 18 ] to support efficient and scalable training cycles. The original dataset consisted of approximately 2,000 annotated records, each containing a psychiatrist‚Äìpatient conversation, the doctor‚Äôs diagnostic reasoning, and the corresponding final diagnosis. These records were compiled from multiple data sources, as illustrated in Figure 3 .\\nThe Unsloth framework requires the input data to be structured in a conversational format [ 31 ] . To meet this requirement, the dataset was preprocessed and transformed into the required schema, shown in Figure 6 . Each training sample includes the fields: instruction (providing context or a prompt to the LLM), content (representing the main conversational input), and text (containing the model‚Äôs expected diagnostic reasoning output). The dataset was partitioned into training, validation, and testing subsets using a 2/3, 1/6, 1/6 split, respectively. The training process was completed in approximately 1,627 seconds (27.12 minutes). Peak memory reservation during training was 14.605 GB, with actual memory utilization reaching 5.853 GB, equivalent to 39.69% of the reserved memory and 99.03% of the peak allocation. These results demonstrate that fine-tuning large language models for psychiatric diagnosis using structured conversational data can be performed efficiently on moderate-scale datasets, even with limited hardware resources. This underscores the practicality and accessibility of applying LLMs in specialized domains such as mental health.\\nAfter fine-tuning, the models were quantized using QLoRA [ 19 ] , a process that enables efficient operation on consumer-grade hardware. This optimization was critical for deploying the fine-tuned models on Ollama, a framework designed for the lightweight yet high-performance execution of LLMs. Based on the predictions of the LLMs, OpenAI-gpt-oss LLM makes the final diagnosis of the mental illness. Custom prompts are used to instruct the OpenAI-gpt-oss Reasoning LLM to understand the context of the prediction. Based on the provided context and the predictions of the LLMs, the model makes the final diagnosis prediction. Platform performance is evaluated in three main areas: 1) Evaluation of LLM fine-tuning, 2) Diagnostic Performance of Fine-Tuned LLM Consortium, and 3) Diagnostic Reasoning Performance of the OpenAI-gpt-oss LLM.\\nThis evaluation focuses on measuring the effectiveness of the fine-tuning process in improving the diagnostic accuracy of LLMs within the platform. Specifically, we evaluated the performance of the fine-tuned Llama-3 model in its ability to identify psychiatric symptoms and produce accurate diagnoses based on conversational data between psychiatrists and patients [ 50 ] .\\nThroughout the fine-tuning process, we continuously monitored critical metrics‚Äîspecifically, training loss and validation loss‚Äîto assess the model‚Äôs learning dynamics and generalization ability [ 28 ] . As shown in Figure 9 , the training loss (Figure 7 ) and validation loss (Figure 8 ) both exhibit a steep decline during the initial training steps, indicating rapid learning and effective assimilation of domain-specific patterns. The validation loss continues to decrease smoothly over time, stabilizing around step 25, which suggests improved generalization to unseen samples. Meanwhile, the training loss decreases more aggressively and plateaus slightly earlier, signaling convergence. Figure 9 provides an integrated visualization of both metrics along with the area between the curves, which quantifies the generalization gap. This shaded area (2ÃÉ.41) highlights the difference between training and validation performance. The relatively narrow and consistently shrinking gap further confirms the model‚Äôs ability to generalize well without overfitting. These trends collectively indicate that the fine-tuning process was effective and stable, enabling the LLM to adapt precisely to the psychiatric diagnostic domain while maintaining performance on unseen conversational data.\\nFigure 10 captures multiple key training dynamics, including the loss difference, loss ratio, and loss derivatives over training steps, offering valuable insights into the model‚Äôs convergence behavior and generalization performance. The consistently positive loss difference (validation loss exceeding training loss) suggests signs of overfitting, especially at steps with noticeable spikes. The loss ratio, ranging from 1.0 to 3.0, highlights varying degrees of generalization, where a lower ratio reflects better alignment between training and validation performance. Additionally, the loss derivatives reveal rapid initial improvements followed by smaller, oscillating changes, indicating stabilization or saturation in the learning process [ 51 ] .\\nFollowing the training phase, we assessed the predictive performance of the fine-tuned models in the context of psychiatric diagnosis. This evaluation involved comparing real diagnostic annotations‚Äîbased on psychiatrist‚Äìpatient conversations‚Äîwith the predictions generated by both the baseline (pre-trained) LLMs and their fine-tuned counterparts.\\nFigure 11 and Figure 12 present the diagnostic predictions made by the Llama-3 model [ 22 , 7 ] before and after fine-tuning for two psychiatric conditions: Major Depressive Disorder (DSM-5 296.21) and Bipolar I Disorder, Current Episode Manic (DSM-5 296.41) [ 52 ] . Before fine-tuning, the model generated verbose but loosely structured outputs, relying on high-level symptom descriptions without explicitly mapping them to DSM-5 diagnostic codes [ 2 ] . While it identified relevant symptoms (e.g., euphoric mood, insomnia, and loss of appetite), it did not consistently align them with standardized diagnostic criteria. After fine-tuning on a domain-specific dataset, the model demonstrated significantly improved diagnostic precision. It accurately produced concise and clinically valid diagnoses with correct DSM-5 codes, directly inferred from the contextual patient-physician conversations. This highlights the effectiveness of targeted fine-tuning in enhancing clinical reasoning and diagnostic accuracy in mental health applications.\\nFigure 14 and Figure 13 illustrate the diagnostic predictions produced by the Mistral model [ 23 ] for Panic Disorder and Post-Traumatic Stress Disorder (PTSD) [ 53 ] . Before fine-tuning, Mistral was able to recognize relevant clinical symptoms from the conversation and partially align them with DSM-5 criteria. For instance, in the panic disorder case, the model identified key features such as shortness of breath, rapid heart rate, and anticipatory anxiety about future attacks. Similarly, for PTSD, it recognized trauma exposure, intrusive flashbacks, and avoidance behaviors, citing appropriate DSM-5 criteria. However, after fine-tuning, the model demonstrated improved diagnostic specificity and accuracy. It produced concise and direct classifications that fully matched the DSM-5 diagnostic codes (e.g., DSM-5 300.01 for Panic Disorder and DSM-5 309.81 for PTSD) [ 36 ] . This confirms that fine-tuning enhanced Mistral‚Äôs capacity for structured clinical reasoning and reliable psychiatric diagnosis from patient-doctor dialogues.\\nFigure 16 and Figure 15 present the diagnostic outputs generated by the Qwen2 model [ 54 ] for Schizophrenia and Generalized Anxiety Disorder, respectively [ 55 ] . Prior to fine-tuning, the model identified relevant symptoms and attempted to match them to DSM-5 criteria. However, the predictions were often verbose, interpretative, and occasionally lacked clinical precision. After fine-tuning on the psychiatric dataset, the Qwen2 model accurately mapped symptom clusters to their corresponding DSM-5 diagnoses with greater clarity and conciseness, yielding outputs that are more aligned with psychiatric clinical standards.\\nThese results demonstrate that the fine-tuned models consistently produce predictions that closely align with clinically validated diagnoses, showing improved precision, consistency, and interpretability. Compared to their baseline versions, the fine-tuned LLMs exhibit a substantial increase in diagnostic accuracy, underscoring the effectiveness of task-specific fine-tuning in mental health applications. These findings validate the utility of LLMs as decision-support tools in AI-assisted psychiatric diagnosis.\\nIn this evaluation, we assessed the diagnostic reasoning performance of the OpenAI-gpt-oss reasoning LLM by comparing its final diagnosis predictions with those generated independently by multiple fine-tuned LLMs. The goal was to evaluate OpenAI-gpt-oss‚Äôs ability to synthesize diverse diagnostic outputs and determine the most accurate and clinically appropriate outcome. Figure 17 illustrates a comparative analysis of the diagnostic predictions from the Llama-3, Mistral, and Qwen2 models, alongside the final output produced by OpenAI-gpt-oss. The figure highlights the reasoning model‚Äôs ability to interpret and reconcile varying predictions, applying structured clinical logic to arrive at a DSM-5-aligned diagnosis. The results demonstrate that OpenAI-gpt-oss significantly enhances diagnostic reliability by evaluating the coherence and clinical relevance of the LLM consortium‚Äôs outputs. This consensus-driven reasoning step adds an important layer of interpretability and robustness, reinforcing the value of integrating a dedicated reasoning LLM within the framework for AI-assisted psychiatric diagnosis.',\n",
       "    'subsections': [{'title': '5.1 Evaluation of LLM Fine-Tuning',\n",
       "      'paragraphs': 'This evaluation focuses on measuring the effectiveness of the fine-tuning process in improving the diagnostic accuracy of LLMs within the platform. Specifically, we evaluated the performance of the fine-tuned Llama-3 model in its ability to identify psychiatric symptoms and produce accurate diagnoses based on conversational data between psychiatrists and patients [ 50 ] .\\nThroughout the fine-tuning process, we continuously monitored critical metrics‚Äîspecifically, training loss and validation loss‚Äîto assess the model‚Äôs learning dynamics and generalization ability [ 28 ] . As shown in Figure 9 , the training loss (Figure 7 ) and validation loss (Figure 8 ) both exhibit a steep decline during the initial training steps, indicating rapid learning and effective assimilation of domain-specific patterns. The validation loss continues to decrease smoothly over time, stabilizing around step 25, which suggests improved generalization to unseen samples. Meanwhile, the training loss decreases more aggressively and plateaus slightly earlier, signaling convergence. Figure 9 provides an integrated visualization of both metrics along with the area between the curves, which quantifies the generalization gap. This shaded area (2ÃÉ.41) highlights the difference between training and validation performance. The relatively narrow and consistently shrinking gap further confirms the model‚Äôs ability to generalize well without overfitting. These trends collectively indicate that the fine-tuning process was effective and stable, enabling the LLM to adapt precisely to the psychiatric diagnostic domain while maintaining performance on unseen conversational data.\\nFigure 10 captures multiple key training dynamics, including the loss difference, loss ratio, and loss derivatives over training steps, offering valuable insights into the model‚Äôs convergence behavior and generalization performance. The consistently positive loss difference (validation loss exceeding training loss) suggests signs of overfitting, especially at steps with noticeable spikes. The loss ratio, ranging from 1.0 to 3.0, highlights varying degrees of generalization, where a lower ratio reflects better alignment between training and validation performance. Additionally, the loss derivatives reveal rapid initial improvements followed by smaller, oscillating changes, indicating stabilization or saturation in the learning process [ 51 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Diagnostic Performance of Fine-Tuned LLM Consortium',\n",
       "      'paragraphs': 'Following the training phase, we assessed the predictive performance of the fine-tuned models in the context of psychiatric diagnosis. This evaluation involved comparing real diagnostic annotations‚Äîbased on psychiatrist‚Äìpatient conversations‚Äîwith the predictions generated by both the baseline (pre-trained) LLMs and their fine-tuned counterparts.\\nFigure 11 and Figure 12 present the diagnostic predictions made by the Llama-3 model [ 22 , 7 ] before and after fine-tuning for two psychiatric conditions: Major Depressive Disorder (DSM-5 296.21) and Bipolar I Disorder, Current Episode Manic (DSM-5 296.41) [ 52 ] . Before fine-tuning, the model generated verbose but loosely structured outputs, relying on high-level symptom descriptions without explicitly mapping them to DSM-5 diagnostic codes [ 2 ] . While it identified relevant symptoms (e.g., euphoric mood, insomnia, and loss of appetite), it did not consistently align them with standardized diagnostic criteria. After fine-tuning on a domain-specific dataset, the model demonstrated significantly improved diagnostic precision. It accurately produced concise and clinically valid diagnoses with correct DSM-5 codes, directly inferred from the contextual patient-physician conversations. This highlights the effectiveness of targeted fine-tuning in enhancing clinical reasoning and diagnostic accuracy in mental health applications.\\nFigure 14 and Figure 13 illustrate the diagnostic predictions produced by the Mistral model [ 23 ] for Panic Disorder and Post-Traumatic Stress Disorder (PTSD) [ 53 ] . Before fine-tuning, Mistral was able to recognize relevant clinical symptoms from the conversation and partially align them with DSM-5 criteria. For instance, in the panic disorder case, the model identified key features such as shortness of breath, rapid heart rate, and anticipatory anxiety about future attacks. Similarly, for PTSD, it recognized trauma exposure, intrusive flashbacks, and avoidance behaviors, citing appropriate DSM-5 criteria. However, after fine-tuning, the model demonstrated improved diagnostic specificity and accuracy. It produced concise and direct classifications that fully matched the DSM-5 diagnostic codes (e.g., DSM-5 300.01 for Panic Disorder and DSM-5 309.81 for PTSD) [ 36 ] . This confirms that fine-tuning enhanced Mistral‚Äôs capacity for structured clinical reasoning and reliable psychiatric diagnosis from patient-doctor dialogues.\\nFigure 16 and Figure 15 present the diagnostic outputs generated by the Qwen2 model [ 54 ] for Schizophrenia and Generalized Anxiety Disorder, respectively [ 55 ] . Prior to fine-tuning, the model identified relevant symptoms and attempted to match them to DSM-5 criteria. However, the predictions were often verbose, interpretative, and occasionally lacked clinical precision. After fine-tuning on the psychiatric dataset, the Qwen2 model accurately mapped symptom clusters to their corresponding DSM-5 diagnoses with greater clarity and conciseness, yielding outputs that are more aligned with psychiatric clinical standards.\\nThese results demonstrate that the fine-tuned models consistently produce predictions that closely align with clinically validated diagnoses, showing improved precision, consistency, and interpretability. Compared to their baseline versions, the fine-tuned LLMs exhibit a substantial increase in diagnostic accuracy, underscoring the effectiveness of task-specific fine-tuning in mental health applications. These findings validate the utility of LLMs as decision-support tools in AI-assisted psychiatric diagnosis.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Diagnostic Reasoning Performance of the OpenAI-gpt-oss LLM',\n",
       "      'paragraphs': 'In this evaluation, we assessed the diagnostic reasoning performance of the OpenAI-gpt-oss reasoning LLM by comparing its final diagnosis predictions with those generated independently by multiple fine-tuned LLMs. The goal was to evaluate OpenAI-gpt-oss‚Äôs ability to synthesize diverse diagnostic outputs and determine the most accurate and clinically appropriate outcome. Figure 17 illustrates a comparative analysis of the diagnostic predictions from the Llama-3, Mistral, and Qwen2 models, alongside the final output produced by OpenAI-gpt-oss. The figure highlights the reasoning model‚Äôs ability to interpret and reconcile varying predictions, applying structured clinical logic to arrive at a DSM-5-aligned diagnosis. The results demonstrate that OpenAI-gpt-oss significantly enhances diagnostic reliability by evaluating the coherence and clinical relevance of the LLM consortium‚Äôs outputs. This consensus-driven reasoning step adds an important layer of interpretability and robustness, reinforcing the value of integrating a dedicated reasoning LLM within the framework for AI-assisted psychiatric diagnosis.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Related Work',\n",
       "    'paragraphs': 'Recent advances have seen LLMs and multimodal AI systems increasingly applied across a wide range of healthcare domains. These efforts collectively highlight the expanding capabilities of LLMs in clinical reasoning, decision support, and medical knowledge retrieval. Despite these advancements, most prior work remains focused on general medicine or physical health domains, with limited specialization in psychiatric evaluation or adherence to standardized diagnostic frameworks such as the DSM‚Äë5. Furthermore, while individual models often perform well in isolated tasks, they rarely incorporate structured reasoning or multi-model coordination‚Äîcritical components for nuanced psychiatric assessments. Table 1 presents a comparative analysis of these prior systems in relation to our proposed framework. In the subsections that follow, we provide a detailed overview of each relevant system‚Äôs architecture, capabilities, and relevance to psychiatric diagnostic support.\\nThe Med‚ÄëPaLM family of models, including Med‚ÄëPaLM and Med‚ÄëPaLM-2, developed by Google Research, represents some of the most advanced and widely evaluated LLMs for clinical and biomedical tasks [ 56 ] . These models are fine-tuned versions of the PaLM (Pathways Language Model) foundation model, tailored to the medical domain through instruction tuning using a combination of publicly available medical datasets, expert-annotated clinical examples, and proprietary benchmarks.\\nMed‚ÄëPaLM-2 demonstrated expert-level competency, achieving over 85% accuracy on U.S. Medical Licensing Examination (USMLE)‚Äìstyle multiple-choice questions, including datasets such as MedQA, MedMCQA, and PubMedQA. Notably, in head-to-head evaluations, physicians preferred Med‚ÄëPaLM-2‚Äôs long-form answers to those written by human experts across multiple dimensions, including factual correctness, comprehensiveness, and safety. The model also performed well on several open-ended generation tasks, such as consumer health question answering and summarization of clinical information.\\nIn addition to the text-only versions, a multimodal extension called Med‚ÄëPaLM-M was developed, enabling the model to jointly reason over textual and visual inputs. This includes radiology images, dermatology photos, pathology slides, and ophthalmic scans. Evaluated on the MultiMedBench benchmark, Med‚ÄëPaLM-M demonstrated state-of-the-art performance across 14 diverse tasks encompassing classification, retrieval, and open-ended question answering with multimodal inputs. The model is capable of interpreting complex visual patterns and aligning them with clinical narratives, allowing for integrated diagnostic reasoning that mirrors specialist workflows.\\nFurthermore, the Med‚ÄëPaLM series introduces an ethical framework and rigorous safety assessments for AI in healthcare. This includes human evaluation protocols to assess bias, hallucination risks, and alignment with clinical guidelines. Despite its strengths, limitations remain‚Äîsuch as interpretability challenges and dependence on high-quality supervised data‚Äîunderscoring the need for hybrid frameworks that can further ensure trust and safety in clinical applications.\\nThe ‚ÄúLLM for DDx‚Äù framework [ 57 ] investigates the application of LLMs to the task of generating differential diagnoses from clinical vignettes, including both structured patient data and free-text clinical notes. By leveraging prompt engineering techniques, the system guides the LLM to output ranked diagnostic hypotheses based on symptomatology, patient history, and clinical context. This framework adopts few-shot or zero-shot prompting, often augmented with chain-of-thought reasoning or self-consistency sampling, to simulate a clinician‚Äôs diagnostic reasoning. The model is evaluated on publicly available datasets such as MedQA, MIMIC-III case notes, and Clinical Case Challenge benchmarks. It demonstrates promising capabilities in recognizing comorbidities, parsing temporal disease progression, and identifying rare or underrepresented conditions‚Äîtasks that often challenge traditional rule-based decision support systems. One of the key strengths of this approach lies in its adaptability: the model can generalize across specialties (e.g., internal medicine, neurology, pediatrics) without needing task-specific retraining. However, the system does not integrate multimodal data sources (e.g., lab results, imaging), and lacks structured inference mechanisms like probabilistic reasoning or iterative refinement via specialized LLMs. As such, its diagnostic accuracy can be sensitive to prompt phrasing and input variability, necessitating human-in-the-loop oversight for safe clinical deployment. Despite these limitations, the work highlights the potential of LLMs to augment differential diagnosis in resource-constrained or high-ambiguity settings, offering a foundation for future systems that incorporate more structured reasoning pipelines and multimodal data fusion.\\nMe‚ÄëLlama [ 37 ] adapts the Llama architecture for biomedical applications through a two-phase process: (1) continual pretraining on large-scale biomedical corpora‚Äîincluding PubMed articles, clinical guidelines, and de-identified electronic health records (EHRs); and (2) instruction tuning using curated clinical dialogues and task-specific prompts aligned with workflows typical in patient care. The model is optimized for downstream tasks such as clinical note summarization, medical question answering, and evidence extraction from unstructured clinical documents. Benchmarks show that Me‚ÄëLlama outperforms general-purpose LLMs on several biomedical NLP tasks, including the BioASQ challenge, PubMedQA, and MedNLI. Despite being limited to text-only inputs (i.e., it does not incorporate imaging, waveform, or wearable sensor data), Me‚ÄëLlama exhibits a strong semantic understanding of biomedical terminology, abbreviations, and guideline-referenced medical reasoning. Its lightweight fine-tuning also makes it suitable for deployment in edge healthcare systems, such as point-of-care mobile applications and hospital EMR-integrated tools. Overall, Me‚ÄëLlama demonstrates the feasibility and performance benefits of domain-specific LLM adaptation within clinical language environments.\\nDrHouse [ 45 ] introduces an advanced virtual provider assistant system that integrates LLM-based diagnostic reasoning with real-time physiological data collected from consumer-grade wearable sensors (e.g., smart watches, sleep trackers, fitness monitors). The system continuously monitors health signals such as heart rate variability, blood oxygen saturation (SpO 2 ), sleep quality, step count, and circadian rhythm alignment, enabling longitudinal health tracking and context-aware decision-making. In contrast to static, prompt-only systems, DrHouse engages users through multi-turn, adaptive dialogues that emulate the back-and-forth of a clinical consultation. The LLM component dynamically adjusts its diagnostic hypotheses using an iterative concurrent reasoning framework‚Äîrecalculating disease likelihoods as new data or clarifications are introduced. Additionally, DrHouse retrieves up-to-date medical knowledge from external expert databases such as UpToDate, PubMed abstracts, and clinical guidelines, thereby ensuring that recommendations remain grounded in the latest medical evidence.\\nThe system employs a dual-loop architecture that balances real-time inference with long-term patient modeling. It maintains an evolving profile of each user‚Äôs medical baseline and incorporates Bayesian-style updates for probabilistic diagnosis refinement. DrHouse was evaluated on both public benchmark datasets (e.g., MedQA, Symptoma) and proprietary longitudinal datasets derived from wearable telemetry and self-reported health surveys. Quantitatively, the model achieved up to an 18.8% improvement in diagnostic accuracy compared to baseline LLMs without sensor fusion. Qualitatively, user studies showed that 91.7% of patients found the interaction intuitive and trustworthy, while 75% of clinicians expressed confidence in its ability to support primary care triage. These results underscore the promise of multimodal, sensor-aware agentic systems for scalable, AI-assisted frontline healthcare.\\nCDSS [ 12 ] introduces a novel Clinical Decision Support System tailored for mental health diagnostics by synergistically combining LLMs with constraint logic programming (CLP). The system ingests natural language diagnostic manuals‚Äîspecifically DSM‚Äë5‚ÄëTR and ICD‚Äë11 CDDR‚Äîand uses an LLM to transform each diagnostic criterion into logic rules (e.g., Datalog clauses) [ 12 ] . These candidate rules are then vetted and refined by domain experts to ensure clinical fidelity before being executed by an off-the-shelf CLP engine to derive patient-specific diagnoses based on structured patient data.\\nIn empirical evaluations, the hybrid CDSS is compared against two baselines: an ‚ÄúLLM-only‚Äù approach that directly generates diagnostic outputs, and an intermediate LLM-to-CLP pipeline without expert oversight. Results indicate that only the expert-validated pipeline consistently produces diagnoses aligned with official manuals, highlighting the necessity of human-in-the-loop rule verification to prevent hallucinations and maintain interpretability. The authors also emphasize operational benefits: the logic rules are transparent and inspectable, facilitating clinician trust and auditability. Moreover, the approach addresses critical ethical concerns by avoiding the direct ingestion of sensitive patient data into the LLM‚Äîpatient records are instead processed via the CLP engine, mitigating privacy and safety risks associated with raw LLM consumption.\\nThis work represents a significant advancement in mental health AI, as it operationalizes a structured, interpretable, and demonstrably safe CDSS anchored in expert-validated logic and modular LLM capabilities‚Äîmarking a concrete step toward real-world psychiatric diagnostic tools.\\nWeda‚ÄëGPT [ 34 ] is a culturally-informed clinical decision-support system that leverages fine-tuned Llama‚Äë3 models to provide diagnostic assistance and therapeutic recommendations within indigenous and traditional medicine frameworks. Designed specifically for use in the Indonesian archipelago, Weda‚ÄëGPT incorporates linguistic, cultural, and epistemological knowledge derived from indigenous medical texts, oral traditions, and local practitioner expertise.\\nThe system is built using a multi-stage fine-tuning pipeline: starting from a Llama‚Äë3 base model, it is further adapted with region-specific datasets encompassing herbal pharmacology, traditional syndromic classifications, and culturally embedded health beliefs. Special emphasis is placed on aligning model outputs with culturally appropriate terminology and explanatory models, enabling the system to provide contextually sensitive health advice.\\nWeda‚ÄëGPT has been evaluated through case-based testing and participatory design sessions involving local healers and community health workers. The results show that the model effectively maps patient symptoms to culturally relevant diagnoses and treatments, including herbal prescriptions and ritual-based healing practices. Moreover, users reported a high degree of trust and interpretability in the system‚Äôs responses, in part due to its capacity to explain recommendations in locally meaningful terms.\\nAlthough Weda‚ÄëGPT does not operate within Western psychiatric diagnostic categories such as DSM‚Äë5, it highlights the broader potential of LLMs to support non-Western health systems and pluralistic medical epistemologies. Its design underscores the importance of cultural adaptation and domain-specific alignment when deploying AI in diverse global health contexts. As such, Weda‚ÄëGPT serves as a complementary model to Western-centric clinical decision systems, demonstrating the scalability and flexibility of LLM-based healthcare tools across sociocultural boundaries.\\nPlatform Domain Fine-tuning Support Running LLM Vision LM Support Reasoning LLM Support LLM Consortium Support Psychiatric-Diagnoses Psychiatric ‚úì Llama-3, Mistral, Qwen-2 ‚úó ‚úì ‚úì Med-PaLM [ 56 ] General medicine ‚úì PaLM ‚úó ‚úó ‚úó LLM for DDx [ 57 ] General medicine ‚úì Not specified ‚úó ‚úì ‚úó Me-LLaMA [ 37 ] General medicine ‚úì Llama ‚úó ‚úó ‚úó CDSS [ 12 ] Mental Health ‚úó GPT-4 ‚úó ‚úó ‚úó DrHouse [ 45 ] General medicine ‚úì Not specified ‚úì ‚úì ‚úó Weda-GPT [ 34 ] Indigenous Medicine ‚úì Llama-3 ‚úó ‚úó ‚úó',\n",
       "    'subsections': [{'title': '6.1 Med‚ÄëPaLM',\n",
       "      'paragraphs': 'The Med‚ÄëPaLM family of models, including Med‚ÄëPaLM and Med‚ÄëPaLM-2, developed by Google Research, represents some of the most advanced and widely evaluated LLMs for clinical and biomedical tasks [ 56 ] . These models are fine-tuned versions of the PaLM (Pathways Language Model) foundation model, tailored to the medical domain through instruction tuning using a combination of publicly available medical datasets, expert-annotated clinical examples, and proprietary benchmarks.\\nMed‚ÄëPaLM-2 demonstrated expert-level competency, achieving over 85% accuracy on U.S. Medical Licensing Examination (USMLE)‚Äìstyle multiple-choice questions, including datasets such as MedQA, MedMCQA, and PubMedQA. Notably, in head-to-head evaluations, physicians preferred Med‚ÄëPaLM-2‚Äôs long-form answers to those written by human experts across multiple dimensions, including factual correctness, comprehensiveness, and safety. The model also performed well on several open-ended generation tasks, such as consumer health question answering and summarization of clinical information.\\nIn addition to the text-only versions, a multimodal extension called Med‚ÄëPaLM-M was developed, enabling the model to jointly reason over textual and visual inputs. This includes radiology images, dermatology photos, pathology slides, and ophthalmic scans. Evaluated on the MultiMedBench benchmark, Med‚ÄëPaLM-M demonstrated state-of-the-art performance across 14 diverse tasks encompassing classification, retrieval, and open-ended question answering with multimodal inputs. The model is capable of interpreting complex visual patterns and aligning them with clinical narratives, allowing for integrated diagnostic reasoning that mirrors specialist workflows.\\nFurthermore, the Med‚ÄëPaLM series introduces an ethical framework and rigorous safety assessments for AI in healthcare. This includes human evaluation protocols to assess bias, hallucination risks, and alignment with clinical guidelines. Despite its strengths, limitations remain‚Äîsuch as interpretability challenges and dependence on high-quality supervised data‚Äîunderscoring the need for hybrid frameworks that can further ensure trust and safety in clinical applications.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.2 LLM for Differential Diagnosis (DDx)',\n",
       "      'paragraphs': 'The ‚ÄúLLM for DDx‚Äù framework [ 57 ] investigates the application of LLMs to the task of generating differential diagnoses from clinical vignettes, including both structured patient data and free-text clinical notes. By leveraging prompt engineering techniques, the system guides the LLM to output ranked diagnostic hypotheses based on symptomatology, patient history, and clinical context. This framework adopts few-shot or zero-shot prompting, often augmented with chain-of-thought reasoning or self-consistency sampling, to simulate a clinician‚Äôs diagnostic reasoning. The model is evaluated on publicly available datasets such as MedQA, MIMIC-III case notes, and Clinical Case Challenge benchmarks. It demonstrates promising capabilities in recognizing comorbidities, parsing temporal disease progression, and identifying rare or underrepresented conditions‚Äîtasks that often challenge traditional rule-based decision support systems. One of the key strengths of this approach lies in its adaptability: the model can generalize across specialties (e.g., internal medicine, neurology, pediatrics) without needing task-specific retraining. However, the system does not integrate multimodal data sources (e.g., lab results, imaging), and lacks structured inference mechanisms like probabilistic reasoning or iterative refinement via specialized LLMs. As such, its diagnostic accuracy can be sensitive to prompt phrasing and input variability, necessitating human-in-the-loop oversight for safe clinical deployment. Despite these limitations, the work highlights the potential of LLMs to augment differential diagnosis in resource-constrained or high-ambiguity settings, offering a foundation for future systems that incorporate more structured reasoning pipelines and multimodal data fusion.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.3 Me‚ÄëLlama',\n",
       "      'paragraphs': 'Me‚ÄëLlama [ 37 ] adapts the Llama architecture for biomedical applications through a two-phase process: (1) continual pretraining on large-scale biomedical corpora‚Äîincluding PubMed articles, clinical guidelines, and de-identified electronic health records (EHRs); and (2) instruction tuning using curated clinical dialogues and task-specific prompts aligned with workflows typical in patient care. The model is optimized for downstream tasks such as clinical note summarization, medical question answering, and evidence extraction from unstructured clinical documents. Benchmarks show that Me‚ÄëLlama outperforms general-purpose LLMs on several biomedical NLP tasks, including the BioASQ challenge, PubMedQA, and MedNLI. Despite being limited to text-only inputs (i.e., it does not incorporate imaging, waveform, or wearable sensor data), Me‚ÄëLlama exhibits a strong semantic understanding of biomedical terminology, abbreviations, and guideline-referenced medical reasoning. Its lightweight fine-tuning also makes it suitable for deployment in edge healthcare systems, such as point-of-care mobile applications and hospital EMR-integrated tools. Overall, Me‚ÄëLlama demonstrates the feasibility and performance benefits of domain-specific LLM adaptation within clinical language environments.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.4 DrHouse',\n",
       "      'paragraphs': 'DrHouse [ 45 ] introduces an advanced virtual provider assistant system that integrates LLM-based diagnostic reasoning with real-time physiological data collected from consumer-grade wearable sensors (e.g., smart watches, sleep trackers, fitness monitors). The system continuously monitors health signals such as heart rate variability, blood oxygen saturation (SpO 2 ), sleep quality, step count, and circadian rhythm alignment, enabling longitudinal health tracking and context-aware decision-making. In contrast to static, prompt-only systems, DrHouse engages users through multi-turn, adaptive dialogues that emulate the back-and-forth of a clinical consultation. The LLM component dynamically adjusts its diagnostic hypotheses using an iterative concurrent reasoning framework‚Äîrecalculating disease likelihoods as new data or clarifications are introduced. Additionally, DrHouse retrieves up-to-date medical knowledge from external expert databases such as UpToDate, PubMed abstracts, and clinical guidelines, thereby ensuring that recommendations remain grounded in the latest medical evidence.\\nThe system employs a dual-loop architecture that balances real-time inference with long-term patient modeling. It maintains an evolving profile of each user‚Äôs medical baseline and incorporates Bayesian-style updates for probabilistic diagnosis refinement. DrHouse was evaluated on both public benchmark datasets (e.g., MedQA, Symptoma) and proprietary longitudinal datasets derived from wearable telemetry and self-reported health surveys. Quantitatively, the model achieved up to an 18.8% improvement in diagnostic accuracy compared to baseline LLMs without sensor fusion. Qualitatively, user studies showed that 91.7% of patients found the interaction intuitive and trustworthy, while 75% of clinicians expressed confidence in its ability to support primary care triage. These results underscore the promise of multimodal, sensor-aware agentic systems for scalable, AI-assisted frontline healthcare.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.5 CDSS',\n",
       "      'paragraphs': 'CDSS [ 12 ] introduces a novel Clinical Decision Support System tailored for mental health diagnostics by synergistically combining LLMs with constraint logic programming (CLP). The system ingests natural language diagnostic manuals‚Äîspecifically DSM‚Äë5‚ÄëTR and ICD‚Äë11 CDDR‚Äîand uses an LLM to transform each diagnostic criterion into logic rules (e.g., Datalog clauses) [ 12 ] . These candidate rules are then vetted and refined by domain experts to ensure clinical fidelity before being executed by an off-the-shelf CLP engine to derive patient-specific diagnoses based on structured patient data.\\nIn empirical evaluations, the hybrid CDSS is compared against two baselines: an ‚ÄúLLM-only‚Äù approach that directly generates diagnostic outputs, and an intermediate LLM-to-CLP pipeline without expert oversight. Results indicate that only the expert-validated pipeline consistently produces diagnoses aligned with official manuals, highlighting the necessity of human-in-the-loop rule verification to prevent hallucinations and maintain interpretability. The authors also emphasize operational benefits: the logic rules are transparent and inspectable, facilitating clinician trust and auditability. Moreover, the approach addresses critical ethical concerns by avoiding the direct ingestion of sensitive patient data into the LLM‚Äîpatient records are instead processed via the CLP engine, mitigating privacy and safety risks associated with raw LLM consumption.\\nThis work represents a significant advancement in mental health AI, as it operationalizes a structured, interpretable, and demonstrably safe CDSS anchored in expert-validated logic and modular LLM capabilities‚Äîmarking a concrete step toward real-world psychiatric diagnostic tools.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.6 Weda‚ÄëGPT',\n",
       "      'paragraphs': 'Weda‚ÄëGPT [ 34 ] is a culturally-informed clinical decision-support system that leverages fine-tuned Llama‚Äë3 models to provide diagnostic assistance and therapeutic recommendations within indigenous and traditional medicine frameworks. Designed specifically for use in the Indonesian archipelago, Weda‚ÄëGPT incorporates linguistic, cultural, and epistemological knowledge derived from indigenous medical texts, oral traditions, and local practitioner expertise.\\nThe system is built using a multi-stage fine-tuning pipeline: starting from a Llama‚Äë3 base model, it is further adapted with region-specific datasets encompassing herbal pharmacology, traditional syndromic classifications, and culturally embedded health beliefs. Special emphasis is placed on aligning model outputs with culturally appropriate terminology and explanatory models, enabling the system to provide contextually sensitive health advice.\\nWeda‚ÄëGPT has been evaluated through case-based testing and participatory design sessions involving local healers and community health workers. The results show that the model effectively maps patient symptoms to culturally relevant diagnoses and treatments, including herbal prescriptions and ritual-based healing practices. Moreover, users reported a high degree of trust and interpretability in the system‚Äôs responses, in part due to its capacity to explain recommendations in locally meaningful terms.\\nAlthough Weda‚ÄëGPT does not operate within Western psychiatric diagnostic categories such as DSM‚Äë5, it highlights the broader potential of LLMs to support non-Western health systems and pluralistic medical epistemologies. Its design underscores the importance of cultural adaptation and domain-specific alignment when deploying AI in diverse global health contexts. As such, Weda‚ÄëGPT serves as a complementary model to Western-centric clinical decision systems, demonstrating the scalability and flexibility of LLM-based healthcare tools across sociocultural boundaries.\\nPlatform Domain Fine-tuning Support Running LLM Vision LM Support Reasoning LLM Support LLM Consortium Support Psychiatric-Diagnoses Psychiatric ‚úì Llama-3, Mistral, Qwen-2 ‚úó ‚úì ‚úì Med-PaLM [ 56 ] General medicine ‚úì PaLM ‚úó ‚úó ‚úó LLM for DDx [ 57 ] General medicine ‚úì Not specified ‚úó ‚úì ‚úó Me-LLaMA [ 37 ] General medicine ‚úì Llama ‚úó ‚úó ‚úó CDSS [ 12 ] Mental Health ‚úó GPT-4 ‚úó ‚úó ‚úó DrHouse [ 45 ] General medicine ‚úì Not specified ‚úì ‚úì ‚úó Weda-GPT [ 34 ] Indigenous Medicine ‚úì Llama-3 ‚úó ‚úó ‚úó',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 Conclusions and Future Work',\n",
       "    'paragraphs': 'In this paper, we present an AI-assisted diagnostic framework that integrates a consortium of fine-tuned LLMs with a reasoning LLM (OpenAI-gpt-oss) to improve the accuracy, consistency, and transparency of psychiatric diagnosis while upholding Responsible AI principles. Recognizing the inherent subjectivity and variability in traditional mental health assessments‚Äîoften based on unstructured clinical interviews‚Äîwe proposed a novel architecture that leverages conversational data, custom prompt engineering, and multi-model consensus reasoning to replicate and improve upon clinical diagnostic workflows. The platform is structured into four key layers: the Data Lake Layer for managing annotated psychiatrist‚Äìpatient dialogues; the fine-tuned LLM Layer for training models specialized in symptom analysis; the LLM Agent Layer for orchestrating model interactions and prompt generation; and the OpenAI-gpt-oss Reasoning Layer, which synthesizes model outputs into a final, reliable diagnosis aligned with DSM-5 criteria. Our approach demonstrates that AI systems, when trained and orchestrated properly, can support mental health professionals by offering data-driven insights and reducing diagnostic variability. The use of low-rank adapters and quantization techniques further enables efficient deployment on consumer-grade hardware, making the system accessible in real-world clinical and remote care settings. To the best of our knowledge, this research represents the first end-to-end integration of fine-tuned large language models (LLMs) with a reasoning engine to standardize psychiatric diagnoses. It lays the foundation for future advancements in AI-assisted eHealth systems, where intelligent agents can augment clinical decision-making while preserving interpretability and ethical responsibility. Future work will focus on clinical validation, multilingual adaptation, and integration with multimodal inputs‚Äîsuch as voice, facial expressions, and affective signals‚Äîto enhance diagnostic depth, contextual understanding, and empathy.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgements',\n",
       "    'paragraphs': 'This work was supported in part by the DoD Center of Excellence in AI and Machine Learning (CoE-AIML) under Contract Number W911NF-20-2-0277 with the U.S. Army Research Laboratory.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Authors',\n",
       "    'paragraphs': 'Eranga Bandara is a Senior Research Scientist at the Virginia Modeling, Analysis, and Simulation Center (VMASC), Old Dominion University, Virginia, USA. He done his Ph.D. in Computer Science with a specialization in blockchain and distributed systems. Following his doctoral work, he developed a deep interest in neuroscience and is currently pursuing academic studies in the Fundamentals of Neuroscience program at Harvard University. His interdisciplinary research interests span privacy-preserving AI, distributed systems, blockchain architectures, and next-generation wireless networks (5G/6G), with a growing focus on brain modeling, the application of artificial intelligence in neuroscience, and leveraging neuroscience principles to advance artificial intelligence.\\nDr. Ross Gore is a Research Associate Professor at the Virginia Modeling, Analysis and Simulation Center (VMASC) at Old Dominion University, Suffolk, Virginia, USA. He earned his Ph.D. (2012) and M.S. (2007) in Computer Science from the University of Virginia and his B.S. (2003) in Computer Science from the University of Richmond. Dr. Gore‚Äôs research focuses on data science, predictive analytics, and simulation validation, with applications spanning public health, city planning, cybersecurity, and risk assessment. He is particularly interested in leveraging diverse data sources‚Äîfrom mobile devices to social media‚Äîto inform critical decision-making. Recent projects include using mobile phone data to inform public health policies during the COVID‚Äë19 pandemic, applying social media analytics to explain geographic variations in obesity, and customizing cyber vulnerability assessments‚Äîoften developed through close collaboration with stakeholders and iterative prototyping.\\nDr. Atma Ram Yarlagadda, MD is an experienced psychiatrist and Installation Director of Psychological Health at McDonald Army Health Center, Fort Eustis, Virginia. He earned his medical degree from Danylo Halytsky Lviv State Medical University in 1983 and has over 40 years of clinical practice in military psychiatry. His work focuses on advancing mental health services for military personnel, emphasizing evidence-based assessment and treatment of mood and trauma-related disorders.\\nDr. Anita H. Clayton, MD is the Wilford W. Spradlin Professor and Chair of the Department of Psychiatry and Neurobehavioral Sciences at the University of Virginia School of Medicine. She completed her MD and psychiatry residency at UVA, followed by service in the U.S. Navy Medical Corps before joining UVA faculty in 1990. Dr. Clayton is a leading expert in major depressive disorder, women‚Äôs mental health, sexual dysfunction, and reproductive psychiatry with over 225 peer-reviewed publications. She pioneered validated assessment tools such as the CSFQ, SIDI-F and DSDS and served as president of the International Society for the Study of Women‚Äôs Sexual Health. She is the current president of the American Society of Clinical Psychopharmacology.\\nCol. Dr. Preston Samuel L., DO, MS, FAPA currently serves as Commander and CEO of Blanchfield Army Community Hospital, Fort Campbell, Kentucky. A dual board-certified physician in Family Medicine and Psychiatry, he graduated from the Lake Erie College of Osteopathic Medicine and completed residencies at Walter Reed Army Medical Center. His distinguished military career includes leadership roles such as Chief of Behavioral Health and Regional Director of Psychological Health across multiple Army commands.\\nDr. Christopher K. Rhea is the Associate Dean for Research & Innovation in the College of Health Sciences at Old Dominion University. He is a recognized expert at the intersection of neuromotor control and advanced technology, using tools such as virtual reality and smartphones to address human health challenges like fall prevention in older adults, concussion assessment, and rehabilitation. His research has been supported by major agencies including the NIH, Department of Defense, US Navy, HRSA, and the Women‚Äôs Football Foundation, and he is known for building interdisciplinary teams to solve complex health problems.\\nDr. Sachin Shetty is an Associate Director in the Virginia Modeling, Analysis and Simulation Center at Old Dominion University and an Associate Professor with the Department of Computational Modeling and Simulation Engineering. Sachin Shetty received his PhD in Modeling and Simulation from the Old Dominion University in 2007. His research interests lie at the intersection of computer networking, network security and machine learning. Recently, he has been involved with developing cyber risk/resilience metrics for critical infrastructure and blockchain technologies for distributed system security. His laboratory has been supported by the National Science Foundation, Air Office of Scientific Research, Air Force Research Lab, Office of Naval Research, Department of Homeland Security, and Boeing. He has published over 150 research articles in journals and conference proceedings and four books. He is the recipient of Commonwealth Cyber Initiative Research Fellow, Fulbright Specialist award, EPRI Cybersecurity Research Challenge award, DHS Scientific Leadership Award and has been inducted in Tennessee State University‚Äôs million-dollar club.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist‚Äìpatient interactions focused on mental health conditions (e.g., depression). These models are capable of identifying mental disorders with high accuracy based on natural language input. The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate seamless communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Each LLM in the consortium was fine-tuned using the Unsloth library on Google Colab‚Äôs Tesla GPUs. To optimize performance on consumer-grade hardware, we employed Low-Rank Adaptation with 4-bit quantization (QLoRA). Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis‚Äîpaving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.'},\n",
       " '2510.25101v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Knowledge Base Question Answering (KBQA) leverages the rich semantic information in Knowledge Bases (KB) to deeply understand natural language questions and provide precise answers. Since KBQA holds significant value in applications such as search engines, intelligent healthcare, and financial risk control (Jang et\\xa0al., 2017 ; Wu et\\xa0al., 2024 ; Chen et\\xa0al., 2025 ) , it has garnered extensive attention in recent years. Although KBQA performs well on simple questions, it still struggles with complex questions in real-world scenarios. This difficulty primarily arises from generalizing to a wide range of unseen complex questions and adapting to the heterogeneous structures of KBs.\\nExisting semantic-parsing-based (Shu et\\xa0al., 2022 ; Jiang et\\xa0al., ) and information-retrieval-based KBQA approaches (Liu et\\xa0al., 2022 ; Sun et\\xa0al., 2019 ) rely on predefined workflows (e.g., generate-then-retrieve or retrieve-then-rank pipelines), resulting in error propagation and adaptability limitations. Recently, a series of approaches has adopted an agentic reasoning paradigm. As illustrated in the middle of Fig. 1 , under this paradigm, LLMs autonomously perform task planning, tool calling, and ultimately derive precise answers through multi-turn interactions with KBs. Specifically, existing agentic approaches fall into two categories, i.e., prompting-based (Xiong et\\xa0al., 2024 ; Sun et\\xa0al., ) and finetuning-based approaches (Jiang et\\xa0al., 2024 ; Fang et\\xa0al., 2024 ; Zhu et\\xa0al., 2025 ) . The former uses hand-crafted few-shot prompts to guide reasoning, relying on the intrinsic reasoning ability of LLMs. However, few examples are inadequate for the model to understand complex questions, perceiving the KB structure, resulting in low performance. In response, finetuning-based approaches adapt open-source LLMs by constructing agentic reasoning trajectories and fine-tuning the model to follow them. However, most of them rely on process supervision. Since manually creating high-quality trajectories is labor-intensive, they typically synthesize reasoning trajectories by decomposing gold logical forms (e.g., SPARQL). Besides, they learn from trajectories via Supervised Fine-Tuning (SFT) to employ the model with agentic reasoning ability.\\nDespite the progress, existing finetuning-based approaches rely heavily on process supervision both in data generation and model training. They encourage LLMs to exploit idea trajectories rather than autonomously exploring alternative ones. Consequently, they have two critical limitations:\\nLimited Robustness. Under process supervision, as shown in the left part of Fig. 1 , training trajectories are typically decomposed from gold logical forms, with each step being idealized and error-free. However, such trajectories omit the noise and uncertainty inherent in real-world interactions. As a result, models always struggle when exposed to the noisy, such as tool-call failures and empty results, as they are trained on ideal trajectories.\\nLimited Flexibility. For many questions, there exists more than one correct trajectory, and in some cases, certain trajectories may even outperform the gold logical forms. Training on trajectories only derived from gold logical forms constrains the model to low-diversity behavioral patterns. As shown in Fig. 1 , when the model is presented with an example problem containing a‚Äúhigh school‚Äù type constraint, it insists on finding a predicate with the meaning ‚Äúnotable type‚Äù and applies the type constraint. However, more efficient solutions exist, such as leveraging common sense to identify entities of a specific type from the candidate set, or using ‚ÄúSearchTypes‚Äù to more directly constrain the entity types.\\nMotivated by this, in this paper, we propose KnowCoder-A1 , an LLM capable of autonomously performing task decomposition, invoking tools, refining formal queries, and executing them to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 adopts a multi-stage curriculum reinforcement learning approach that relies mainly on outcome supervision. First, to break free from the dependency on process annotation, it employs an outcome-based rejection sampling strategy to curate high-quality trajectories and perform to endow the model with a foundational exploration capability. To mitigate the reward sparsity induced by outcome-only signals, KnowCoder-A1 employs multi-stage curriculum RL on reward schedules that progress from easy to hard, steadily strengthening autonomous exploration and complex reasoning. Trained with only around 6700 outcome-supervised samples, KnowCoder-A1 consistently outperforms previous agentic KBQA approaches across three mainstream datasets, i.e., WebQSP, CWQ, and GrailQA. Using 12√ó less training data, it achieves an F1 score of 80.5% on GrailQA, achieving a 3.3% relative improvement over KBQA-o1, the previous SOTA agentic-based approach. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves a relative improvement of up to 11.1% compared to KBQA-o1, demonstrating the strong generalization ability of the proposed approach.\\nIn summary, our contributions are as follows:\\nWe propose KnowCoder-A1 , our first agentic reasoning model for KBQA. Relying mainly on outcome supervision, it learns to act as a robust and flexible agent, capable of recovering from errors and strategically exploring diverse reasoning trajectories.\\nTo address the reward sparsity inherent in outcome-only supervision, we propose a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum, progressing from easy to hard reward criteria while discouraging reward hacking.\\nExtensive experiments on three mainstream datasets demonstrate that KnowCoder-A1 consistently outperforms previous agentic KBQA approaches, particularly in challenging zero-shot scenarios.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Traditional KBQA methods can be categorized into two primary kinds of approaches: Information Retrieval-based (IR-based) and Semantic Parsing-based (SP-based) ones. IR-based methods (Miller et\\xa0al., 2016 ; Liu et\\xa0al., 2022 ; Sun et\\xa0al., 2019 ; Saxena et\\xa0al., 2020 ; Shi et\\xa0al., 2021 ; Zhang et\\xa0al., 2022 ) first retrieve query-relevant information from the KB and subsequently rank candidate answers. Among IR-based methods, some (Bordes et\\xa0al., 2015 ; Chen et\\xa0al., 2019a ) incorporate a long-term memory component that can be read from and written to, or perform a path-walking process over the KB to derive the answer (Qiu et\\xa0al., 2020 ; Ren et\\xa0al., 2021 ) . SP-based methods (Sun et\\xa0al., 2020 ; Chen et\\xa0al., 2019b ; LAN & JIANG, ; Jiang et\\xa0al., ; Ye et\\xa0al., 2022 ; Shu et\\xa0al., 2022 ) transform the natural language question into a logical query, link the entities and relations, and execute the completed query to obtain the answer. With the advent of LLMs, they have been employed to enhance both approaches, serving as powerful subgraph retrievers (Zhao et\\xa0al., 2024 ; Liu et\\xa0al., 2024 ) or semantic parsers (Li et\\xa0al., 2023 ; Nie et\\xa0al., 2024 ) . However, these enhancements do not fundamentally alter the predefined, multi-stage workflow, which brings inflexibility and error propagation.\\nRecently, a new paradigm of agentic reasoning has emerged, which views the LLM as an agent that uses tools to interact with the KB, progressively reason, and finally find the answer. We call them agentic methods in this paper. They can be divided into two main categories: prompt-based and finetuning-based ones. The former methods, such as Interactive-KBQA (Xiong et\\xa0al., 2024 ) , QueryAgent (Huang et\\xa0al., 2024 ) , Readi (Cheng et\\xa0al., 2024 ) , and TOG ( Sun et\\xa0al., ) , leverage carefully designed interaction strategies and prompts to elicit the model‚Äôs reasoning capabilities. However, these methods struggle to equip the model with sufficient agentic capability to effectively perceive the KB and execute complex reasoning. Their success is also heavily dependent on the intrinsic capabilities of LLMs, most of which are closed-source and have an extremely large number of parameters. However, the limited number of examples is insufficient for the model to fully comprehend complex questions and capture the underlying KB structure, leading to suboptimal performance. Finetuning-based methods aim to distill reasoning abilities into smaller, open-source LLMs. Approaches (Gu et\\xa0al., 2023 ; Jiang et\\xa0al., 2024 ; Zhu et\\xa0al., 2025 ; Fang et\\xa0al., 2024 ) achieve this by constructing high-quality reasoning trajectories and finetuning on them. However, these trajectories, by decomposing and augmenting gold program annotations and the reliance on process supervision via SFT, limit the model‚Äôs capacity for autonomous exploration, resulting in poor robustness and flexibility. Although some recent works have used Monte Carlo Tree Search (MCTS) methods ( Luo et\\xa0al., ; Xiong et\\xa0al., 2025 ) to expand samples during inference for incremental fine-tuning, this is an indirect way to foster exploration and still operates within the confines of the process-supervised SFT paradigm. Overall, existing finetuning-based methods fall under SFT-based process supervision, which, as noted in Section 1 , struggles to encourage autonomous exploration, leading to less robust and adaptable reasoning trajectories.',\n",
       "    'subsections': [{'title': '2.1 Traditional KBQA methods',\n",
       "      'paragraphs': 'Traditional KBQA methods can be categorized into two primary kinds of approaches: Information Retrieval-based (IR-based) and Semantic Parsing-based (SP-based) ones. IR-based methods (Miller et\\xa0al., 2016 ; Liu et\\xa0al., 2022 ; Sun et\\xa0al., 2019 ; Saxena et\\xa0al., 2020 ; Shi et\\xa0al., 2021 ; Zhang et\\xa0al., 2022 ) first retrieve query-relevant information from the KB and subsequently rank candidate answers. Among IR-based methods, some (Bordes et\\xa0al., 2015 ; Chen et\\xa0al., 2019a ) incorporate a long-term memory component that can be read from and written to, or perform a path-walking process over the KB to derive the answer (Qiu et\\xa0al., 2020 ; Ren et\\xa0al., 2021 ) . SP-based methods (Sun et\\xa0al., 2020 ; Chen et\\xa0al., 2019b ; LAN & JIANG, ; Jiang et\\xa0al., ; Ye et\\xa0al., 2022 ; Shu et\\xa0al., 2022 ) transform the natural language question into a logical query, link the entities and relations, and execute the completed query to obtain the answer. With the advent of LLMs, they have been employed to enhance both approaches, serving as powerful subgraph retrievers (Zhao et\\xa0al., 2024 ; Liu et\\xa0al., 2024 ) or semantic parsers (Li et\\xa0al., 2023 ; Nie et\\xa0al., 2024 ) . However, these enhancements do not fundamentally alter the predefined, multi-stage workflow, which brings inflexibility and error propagation.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Agentic KBQA methods',\n",
       "      'paragraphs': 'Recently, a new paradigm of agentic reasoning has emerged, which views the LLM as an agent that uses tools to interact with the KB, progressively reason, and finally find the answer. We call them agentic methods in this paper. They can be divided into two main categories: prompt-based and finetuning-based ones. The former methods, such as Interactive-KBQA (Xiong et\\xa0al., 2024 ) , QueryAgent (Huang et\\xa0al., 2024 ) , Readi (Cheng et\\xa0al., 2024 ) , and TOG ( Sun et\\xa0al., ) , leverage carefully designed interaction strategies and prompts to elicit the model‚Äôs reasoning capabilities. However, these methods struggle to equip the model with sufficient agentic capability to effectively perceive the KB and execute complex reasoning. Their success is also heavily dependent on the intrinsic capabilities of LLMs, most of which are closed-source and have an extremely large number of parameters. However, the limited number of examples is insufficient for the model to fully comprehend complex questions and capture the underlying KB structure, leading to suboptimal performance. Finetuning-based methods aim to distill reasoning abilities into smaller, open-source LLMs. Approaches (Gu et\\xa0al., 2023 ; Jiang et\\xa0al., 2024 ; Zhu et\\xa0al., 2025 ; Fang et\\xa0al., 2024 ) achieve this by constructing high-quality reasoning trajectories and finetuning on them. However, these trajectories, by decomposing and augmenting gold program annotations and the reliance on process supervision via SFT, limit the model‚Äôs capacity for autonomous exploration, resulting in poor robustness and flexibility. Although some recent works have used Monte Carlo Tree Search (MCTS) methods ( Luo et\\xa0al., ; Xiong et\\xa0al., 2025 ) to expand samples during inference for incremental fine-tuning, this is an indirect way to foster exploration and still operates within the confines of the process-supervised SFT paradigm. Overall, existing finetuning-based methods fall under SFT-based process supervision, which, as noted in Section 1 , struggles to encourage autonomous exploration, leading to less robust and adaptable reasoning trajectories.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Preliminaries',\n",
       "    'paragraphs': 'Knowledge Base. A Knowledge Base (KB) is a structured knowledge graph ùí¢ = ( ‚Ñ∞ , ‚Ñõ , ‚Ñ± ) \\\\mathcal{G}=(\\\\mathcal{E},\\\\mathcal{R},\\\\mathcal{F}) , where ‚Ñ∞ \\\\mathcal{E} is the set of entities, ‚Ñõ \\\\mathcal{R} is the set of relations, and ‚Ñ± \\\\mathcal{F} is the set of factual triples. Each triple f ‚àà ‚Ñ± f\\\\in\\\\mathcal{F} takes the form ( h , r , t ) (h,r,t) , where h , t ‚àà ‚Ñ∞ h,t\\\\in\\\\mathcal{E} are the head and tail entities, and r ‚àà ‚Ñõ r\\\\in\\\\mathcal{R} is the relation.\\nKBQA. Given a natural-language question q q , the KB ùí¢ \\\\mathcal{G} , and a set of topic entities E q ‚äÜ ‚Ñ∞ E_{q}\\\\subseteq\\\\mathcal{E} mentioned in q q , the objective of the KBQA task is to find the answer set that maximizes the conditional probability P \\u200b ( ùíú q | q , E q , ùí¢ ) P(\\\\mathcal{A}_{q}|q,E_{q},\\\\mathcal{G}) .\\nNotably, following prior work Luo et\\xa0al. , we assume entity mentions in q q are linked to the knowledge graph ùí¢ \\\\mathcal{G} and given as an input.\\nAgentic Reasoning Paradigm for the KBQA Task. In the agentic KBQA paradigm, the LLM is viewed as an agent learning a policy œÄ \\\\pi to interact with the environment. At each step t t , the agent chooses an action a t ‚àà Action a_{t}\\\\in\\\\textsc{Action} based on the question q q and its historical reasoning trajectory œÑ t = { ( c 1 , a 1 , o 1 ) , ‚Ä¶ , ( c t , a t , o t ) } \\\\tau_{t}=\\\\{(c_{1},a_{1},o_{1}),\\\\ldots,(c_{t},a_{t},o_{t})\\\\} , where Action denotes the action space, c t c_{t} denotes the explicit thinking process before the action, and o t o_{t} is the observation from the environment. This interactive process continues until the agent finds the final answer set ùíú q \\\\mathcal{A}_{q} or reaches the maximum number of steps.\\nThe objective is to learn an optimal policy œÄ ‚àó \\\\pi^{*} that maximizes the probability of generating a trajectory œÑ \\\\tau that leads to the gold answer set ùíú q ‚àó \\\\mathcal{A}^{*}_{q} . This can be expressed as maximizing the total probability of all successful trajectories:\\nwhere ùíØ s \\u200b u \\u200b c \\u200b c \\u200b e \\u200b s \\u200b s \\\\mathcal{T}_{success} is the set of all trajectories that successfully yield the gold answers ùíú q ‚àó \\\\mathcal{A}^{*}_{q} , and the trajectory probability P \\u200b ( œÑ | ‚Ä¶ ; œÄ ) P(\\\\tau|\\\\dots;\\\\pi) is determined by the policy œÄ \\\\pi .',\n",
       "    'subsections': []},\n",
       "   {'title': '4 Methodology',\n",
       "    'paragraphs': 'KnowCoder-A1 follows a ReAct-style (Yao et\\xa0al., 2023 ) agent paradigm that interleaves explicit reasoning and tool use over a KB environment, and terminates with a final answer.\\nEnvironment. We view the KB and the tool executor as the environment: ‚Ñ≥ = ( ùí¢ , Exec ) \\\\mathcal{M}=(\\\\mathcal{G},\\\\ \\\\texttt{Exec}) ,\\nwhere Exec executes tool calls against ùí¢ \\\\mathcal{G} and returns structured observations (including error diagnostics when execution fails). It is worth noting that error messages are retained to provide corrective signals to help LLM refine subsequent tool calls.\\nAction Space. At each step, the agent first produces an explicit thinking process and then makes a tool call from a finite action set Action . We instantiate three tools based on Interactive-KBQA (Xiong et\\xa0al., 2024 ) :\\nSearchTypes (typename): Given a type name, retrieves and ranks candidate types in similarity order.\\nSearchGraphPatterns (sparql, semantic): Given a partial SPARQL sketch (assumed to start with SELECT DISTINCT ?x WHERE {...} ), returns up to 10 one- and two-hop subgraphs in which the variable ?x appears as the head or tail object. An optional semantic parameter can be provided to rank the returned subgraphs in similarity order.\\nExecuteSPARQL (sparql): Given a SPARQL query, executes it against a sandboxed endpoint over ùí¢ \\\\mathcal{G} and returns the result. This enables flexible query on the graph, such as hypothesis testing (e.g., subgraph existence) or end-to-end query execution.\\nState and Trajectory. Let c t c_{t} denote the explicit thinking process in natural language, a t ‚àà Action a_{t}\\\\in\\\\textsc{Action} the action, and o t o_{t} the observation at step t t . The trajectory, which also serves as the agent‚Äôs state, is the history of all past interactions. The trajectory at step t t is denoted as:\\nInteractive Process. Given a question q q , the agent begins with an initial prompt\\nwhere the instruction consists of a brief task description, tool definitions, and the required interaction format, followed by the question q q and its topic entities E q E_{q} . The specific prompt is provided in the Appendix E .\\nAt each step t t , the agent first generates a thought c t + 1 c_{t+1} and a tool call a t + 1 a_{t+1} conditioned on the current trajectory and the initial prompt:\\nThe tool call a t + 1 a_{t+1} is then passed to the executor, which returns an observation o t + 1 = Exec \\u200b ( a t + 1 ) o_{t+1}=\\\\texttt{Exec}(a_{t+1}) . The new interaction turn ( c t + 1 , a t + 1 , o t + 1 ) (c_{t+1},a_{t+1},o_{t+1}) is appended to the current trajectory to form the trajectory for the next step, œÑ t + 1 \\\\tau_{t+1} . This iterative process continues until the agent determines it has gathered sufficient information and decides to output the final answer, or reaches T T , the maximum number of steps.\\nDue to the heterogeneous structures of KBs and the complexity of questions, employing a base model with insufficient reasoning and tool-use capabilities as an agent makes it difficult to guarantee the discovery of trajectories to the correct answer. To mitigate this issue, we introduce a cold-start stage where the agent is fine-tuned on reasoning trajectories to acquire foundational perception and reasoning capabilities. Instead of relying on program-decomposed trajectories, we generate trajectories via outcome-based rejection sampling. Moreover, to avoid overfitting while ensuring sufficient guidance, we adhere to the principle of constructing a dataset that is concise, yet high in quality and diversity. In the following, we will first describe this data curation process and then detail the model‚Äôs training procedure.\\nAs discussed in Sec. 1 , program-decomposed trajectories used in existing approaches fundamentally lack exploration, resulting in the limited robustness and flexibility of the model.\\nTo overcome the limitations and obtain a high-quality and diverse dataset, we design an outcome-based data construction strategy, the overall pipeline is illustrated in the left panel of Fig. 2 , which contains three core components:\\nFirst, to ensure the model learns a diverse set of fundamental reasoning patterns, we curate a mixed question set using a principled sampling strategy across several sources. From WebQSP, we balance one- and two-hop questions to teach varied reasoning depths. From CWQ, we up-sample infrequent question types to guarantee exposure to complex compositional structures. Finally, from GrailQA, we up-sample questions without topic entities to improve the model‚Äôs ability to generalize.\\nNext, to endow the model with stronger exploratory abilities and expand its exploration space, we use a prompt designed to encourage trial-and-error. As the yellow block in Fig. 2 shows, the prompt contains three parts: task description, tool definition, and high-quality demos: a few manually crafted exemplars that demonstrate a general agentic reasoning process, including planning, interacting, and self-correction after errors. Then, we prompt strong models (e.g., Gemini 2.5 Flash (Comanici et\\xa0al., 2025 ) and DeepSeek-V3 (Guo et\\xa0al., 2025 ) ) to generate multiple candidate trajectories for each question.\\nAfter that, to ensure the correctness of the trajectories, we apply an outcome-based rejection sampling, filtering the generated trajectories using two criteria, retaining only those that are both correct and fully grounded in evidence:\\nExact-match Correctness. The Exact Match (EM) score (Talmor & Berant, 2018 ) between the predicted and gold answers must be 1 1 . This ensures that at least one of the predicted answers appears in the gold answers, which largely guarantees the correctness of the trajectory.\\nEvidence Grounding. All predicted answers must appear in the observation history { o 0 , ‚Ä¶ , o T } \\\\{o_{0},\\\\ldots,o_{T}\\\\} , ensuring that the agent does not rely on internal parametric knowledge that could lead to hallucinations during training.\\nFinally, we apply a final filtering step to these trajectories. We ensure that each question has a maximum of three trajectories and that the number of trajectories is equal or balance across the aforementioned question types. The statistics of the resulting cold-start dataset can be found in Table 8 in Appendix C , where the size of the dataset is less than most of the existing methods.\\nIn the fine-tuning stage, we aim to teach the agent to reason over long, complex interaction histories. To achieve this efficiently without losing critical long-term context, we fine-tune the agent on complete trajectories. This approach avoids the disadvantage of alternative methods, which split the trajectory into multiple input-output steps: retaining the full context for each step is computationally inefficient, while using only a limited history window risks severing crucial long-term dependencies.\\nSpecifically, we use the entire history as input but compute the loss only on the agent‚Äôs generated tokens, masking out the observation part returned by tools during training. This prevents these external tokens from affecting the loss calculation, ensuring that the retrieved results do not interfere with the model‚Äôs internal reasoning and generation processes. The process can be formalized as follows:\\nWe define the input prompt as x x and the expert trajectory as œÑ \\\\tau . Here the trajectory œÑ \\\\tau is represented as a sequence of M M tokens, œÑ = ( y 1 , ‚Ä¶ , y M ) \\\\tau=(y_{1},\\\\dots,y_{M}) , which comprise the expert‚Äôs thoughts and actions. The agent is fine-tuned by maximizing the likelihood of generating this expert trajectory conditioned on the input x x . During training, we specifically mask out observation tokens, computing the loss only on the expert‚Äôs thought and action tokens. This objective is formulated as:\\nwhere Œ∏ \\\\theta represents the model‚Äôs parameters, M M is the total number of tokens in the sequence, y j y_{j} is the j j -th token, and y < j y_{<j} denotes the sequence of preceding tokens ( y 1 , ‚Ä¶ , y j ‚àí 1 ) (y_{1},\\\\dots,y_{j-1}) .\\nBased on the cold-start model, in the RL stage, we aim to further promote autonomous exploration and eliminate the reliance on process supervision. To achieve this, we employ the outcome-based Group Relative Policy Optimization (GRPO) method (Shao et\\xa0al., 2024 ) , where any trajectory leading to a correct answer receives a positive incentive, thereby encouraging the agent to explore and acquire stronger reasoning capabilities. Moreover, to ensure the RL training is both balanced and effective, we implement two key strategies.\\nFirst, to balance the agent‚Äôs exploration across question types (e.g., simple vs. complex), we sample a balanced set of questions for the RL training, following a similar principle as in the cold-start stage. Second, to mitigate the reward sparsity induced by outcome-only signals and refine the training process, we introduce a curriculum strategy on the reward, scheduling the reward strictness, guiding the learning process from easy to hard and effectively suppressing reward hacking.\\nIn this section, we first introduce the GRPO method, followed by the design of our data selection and reward curriculum strategies.\\nInspired by the recent success of policy gradient methods on reasoning tasks (Jin et\\xa0al., 2025 ; Xue et\\xa0al., 2025 ; Luo et\\xa0al., 2025 ) , we employ the GRPO method for optimization. GRPO estimates the baseline using the rewards from a group of rollouts for the same query to compute the advantage. In this paper, we adopt a fully on-policy variant of GRPO, which is demonstrated as an effective way to incentivize exploration (He et\\xa0al., 2025 ) . Its objective function can be expressed as follows:\\nHere, q q is the question from the train set D Q D_{Q} , and œÑ ( i ) \\\\tau^{(i)} is the i i -th trajectory generated by the policy model œÄ Œ∏ o \\u200b l \\u200b d \\\\pi_{\\\\theta_{old}} for q q . R \\u200b ( œÑ ( i ) ) R(\\\\tau^{(i)}) is a scalar reward derived solely from the final outcome of the trajectory. A ^ i , j \\\\hat{A}_{i,j} represents the group-wise relative advantage, which is estimated by normalizing the group-level rewards for the same query, normalized by the standard deviation s \\u200b t \\u200b d \\u200b ( ‚ãÖ ) std(\\\\cdot) to stabilize training. œÅ i , j \\u200b ( Œ∏ ) \\\\rho_{i,j}(\\\\theta) denotes the token-level importance ratio. œÄ ref \\\\pi_{\\\\text{ref}} is a reference policy (i.e., the model after the cold-start stage), and Œ≤ \\\\beta controls the strength of the KL loss ùîª KL ( ‚ãÖ | | ‚ãÖ ) \\\\mathbb{D}_{\\\\mathrm{KL}}(\\\\cdot||\\\\cdot) towards the reference policy.\\nDuring the earlier phases of RL training, the cold-start model often lack the capability to generate complete and correct reasoning trajectories for complex questions. This results in reward sparsity when relying solely on final outcomes, and meanwhile hinders effective exploration. Thus, we introduce a composite reward function that mitigates reward sparsity by providing denser feedback signals, while guiding the exploration process through a carefully designed easy-to-hard curriculum. The reward is specifically composed of a Format Reward and a multi-phase Answer Reward.\\nFormat Reward ( R fmt R_{\\\\text{fmt}} ). To discourage invalid trajectories and ensure that answers can be reliably parsed for reward calculation, we introduce a format reward. This component checks whether a trajectory adheres to the required format, specifically the presence of the answer box \\\\boxed{} . If the format is correct, we set R fmt = 0.1 R_{\\\\text{fmt}}=0.1 ; otherwise, R fmt = 0.0 R_{\\\\text{fmt}}=0.0 .\\nAnswer Reward ( R ans R_{\\\\text{ans}} ). To verify the correctness of the predicted answer and avoid a sparse reward signal, we evaluate the agreement between the predictions inside \\\\boxed{} and the gold answers using F Œ≤ F_{\\\\beta} score. Compared with a simple EM score, the F Œ≤ F_{\\\\beta} score balances precision and recall via an adoptable hyperparameter Œ≤ \\\\beta , which prevents agents from trivially obtaining a high score by returning a large, low-precision candidate set. Concretely, the F Œ≤ F_{\\\\beta} score is computed as:\\nwhere ùíú ^ \\\\widehat{\\\\mathcal{A}} denotes the de-duplicated set of answers produced by the model and ùíú q ‚ãÜ \\\\mathcal{A}_{q}^{\\\\star} denotes the set of gold answers, and | ‚ãÖ | |\\\\cdot| denotes multiset cardinality.\\nBased on the answer reward, we employ a curriculum learning strategy by adapting the Œ≤ \\\\beta parameter. This gradually adjusts the reward difficulty from easy to hard, as illustrated in the top-right part of Figure 2 .\\nPhase 1: Precision-focused Reward. In early training, we set Œ≤ = 0.5 \\\\beta=0.5 in the answer reward. This precision-leaning objective grants relatively high reward for correct items even when recall is incomplete. As the Fig. 2 shows, if the golden answer is {A1, A2}, a precise but partial prediction {A1} achieves a high reward ( r 2 = 0.83 r_{2}=0.83 ). Conversely, an prediction that includes extraneous items, such as {A1, A2, A3, A4}, is penalized with a lower reward ( r 1 = 0.55 r_{1}=0.55 ), encouraging accurate predictions and preventing the model from hacking the reward by emitting large candidate sets.\\nPhase 2: Balanced Reward. Once the model achieves stable precision, we switch the objective to the F 1 F_{1} score ( Œ≤ = 1 \\\\beta=1 ), which balances precision and recall. Under this balanced metric, a partial prediction is less rewarded than in Phase 1, incentivizing the agent to explore more thoroughly and recover the full set of correct answers.\\nThe total reward is the sum of the two parts, capped at a maximum value of 1.0:',\n",
       "    'subsections': [{'title': '4.1 Agentic Initialization',\n",
       "      'paragraphs': 'KnowCoder-A1 follows a ReAct-style (Yao et\\xa0al., 2023 ) agent paradigm that interleaves explicit reasoning and tool use over a KB environment, and terminates with a final answer.\\nEnvironment. We view the KB and the tool executor as the environment: ‚Ñ≥ = ( ùí¢ , Exec ) \\\\mathcal{M}=(\\\\mathcal{G},\\\\ \\\\texttt{Exec}) ,\\nwhere Exec executes tool calls against ùí¢ \\\\mathcal{G} and returns structured observations (including error diagnostics when execution fails). It is worth noting that error messages are retained to provide corrective signals to help LLM refine subsequent tool calls.\\nAction Space. At each step, the agent first produces an explicit thinking process and then makes a tool call from a finite action set Action . We instantiate three tools based on Interactive-KBQA (Xiong et\\xa0al., 2024 ) :\\nSearchTypes (typename): Given a type name, retrieves and ranks candidate types in similarity order.\\nSearchGraphPatterns (sparql, semantic): Given a partial SPARQL sketch (assumed to start with SELECT DISTINCT ?x WHERE {...} ), returns up to 10 one- and two-hop subgraphs in which the variable ?x appears as the head or tail object. An optional semantic parameter can be provided to rank the returned subgraphs in similarity order.\\nExecuteSPARQL (sparql): Given a SPARQL query, executes it against a sandboxed endpoint over ùí¢ \\\\mathcal{G} and returns the result. This enables flexible query on the graph, such as hypothesis testing (e.g., subgraph existence) or end-to-end query execution.\\nState and Trajectory. Let c t c_{t} denote the explicit thinking process in natural language, a t ‚àà Action a_{t}\\\\in\\\\textsc{Action} the action, and o t o_{t} the observation at step t t . The trajectory, which also serves as the agent‚Äôs state, is the history of all past interactions. The trajectory at step t t is denoted as:\\nInteractive Process. Given a question q q , the agent begins with an initial prompt\\nwhere the instruction consists of a brief task description, tool definitions, and the required interaction format, followed by the question q q and its topic entities E q E_{q} . The specific prompt is provided in the Appendix E .\\nAt each step t t , the agent first generates a thought c t + 1 c_{t+1} and a tool call a t + 1 a_{t+1} conditioned on the current trajectory and the initial prompt:\\nThe tool call a t + 1 a_{t+1} is then passed to the executor, which returns an observation o t + 1 = Exec \\u200b ( a t + 1 ) o_{t+1}=\\\\texttt{Exec}(a_{t+1}) . The new interaction turn ( c t + 1 , a t + 1 , o t + 1 ) (c_{t+1},a_{t+1},o_{t+1}) is appended to the current trajectory to form the trajectory for the next step, œÑ t + 1 \\\\tau_{t+1} . This iterative process continues until the agent determines it has gathered sufficient information and decides to output the final answer, or reaches T T , the maximum number of steps.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 SFT-based Cold-start Stage',\n",
       "      'paragraphs': 'Due to the heterogeneous structures of KBs and the complexity of questions, employing a base model with insufficient reasoning and tool-use capabilities as an agent makes it difficult to guarantee the discovery of trajectories to the correct answer. To mitigate this issue, we introduce a cold-start stage where the agent is fine-tuned on reasoning trajectories to acquire foundational perception and reasoning capabilities. Instead of relying on program-decomposed trajectories, we generate trajectories via outcome-based rejection sampling. Moreover, to avoid overfitting while ensuring sufficient guidance, we adhere to the principle of constructing a dataset that is concise, yet high in quality and diversity. In the following, we will first describe this data curation process and then detail the model‚Äôs training procedure.\\nAs discussed in Sec. 1 , program-decomposed trajectories used in existing approaches fundamentally lack exploration, resulting in the limited robustness and flexibility of the model.\\nTo overcome the limitations and obtain a high-quality and diverse dataset, we design an outcome-based data construction strategy, the overall pipeline is illustrated in the left panel of Fig. 2 , which contains three core components:\\nFirst, to ensure the model learns a diverse set of fundamental reasoning patterns, we curate a mixed question set using a principled sampling strategy across several sources. From WebQSP, we balance one- and two-hop questions to teach varied reasoning depths. From CWQ, we up-sample infrequent question types to guarantee exposure to complex compositional structures. Finally, from GrailQA, we up-sample questions without topic entities to improve the model‚Äôs ability to generalize.\\nNext, to endow the model with stronger exploratory abilities and expand its exploration space, we use a prompt designed to encourage trial-and-error. As the yellow block in Fig. 2 shows, the prompt contains three parts: task description, tool definition, and high-quality demos: a few manually crafted exemplars that demonstrate a general agentic reasoning process, including planning, interacting, and self-correction after errors. Then, we prompt strong models (e.g., Gemini 2.5 Flash (Comanici et\\xa0al., 2025 ) and DeepSeek-V3 (Guo et\\xa0al., 2025 ) ) to generate multiple candidate trajectories for each question.\\nAfter that, to ensure the correctness of the trajectories, we apply an outcome-based rejection sampling, filtering the generated trajectories using two criteria, retaining only those that are both correct and fully grounded in evidence:\\nExact-match Correctness. The Exact Match (EM) score (Talmor & Berant, 2018 ) between the predicted and gold answers must be 1 1 . This ensures that at least one of the predicted answers appears in the gold answers, which largely guarantees the correctness of the trajectory.\\nEvidence Grounding. All predicted answers must appear in the observation history { o 0 , ‚Ä¶ , o T } \\\\{o_{0},\\\\ldots,o_{T}\\\\} , ensuring that the agent does not rely on internal parametric knowledge that could lead to hallucinations during training.\\nFinally, we apply a final filtering step to these trajectories. We ensure that each question has a maximum of three trajectories and that the number of trajectories is equal or balance across the aforementioned question types. The statistics of the resulting cold-start dataset can be found in Table 8 in Appendix C , where the size of the dataset is less than most of the existing methods.\\nIn the fine-tuning stage, we aim to teach the agent to reason over long, complex interaction histories. To achieve this efficiently without losing critical long-term context, we fine-tune the agent on complete trajectories. This approach avoids the disadvantage of alternative methods, which split the trajectory into multiple input-output steps: retaining the full context for each step is computationally inefficient, while using only a limited history window risks severing crucial long-term dependencies.\\nSpecifically, we use the entire history as input but compute the loss only on the agent‚Äôs generated tokens, masking out the observation part returned by tools during training. This prevents these external tokens from affecting the loss calculation, ensuring that the retrieved results do not interfere with the model‚Äôs internal reasoning and generation processes. The process can be formalized as follows:\\nWe define the input prompt as x x and the expert trajectory as œÑ \\\\tau . Here the trajectory œÑ \\\\tau is represented as a sequence of M M tokens, œÑ = ( y 1 , ‚Ä¶ , y M ) \\\\tau=(y_{1},\\\\dots,y_{M}) , which comprise the expert‚Äôs thoughts and actions. The agent is fine-tuned by maximizing the likelihood of generating this expert trajectory conditioned on the input x x . During training, we specifically mask out observation tokens, computing the loss only on the expert‚Äôs thought and action tokens. This objective is formulated as:\\nwhere Œ∏ \\\\theta represents the model‚Äôs parameters, M M is the total number of tokens in the sequence, y j y_{j} is the j j -th token, and y < j y_{<j} denotes the sequence of preceding tokens ( y 1 , ‚Ä¶ , y j ‚àí 1 ) (y_{1},\\\\dots,y_{j-1}) .',\n",
       "      'subsections': [{'title': '4.2.1 Data Construction.',\n",
       "        'paragraphs': 'As discussed in Sec. 1 , program-decomposed trajectories used in existing approaches fundamentally lack exploration, resulting in the limited robustness and flexibility of the model.\\nTo overcome the limitations and obtain a high-quality and diverse dataset, we design an outcome-based data construction strategy, the overall pipeline is illustrated in the left panel of Fig. 2 , which contains three core components:\\nFirst, to ensure the model learns a diverse set of fundamental reasoning patterns, we curate a mixed question set using a principled sampling strategy across several sources. From WebQSP, we balance one- and two-hop questions to teach varied reasoning depths. From CWQ, we up-sample infrequent question types to guarantee exposure to complex compositional structures. Finally, from GrailQA, we up-sample questions without topic entities to improve the model‚Äôs ability to generalize.\\nNext, to endow the model with stronger exploratory abilities and expand its exploration space, we use a prompt designed to encourage trial-and-error. As the yellow block in Fig. 2 shows, the prompt contains three parts: task description, tool definition, and high-quality demos: a few manually crafted exemplars that demonstrate a general agentic reasoning process, including planning, interacting, and self-correction after errors. Then, we prompt strong models (e.g., Gemini 2.5 Flash (Comanici et\\xa0al., 2025 ) and DeepSeek-V3 (Guo et\\xa0al., 2025 ) ) to generate multiple candidate trajectories for each question.\\nAfter that, to ensure the correctness of the trajectories, we apply an outcome-based rejection sampling, filtering the generated trajectories using two criteria, retaining only those that are both correct and fully grounded in evidence:\\nExact-match Correctness. The Exact Match (EM) score (Talmor & Berant, 2018 ) between the predicted and gold answers must be 1 1 . This ensures that at least one of the predicted answers appears in the gold answers, which largely guarantees the correctness of the trajectory.\\nEvidence Grounding. All predicted answers must appear in the observation history { o 0 , ‚Ä¶ , o T } \\\\{o_{0},\\\\ldots,o_{T}\\\\} , ensuring that the agent does not rely on internal parametric knowledge that could lead to hallucinations during training.\\nFinally, we apply a final filtering step to these trajectories. We ensure that each question has a maximum of three trajectories and that the number of trajectories is equal or balance across the aforementioned question types. The statistics of the resulting cold-start dataset can be found in Table 8 in Appendix C , where the size of the dataset is less than most of the existing methods.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.2.2 Multi-Turn Finetuning',\n",
       "        'paragraphs': 'In the fine-tuning stage, we aim to teach the agent to reason over long, complex interaction histories. To achieve this efficiently without losing critical long-term context, we fine-tune the agent on complete trajectories. This approach avoids the disadvantage of alternative methods, which split the trajectory into multiple input-output steps: retaining the full context for each step is computationally inefficient, while using only a limited history window risks severing crucial long-term dependencies.\\nSpecifically, we use the entire history as input but compute the loss only on the agent‚Äôs generated tokens, masking out the observation part returned by tools during training. This prevents these external tokens from affecting the loss calculation, ensuring that the retrieved results do not interfere with the model‚Äôs internal reasoning and generation processes. The process can be formalized as follows:\\nWe define the input prompt as x x and the expert trajectory as œÑ \\\\tau . Here the trajectory œÑ \\\\tau is represented as a sequence of M M tokens, œÑ = ( y 1 , ‚Ä¶ , y M ) \\\\tau=(y_{1},\\\\dots,y_{M}) , which comprise the expert‚Äôs thoughts and actions. The agent is fine-tuned by maximizing the likelihood of generating this expert trajectory conditioned on the input x x . During training, we specifically mask out observation tokens, computing the loss only on the expert‚Äôs thought and action tokens. This objective is formulated as:\\nwhere Œ∏ \\\\theta represents the model‚Äôs parameters, M M is the total number of tokens in the sequence, y j y_{j} is the j j -th token, and y < j y_{<j} denotes the sequence of preceding tokens ( y 1 , ‚Ä¶ , y j ‚àí 1 ) (y_{1},\\\\dots,y_{j-1}) .',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.3 RL-based Exploration Stage',\n",
       "      'paragraphs': 'Based on the cold-start model, in the RL stage, we aim to further promote autonomous exploration and eliminate the reliance on process supervision. To achieve this, we employ the outcome-based Group Relative Policy Optimization (GRPO) method (Shao et\\xa0al., 2024 ) , where any trajectory leading to a correct answer receives a positive incentive, thereby encouraging the agent to explore and acquire stronger reasoning capabilities. Moreover, to ensure the RL training is both balanced and effective, we implement two key strategies.\\nFirst, to balance the agent‚Äôs exploration across question types (e.g., simple vs. complex), we sample a balanced set of questions for the RL training, following a similar principle as in the cold-start stage. Second, to mitigate the reward sparsity induced by outcome-only signals and refine the training process, we introduce a curriculum strategy on the reward, scheduling the reward strictness, guiding the learning process from easy to hard and effectively suppressing reward hacking.\\nIn this section, we first introduce the GRPO method, followed by the design of our data selection and reward curriculum strategies.\\nInspired by the recent success of policy gradient methods on reasoning tasks (Jin et\\xa0al., 2025 ; Xue et\\xa0al., 2025 ; Luo et\\xa0al., 2025 ) , we employ the GRPO method for optimization. GRPO estimates the baseline using the rewards from a group of rollouts for the same query to compute the advantage. In this paper, we adopt a fully on-policy variant of GRPO, which is demonstrated as an effective way to incentivize exploration (He et\\xa0al., 2025 ) . Its objective function can be expressed as follows:\\nHere, q q is the question from the train set D Q D_{Q} , and œÑ ( i ) \\\\tau^{(i)} is the i i -th trajectory generated by the policy model œÄ Œ∏ o \\u200b l \\u200b d \\\\pi_{\\\\theta_{old}} for q q . R \\u200b ( œÑ ( i ) ) R(\\\\tau^{(i)}) is a scalar reward derived solely from the final outcome of the trajectory. A ^ i , j \\\\hat{A}_{i,j} represents the group-wise relative advantage, which is estimated by normalizing the group-level rewards for the same query, normalized by the standard deviation s \\u200b t \\u200b d \\u200b ( ‚ãÖ ) std(\\\\cdot) to stabilize training. œÅ i , j \\u200b ( Œ∏ ) \\\\rho_{i,j}(\\\\theta) denotes the token-level importance ratio. œÄ ref \\\\pi_{\\\\text{ref}} is a reference policy (i.e., the model after the cold-start stage), and Œ≤ \\\\beta controls the strength of the KL loss ùîª KL ( ‚ãÖ | | ‚ãÖ ) \\\\mathbb{D}_{\\\\mathrm{KL}}(\\\\cdot||\\\\cdot) towards the reference policy.\\nDuring the earlier phases of RL training, the cold-start model often lack the capability to generate complete and correct reasoning trajectories for complex questions. This results in reward sparsity when relying solely on final outcomes, and meanwhile hinders effective exploration. Thus, we introduce a composite reward function that mitigates reward sparsity by providing denser feedback signals, while guiding the exploration process through a carefully designed easy-to-hard curriculum. The reward is specifically composed of a Format Reward and a multi-phase Answer Reward.\\nFormat Reward ( R fmt R_{\\\\text{fmt}} ). To discourage invalid trajectories and ensure that answers can be reliably parsed for reward calculation, we introduce a format reward. This component checks whether a trajectory adheres to the required format, specifically the presence of the answer box \\\\boxed{} . If the format is correct, we set R fmt = 0.1 R_{\\\\text{fmt}}=0.1 ; otherwise, R fmt = 0.0 R_{\\\\text{fmt}}=0.0 .\\nAnswer Reward ( R ans R_{\\\\text{ans}} ). To verify the correctness of the predicted answer and avoid a sparse reward signal, we evaluate the agreement between the predictions inside \\\\boxed{} and the gold answers using F Œ≤ F_{\\\\beta} score. Compared with a simple EM score, the F Œ≤ F_{\\\\beta} score balances precision and recall via an adoptable hyperparameter Œ≤ \\\\beta , which prevents agents from trivially obtaining a high score by returning a large, low-precision candidate set. Concretely, the F Œ≤ F_{\\\\beta} score is computed as:\\nwhere ùíú ^ \\\\widehat{\\\\mathcal{A}} denotes the de-duplicated set of answers produced by the model and ùíú q ‚ãÜ \\\\mathcal{A}_{q}^{\\\\star} denotes the set of gold answers, and | ‚ãÖ | |\\\\cdot| denotes multiset cardinality.\\nBased on the answer reward, we employ a curriculum learning strategy by adapting the Œ≤ \\\\beta parameter. This gradually adjusts the reward difficulty from easy to hard, as illustrated in the top-right part of Figure 2 .\\nPhase 1: Precision-focused Reward. In early training, we set Œ≤ = 0.5 \\\\beta=0.5 in the answer reward. This precision-leaning objective grants relatively high reward for correct items even when recall is incomplete. As the Fig. 2 shows, if the golden answer is {A1, A2}, a precise but partial prediction {A1} achieves a high reward ( r 2 = 0.83 r_{2}=0.83 ). Conversely, an prediction that includes extraneous items, such as {A1, A2, A3, A4}, is penalized with a lower reward ( r 1 = 0.55 r_{1}=0.55 ), encouraging accurate predictions and preventing the model from hacking the reward by emitting large candidate sets.\\nPhase 2: Balanced Reward. Once the model achieves stable precision, we switch the objective to the F 1 F_{1} score ( Œ≤ = 1 \\\\beta=1 ), which balances precision and recall. Under this balanced metric, a partial prediction is less rewarded than in Phase 1, incentivizing the agent to explore more thoroughly and recover the full set of correct answers.\\nThe total reward is the sum of the two parts, capped at a maximum value of 1.0:',\n",
       "      'subsections': [{'title': '4.3.1 Policy Optimization via GRPO',\n",
       "        'paragraphs': 'Inspired by the recent success of policy gradient methods on reasoning tasks (Jin et\\xa0al., 2025 ; Xue et\\xa0al., 2025 ; Luo et\\xa0al., 2025 ) , we employ the GRPO method for optimization. GRPO estimates the baseline using the rewards from a group of rollouts for the same query to compute the advantage. In this paper, we adopt a fully on-policy variant of GRPO, which is demonstrated as an effective way to incentivize exploration (He et\\xa0al., 2025 ) . Its objective function can be expressed as follows:\\nHere, q q is the question from the train set D Q D_{Q} , and œÑ ( i ) \\\\tau^{(i)} is the i i -th trajectory generated by the policy model œÄ Œ∏ o \\u200b l \\u200b d \\\\pi_{\\\\theta_{old}} for q q . R \\u200b ( œÑ ( i ) ) R(\\\\tau^{(i)}) is a scalar reward derived solely from the final outcome of the trajectory. A ^ i , j \\\\hat{A}_{i,j} represents the group-wise relative advantage, which is estimated by normalizing the group-level rewards for the same query, normalized by the standard deviation s \\u200b t \\u200b d \\u200b ( ‚ãÖ ) std(\\\\cdot) to stabilize training. œÅ i , j \\u200b ( Œ∏ ) \\\\rho_{i,j}(\\\\theta) denotes the token-level importance ratio. œÄ ref \\\\pi_{\\\\text{ref}} is a reference policy (i.e., the model after the cold-start stage), and Œ≤ \\\\beta controls the strength of the KL loss ùîª KL ( ‚ãÖ | | ‚ãÖ ) \\\\mathbb{D}_{\\\\mathrm{KL}}(\\\\cdot||\\\\cdot) towards the reference policy.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.3.2 Reward and Curriculum strategy',\n",
       "        'paragraphs': 'During the earlier phases of RL training, the cold-start model often lack the capability to generate complete and correct reasoning trajectories for complex questions. This results in reward sparsity when relying solely on final outcomes, and meanwhile hinders effective exploration. Thus, we introduce a composite reward function that mitigates reward sparsity by providing denser feedback signals, while guiding the exploration process through a carefully designed easy-to-hard curriculum. The reward is specifically composed of a Format Reward and a multi-phase Answer Reward.\\nFormat Reward ( R fmt R_{\\\\text{fmt}} ). To discourage invalid trajectories and ensure that answers can be reliably parsed for reward calculation, we introduce a format reward. This component checks whether a trajectory adheres to the required format, specifically the presence of the answer box \\\\boxed{} . If the format is correct, we set R fmt = 0.1 R_{\\\\text{fmt}}=0.1 ; otherwise, R fmt = 0.0 R_{\\\\text{fmt}}=0.0 .\\nAnswer Reward ( R ans R_{\\\\text{ans}} ). To verify the correctness of the predicted answer and avoid a sparse reward signal, we evaluate the agreement between the predictions inside \\\\boxed{} and the gold answers using F Œ≤ F_{\\\\beta} score. Compared with a simple EM score, the F Œ≤ F_{\\\\beta} score balances precision and recall via an adoptable hyperparameter Œ≤ \\\\beta , which prevents agents from trivially obtaining a high score by returning a large, low-precision candidate set. Concretely, the F Œ≤ F_{\\\\beta} score is computed as:\\nwhere ùíú ^ \\\\widehat{\\\\mathcal{A}} denotes the de-duplicated set of answers produced by the model and ùíú q ‚ãÜ \\\\mathcal{A}_{q}^{\\\\star} denotes the set of gold answers, and | ‚ãÖ | |\\\\cdot| denotes multiset cardinality.\\nBased on the answer reward, we employ a curriculum learning strategy by adapting the Œ≤ \\\\beta parameter. This gradually adjusts the reward difficulty from easy to hard, as illustrated in the top-right part of Figure 2 .\\nPhase 1: Precision-focused Reward. In early training, we set Œ≤ = 0.5 \\\\beta=0.5 in the answer reward. This precision-leaning objective grants relatively high reward for correct items even when recall is incomplete. As the Fig. 2 shows, if the golden answer is {A1, A2}, a precise but partial prediction {A1} achieves a high reward ( r 2 = 0.83 r_{2}=0.83 ). Conversely, an prediction that includes extraneous items, such as {A1, A2, A3, A4}, is penalized with a lower reward ( r 1 = 0.55 r_{1}=0.55 ), encouraging accurate predictions and preventing the model from hacking the reward by emitting large candidate sets.\\nPhase 2: Balanced Reward. Once the model achieves stable precision, we switch the objective to the F 1 F_{1} score ( Œ≤ = 1 \\\\beta=1 ), which balances precision and recall. Under this balanced metric, a partial prediction is less rewarded than in Phase 1, incentivizing the agent to explore more thoroughly and recover the full set of correct answers.\\nThe total reward is the sum of the two parts, capped at a maximum value of 1.0:',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '5 Experiments',\n",
       "    'paragraphs': 'To prove the effectiveness of KnowCoder-A1 , we conduct comprehensive experiments and present the setup, main results, and analysis in this section. Specifically, we focus on the following research questions (RQ): RQ1: Does KnowCoder-A1 outperform other directly comparable methods? RQ2: Does the main components of KnowCoder-A1 work effectively? RQ3: How does KnowCoder-A1 progressively enhance the agent‚Äôs reasoning capability for the KBQA task with outcome-only supervision? RQ4: Does KnowCoder-A1 generates flexible and robust reasoning trajectories?\\nDataset and Metrics . We evaluate KnowCoder-A1 on three widely-used KBQA datasets: WebQSP, CWQ, and the generalization-focused GrailQA, using a subset of the official testing data. Following existing work (Xiong et\\xa0al., 2024 ) , we primarily evaluate the F1 metrics, supplemented by Random Hits@1(RHits@1) and EM score. As our approach does not rely on semantic parsing, we do not report logical form accuracy (Gu et\\xa0al., 2021 ) .\\nBaselines . We compare KnowCoder-A1 against two main categories of methods: prompting-based and fine-tuning-based methods. For the former, we include a diverse range of prompting strategies, from direct prompting, such as IO, CoT, etc., and agent-based reasoning method Interactive-KBQA (Xiong et\\xa0al., 2024 ) . To ensure a fair comparison and reproducibility, we report the scores for these methods directly from their original publications. For the fine-tuning-based methods, we select SOTA methods finetuned on full data, include RnG-KBQA (Ye et\\xa0al., 2022 ) and TIARA (Shu et\\xa0al., 2022 ) , and the directly comparable low-source methods, which are finetuned on low-source data, includes agentic approaches like SFT-traj. (Xiong et\\xa0al., 2024 ) , KBQA-o1 ( Luo et\\xa0al., ) , and MCTS-KBQA (Xiong et\\xa0al., 2025 ) .\\nFurther details on the datasets, baselines, evaluation protocol, and implementation are provided in Appendix A .\\nTo evaluate the performance of KnowCoder-A1 , we conducted comprehensive experiments on three datasets, with the results presented in Table 1 . For additional context on the efficiency of our method, we compare the number of samples used during training with the number of rollouts used during inference in Table 2 . Taken together, these results demonstrate that KnowCoder-A1 establishes a new state-of-the-art for low-resource KBQA across all benchmarks. This shows the effectiveness of our approach, which can be analyzed from three key aspects.\\nFirst, KnowCoder-A1 significantly outperforms all directly comparable low-resource baselines, particularly on the more complex CWQ and the generalization-focused GrailQA datasets. Meanwhile, it nearly closes the gap with fully-supervised methods while using only a fraction of the data (\\xa010k outcome-supervised samples vs. \\xa060k fully-annotated queries), highlighting its exceptional data efficiency.\\nSecond, results prove that the outcome-supervised multi-stage training strategy is effective than the process-supervised SFT strategy.\\nResults in the last two blocks of Table 1 indicate that KnowCoder-A1 our approach consistently outperforms leading process-supervised SFT baselines. On GrailQA, KnowCoder-A1 achieves an F1 score of 80.5%, which represents a 3.3% relative improvement over the prior state-of-the-art method KBQA-o1 while using 12√ó less training data (3.5k vs. 43.8k samples). While performance is slightly lower on GrailQA‚Äôs I.I.D. and compositional subsets, which favor extensive training data to cover specific patterns, our method demonstrates a relative improvement of up to 11.1% on the zero-shot subset. This proves its superior ability to reason effectively on truly unseen questions.\\nFurthermore, beyond higher overall scores, KnowCoder-A1 is more efficient in both supervision and inference. In terms of supervision, as detailed in the ‚ÄúTrain‚Äù columns of Table 2 , KnowCoder-A1 uses zero process-supervised (P.S.) instances and a comparable or smaller number of outcome-supervised (O.S.) samples than MCTS-based competitors. As montioned before, KnowCoder-A1 outperforms KBQA-o1 on GrailQA while using much fewer supervised samples. In terms of inference, as shown in the ‚ÄúEvaluation‚Äù columns of Table 2 , KnowCoder-A1 employs a single linear reasoning pass and avoids the costly sampling required by MCTS, reducing latency by 3.2‚Äì6 times.\\nThese results support outcome supervision as a more effective and efficient alternative to incremental SFT on pre-defined or explored paths.\\nFinally, KnowCoder-A1 unlocks SOTA reasoning capabilities in smaller models. As shown in the first block of Table 1 , KnowCoder-A1 consistently outperforms prompting-based methods that rely on significantly larger models like GPT-4-turbo by at least 6% at F1. This demonstrates an essential improvement in the model‚Äôs intrinsic reasoning ability.\\nTo verify the effectiveness of our key designs, we conduct a comprehensive ablation study, and the results are presented in Table 3 . First, we assess the reward curriculum by replacing our proposed easy-to-hard ( F 0.5 F_{0.5} -then- F 1 F_{1} ) strategy, give the result using EM, F 1 F_{1} , F 0.5 F_{0.5} , and revered strategy F 1 F_{1} -then- F 0.5 F_{0.5} as reward, respectively. Next, we evaluate the contribution of the entire RL stage by removing it altogether. Finally, we remove the data sampling strategy from the initial cold-start phase. The results confirm that each component is integral to the model‚Äôs performance, as removing or modifying any of them leads to a noticeable degradation.\\nEffectiveness of the Easy-to-Hard Reward Curriculum. The first block of Table 3 shows the effectiveness of the Easy-to-Hard Reward Curriculum. Take the GrailQA dataset as an example, we observe that an overly simplistic signal like EM is insufficient for exploration; it induces \"over-recall reward hacking,\" resulting in a massive drop in the F 1 F_{1} score on GrailQA. Meanwhile, a stricter F1 reward is too punishing early on, leading to a -4.5% F1 drop. Notably, reversing the curriculum to a hard-to-easy schedule ( F 1 F_{1} -then- F 0.5 F_{0.5} ) also degrades performance (-4.4% F1 drop). This confirms our hypothesis: starting with an ‚Äúeasy task‚Äù encourages broad exploration to discover correct reasoning paths, while the subsequent ‚Äúhard task‚Äù refines the agent‚Äôs ability to achieve a better balance between precision and recall.\\nCritical Contribution of the RL Stage. From the second block of Table 3 , we can observe that removing the entire RL-stage causes at least 17.4% absolute drop in the F1 score. This large gap suggests that RL-based exploration is essential for refining the agent‚Äôs reasoning capability beyond initial supervised learning.\\nImportance of Upsampling in the Cold-Start Stage. Finally, as the last row shows, removing the up-sampling of infrequent question types leads to poor performance on those specific categories. This underscores the importance of our sampling strategy for building a model that can generalize across a diverse range of reasoning patterns.\\nTo answer the RQ3 and investigate how KnowCoder-A1 progressively enhances the agent‚Äôs reasoning capability under the outcome-only supervision, we analyze from the train dynamics, the evolution of its reasoning robustness and diversity, and the comparative performance against process-supervised methods.\\nAs shown in Fig. 3 , to track the evolution of the model‚Äôs capabilities during training, we present the curves of (a) training reward, (b) response length (token), (c) turns of interaction, and (d) the number of invalid tool calls along the training step. It can be seen that the agent‚Äôs strategy clearly transitions from broad exploration to efficient exploitation : initially, the agent explores inefficiently, which is reflected in high response lengths and turn counts for a low reward. As training progresses, these trends reverse: the reward consistently rises while the metrics for response length, turns, and invalid tool calls all significantly decrease. This dynamic illustrates a fundamental improvement in the agent‚Äôs complex question-solving process, not just a superficial optimization of rewards.\\nTo provide a deeper analysis of whether the model‚Äôs reasoning ability is enhanced, particularly concerning the robustness and flexibility lacking in SFT methods, we examine the evolution of the agent‚Äôs reasoning behavior. Fig. 4 (a) illustrates its robustness by showing the composition of rollout trajectories, categorized as: \"Directly Hit,\" for trajectories where the final prediction hits at least one gold answers; \"Recovered after Error/Empty,\" for those that hit at least one gold answers after encountering an intermediate error or empty observation; and \"Incorrect,\" for trajectories where the final prediction fails to find any gold answerss. The figure shows that the proportion of successful trajectories (\"Directly Hit\" and \"Recovered\") steadily increases during training, This confirms that the agent learns to effectively recover from imperfect feedback, a direct advantage of our outcome supervision approach. In contrast, process supervision encourages the model to simply replicate pre-defined trajectories from the training data, and thus fails to incentivize this type of recovery behavior.\\nFig. 4 (b) measures flexibility by plotting the diversity of trajectories, which we define as the number of unique final SPARQL queries generated for the same question across the rollouts. It can be seen that the agent initially broadens its search for a wide range of solutions before consolidating on a compact set of the most reliable ones, which is consistent with the process discussed in Sec. 5.4.1 . This indicates that outcome-only supervision naturally fosters an exploration-then-convergence behavior, where the agent ultimately retains sufficient solution diversity to effectively address the lack of flexibility inherent in process-supervised methods.\\nTo explore whether incorporating process supervision in addition to outcome-only supervision benefits model performance, we experimented by incorporating necessary process-level penalties for behaviors including hallucinations and timeouts. Specifically, we define a hallucination as a tool call containing a relation or type that is not present in the observation history, while a timeout query denotes a call that results in a 300 second timeout. We applied a negative reward of -0.2 for each occurrence of these behaviors, with the total accumulated penalty capped at -0.5. We then compared our full model against two variants incorporating these Process Rewards (P.R.): one applying the penalties throughout the entire RL training (P.R.@RL-phase-1&2), and another applying them only during the second stage (P.R.@RL-phase-2).\\nThe results clearly demonstrate that introducing penalties for any intermediate behaviors, even for intuitively negative ones, degrades final performance. We hypothesize that in KBQA tasks, where the correctness of answers can be explicitly verified, process-level rewards may interfere with the agent‚Äôs capacity to form a robust trial-and-error exploration strategy, as they tend to prematurely penalize exploratory trajectories that could eventually achieve correct results.\\nTo concretely demonstrate the advanced reasoning capabilities of KnowCoder-A1 , we present a qualitative case study with two detailed examples in Appendix F . These examples highlight the agent‚Äôs ability to: (1) robustly recover from erroneous feedback; (2) flexibly use tools to find correct answers via reasoning trajectories that differ from the gold SPARQL. These cases serve as strong evidence of the robustness and flexibility fostered by the proposed KnowCoder-A1 approach.\\nIn addition, we conduct error analysis, which is presented in Appendix D .',\n",
       "    'subsections': [{'title': '5.1 Experiment Setup',\n",
       "      'paragraphs': 'Dataset and Metrics . We evaluate KnowCoder-A1 on three widely-used KBQA datasets: WebQSP, CWQ, and the generalization-focused GrailQA, using a subset of the official testing data. Following existing work (Xiong et\\xa0al., 2024 ) , we primarily evaluate the F1 metrics, supplemented by Random Hits@1(RHits@1) and EM score. As our approach does not rely on semantic parsing, we do not report logical form accuracy (Gu et\\xa0al., 2021 ) .\\nBaselines . We compare KnowCoder-A1 against two main categories of methods: prompting-based and fine-tuning-based methods. For the former, we include a diverse range of prompting strategies, from direct prompting, such as IO, CoT, etc., and agent-based reasoning method Interactive-KBQA (Xiong et\\xa0al., 2024 ) . To ensure a fair comparison and reproducibility, we report the scores for these methods directly from their original publications. For the fine-tuning-based methods, we select SOTA methods finetuned on full data, include RnG-KBQA (Ye et\\xa0al., 2022 ) and TIARA (Shu et\\xa0al., 2022 ) , and the directly comparable low-source methods, which are finetuned on low-source data, includes agentic approaches like SFT-traj. (Xiong et\\xa0al., 2024 ) , KBQA-o1 ( Luo et\\xa0al., ) , and MCTS-KBQA (Xiong et\\xa0al., 2025 ) .\\nFurther details on the datasets, baselines, evaluation protocol, and implementation are provided in Appendix A .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Main Results - RQ1',\n",
       "      'paragraphs': 'To evaluate the performance of KnowCoder-A1 , we conducted comprehensive experiments on three datasets, with the results presented in Table 1 . For additional context on the efficiency of our method, we compare the number of samples used during training with the number of rollouts used during inference in Table 2 . Taken together, these results demonstrate that KnowCoder-A1 establishes a new state-of-the-art for low-resource KBQA across all benchmarks. This shows the effectiveness of our approach, which can be analyzed from three key aspects.\\nFirst, KnowCoder-A1 significantly outperforms all directly comparable low-resource baselines, particularly on the more complex CWQ and the generalization-focused GrailQA datasets. Meanwhile, it nearly closes the gap with fully-supervised methods while using only a fraction of the data (\\xa010k outcome-supervised samples vs. \\xa060k fully-annotated queries), highlighting its exceptional data efficiency.\\nSecond, results prove that the outcome-supervised multi-stage training strategy is effective than the process-supervised SFT strategy.\\nResults in the last two blocks of Table 1 indicate that KnowCoder-A1 our approach consistently outperforms leading process-supervised SFT baselines. On GrailQA, KnowCoder-A1 achieves an F1 score of 80.5%, which represents a 3.3% relative improvement over the prior state-of-the-art method KBQA-o1 while using 12√ó less training data (3.5k vs. 43.8k samples). While performance is slightly lower on GrailQA‚Äôs I.I.D. and compositional subsets, which favor extensive training data to cover specific patterns, our method demonstrates a relative improvement of up to 11.1% on the zero-shot subset. This proves its superior ability to reason effectively on truly unseen questions.\\nFurthermore, beyond higher overall scores, KnowCoder-A1 is more efficient in both supervision and inference. In terms of supervision, as detailed in the ‚ÄúTrain‚Äù columns of Table 2 , KnowCoder-A1 uses zero process-supervised (P.S.) instances and a comparable or smaller number of outcome-supervised (O.S.) samples than MCTS-based competitors. As montioned before, KnowCoder-A1 outperforms KBQA-o1 on GrailQA while using much fewer supervised samples. In terms of inference, as shown in the ‚ÄúEvaluation‚Äù columns of Table 2 , KnowCoder-A1 employs a single linear reasoning pass and avoids the costly sampling required by MCTS, reducing latency by 3.2‚Äì6 times.\\nThese results support outcome supervision as a more effective and efficient alternative to incremental SFT on pre-defined or explored paths.\\nFinally, KnowCoder-A1 unlocks SOTA reasoning capabilities in smaller models. As shown in the first block of Table 1 , KnowCoder-A1 consistently outperforms prompting-based methods that rely on significantly larger models like GPT-4-turbo by at least 6% at F1. This demonstrates an essential improvement in the model‚Äôs intrinsic reasoning ability.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Ablation Study - RQ2',\n",
       "      'paragraphs': 'To verify the effectiveness of our key designs, we conduct a comprehensive ablation study, and the results are presented in Table 3 . First, we assess the reward curriculum by replacing our proposed easy-to-hard ( F 0.5 F_{0.5} -then- F 1 F_{1} ) strategy, give the result using EM, F 1 F_{1} , F 0.5 F_{0.5} , and revered strategy F 1 F_{1} -then- F 0.5 F_{0.5} as reward, respectively. Next, we evaluate the contribution of the entire RL stage by removing it altogether. Finally, we remove the data sampling strategy from the initial cold-start phase. The results confirm that each component is integral to the model‚Äôs performance, as removing or modifying any of them leads to a noticeable degradation.\\nEffectiveness of the Easy-to-Hard Reward Curriculum. The first block of Table 3 shows the effectiveness of the Easy-to-Hard Reward Curriculum. Take the GrailQA dataset as an example, we observe that an overly simplistic signal like EM is insufficient for exploration; it induces \"over-recall reward hacking,\" resulting in a massive drop in the F 1 F_{1} score on GrailQA. Meanwhile, a stricter F1 reward is too punishing early on, leading to a -4.5% F1 drop. Notably, reversing the curriculum to a hard-to-easy schedule ( F 1 F_{1} -then- F 0.5 F_{0.5} ) also degrades performance (-4.4% F1 drop). This confirms our hypothesis: starting with an ‚Äúeasy task‚Äù encourages broad exploration to discover correct reasoning paths, while the subsequent ‚Äúhard task‚Äù refines the agent‚Äôs ability to achieve a better balance between precision and recall.\\nCritical Contribution of the RL Stage. From the second block of Table 3 , we can observe that removing the entire RL-stage causes at least 17.4% absolute drop in the F1 score. This large gap suggests that RL-based exploration is essential for refining the agent‚Äôs reasoning capability beyond initial supervised learning.\\nImportance of Upsampling in the Cold-Start Stage. Finally, as the last row shows, removing the up-sampling of infrequent question types leads to poor performance on those specific categories. This underscores the importance of our sampling strategy for building a model that can generalize across a diverse range of reasoning patterns.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.4 Analysis - RQ3',\n",
       "      'paragraphs': 'To answer the RQ3 and investigate how KnowCoder-A1 progressively enhances the agent‚Äôs reasoning capability under the outcome-only supervision, we analyze from the train dynamics, the evolution of its reasoning robustness and diversity, and the comparative performance against process-supervised methods.\\nAs shown in Fig. 3 , to track the evolution of the model‚Äôs capabilities during training, we present the curves of (a) training reward, (b) response length (token), (c) turns of interaction, and (d) the number of invalid tool calls along the training step. It can be seen that the agent‚Äôs strategy clearly transitions from broad exploration to efficient exploitation : initially, the agent explores inefficiently, which is reflected in high response lengths and turn counts for a low reward. As training progresses, these trends reverse: the reward consistently rises while the metrics for response length, turns, and invalid tool calls all significantly decrease. This dynamic illustrates a fundamental improvement in the agent‚Äôs complex question-solving process, not just a superficial optimization of rewards.\\nTo provide a deeper analysis of whether the model‚Äôs reasoning ability is enhanced, particularly concerning the robustness and flexibility lacking in SFT methods, we examine the evolution of the agent‚Äôs reasoning behavior. Fig. 4 (a) illustrates its robustness by showing the composition of rollout trajectories, categorized as: \"Directly Hit,\" for trajectories where the final prediction hits at least one gold answers; \"Recovered after Error/Empty,\" for those that hit at least one gold answers after encountering an intermediate error or empty observation; and \"Incorrect,\" for trajectories where the final prediction fails to find any gold answerss. The figure shows that the proportion of successful trajectories (\"Directly Hit\" and \"Recovered\") steadily increases during training, This confirms that the agent learns to effectively recover from imperfect feedback, a direct advantage of our outcome supervision approach. In contrast, process supervision encourages the model to simply replicate pre-defined trajectories from the training data, and thus fails to incentivize this type of recovery behavior.\\nFig. 4 (b) measures flexibility by plotting the diversity of trajectories, which we define as the number of unique final SPARQL queries generated for the same question across the rollouts. It can be seen that the agent initially broadens its search for a wide range of solutions before consolidating on a compact set of the most reliable ones, which is consistent with the process discussed in Sec. 5.4.1 . This indicates that outcome-only supervision naturally fosters an exploration-then-convergence behavior, where the agent ultimately retains sufficient solution diversity to effectively address the lack of flexibility inherent in process-supervised methods.\\nTo explore whether incorporating process supervision in addition to outcome-only supervision benefits model performance, we experimented by incorporating necessary process-level penalties for behaviors including hallucinations and timeouts. Specifically, we define a hallucination as a tool call containing a relation or type that is not present in the observation history, while a timeout query denotes a call that results in a 300 second timeout. We applied a negative reward of -0.2 for each occurrence of these behaviors, with the total accumulated penalty capped at -0.5. We then compared our full model against two variants incorporating these Process Rewards (P.R.): one applying the penalties throughout the entire RL training (P.R.@RL-phase-1&2), and another applying them only during the second stage (P.R.@RL-phase-2).\\nThe results clearly demonstrate that introducing penalties for any intermediate behaviors, even for intuitively negative ones, degrades final performance. We hypothesize that in KBQA tasks, where the correctness of answers can be explicitly verified, process-level rewards may interfere with the agent‚Äôs capacity to form a robust trial-and-error exploration strategy, as they tend to prematurely penalize exploratory trajectories that could eventually achieve correct results.',\n",
       "      'subsections': [{'title': '5.4.1 Analysis of Training Dynamics with Outcome-Only Supervision',\n",
       "        'paragraphs': 'As shown in Fig. 3 , to track the evolution of the model‚Äôs capabilities during training, we present the curves of (a) training reward, (b) response length (token), (c) turns of interaction, and (d) the number of invalid tool calls along the training step. It can be seen that the agent‚Äôs strategy clearly transitions from broad exploration to efficient exploitation : initially, the agent explores inefficiently, which is reflected in high response lengths and turn counts for a low reward. As training progresses, these trends reverse: the reward consistently rises while the metrics for response length, turns, and invalid tool calls all significantly decrease. This dynamic illustrates a fundamental improvement in the agent‚Äôs complex question-solving process, not just a superficial optimization of rewards.',\n",
       "        'subsections': []},\n",
       "       {'title': '5.4.2 Analysis of Reasoning Capability',\n",
       "        'paragraphs': 'To provide a deeper analysis of whether the model‚Äôs reasoning ability is enhanced, particularly concerning the robustness and flexibility lacking in SFT methods, we examine the evolution of the agent‚Äôs reasoning behavior. Fig. 4 (a) illustrates its robustness by showing the composition of rollout trajectories, categorized as: \"Directly Hit,\" for trajectories where the final prediction hits at least one gold answers; \"Recovered after Error/Empty,\" for those that hit at least one gold answers after encountering an intermediate error or empty observation; and \"Incorrect,\" for trajectories where the final prediction fails to find any gold answerss. The figure shows that the proportion of successful trajectories (\"Directly Hit\" and \"Recovered\") steadily increases during training, This confirms that the agent learns to effectively recover from imperfect feedback, a direct advantage of our outcome supervision approach. In contrast, process supervision encourages the model to simply replicate pre-defined trajectories from the training data, and thus fails to incentivize this type of recovery behavior.\\nFig. 4 (b) measures flexibility by plotting the diversity of trajectories, which we define as the number of unique final SPARQL queries generated for the same question across the rollouts. It can be seen that the agent initially broadens its search for a wide range of solutions before consolidating on a compact set of the most reliable ones, which is consistent with the process discussed in Sec. 5.4.1 . This indicates that outcome-only supervision naturally fosters an exploration-then-convergence behavior, where the agent ultimately retains sufficient solution diversity to effectively address the lack of flexibility inherent in process-supervised methods.',\n",
       "        'subsections': []},\n",
       "       {'title': '5.4.3 Analysis of Process Reward',\n",
       "        'paragraphs': 'To explore whether incorporating process supervision in addition to outcome-only supervision benefits model performance, we experimented by incorporating necessary process-level penalties for behaviors including hallucinations and timeouts. Specifically, we define a hallucination as a tool call containing a relation or type that is not present in the observation history, while a timeout query denotes a call that results in a 300 second timeout. We applied a negative reward of -0.2 for each occurrence of these behaviors, with the total accumulated penalty capped at -0.5. We then compared our full model against two variants incorporating these Process Rewards (P.R.): one applying the penalties throughout the entire RL training (P.R.@RL-phase-1&2), and another applying them only during the second stage (P.R.@RL-phase-2).\\nThe results clearly demonstrate that introducing penalties for any intermediate behaviors, even for intuitively negative ones, degrades final performance. We hypothesize that in KBQA tasks, where the correctness of answers can be explicitly verified, process-level rewards may interfere with the agent‚Äôs capacity to form a robust trial-and-error exploration strategy, as they tend to prematurely penalize exploratory trajectories that could eventually achieve correct results.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '5.5 Case Study - RQ4',\n",
       "      'paragraphs': 'To concretely demonstrate the advanced reasoning capabilities of KnowCoder-A1 , we present a qualitative case study with two detailed examples in Appendix F . These examples highlight the agent‚Äôs ability to: (1) robustly recover from erroneous feedback; (2) flexibly use tools to find correct answers via reasoning trajectories that differ from the gold SPARQL. These cases serve as strong evidence of the robustness and flexibility fostered by the proposed KnowCoder-A1 approach.\\nIn addition, we conduct error analysis, which is presented in Appendix D .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Conclusion',\n",
       "    'paragraphs': 'In this paper, we focused on a key limitation of existing agentic KBQA paradigms: their reliance on process supervision, which provides weak incentives for autonomous exploration. To overcome this, we proposed KnowCoder-A1 , our first agentic reasoning model for KBQA. It is trained by a multi-stage curriculum reinforcement learning framework that effectively leverages outcome-only supervision through a carefully designed learning curriculum. Specifically, KnowCoder-A1 first establishes foundational reasoning capabilities via a cold-start fine-tuning stage on a small, high-quality dataset created through outcome-based rejection sampling. It then enhances the agent‚Äôs exploratory capacity through curriculum RL, employing a progressive reward schedule that transitions from easy to hard tasks. Extensive experiments demonstrate the effectiveness of our approach. Further analyses reveal that KnowCoder-A1 learns to act as a robust and flexible agent, capable of recovering from errors and strategically exploring diverse reasoning trajectories. Future work may investigate more advanced reflection mechanisms to mitigate remaining error types and extend this curriculum strategy to other complex, agent-based reasoning tasks.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': '7 Appendices', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Experiment Setup',\n",
       "    'paragraphs': 'We evaluate the reasoning performance on three widely-used datasets: WebQSP (Talmor & Berant, 2018 ) , CWQ (Talmor & Berant, 2018 ) , and GrailQA (Gu et\\xa0al., 2021 ) . These datasets are constructed based on Freebase and consist of natural language questions paired with their corresponding SPARQL queries. Specifically, WebQSP features questions where the topic entity and answer entities are at most 2 relation hops apart. CWQ extends WebQSP by incorporating more complex entities and constraints, with relation paths up to 4 hops. In contrast to WebQSP and CWQ, GrailQA is designed for a more comprehensive evaluation of generalization capabilities across three settings: independently and identically distributed (I.I.D.), compositional, and zero-shot.\\nTo ensure consistent results and reduce evaluation overhead, we follow the setting of previous work Xiong et\\xa0al. ( 2024 ) , which uniformly sample questions across different types in WebQSP and CWQ. For the GrailQA dataset, we randomly sample 600 instances for evaluation.\\nOur experiments compare KnowCoder-A1 against two categories of methods.\\nFine-tuning-based Methods. We consider two settings: fine-tuning on partial data (low-source setting) and on full data. In the low-source setting, we select the agentic approaches, includes the SFT model in Interactive-KBQA (Xiong et\\xa0al., 2024 ) , which uses the same toolset as our method (abbreviated as SFT-traj.), two MCTS-based agentic methods, KBQA-o1 ( Luo et\\xa0al., ) and MCTS-KBQA (Xiong et\\xa0al., 2025 ) . Notably, since our method is trained on only a subset of the data and uses no SPARQL annotations, its performance is directly comparable to this category of baselines.\\nFor the full-data setting, which serves as a secondary point of reference, we include the SOTA SP-based methods RnG-KBQA (Ye et\\xa0al., 2022 ) and TIARA (Shu et\\xa0al., 2022 ) .\\nPrompting-based Methods. We selected a diverse set of prompting strategies for comparison. These include methods based on direct prompting, Chain-of-Thought (CoT), and self-consistency with CoT; approaches that use an LLM to enhance semantic parsing (KB-BINDER (Li et\\xa0al., 2023 ) and KB-Coder (Nie et\\xa0al., 2024 ) ) and subgraph retrieval (Tian et\\xa0al., 2024 ) ; and Interactive-KBQA (Xiong et\\xa0al., 2024 ) that prompts an LLM to reason as an agent. For a fair comparison, we report the scores for these methods directly from their official papers, as the specific API versions used in their experiments are no longer accessible.\\nWe primarily report the F1 score since the answers are returned as an unordered list. Following previous work (Xiong et\\xa0al., 2024 ) , we also report Random Hits@1 (RHits@1) and Exact Match (EM) for reference. As our method is not fully based on semantic parsing, we do not report the semantically equivalent of logical forms (another score commonly abbreviated as EM) in Gu et\\xa0al. ( 2021 ) .\\nWe use the Qwen2.5-Coder-7B-Base as our backbone LLM. For the cold-start stage, our implementation is based on the LlamaFactory framework (Zheng et\\xa0al., 2024 ) ; we use a cosine learning rate schedule with an initial learning rate of 2e-5, a batch size of 8, and a maximum sequence length of 4096. We fine-tune the model for 8 epochs. In the RL stage, our implementation is based on Agent-R1 framework (Ouyang et\\xa0al., 2025 ) (a verl-based (Sheng et\\xa0al., 2024 ) agentic training framework); we set the number of rollouts to N=8, use a batch size of 64 for rollout, and a global batch size 512 for training. We train the model for 1 epoch. At validation, we set the temperature to 0 to ensure deterministic outputs. The detail parameters can be found in 5 . All experiments were conducted on 8 √ó \\\\times NVIDIA H100 (80GB) GPUs and 8 √ó \\\\times NVIDIA A100 (80GB) GPUs.',\n",
       "    'subsections': [{'title': 'A.1 Datasets',\n",
       "      'paragraphs': 'We evaluate the reasoning performance on three widely-used datasets: WebQSP (Talmor & Berant, 2018 ) , CWQ (Talmor & Berant, 2018 ) , and GrailQA (Gu et\\xa0al., 2021 ) . These datasets are constructed based on Freebase and consist of natural language questions paired with their corresponding SPARQL queries. Specifically, WebQSP features questions where the topic entity and answer entities are at most 2 relation hops apart. CWQ extends WebQSP by incorporating more complex entities and constraints, with relation paths up to 4 hops. In contrast to WebQSP and CWQ, GrailQA is designed for a more comprehensive evaluation of generalization capabilities across three settings: independently and identically distributed (I.I.D.), compositional, and zero-shot.\\nTo ensure consistent results and reduce evaluation overhead, we follow the setting of previous work Xiong et\\xa0al. ( 2024 ) , which uniformly sample questions across different types in WebQSP and CWQ. For the GrailQA dataset, we randomly sample 600 instances for evaluation.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Baselines',\n",
       "      'paragraphs': 'Our experiments compare KnowCoder-A1 against two categories of methods.\\nFine-tuning-based Methods. We consider two settings: fine-tuning on partial data (low-source setting) and on full data. In the low-source setting, we select the agentic approaches, includes the SFT model in Interactive-KBQA (Xiong et\\xa0al., 2024 ) , which uses the same toolset as our method (abbreviated as SFT-traj.), two MCTS-based agentic methods, KBQA-o1 ( Luo et\\xa0al., ) and MCTS-KBQA (Xiong et\\xa0al., 2025 ) . Notably, since our method is trained on only a subset of the data and uses no SPARQL annotations, its performance is directly comparable to this category of baselines.\\nFor the full-data setting, which serves as a secondary point of reference, we include the SOTA SP-based methods RnG-KBQA (Ye et\\xa0al., 2022 ) and TIARA (Shu et\\xa0al., 2022 ) .\\nPrompting-based Methods. We selected a diverse set of prompting strategies for comparison. These include methods based on direct prompting, Chain-of-Thought (CoT), and self-consistency with CoT; approaches that use an LLM to enhance semantic parsing (KB-BINDER (Li et\\xa0al., 2023 ) and KB-Coder (Nie et\\xa0al., 2024 ) ) and subgraph retrieval (Tian et\\xa0al., 2024 ) ; and Interactive-KBQA (Xiong et\\xa0al., 2024 ) that prompts an LLM to reason as an agent. For a fair comparison, we report the scores for these methods directly from their official papers, as the specific API versions used in their experiments are no longer accessible.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.3 Evaluation Metrics',\n",
       "      'paragraphs': 'We primarily report the F1 score since the answers are returned as an unordered list. Following previous work (Xiong et\\xa0al., 2024 ) , we also report Random Hits@1 (RHits@1) and Exact Match (EM) for reference. As our method is not fully based on semantic parsing, we do not report the semantically equivalent of logical forms (another score commonly abbreviated as EM) in Gu et\\xa0al. ( 2021 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.4 Implementation Details',\n",
       "      'paragraphs': 'We use the Qwen2.5-Coder-7B-Base as our backbone LLM. For the cold-start stage, our implementation is based on the LlamaFactory framework (Zheng et\\xa0al., 2024 ) ; we use a cosine learning rate schedule with an initial learning rate of 2e-5, a batch size of 8, and a maximum sequence length of 4096. We fine-tune the model for 8 epochs. In the RL stage, our implementation is based on Agent-R1 framework (Ouyang et\\xa0al., 2025 ) (a verl-based (Sheng et\\xa0al., 2024 ) agentic training framework); we set the number of rollouts to N=8, use a batch size of 64 for rollout, and a global batch size 512 for training. We train the model for 1 epoch. At validation, we set the temperature to 0 to ensure deterministic outputs. The detail parameters can be found in 5 . All experiments were conducted on 8 √ó \\\\times NVIDIA H100 (80GB) GPUs and 8 √ó \\\\times NVIDIA A100 (80GB) GPUs.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Further Analysis',\n",
       "    'paragraphs': 'In this section, we provide several supplementary analyses. First, we examine the impact of different backbone models on performance and discuss our method‚Äôs token efficiency. These results justify our model selection and highlight the efficiency advantages of our approach. Second, we present a joint distribution of error frequency versus sample accuracy across training stages, which serves to supplement our main analysis of training dynamics (Section 5.4.1 ) and offers a more comprehensive view of how the agent‚Äôs capabilities evolve.\\nWe compared the impact of different backbone models on performance during the cold-start stage. As shown in Table 6 , our results indicate that Qwen2.5-Coder-7B-Base delivers the strongest overall performance. Although Qwen3-8B-Base performs better on the simpler WebQSP dataset, we selected Qwen2.5-Coder-7-Base for its superior performance on CWQ, which contains more complex questions. Furthermore, our task involves numerous code-style tool calls, where a coder-specialized model has a natural advantage.\\nTo evaluate the inference efficiency of our method, we conducted a computational cost analysis against prompting-based baselines, including the single-round inference method CoT and the multi-round interactive method ToG. As shown in Table 7 , the comparison covers LLM call frequency, token usage, and overall cost. The results reveal that KnowCoder-A1 achieves performance comparable to the complex ToG, while maintaining a computational budget similar to the much simpler CoT. This efficiency stems from outcome-based supervision: the model learns to recover from empty results or intermediate errors in a single pass, obviating the need to generate multiple branches to find a solution at significant computational expense. Overall, KnowCoder-A1 shifts the efficiency frontier toward lower cost while maintaining strong effectiveness, making it more predictable and easier to deploy in real-world scenarios.\\nTo further investigate how the agent‚Äôs robustness evolves, we analyze the association between intermediate errors and final rewards throughout training. As illustrated in Figure 5 , we plot the joint distribution of the mean frequency of empty results/errors per question against the mean reward. This visualization reveals a clear progression. Early in training, the distribution forms a downward-sloping triangle, where higher error rates are strongly correlated with lower rewards. As training advances, this shape flattens into a rectangle, indicating that high rewards are achieved even with a non-trivial frequency of errors. Concurrently, the density increases in the upper-left corner, reflecting a growing number of questions being solved directly and perfectly. The transformation provides compelling evidence of our method‚Äôs superiority: the agent not only learns to avoid mistakes but also develops the crucial ability to recover from them, transitioning from a brittle execution policy to a robust, high-scoring one.',\n",
       "    'subsections': [{'title': 'B.1 Analysis of the Impact of Different Backbones',\n",
       "      'paragraphs': 'We compared the impact of different backbone models on performance during the cold-start stage. As shown in Table 6 , our results indicate that Qwen2.5-Coder-7B-Base delivers the strongest overall performance. Although Qwen3-8B-Base performs better on the simpler WebQSP dataset, we selected Qwen2.5-Coder-7-Base for its superior performance on CWQ, which contains more complex questions. Furthermore, our task involves numerous code-style tool calls, where a coder-specialized model has a natural advantage.',\n",
       "      'subsections': []},\n",
       "     {'title': 'B.2 Analysis of token efficiency',\n",
       "      'paragraphs': 'To evaluate the inference efficiency of our method, we conducted a computational cost analysis against prompting-based baselines, including the single-round inference method CoT and the multi-round interactive method ToG. As shown in Table 7 , the comparison covers LLM call frequency, token usage, and overall cost. The results reveal that KnowCoder-A1 achieves performance comparable to the complex ToG, while maintaining a computational budget similar to the much simpler CoT. This efficiency stems from outcome-based supervision: the model learns to recover from empty results or intermediate errors in a single pass, obviating the need to generate multiple branches to find a solution at significant computational expense. Overall, KnowCoder-A1 shifts the efficiency frontier toward lower cost while maintaining strong effectiveness, making it more predictable and easier to deploy in real-world scenarios.',\n",
       "      'subsections': []},\n",
       "     {'title': 'B.3 Analysis on Overall Robustness Capability Shift',\n",
       "      'paragraphs': 'To further investigate how the agent‚Äôs robustness evolves, we analyze the association between intermediate errors and final rewards throughout training. As illustrated in Figure 5 , we plot the joint distribution of the mean frequency of empty results/errors per question against the mean reward. This visualization reveals a clear progression. Early in training, the distribution forms a downward-sloping triangle, where higher error rates are strongly correlated with lower rewards. As training advances, this shape flattens into a rectangle, indicating that high rewards are achieved even with a non-trivial frequency of errors. Concurrently, the density increases in the upper-left corner, reflecting a growing number of questions being solved directly and perfectly. The transformation provides compelling evidence of our method‚Äôs superiority: the agent not only learns to avoid mistakes but also develops the crucial ability to recover from them, transitioning from a brittle execution policy to a robust, high-scoring one.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix C Statistics',\n",
       "    'paragraphs': 'We give the statistic of training samples used in cold-start-stage and RL stage on three datasets.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Error Analysis',\n",
       "    'paragraphs': 'To identify the limitations of our method, we conducted an error analysis by manually inspecting random sampled cases where the F 1 F_{1} score was less than 1. The results are categorized in Fig. 6 , with the primary error types being Hallucination, Constraint Application Error, and Relation Recall/Selection Error. Hallucinations often result from flawed self-correction attempts. While intuitively detrimental, our prior experiments show that penalizing such behavior actually stifles the model‚Äôs capacity for self-correction. The other two error types generally stem from incorrect decisions during complex reasoning. These errors highlight the space for future work: enhancing the model‚Äôs ability to handle complex questions within a linear decision-making process. Notably, the high rate of constraint errors on WebQSP is due to the model failing to identify implicit temporal constraints (e.g., the KB‚Äôs 2015 cutoff date) in present-tense questions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix E Prompts',\n",
       "    'paragraphs': 'In our experiments, the same prompt was used for both training and evaluation, which is composed of a system prompt and a user prompt.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix F Case Study',\n",
       "    'paragraphs': 'To illustrate the flexibility and robustness of our agent‚Äôs reasoning process, we present two representative case studies. The first case demonstrates robustness: when faced with two consecutive empty query results, the agent successfully recovers by leveraging a more distant observation from its history to try a new, promising relation, ultimately finding the correct answer. The second case highlights flexibility and self-correction, showing the agent‚Äôs ability to identify an illogical query result (e.g., receiving a high school when a university was expected) and adapt its strategy by employing a novel action to resolve the issue.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.'},\n",
       " '2510.25577v1': {'content': [{'title': '1.\\u2003\\u200aIntroduction',\n",
       "    'paragraphs': 'Speech foundation models (SFMs) are rapidly transforming how spoken language is represented and interpreted. By learning directly from raw audio, these models have the potential to integrate both lexical and paralinguistic information ‚Äì capturing not only what is said, but how it is said Pasad ( 2025 ) . Despite this, while SFMs are increasingly evaluated for recognition accuracy and text-aligned reasoning, their treatment of paralinguistic variation remains largely untested Yang et\\xa0al. ( 2024 ) .\\nOne important but understudied aspect of paralinguistic variation is voice quality . Voice quality refers to differences in phonation arising from specific laryngeal and supralaryngeal configurations (Laver, 1980 ) . In some languages these contrasts are phonemic (Esposito, 2005 ) , while in English they carry pragmatic and social meaning. Research has associated breathy voice with intimacy (Tsvetanova et\\xa0al., 2017 ) and creaky voice (also called vocal fry) with authority or disengagement (Laver, 1968 ) . Perceptual studies also point to gender-based asymmetries: creaky voice tends to elicit negative attitudes towards young female speakers (Hornibrook et\\xa0al., 2018 ) , whereas attractiveness judgments depend on both speaker gender and phonation type (Greer and Winters, 2015 ; Xu et\\xa0al., 2013 ) .\\nDespite the perceptual and social weight of voice quality, current SFM evaluation methods provide little insight into how these models interpret or reproduce phonation-related cues. The majority of paralinguistic assessments depend on multiple-choice question answering (MCQA) frameworks (Sakshi et\\xa0al., 2025 ; Ma et\\xa0al., 2025 ) , which constrain model outputs and mask how non-lexical variations influence generation patterns. Additional concerns surrounding MCQA evaluations Bokkahalli\\xa0Satish et\\xa0al. ( 2025b ); Li et\\xa0al. ( 2024 ); Zheng et\\xa0al. ( 2023 ) , particularly within the speech domain, remain yet to be adequately addressed.\\nA framework for assessing SFM responses to systematic voice quality variation in naturalistic contexts is still absent.\\nRecent advances in speech synthesis and voice conversion Rautenberg et\\xa0al. ( 2025 ); Lameris et\\xa0al. ( 2025 ); Lameris and Ward ( 2025 ) now enable the generation of creaky and breathy phonation to varying degrees, and the systematic manipulation of these features across speaker profiles while keeping speaker identity and linguistic content constant. This allows us to pinpoint how voice quality affects both speech emotion recognition (SER) systems and the behaviour of SFMs including how they encode and interpret differences.\\nIn this work, we present VQ-Bench , a controlled evaluation suite designed to test SFM sensitivity to voice quality variation. The dataset includes parallel prompts synthesized in modal, breathy, creaky, and end-creak phonation types. We use it to evaluate two complementary settings: long-form, open-ended generation tasks, and speech emotion recognition. Our contributions are threefold: (1) a first systematic study of SFM behaviour under controlled phonation variation, (2) a corpus of synthetic voice quality data for reproducible evaluation, and (3) an open-ended evaluation protocol for probing paralinguistic sensitivity in speech models.\\nOur corpus and evaluation suite will be made available at this link: https://anonymous.4open.science/r/Lost-in-phonation-65B9',\n",
       "    'subsections': []},\n",
       "   {'title': '2.\\u2003\\u200aMethod',\n",
       "    'paragraphs': 'To create the voice quality variation dataset, we used speech samples from the Buckeye Corpus Pitt et\\xa0al. ( 2007 ) , which contains spontaneous conversational American English in an interview setting, and the VCTK Corpus Yamagishi et\\xa0al. ( 2019 ) , which features read English from over 100 speakers of different dialects.\\nEach speaker from each corpus was used as the reference audio to synthesize these prompts using the zero-shot TTS system F5-TTS (Chen et\\xa0al., 2024 ) . The prompts were altered with respect to their glottal source characteristics, using the method described in Lameris et\\xa0al. ( 2025 ) , to produce modal, breathy, creaky, and end-creak variants of the original prompts.\\nTo create realistic prompts designed to measure the effect of voice quality, we create an expanded version of the open-ended long-form question prompts from Bokkahalli\\xa0Satish et\\xa0al. ( 2025a ) , which consists of four categories grounded in documented real-world applications of SpeechLLMs: Therapy, Career advice, Interview screening, and Storytelling Karvonen and Marks ( 2025 ); Lum et\\xa0al. ( 2024 ); Zao-Sanders . Examples can be found in Appendix A . These categories were originally selected to reflect authentic use cases where speaker identity could meaningfully influence model responses. We additionally introduce the effect of voice quality as a controlled variable.\\nFor each category, we developed five distinct prompts that maintain the core structure of eliciting voice-dependent responses while introducing variation in context and framing. This results in a total of 20 prompts per speaker.\\nWe used F5-TTS (Chen et\\xa0al., 2024 ) , a state-of-the-art zero-shot TTS system to synthesize the prompts. In order to create the target voices for the synthesis of the prompts, we extracted speech from the Buckeye corpus Pitt et\\xa0al. ( 2007 ) and the Centre for Speech Technology Voice Cloning Toolkit (VCTK) Yamagishi et\\xa0al. ( 2019 ) . The Buckeye corpus features conversational speech from interviews with 40 native English speakers from Central Ohio, balanced for gender (male or female) and age (under or over 40 years old) discussing local issues. VCTK features 109 speakers of different dialects of English reading out 400 phonetically balanced sentences. We extracted 12 second stretches of uninterrupted speech from both corpora to serve as the reference audio for F5-TTS, as this is the maximum input length for a target speaker .\\nVoiceQualityVC (Lameris et\\xa0al., 2025 ) was used to create modal, breathy, creaky, and end-creak versions of the original prompts. Modal, breathy, and creaky voice quality account for up to 90% of English speech Podesva ( 2011 ) , and breathy and creaky voice have established perceived paralinguistic functions compared to modal voice, the standard phonation type. Rather than paralinguistic information, end-creak contains pragmatic information, indicating phrase or turn finality Lameris et\\xa0al. ( 2024 ) . To generate end-creak, the sentence‚Äôs first half was converted using modal voice parameters, followed by linear interpolation to the end-creak values. All conversions utilized acoustic parameter values from Table 1 , which were derived and selected based on prior work in voice quality (Lameris et\\xa0al., 2025 ; Lameris and Ward, 2025 ) . We measured the output of two acoustic parameters that aid in distinguishing creaky and breathy phonation types, H1‚ÄìH2 and H1‚ÄìA3, to ensure that the values were different for each of the voice qualities. The results of those measurements can be found in Figures 1 and 2 .\\nNote. Significance levels: *** p < .001 p<.001 , ** p < .01 p<.01 , * p < .05 p<.05 , p < .10 p<.10 ; = not significant; ‚Äì no effect. Arrows indicate direction: ‚Üë positive effect, ‚Üì negative effect. Gender effects are identical across both corpora.',\n",
       "    'subsections': [{'title': '2.1.\\u2003\\u200aVQ-Bench Creation',\n",
       "      'paragraphs': 'To create realistic prompts designed to measure the effect of voice quality, we create an expanded version of the open-ended long-form question prompts from Bokkahalli\\xa0Satish et\\xa0al. ( 2025a ) , which consists of four categories grounded in documented real-world applications of SpeechLLMs: Therapy, Career advice, Interview screening, and Storytelling Karvonen and Marks ( 2025 ); Lum et\\xa0al. ( 2024 ); Zao-Sanders . Examples can be found in Appendix A . These categories were originally selected to reflect authentic use cases where speaker identity could meaningfully influence model responses. We additionally introduce the effect of voice quality as a controlled variable.\\nFor each category, we developed five distinct prompts that maintain the core structure of eliciting voice-dependent responses while introducing variation in context and framing. This results in a total of 20 prompts per speaker.\\nWe used F5-TTS (Chen et\\xa0al., 2024 ) , a state-of-the-art zero-shot TTS system to synthesize the prompts. In order to create the target voices for the synthesis of the prompts, we extracted speech from the Buckeye corpus Pitt et\\xa0al. ( 2007 ) and the Centre for Speech Technology Voice Cloning Toolkit (VCTK) Yamagishi et\\xa0al. ( 2019 ) . The Buckeye corpus features conversational speech from interviews with 40 native English speakers from Central Ohio, balanced for gender (male or female) and age (under or over 40 years old) discussing local issues. VCTK features 109 speakers of different dialects of English reading out 400 phonetically balanced sentences. We extracted 12 second stretches of uninterrupted speech from both corpora to serve as the reference audio for F5-TTS, as this is the maximum input length for a target speaker .\\nVoiceQualityVC (Lameris et\\xa0al., 2025 ) was used to create modal, breathy, creaky, and end-creak versions of the original prompts. Modal, breathy, and creaky voice quality account for up to 90% of English speech Podesva ( 2011 ) , and breathy and creaky voice have established perceived paralinguistic functions compared to modal voice, the standard phonation type. Rather than paralinguistic information, end-creak contains pragmatic information, indicating phrase or turn finality Lameris et\\xa0al. ( 2024 ) . To generate end-creak, the sentence‚Äôs first half was converted using modal voice parameters, followed by linear interpolation to the end-creak values. All conversions utilized acoustic parameter values from Table 1 , which were derived and selected based on prior work in voice quality (Lameris et\\xa0al., 2025 ; Lameris and Ward, 2025 ) . We measured the output of two acoustic parameters that aid in distinguishing creaky and breathy phonation types, H1‚ÄìH2 and H1‚ÄìA3, to ensure that the values were different for each of the voice qualities. The results of those measurements can be found in Figures 1 and 2 .\\nNote. Significance levels: *** p < .001 p<.001 , ** p < .01 p<.01 , * p < .05 p<.05 , p < .10 p<.10 ; = not significant; ‚Äì no effect. Arrows indicate direction: ‚Üë positive effect, ‚Üì negative effect. Gender effects are identical across both corpora.',\n",
       "      'subsections': [{'title': '2.1.1.\\u2003\\u200aLong-form task creation',\n",
       "        'paragraphs': 'To create realistic prompts designed to measure the effect of voice quality, we create an expanded version of the open-ended long-form question prompts from Bokkahalli\\xa0Satish et\\xa0al. ( 2025a ) , which consists of four categories grounded in documented real-world applications of SpeechLLMs: Therapy, Career advice, Interview screening, and Storytelling Karvonen and Marks ( 2025 ); Lum et\\xa0al. ( 2024 ); Zao-Sanders . Examples can be found in Appendix A . These categories were originally selected to reflect authentic use cases where speaker identity could meaningfully influence model responses. We additionally introduce the effect of voice quality as a controlled variable.\\nFor each category, we developed five distinct prompts that maintain the core structure of eliciting voice-dependent responses while introducing variation in context and framing. This results in a total of 20 prompts per speaker.',\n",
       "        'subsections': []},\n",
       "       {'title': '2.1.2.\\u2003\\u200aTarget voice synthesis',\n",
       "        'paragraphs': 'We used F5-TTS (Chen et\\xa0al., 2024 ) , a state-of-the-art zero-shot TTS system to synthesize the prompts. In order to create the target voices for the synthesis of the prompts, we extracted speech from the Buckeye corpus Pitt et\\xa0al. ( 2007 ) and the Centre for Speech Technology Voice Cloning Toolkit (VCTK) Yamagishi et\\xa0al. ( 2019 ) . The Buckeye corpus features conversational speech from interviews with 40 native English speakers from Central Ohio, balanced for gender (male or female) and age (under or over 40 years old) discussing local issues. VCTK features 109 speakers of different dialects of English reading out 400 phonetically balanced sentences. We extracted 12 second stretches of uninterrupted speech from both corpora to serve as the reference audio for F5-TTS, as this is the maximum input length for a target speaker .',\n",
       "        'subsections': []},\n",
       "       {'title': '2.1.3.\\u2003\\u200aVoice quality conversion',\n",
       "        'paragraphs': 'VoiceQualityVC (Lameris et\\xa0al., 2025 ) was used to create modal, breathy, creaky, and end-creak versions of the original prompts. Modal, breathy, and creaky voice quality account for up to 90% of English speech Podesva ( 2011 ) , and breathy and creaky voice have established perceived paralinguistic functions compared to modal voice, the standard phonation type. Rather than paralinguistic information, end-creak contains pragmatic information, indicating phrase or turn finality Lameris et\\xa0al. ( 2024 ) . To generate end-creak, the sentence‚Äôs first half was converted using modal voice parameters, followed by linear interpolation to the end-creak values. All conversions utilized acoustic parameter values from Table 1 , which were derived and selected based on prior work in voice quality (Lameris et\\xa0al., 2025 ; Lameris and Ward, 2025 ) . We measured the output of two acoustic parameters that aid in distinguishing creaky and breathy phonation types, H1‚ÄìH2 and H1‚ÄìA3, to ensure that the values were different for each of the voice qualities. The results of those measurements can be found in Figures 1 and 2 .\\nNote. Significance levels: *** p < .001 p<.001 , ** p < .01 p<.01 , * p < .05 p<.05 , p < .10 p<.10 ; = not significant; ‚Äì no effect. Arrows indicate direction: ‚Üë positive effect, ‚Üì negative effect. Gender effects are identical across both corpora.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '3.\\u2003\\u200aExperiments',\n",
       "    'paragraphs': 'To evaluate models using VQ-Bench, we consider two tasks: (1) A long-form task in which two SFMs ( OpenAI speech-to-speech API 1 1 1 OpenAI speech-to-speech API and LFMAudio2-1.5B 2 2 2 LFMAudio2-1.5B ) were prompted to provide a response for the four categories in Section 2.1.1 . The SFMs were asked to provide constructive advice in a therapy task, suggest career options to a speaker based on their interests, decide whether a speaker should be promoted in the interview task and generate stories. The responses are then evaluated using an LLM judge ( gemini-2.5-flash-lite 3 3 3 Gemini-2.5-flash-lite API ), where we ask the judge to rate the responses on multiple evaluation dimensions (Table 2 ) on a scale of 1‚Äì5 following the procedure in Bokkahalli\\xa0Satish et\\xa0al. ( 2025a ) .\\n(2) Additionally, we use these prompts for a speech emotion recognition (SER) task using xlsr-en-speech-emotion-recognition 4 4 4 xlsr-speech-emotion-recognition , a Wav2Vec 2.0 model fine-tuned to predict eight emotions: angry, calm, disgust, fearful, happy, neutral, sad, and surprised. Given the link between voice quality and emotion, we test whether predicted emotion classes vary by phonation type, using only the Buckeye subset, because VCTK is designed to be neutral.',\n",
       "    'subsections': []},\n",
       "   {'title': '4.\\u2003\\u200aResults',\n",
       "    'paragraphs': 'As an initial validation, we assess whether the SFMs reliably detect speaker gender from audio. The OpenAI real-time speech-to-speech API proves unreliable, defaulting to classifying nearly all samples as male, and consequently shows no systematic response variation across voice qualities. All subsequent results therefore focus on the LFMAudio2 model, which demonstrates reliable gender detection.\\nThe gemini-2.5-flash-lite judge LLM ratings of LFMAudio2-1.5B responses were analyzed using a cumulative link mixed model (CLMM) for each dimension of the long-form generation task. The CLMM included voice quality, gender, and prompt as fixed effects, and speaker as a random intercept. We initially tested for interaction effects between voice quality and gender:\\nInteractions that were not significant based on an ANOVA were removed from the model. Likewise, fixed effects that were not significant were dropped. As an interaction effect was only found for the ‚ÄòHeroic agency‚Äô evaluation dimension for the Buckeye corpus, the interactions were not further investigated. The full results can be found in Table 2 . The effect of voice quality was significant for all evaluation dimensions except ‚ÄòRole status‚Äô and ‚ÄòEmotional validation‚Äô . The direction of the effect differed per task and per voice quality. For the career advice task, in the ‚ÄòSTEM vs. care‚Äô dimension, breathy and end-creak resulted in higher STEM-oriented ratings compared to modal voice, while a creaky voice resulted in more care-oriented ratings. While all voice qualities increased scores of ‚ÄòActionability‚Äô compared to modal voice. For the interview task, all voice qualities resulted in lower scores, except for creaky voice in the Shortlist decision dimension. In the storytelling task, the effect of voice quality is mixed, but consistent across corpora. For the therapy task, all non-modal voice qualities increased scores compared to modal voice if voice quality was found to be significant. Surprisingly, the effect of end-creak aligns more with breathy speech than with creaky voice quality. Female voices were systematically rated lower than male voices in the interview task across both corpora, specifically for ‚ÄòSalary offer‚Äô and ‚ÄòLeadership endorsement‚Äô .\\nThe SER results were analyzed using Bayesian multilevel categorical regression, predicting emotion labels from voice quality, gender, and prompt, with a random intercept per speaker. The model was estimated using four Markov chains, each with 2,000 iterations, including 1,000 warmup iterations. The R-hat values indicated good convergence. Results revealed meaningful effects for several voice quality categories compared to modal voice, as well as gender effects. Breathy voice increased the likelihood of calm and neutral predictions while decreasing the likelihood of fearful and surprised predictions. Creaky voice decreased the likelihood of fearful and happy predictions. End-creak decreased the likelihood of fearful predictions. Female voices positively influenced the prediction of fearful and surprised. All meaningful effects with non-overlapping 0, along with their 95% credible intervals, are in Table 4 .',\n",
       "    'subsections': []},\n",
       "   {'title': '5.\\u2003\\u200aDiscussion',\n",
       "    'paragraphs': 'The results suggest that changes in voice quality can significantly alter SFM responses in long-form response tasks. Additionally, voice quality significantly affects predictions in an SER task. There is a high degree of internal consistency in the results despite the fact that the voices were created from disparate corpora. Both the results for the long-form response and SER task often correspond to descriptions of voice quality in literature, such as associations of breathy voice with non-aggressive, friendly speech (Xu et\\xa0al., 2013 ) as well as prevalent gender bias regarding salary and leadership.\\nIf speech foundation models are not evaluated for bias in terms of how they interpret voice quality, they risk reproducing the same, at times gendered, asymmetries observed in human listeners. Our current analysis is limited to binary gender distinctions, reflecting the structure of the source corpora. However, the inclusion of gender-ambiguous and non-binary voices will be essential to assess whether similar or distinct biases emerge beyond the binary paradigm. Subtle features such as breathiness or creak may be mapped to social judgments of competence in ways that can negatively impact SFM-aided decision making in job interviews that could disproportionately disadvantage female speakers. Thus, the models may not only mirror human biases but also amplify them, as observed with other stereotypes in AI systems Schwartz et\\xa0al. ( 2022 ) .\\nRegarding the mapping between perceived paralinguistic meaning and voice quality, our preliminary evaluation of an SER model indicates that, as these models continue to improve, they could serve as a useful resource for providing empirical evidence supporting the development of hypotheses about the specific communicative functions of individual voice qualities.',\n",
       "    'subsections': []},\n",
       "   {'title': '6.\\u2003\\u200aConclusion',\n",
       "    'paragraphs': 'Our experiments show that controlled shifts in phonation type consistently influence model behaviour across open-ended and classification tasks. In long-form generation, these changes affected how models assigned agency, empathy, and leadership, with breathy and end-creak speech often eliciting more affiliative or care-oriented responses, while creaky voice produced more reserved or authority-linked judgments. Similar trends appeared in speech emotion recognition: breathy voice increased predictions of calm and neutral states, while creaky and end-creak voices reduced fearful classifications.\\nThese patterns mirror well-documented human perceptual biases, including gender-linked asymmetries in how vocal traits are interpreted. VQ-Bench provides a reproducible method for probing this dimension of model behaviour. Our results highlight that paralinguistic variation, particularly voice quality, can meaningfully alter SFM reasoning, evaluation, and emotional mapping. Accounting for this variability is necessary if speech models are to be used responsibly in applications such as hiring, therapy, or dialogue systems, where subtle vocal cues carry social meaning.',\n",
       "    'subsections': []},\n",
       "   {'title': '7.\\u2003\\u200aBibliographical References',\n",
       "    'paragraphs': '',\n",
       "    'subsections': []},\n",
       "   {'title': '', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': '8.\\u2003\\u200aLanguage Resource References',\n",
       "    'paragraphs': '',\n",
       "    'subsections': []},\n",
       "   {'title': '', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Appendix: Model Bias Evaluation Dimensions',\n",
       "    'paragraphs': 'In this section, we provide details about the prompts and evaluation procedures used in assessing Speech Foundation Models (SFMs) on long-form generation tasks.\\nThe speech prompt variations and their corresponding transcripts are presented in Figure 3 . Our evaluation framework uses multi-modal input capabilities of modern SFMs, which accept both audio speech prompts and accompanying text instructions. The two SFMs evaluated in this study differ in their output modalities. LFMAudio2-1.5B generates both textual and corresponding speech outputs, whereas the OpenAI speech-to-speech API produces only speech output, which requires an additional transcription step (with speech recognition) to use the judge LLM.\\nThe prompts in Table 3 are organized into four ecologically valid categories ‚Äì Therapy, Career Advice, Interview Screening, and Storytelling ‚Äì each with five distinct variations. These categories were selected to represent realistic deployment scenarios where voice quality and speaker characteristics might influence model behaviour. For the Therapy category, prompts describe emotional distress and burnout in varied ways, testing whether the model‚Äôs supportive responses are influenced by how the speaker sounds rather than solely what they say. The Career Advice prompts present speakers with different professional interests and decision-making scenarios, allowing us to assess whether voice quality biases the model toward particular career trajectories (e.g., STEM vs. care-oriented roles). Interview Screening prompts feature speakers expressing confidence and leadership capability, enabling measurement of whether phonation type affects hiring recommendations and salary offers. Finally, Storytelling prompts request personalized narratives, revealing how voice quality might shape the agency, heroism, and narrative arc assigned to characters modelled after the speaker.\\nTherapy\\nCareer Advice\\nInterview Screening\\nStory',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Appendix: Speech Emotion Recognition Results',\n",
       "    'paragraphs': 'For the speech emotion recognition task, we utilized only the Buckeye corpus subset of VQ-Bench due to the spontaneous and potentially more emotionally richer nature of conversational speech compared to the read speech in VCTK. The xlsr-en-speech-emotion-recognition model predicts eight emotion categories: angry, calm, disgust, fearful, happy, neutral, sad, and surprised.\\nWe recorded the full logit distributions rather than only the top prediction, enabling Bayesian multilevel analysis of how voice quality shifts the probability mass across all emotion categories. This approach reveals more nuanced effects than simple accuracy metrics‚Äîfor example, breathy voice may not change the top-predicted emotion but could substantially increase the probability assigned to \"calm\" while decreasing \"fearful,\" reflecting systematic shifts in the model‚Äôs affective interpretation that would be missed by classification accuracy alone.\\nThe full results of meaningful effects (those with 95% credible intervals not overlapping zero) are reported in Table 4 , showing how breathy, creaky, and end-creak voice qualities, as well as speaker gender, influence emotion predictions relative to modal voice quality in male speakers.\\n',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.\\n \\n \\n Keywords:\\u2009Speech Foundation Models, Voice Quality, Phonation Types, Paralinguistics'},\n",
       " '2510.25091v1': {'content': [{'title': '1. Introduction',\n",
       "    'paragraphs': 'Stock markets are fundamental to the global financial system, with accurate price prediction directly impacting capital allocation, portfolio optimization, and risk management. While the Efficient Market Hypothesis (Merello et\\xa0al., 2019 ) suggests that prices reflect all available information, making future movements theoretically unpredictable, research has identified systematic inefficiencies‚Äîinformation asymmetry, behavioral biases, and market microstructure effects‚Äîthat create potentially exploitable patterns for those capable of modeling and uncovering insights within complex market dynamics.\\nPredicting stock market movements remains an exceptionally challenging task due to a range of intertwined factors. Financial markets typically exhibit a low signal-to-noise ratio, where meaningful patterns are often obscured by random fluctuations. Their inherently non-stationary nature means that profitable patterns in one regime may fail as conditions change.\\nStock movements also involve complex interdependencies through sectoral correlations and momentum spillovers that evolve dynamically.\\nIn addition, prices react to influences operating across multiple timescales, while relevant information spans diverse forms, from structured numerical data to unstructured text.\\nGraph Neural Networks (GNNs) (Zhang et\\xa0al., 2020 ) have emerged as a powerful framework for stock market prediction by modeling inter-stock relationships through industry affiliations and correlation, enabling information propagation across stocks to capture sectoral influences and spillover effects. However, conventional graph models are inherently limited to pairwise relationships. In contrast, real-world markets often exhibit complex group-wise correlations. Stocks within the same sector tend to move synchronously during sectoral shifts, and related industries experience collective movements under supply chain disruptions. These limitations motivate the adoption of hypergraphs (Agarwal et\\xa0al., 2006 ; Feng et\\xa0al., 2019b ) , where hyperedges can connect multiple nodes simultaneously, naturally encoding group relationships. Hypergraph representations preserve higher-order market structures and facilitate efficient computation by directly modeling group interactions rather than degenerating them into oversimplified binary relations.\\nModels that rely exclusively on numerical data exhibit fundamental epistemic constraints, as they are unable to anticipate phenomena absent from historical data. Corporate disclosures, regulatory shifts, and geopolitical events typically manifest as textual information prior to influencing market prices. The advent of Large Language Models (LLMs) (Chen et\\xa0al., 2023b ) has introduced new opportunities for processing textual data at scale. Equipped with extensive pre-trained knowledge of economics and finance, LLMs can assimilate dynamic news flows, thereby addressing informational gaps that traditional time series models are unable to bridge (Xie et\\xa0al., 2023 ) .\\nRecent research has explored several strategies for integrating LLMs with quantitative models, including alignment method that maps time series into textual embeddings (Ding et\\xa0al., 2023 ) , and prompt-based approach (Li et\\xa0al., 2024a ) that textualizes numerical data. Despite these advances, substantial challenges persist in achieving seamless LLM integration. Existing methods generally treat structural and textual information in isolation, thereby foregoing potential synergies. Furthermore, the inherent mismatch between the discrete, token-based processing of LLMs and the continuous nature of time series remains only partially resolved, underscoring the necessity for sophisticated multimodal fusion frameworks.\\nAs models grow increasingly complex, computational efficiency becomes paramount. The Mixture of Experts (MoEs) framework (Jacobs, 1993 ; Fedus et\\xa0al., 2022 ; Riquelme et\\xa0al., 2021 ) addresses this challenge by dynamically routing inputs to specialized expert networks, activating only relevant model subsets. This selective activation allows different experts to specialize in particular market conditions or sectors, preserving model capacity while maintaining manageable inference costs for practical deployment.\\nHowever, integrating MoEs architectures with advanced financial modeling components remains nontrivial. Current approaches to combining hypergraph structures with transformer architectures often rely on simple feature fusion rather than joint reasoning mechanisms. Moreover, existing MoEs implementations typically fail to capture the hierarchical and multi-scale nature of market dynamics.\\nThese limitations highlight the need for novel architectures capable of unifying relational modeling, modality alignment, textual understanding, and computational efficiency while remaining feasible for deployment in real trading environments.\\nTo tackle these challenges, we propose a novel multi-modal architecture that synergistically integrates multi-context hypergraph modeling, LLM-enhanced semantic reasoning, and style‚Äìstructure expert specialization. Our contributions are threefold:\\nMulti-Context Multimodal Hypergraph : We introduce a hierarchical architecture consisting of a Local Context Hypergraph (LCH) that captures fine-grained spatiotemporal dynamics at the instance level, and a Global Context Hypergraph (GCH) that models persistent structural dependencies across stocks.\\nBoth components utilize shared hyperedges that jointly connect nodes from quantitative and textual modalities, enabling direct interaction between market signals and news narratives. Through hypergraph convolutions, these shared connections facilitate mutual representation enhancement and cross-modal feature learning, achieving deep, integrated multi-modal understanding beyond simple fusion.\\nLLM-Enhanced Reasoning : We incorporate a frozen Large Language Model (Llama-3.2-1B) to bridge the semantic gap between textual and numerical information. Leveraging its pre-trained financial knowledge, the LLM enriches multi-modal representations while preserving efficiency via parameter freezing and lightweight adapter layers.\\nStyle-Structured Mixture of Experts (SSMoEs) : We introduce a MoEs module with learnable style parameters that enables adaptive specialization across different market states and industry conditions via sparse activation. This design maintains computational efficiency while preserving high model capacity and complements the hypergraph for robust regime-aware representations.\\nExtensive experiments on the DJIA, NASDAQ 100, and S&P 100 indices demonstrate our method‚Äôs state-of-the-art performance, achieving the highest risk-adjusted returns with Sharpe ratios of 1.585, 2.100, and 1.351, and Calmar ratios of 3.377, 4.380, and 2.075, respectively, while maintaining the lowest maximum drawdowns (14.81%, 16.17%, and 14.27%).',\n",
       "    'subsections': []},\n",
       "   {'title': '2. Related Work',\n",
       "    'paragraphs': 'Early stock prediction methods primarily relied on statistical models (Wang and Leu, 1996 ; Zivot and Wang, 2006 ) , which assume linear dependencies and thus struggle to capture the complex dynamics of financial markets. Subsequent machine learning approaches (Ballings et\\xa0al., 2015 ) enhanced non-linear modeling capabilities but often treated stocks independently, overlooking inter-stock dependencies. This limitation motivated the adoption of graph-based models to represent the inherent relational structure among stocks.\\nRecognizing that stock movements are highly interconnected, researchers began employing Graph Neural Networks (GNNs) to model inter-stock relationships (Chen et\\xa0al., 2018 ) . Early studies constructed graphs using predefined relationships such as common shareholders, industry sectors, or supply chains. More advanced models, including RSR (Feng et\\xa0al., 2019a ) , which integrates LSTM with graph convolutions, HATS (Kim et\\xa0al., 2019 ) , which introduces multi-relational attention mechanisms, and FinGAT (Hsu et\\xa0al., 2021 ) , which applies dynamic attention to quantify stock interactions, have demonstrated improved performance. Recent approaches have shifted from static, predefined structures to dynamically learned relationships that capture latent dependencies between stocks (Sawhney et\\xa0al., 2021 ) .\\nBeyond pairwise relations, the recognition of group-wise interactions has spurred the development of hypergraph-based models. STHGCN (Sawhney et\\xa0al., 2020 ) jointly models the temporal evolution of stock prices and their industry-level associations, effectively capturing higher-order dependencies. More recently, CI-STHPAN (Xia et\\xa0al., 2024 ) introduced a pre-training framework on stock time series followed by fine-tuning for quantitative stock selection, leveraging self-supervised learning to extract robust spatio-temporal representations.\\nDeep learning has revolutionized stock market prediction through diverse neural architectures. Recurrent Neural Networks (RNNs) (Cho et\\xa0al., 2014 ) , particularly LSTM (Hochreiter and Schmidhuber, 1997 ; Moghar and Hamiche, 2020 ) and GRU (Gupta et\\xa0al., 2022 ) variants, have been widely utilized for their ability to capture sequential dependencies and temporal dynamics. The DA-RNN (Qin et\\xa0al., 2017 ) introduced LSTMs to adaptively extract relevant features, while the State Frequency Memory (SFM) network (Zhang et\\xa0al., 2017 ) decomposed hidden states into multiple frequency components to enhance representational diversity. Moreover, transformer-based architectures have achieved superior performance by effectively modeling complex temporal and cross-asset dependencies. Stockformer (Ma et\\xa0al., 2025 ) integrates wavelet decomposition with dual-frequency spatio-temporal encoders and a fusion attention mechanism to capture both high- and low-frequency financial dynamics.\\nRecently, Large Language Models (LLMs) have emerged as a new paradigm in financial modeling. BloombergGPT (Wu et\\xa0al., 2024 ) represents a seminal effort, comprising 50 billion parameters trained on a hybrid corpus of financial and general-domain texts. FinGPT (Yang et\\xa0al., 2023 ) and DISC-FinLLM (Chen et\\xa0al., 2023a ) introduced instruction fine-tuning and low-rank adaptation, to improve task-specific performance. LLMs also have been leveraged under various paradigms: news-driven approaches utilize sentiment and contextual analysis of market narratives, while reasoning-driven agents like FinMEM (Yu et\\xa0al., 2023 ) and FinAgent (Zhang et\\xa0al., 2024 ) incorporate multimodal data sources, such as earnings calls, regulatory filings, and social media, to augment predictive accuracy and interpretability.\\nTime Series Foundation Models (TSFMs) have further extended the foundation model paradigm to temporal data. Models such as TimesFM (Das et\\xa0al., 2024 ) , Lag-Llama (Rasul et\\xa0al., 2024 ) , and Chronos (Ansari et\\xa0al., 2024 ) exhibit strong generalization across diverse time series domains by pre-training on large-scale temporal corpora. Financially specialized models, including Kronos (Shi et\\xa0al., 2025a ) , address a key limitation of general-purpose TSFMs by focusing exclusively on financial datasets, thereby improving domain relevance. To mitigate computational costs and improve scalability, Mixture of Experts (MoE) architectures have emerged, with Time-MoE (Shi et\\xa0al., 2025b ) scales TSFMs to billions of parameters while achieving 20‚Äì24% performance gains over dense models under equivalent computational budgets.\\nThe integration of numerical and textual data in time series forecasting has progressed from rudimentary keyword-based approaches to advanced architectures that leverage the full representational capacity of large language models. Time-LLM (Jin et\\xa0al., 2024 ) reprograms time series into text-like representations compatible with LLM embedding spaces, facilitating multimodal interaction. Similarly, ChatTime (Wang et\\xa0al., 2024a ) conceptualizes time series as a foreign language, converting continuous numerical sequences into discrete tokens. TGForecaster (Xu et\\xa0al., 2025 ) employs PatchTST encoders (Nie et\\xa0al., 2023 ) for temporal data while incorporating pre-trained text models to process news, achieving efficient cross-modal fusion.\\nFor financial forecasting and stock prediction, MoE architectures have not yet been systematically explored, but they present substantial potential for advancing model adaptability and scalability. Given the inherently heterogeneous nature of financial markets characterized by regime shifts, sectoral dependencies, and varying volatility structures, MoE provides a natural framework for modular specialization. Each expert can be tailored to capture specific temporal patterns, market trends, industry behaviors and personalized sentiment, while sparse gating mechanisms ensure computational efficiency. Despite these advantages, the integration of MoE with multimodal architectures remains an unexplored frontier. Existing approaches typically treat structural representation, temporal modeling, and textual understanding in isolation. A promising research direction lies in developing unified frameworks that jointly incorporate hypergraph-informed structural priors, LLM-based semantic reasoning, and specialized MoE processing, balancing representational richness with efficiency.',\n",
       "    'subsections': [{'title': '2.1. Graph & Hypergraph for Stock Relations',\n",
       "      'paragraphs': 'Early stock prediction methods primarily relied on statistical models (Wang and Leu, 1996 ; Zivot and Wang, 2006 ) , which assume linear dependencies and thus struggle to capture the complex dynamics of financial markets. Subsequent machine learning approaches (Ballings et\\xa0al., 2015 ) enhanced non-linear modeling capabilities but often treated stocks independently, overlooking inter-stock dependencies. This limitation motivated the adoption of graph-based models to represent the inherent relational structure among stocks.\\nRecognizing that stock movements are highly interconnected, researchers began employing Graph Neural Networks (GNNs) to model inter-stock relationships (Chen et\\xa0al., 2018 ) . Early studies constructed graphs using predefined relationships such as common shareholders, industry sectors, or supply chains. More advanced models, including RSR (Feng et\\xa0al., 2019a ) , which integrates LSTM with graph convolutions, HATS (Kim et\\xa0al., 2019 ) , which introduces multi-relational attention mechanisms, and FinGAT (Hsu et\\xa0al., 2021 ) , which applies dynamic attention to quantify stock interactions, have demonstrated improved performance. Recent approaches have shifted from static, predefined structures to dynamically learned relationships that capture latent dependencies between stocks (Sawhney et\\xa0al., 2021 ) .\\nBeyond pairwise relations, the recognition of group-wise interactions has spurred the development of hypergraph-based models. STHGCN (Sawhney et\\xa0al., 2020 ) jointly models the temporal evolution of stock prices and their industry-level associations, effectively capturing higher-order dependencies. More recently, CI-STHPAN (Xia et\\xa0al., 2024 ) introduced a pre-training framework on stock time series followed by fine-tuning for quantitative stock selection, leveraging self-supervised learning to extract robust spatio-temporal representations.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2. LLM & Foundation Models in Finance',\n",
       "      'paragraphs': 'Deep learning has revolutionized stock market prediction through diverse neural architectures. Recurrent Neural Networks (RNNs) (Cho et\\xa0al., 2014 ) , particularly LSTM (Hochreiter and Schmidhuber, 1997 ; Moghar and Hamiche, 2020 ) and GRU (Gupta et\\xa0al., 2022 ) variants, have been widely utilized for their ability to capture sequential dependencies and temporal dynamics. The DA-RNN (Qin et\\xa0al., 2017 ) introduced LSTMs to adaptively extract relevant features, while the State Frequency Memory (SFM) network (Zhang et\\xa0al., 2017 ) decomposed hidden states into multiple frequency components to enhance representational diversity. Moreover, transformer-based architectures have achieved superior performance by effectively modeling complex temporal and cross-asset dependencies. Stockformer (Ma et\\xa0al., 2025 ) integrates wavelet decomposition with dual-frequency spatio-temporal encoders and a fusion attention mechanism to capture both high- and low-frequency financial dynamics.\\nRecently, Large Language Models (LLMs) have emerged as a new paradigm in financial modeling. BloombergGPT (Wu et\\xa0al., 2024 ) represents a seminal effort, comprising 50 billion parameters trained on a hybrid corpus of financial and general-domain texts. FinGPT (Yang et\\xa0al., 2023 ) and DISC-FinLLM (Chen et\\xa0al., 2023a ) introduced instruction fine-tuning and low-rank adaptation, to improve task-specific performance. LLMs also have been leveraged under various paradigms: news-driven approaches utilize sentiment and contextual analysis of market narratives, while reasoning-driven agents like FinMEM (Yu et\\xa0al., 2023 ) and FinAgent (Zhang et\\xa0al., 2024 ) incorporate multimodal data sources, such as earnings calls, regulatory filings, and social media, to augment predictive accuracy and interpretability.\\nTime Series Foundation Models (TSFMs) have further extended the foundation model paradigm to temporal data. Models such as TimesFM (Das et\\xa0al., 2024 ) , Lag-Llama (Rasul et\\xa0al., 2024 ) , and Chronos (Ansari et\\xa0al., 2024 ) exhibit strong generalization across diverse time series domains by pre-training on large-scale temporal corpora. Financially specialized models, including Kronos (Shi et\\xa0al., 2025a ) , address a key limitation of general-purpose TSFMs by focusing exclusively on financial datasets, thereby improving domain relevance. To mitigate computational costs and improve scalability, Mixture of Experts (MoE) architectures have emerged, with Time-MoE (Shi et\\xa0al., 2025b ) scales TSFMs to billions of parameters while achieving 20‚Äì24% performance gains over dense models under equivalent computational budgets.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3. Multimodal Financial Forecasting',\n",
       "      'paragraphs': 'The integration of numerical and textual data in time series forecasting has progressed from rudimentary keyword-based approaches to advanced architectures that leverage the full representational capacity of large language models. Time-LLM (Jin et\\xa0al., 2024 ) reprograms time series into text-like representations compatible with LLM embedding spaces, facilitating multimodal interaction. Similarly, ChatTime (Wang et\\xa0al., 2024a ) conceptualizes time series as a foreign language, converting continuous numerical sequences into discrete tokens. TGForecaster (Xu et\\xa0al., 2025 ) employs PatchTST encoders (Nie et\\xa0al., 2023 ) for temporal data while incorporating pre-trained text models to process news, achieving efficient cross-modal fusion.\\nFor financial forecasting and stock prediction, MoE architectures have not yet been systematically explored, but they present substantial potential for advancing model adaptability and scalability. Given the inherently heterogeneous nature of financial markets characterized by regime shifts, sectoral dependencies, and varying volatility structures, MoE provides a natural framework for modular specialization. Each expert can be tailored to capture specific temporal patterns, market trends, industry behaviors and personalized sentiment, while sparse gating mechanisms ensure computational efficiency. Despite these advantages, the integration of MoE with multimodal architectures remains an unexplored frontier. Existing approaches typically treat structural representation, temporal modeling, and textual understanding in isolation. A promising research direction lies in developing unified frameworks that jointly incorporate hypergraph-informed structural priors, LLM-based semantic reasoning, and specialized MoE processing, balancing representational richness with efficiency.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3. Methodology',\n",
       "    'paragraphs': 'We formulate the d d -day-ahead stock movement prediction as a binary classification problem, aiming to forecast whether the closing price of each constituent stock within the market index will rise after d d trading days.\\nTHe H3M-SSMoEs leverages three complementary modalities:\\n(1) Historical quantitative features, extracted over a T T -day lookback window;\\n(2) Daily financial news, encoded using the frozen Llama-3.2-1B large language model to capture semantic context;\\nand (3) Timestamp embedding, which encode explicit temporal information using the same frozen LLM.\\nDetailed definition is provided in Appendix A .\\nFigure 1 illustrates the overall architecture of the H3M-SSMoEs, which integrates three key components: Multi-Context Multimodal Hypergraph modeling, LLM-driven semantic enhancement with multimodal reasoning, and the Style-Structured Mixture of Experts (SSMoEs) to enable adaptive and context-aware stock movement prediction.\\nTo facilitate cross-modal learning, all heterogeneous input modalities are projected into a unified latent space of dimension D D . These projections enable the alignment of diverse data sources‚Äîquantitative indicators, textual news, and temporal information‚Äîwithin a common representational framework.\\nModality-Specific Projection. Each modality m m (quantitative, news, or timestamps) is transformed through a modality-specific feed-forward network that maps the original input space to the shared latent dimension:\\nwhere ùê± n , t ( m ) \\\\mathbf{x}_{n,t}^{(m)} denotes the input features of modality m m for stock n n at time t t . Specifically, ùê± n , t quant ‚àà ‚Ñù F \\\\mathbf{x}_{n,t}^{\\\\text{quant}}\\\\in\\\\mathbb{R}^{F} corresponds to F F financial indicators, ùê± n , t news ‚àà ‚Ñù D news \\\\mathbf{x}_{n,t}^{\\\\text{news}}\\\\in\\\\mathbb{R}^{D_{\\\\text{news}}} represents pre-computed LLM embeddings of news, and ùê± t time ‚àà ‚Ñù D time \\\\mathbf{x}_{t}^{\\\\text{time}}\\\\in\\\\mathbb{R}^{D_{\\\\text{time}}} encodes timestamps via LLM-processed date representations shared across all stocks.\\nTemporal Encoding Integration. To introduce temporal awareness into quantitative features and strengthen their alignment with daily news embeddings, we incorporate positional encodings as follows:\\nThis formulation explicitly injects temporal context into quantitative representations, enhancing their ability to capture time-dependent market dynamics. In contrast, news embeddings inherently encode temporal semantics through their linguistic content. The resulting representations form two parallel embedding streams, ùêô quant , ùêô news ‚àà ‚Ñù N √ó T √ó D \\\\mathbf{Z}^{\\\\text{quant}},\\\\mathbf{Z}^{\\\\text{news}}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D} , which serve as inputs for cross-modal alignment within subsequent hypergraph-based layers.\\nTraditional graph-based approaches are inherently constrained by predefined static pairwise relationships, which limits their ability to capture dynamic, collective market behaviors where entire sectors often move synchronously. To address this limitation, we introduce a multi-context multimodal hypergraph framework that hierarchically models both local and global interactions through higher-order relationships. Our architecture leverages shared hypergraph to unify intra-modal and cross-modal interactions, facilitating comprehensive information exchange between quantitative signals and semantic news semantics. This design effectively captures the bidirectional interplay between the two modalities‚Äîhow news drives market movements and how market fluctuations, in turn, generate new narratives. By modeling these intertwined dependencies, the proposed framework achieves synergistic multimodal integration and cross-modal alignment, surpassing conventional fusion approaches.\\nThe Local Context Hypergraph (LCH) is designed to model the intricate spatiotemporal dependencies inherent in financial markets, where individual stock movements are jointly influenced by immediate behavioral patterns and market narratives. Unlike conventional methods relying on fixed temporal windows or predefined, static correlations, the LCH flexibly discovers dynamic group relationships of stock‚Äìtime quantitative and textual instances that exhibit coordinated behaviors by representing each stock at each timestamp as a distinct node within a hypergraph. This formulation preserves fine-grained temporal resolution while leveraging hyperedges to model group-wise dependencies that evolve over time.\\nTo achieve unified processing across temporal and spatial dimensions, we flatten the stock and time dimensions to obtain ùêô f \\u200b l \\u200b a \\u200b t ( m ) ‚àà ‚Ñù N ‚ãÖ T √ó D \\\\mathbf{Z}^{(m)}_{flat}\\\\in\\\\mathbb{R}^{N\\\\cdot T\\\\times D} for each modality m ‚àà { quantitative , news } m\\\\in\\\\{\\\\text{quantitative},\\\\text{news}\\\\} , where each row represents a unique stock‚Äìtime instance. The core innovation lies in constructing multimodal sub-hypergraphs that capture distinct types of dependencies: temporal correlations within numerical indicators, semantic coherence across news narratives, and the bidirectional interplay between quantitative and textual modalities. This design explicitly acknowledges that market behaviors are governed by fundamentally heterogeneous forms of relationships and modalities.\\nFor each modality pair ( m i , m j ) (m_{i},m_{j}) (quantitative or news), a specialized sub-hypergraph is learned via adaptive projection:\\nwhere FFN l \\u200b o \\u200b c \\u200b a \\u200b l ( m i , m j ) \\\\text{FFN}_{local}^{(m_{i},m_{j})} is a modality-pair-specific network that learns to identify E 1 E_{1} latent hyperedges, each representing a set of stock‚Äìtime instances exhibiting coordinated behaviors. This formulation yields four distinct sub-hypergraphs corresponding to intra- and inter-modal relationships intertwined with critical market dynamics:\\nQuantitative‚ÄìQuantitative Dynamics : Captures temporal momentum and volatility clustering within numerical market indicators;\\nNews‚ÄìNews Coherence : Models semantic coherence and the propagation of narratives across the news landscape;\\nQuantitative‚ÄìNews Alignment : Aligns market reactions with contemporaneous news events. This cross-modal sub-hypergraph learns how current price patterns co-occur with specific news narratives, contextualizing stock movements within the textual information;\\nNews‚ÄìQuantitative Anticipation : Represents the inverse relationship‚Äîhow news content anticipates market movements. This component captures predictive cues embedded in texts that may not yet manifest in price dynamics.\\nThis symmetric design explicitly differentiates between the market‚Äôs reactive and anticipatory responses to news information.\\nAlthough these sub-hypergraphs encode complementary aspects of market behavior, their relative importance fluctuates with changing market regimes. Rather than treating all relationships uniformly, the LCH employs an adaptive multi-hypergraph fusion mechanism:\\nThis fusion network dynamically weights and integrates the intra- and cross-modal sub-hypergraphs in accordance with the prevailing market context. Subsequently, z-score normalization is applied to each element within the hyperedges, followed by a column-wise softmax operation to ensure that each hyperedge constitutes a probability distribution over nodes, resulting in the unified incidence matrix ùêá l \\u200b o \\u200b c \\u200b a \\u200b l \\\\mathbf{H}_{local} .\\nHowever, not all hyperedges contribute equally to modeling market structure. Some effectively encode distinctive and insightful dependencies, whereas others capture redundant or noisy patterns. To highlight the most informative structures, we introduce an information-theoretic hyperedge weighting scheme based on the Jensen‚ÄìShannon Divergence (JSD). For each pair of hyperedges i i and j j :\\nwhere ùê° i \\\\mathbf{h}_{i} denotes the node distribution of the i i -th hyperedge, ùê¶ i \\u200b j = 1 2 \\u200b ( ùê° i + ùê° j ) \\\\mathbf{m}_{ij}=\\\\frac{1}{2}(\\\\mathbf{h}_{i}+\\\\mathbf{h}_{j}) is their mean distribution, and KL is the Kullback‚ÄìLeibler Divergence. Hyperedges with higher average JSD scores capture more unique relational structures and thus receive larger weights in the diagonal matrix ùêñ ùüè ‚àà ‚Ñù E 1 √ó E 1 \\\\mathbf{W_{1}}\\\\in\\\\mathbb{R}^{E_{1}\\\\times E_{1}} . This adaptive weighting encourages the model to emphasize structurally informative hyperedges while suppressing redundant ones.\\nFinally, information is propagated through the weighted hypergraph structure to model higher-order interactions among stock‚Äìtime instances. For each modality, hypergraph convolution is performed using the shared local hypergraph ùêá l \\u200b o \\u200b c \\u200b a \\u200b l \\\\mathbf{H}^{local} :\\nwhere ùöØ l \\u200b o \\u200b c \\u200b a \\u200b l ( m ) ‚àà ‚Ñù D √ó D \\\\boldsymbol{\\\\Theta}^{(m)}_{local}\\\\in\\\\mathbb{R}^{D\\\\times D} is a learnable modality-specific transformation matrix, and œÉ \\\\sigma denotes a nonlinear activation. This operation facilitates high-order cross-temporal, and cross-modal interactions, enabling implicit alignment between modalities. Consequently, the model learns temporal lead‚Äìlag relationships and cross-stock spillover effects, uncovering latent connections between quantitative market dynamics and textual narratives.\\nThe resulting features ùêô ‚Ä≤ L \\u200b C \\u200b H ( m ) \\\\mathbf{Z^{\\\\prime}}^{(m)}_{LCH} are then reshaped to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} to yield ùêô L \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{LCH} , embedding sophisticated dependencies.\\nWhile the Local Context Hypergraph (LCH) captures fine-grained spatiotemporal patterns, the Global Context Hypergraph (GCH) models persistent structural relationships‚Äîsuch as sector affiliations, supply chain dependencies, and competitive dynamics‚Äîthat span the entire temporal horizon. This global perspective facilitates the identification of stable sectoral structures and long-term dependencies through higher-order group interactions.\\nThe GCH adopts an architecture analogous to that of the LCH but operates at the stock level rather than the instance level, thereby modeling persistent inter-stock relationships across time. The features are first flattened into stock-level representations of shape ‚Ñù N √ó ( T ‚ãÖ D ) \\\\mathbb{R}^{N\\\\times(T\\\\cdot D)} . Subsequently, multi-head self- and cross-attention mechanisms, sub-hypergraph construction, adaptive fusion, and the derivation of the global hypergraph incidence matrix ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} are performed, followed by the JSD-based weighting mechanism and hypergraph convolution to derive enhanced global representations for both modalities. These representations are then reshaped back to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} , yielding ùêô G \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{GCH} . Comprehensive formulations of these processes are provided in Appendix B .\\nThe GCH captures industry-wide trends, sectoral correlations, and market-wide sentiment flows, thereby complementing the micro-level temporal patterns learned by the LCH. Working in concert, these two hypergraphs yield a multi-context representation: the LCH effectively models short-term responses‚Äîsuch as how individual stocks react to technical indicators or news‚Äîwhile the GCH contextualizes these reactions within overarching market dynamics. Collectively, this hypergraph architecture forms a robust foundation for comprehensive market understanding, enabling rich multi-modal and multi-scale representations.\\nFollowing hypergraph processing, we incorporate a frozen Large Language Model (LLM) to enrich semantic representations and facilitate advanced multimodal reasoning across the aligned numerical and textual modalities. Specifically, the quantitative and news embeddings ùêô G \\u200b C \\u200b H quant , ùêô G \\u200b C \\u200b H news ‚àà ‚Ñù N √ó T √ó D \\\\mathbf{Z}^{\\\\text{quant}}_{GCH},\\\\mathbf{Z}^{\\\\text{news}}_{GCH}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D} produced by the GCH are further refined through an LLM-based reasoning layer.\\nWe employ the frozen Llama-3.2-1B model, chosen for its favorable trade-off between semantic reasoning capacity and computational efficiency. Freezing the LLM parameters preserves its extensive linguistic and financial domain-specific knowledge acquired during pre-training, while lightweight adapter layers are utilized to perform modality alignment and feature fusion without imposing significant training costs.\\nFirst, the quantitative and news features are concatenated along the feature dimension:\\nThe concatenated representations are then projected into the LLM input space via a multimodal fusion network:\\nwhere D LLM = 2048 D_{\\\\text{LLM}}=2048 denotes the hidden dimension of the Llama-3.2-1B model. The fused embeddings are subsequently processed by the frozen LLM to yield high-level semantic representations:\\nThis design enables the framework to exploit the LLM‚Äôs pre-trained understanding of finance, market structures, and contextual semantics while maintaining computational efficiency through parameter freezing. The LLM serves as a semantic reasoning engine that enhances multimodal feature representations with deep linguistic and financial knowledge, thereby augmenting the model‚Äôs capacity for nuanced and context-aware market prediction.\\nStock markets exhibit heterogeneous behaviors across multiple scales‚Äîfrom global sentiment shifts to sector-specific momentum. We propose a Style-Structured Mixture of Experts (SSMoEs) architecture comprising two complementary expert pools: Shared Market Experts, which model overarching market regimes, and Industry-specialized Experts, which capture sector-level dynamics. Leveraging sparse activation, only the most relevant experts are dynamically selected based on market context and industry characteristics, ensuring high model expressiveness with efficient computation.\\nA central innovation of this module lies in the parametric style space: each expert incorporates learnable style parameters that enable distinct predictive strategies.\\nDuring training, these parameters naturally differentiate across experts‚Äîyielding diverse strategic orientations such as bullish versus bearish or conservative versus aggressive. This diversity fosters a broad spectrum of adaptive trading perspectives among the experts.\\nIndividual stock dynamics are strongly conditioned by the broader market environment. Distinct market regimes, such as bullish, bearish, or high-volatility phases, exhibit systematic patterns that collectively shape asset behavior. The Shared Market Experts module is designed to infer the prevailing market regime and to adapt its specialization accordingly.\\nTo capture the global market state, we first flatten ùêô L \\u200b L \\u200b M ‚àà ‚Ñù N √ó T √ó D L \\u200b L \\u200b M \\\\mathbf{Z}_{LLM}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D_{LLM}} into ‚Ñù N √ó T ‚ãÖ D L \\u200b L \\u200b M \\\\mathbb{R}^{N\\\\times T\\\\cdot D_{LLM}} , yielding ùêô f \\u200b l \\u200b a \\u200b t \\\\mathbf{Z}_{flat} . This representation is then projected into a lower-dimensional space ‚Ñù N √ó D \\\\mathbb{R}^{N\\\\times D} to reduce dimensionality and computational overhead. Next, we derive the market state by aggregating information across all stocks:\\nwhere W r ‚àà ‚Ñù D √ó D m W_{r}\\\\in\\\\mathbb{R}^{D\\\\times D_{m}} projects features into a market-representative subspace, and W l ‚àà ‚Ñù 1 √ó N W_{l}\\\\in\\\\mathbb{R}^{1\\\\times N} performs cross-stock aggregation. These projections allow the model to learn both which features are most informative for market-state inference and how to optimally aggregate asset-level information.\\nThe resulting market state is then concatenated with individual stock features to construct an augmented representation:\\nensuring that each stock‚Äôs routing decision reflects both local characteristics and global market context.\\nEach Shared Market Expert j j is parameterized by a learnable style vector ùê¨ j m \\u200b k \\u200b t ‚àà ‚Ñù D s \\\\mathbf{s}_{j}^{mkt}\\\\in\\\\mathbb{R}^{D_{s}} , which defines its regime-specific specialization:\\nThrough training, these style vectors evolve into distinct market archetypes‚Äîsuch as bullish, bearish, or neutral‚Äîenabling the ensemble to capture a diverse and adaptive set of prediction and trading behaviors across varying market conditions.\\nBeyond market-level influences, sector-specific forces often drive synchronized movements within industries, shaped by shared fundamentals or supply-chain dependencies. The Industry-specialized Experts complements the market view by modeling these intra-sector dynamics through experts specialized in distinct industry behaviors.\\nThis module leverages the higher-order sectoral relationships ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} learned by the Global Context Hypergraph (GCH) to guide industry-aware routing:\\nThe transformation extracts industry embeddings that capture latent cross-sector dependencies and evolving sectoral relationships. Each stock‚Äôs representation is then augmented as:\\nwhere ùêà i \\\\mathbf{I}_{i} is the i i -th row of ùêà \\\\mathbf{I} , representing the industry-level embedding associated with stock i i . This design provides the routing mechanism with both asset-specific and industrial context. For instance, a semiconductor stock might be routed to experts specializing simultaneously in technology momentum, global supply-chain shifts, and cyclical manufacturing patterns.\\nEach Industry-specialized Expert k k also maintains a learnable style vector ùê¨ k i \\u200b n \\u200b d ‚àà ‚Ñù D s \\\\mathbf{s}_{k}^{ind}\\\\in\\\\mathbb{R}^{D_{s}} :\\nThese style vectors encourage differentiation among experts, promoting the emergence of specialized sectoral archetypes‚Äîsome focusing on defensive industries, others on growth-oriented technology or cyclical manufacturing sectors.\\nFinally, both expert modules employ sparse gating with top-K selection to aggregate outputs from the most relevant experts in their respective pools, yielding ùê° i m \\u200b k \\u200b t , ùê° i i \\u200b n \\u200b d \\\\mathbf{h}^{mkt}_{i},\\\\mathbf{h}^{ind}_{i} for each stock i i , respectively. Detailed formulations of the routing and weighted aggregation processes are provided in Appendix C .\\nThe SSMoEs module integrates complementary insights from both global market and industrial perspectives via an flexible aggregation mechanism. This layer coordinates expert selection and aggregation across both pools, producing final predictions that adapt to multi-scale market structures.\\nSpecifically, the final integration stage adaptively combines outputs from both expert pools through a learnable nonlinear fusion:\\nwhere W mkt , W ind ‚àà ‚Ñù d √ó d W_{\\\\text{mkt}},W_{\\\\text{ind}}\\\\in\\\\mathbb{R}^{d\\\\times d} are learnable weights controlling the relative influence of market and industry signals, and œÉ \\u200b ( ‚ãÖ ) \\\\sigma(\\\\cdot) denotes a nonlinear activation. By integrating broad and granular insights, the SSMoEs captures multi-scale dependencies, yielding representations that reflect each stock‚Äôs unique position within the evolving market ecosystem.\\nThe fused representation ùê≥ i \\\\mathbf{z}_{i} is finally passed through a FFN followed by softmax to generate the binary classification probabilities ùê≤ ^ i = [ y ^ i , 0 , y ^ i , 1 ] \\\\mathbf{\\\\hat{y}}_{i}=[\\\\hat{y}_{i,0},\\\\hat{y}_{i,1}] for each stock i i , indicating the likelihood of an upward price movement d d days ahead.\\nTo optimize the model, we employ a composite loss function that combines the classification objective with two auxiliary losses for balanced expert utilization. The classification component adopts the cross-entropy loss:\\nwhere y i ‚àà { 0 , 1 } y_{i}\\\\in\\\\{0,1\\\\} denotes the ground-truth label for the i i -th stock.\\nTo encourage balanced expert utilization within both expert pools, we incorporate sequence-wise auxiliary losses (Liu et\\xa0al., 2024a ) . For each module, the auxiliary loss is formulated as:\\nwhere f i f_{i} represents the fraction of stocks routed to expert i i , P i P_{i} denotes the average routing probability assigned to expert i i , and N e N_{e} is the total number of experts. This term promotes uniform expert utilization while preserving specialization among experts.\\nThe overall loss function combines all components as:\\nwhere Œ± \\\\alpha and Œ≤ \\\\beta are the balance factors.',\n",
       "    'subsections': [{'title': '3.1. Feature Embedding',\n",
       "      'paragraphs': 'To facilitate cross-modal learning, all heterogeneous input modalities are projected into a unified latent space of dimension D D . These projections enable the alignment of diverse data sources‚Äîquantitative indicators, textual news, and temporal information‚Äîwithin a common representational framework.\\nModality-Specific Projection. Each modality m m (quantitative, news, or timestamps) is transformed through a modality-specific feed-forward network that maps the original input space to the shared latent dimension:\\nwhere ùê± n , t ( m ) \\\\mathbf{x}_{n,t}^{(m)} denotes the input features of modality m m for stock n n at time t t . Specifically, ùê± n , t quant ‚àà ‚Ñù F \\\\mathbf{x}_{n,t}^{\\\\text{quant}}\\\\in\\\\mathbb{R}^{F} corresponds to F F financial indicators, ùê± n , t news ‚àà ‚Ñù D news \\\\mathbf{x}_{n,t}^{\\\\text{news}}\\\\in\\\\mathbb{R}^{D_{\\\\text{news}}} represents pre-computed LLM embeddings of news, and ùê± t time ‚àà ‚Ñù D time \\\\mathbf{x}_{t}^{\\\\text{time}}\\\\in\\\\mathbb{R}^{D_{\\\\text{time}}} encodes timestamps via LLM-processed date representations shared across all stocks.\\nTemporal Encoding Integration. To introduce temporal awareness into quantitative features and strengthen their alignment with daily news embeddings, we incorporate positional encodings as follows:\\nThis formulation explicitly injects temporal context into quantitative representations, enhancing their ability to capture time-dependent market dynamics. In contrast, news embeddings inherently encode temporal semantics through their linguistic content. The resulting representations form two parallel embedding streams, ùêô quant , ùêô news ‚àà ‚Ñù N √ó T √ó D \\\\mathbf{Z}^{\\\\text{quant}},\\\\mathbf{Z}^{\\\\text{news}}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D} , which serve as inputs for cross-modal alignment within subsequent hypergraph-based layers.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2. Multi-Context Multimodal Hypergraph',\n",
       "      'paragraphs': 'Traditional graph-based approaches are inherently constrained by predefined static pairwise relationships, which limits their ability to capture dynamic, collective market behaviors where entire sectors often move synchronously. To address this limitation, we introduce a multi-context multimodal hypergraph framework that hierarchically models both local and global interactions through higher-order relationships. Our architecture leverages shared hypergraph to unify intra-modal and cross-modal interactions, facilitating comprehensive information exchange between quantitative signals and semantic news semantics. This design effectively captures the bidirectional interplay between the two modalities‚Äîhow news drives market movements and how market fluctuations, in turn, generate new narratives. By modeling these intertwined dependencies, the proposed framework achieves synergistic multimodal integration and cross-modal alignment, surpassing conventional fusion approaches.\\nThe Local Context Hypergraph (LCH) is designed to model the intricate spatiotemporal dependencies inherent in financial markets, where individual stock movements are jointly influenced by immediate behavioral patterns and market narratives. Unlike conventional methods relying on fixed temporal windows or predefined, static correlations, the LCH flexibly discovers dynamic group relationships of stock‚Äìtime quantitative and textual instances that exhibit coordinated behaviors by representing each stock at each timestamp as a distinct node within a hypergraph. This formulation preserves fine-grained temporal resolution while leveraging hyperedges to model group-wise dependencies that evolve over time.\\nTo achieve unified processing across temporal and spatial dimensions, we flatten the stock and time dimensions to obtain ùêô f \\u200b l \\u200b a \\u200b t ( m ) ‚àà ‚Ñù N ‚ãÖ T √ó D \\\\mathbf{Z}^{(m)}_{flat}\\\\in\\\\mathbb{R}^{N\\\\cdot T\\\\times D} for each modality m ‚àà { quantitative , news } m\\\\in\\\\{\\\\text{quantitative},\\\\text{news}\\\\} , where each row represents a unique stock‚Äìtime instance. The core innovation lies in constructing multimodal sub-hypergraphs that capture distinct types of dependencies: temporal correlations within numerical indicators, semantic coherence across news narratives, and the bidirectional interplay between quantitative and textual modalities. This design explicitly acknowledges that market behaviors are governed by fundamentally heterogeneous forms of relationships and modalities.\\nFor each modality pair ( m i , m j ) (m_{i},m_{j}) (quantitative or news), a specialized sub-hypergraph is learned via adaptive projection:\\nwhere FFN l \\u200b o \\u200b c \\u200b a \\u200b l ( m i , m j ) \\\\text{FFN}_{local}^{(m_{i},m_{j})} is a modality-pair-specific network that learns to identify E 1 E_{1} latent hyperedges, each representing a set of stock‚Äìtime instances exhibiting coordinated behaviors. This formulation yields four distinct sub-hypergraphs corresponding to intra- and inter-modal relationships intertwined with critical market dynamics:\\nQuantitative‚ÄìQuantitative Dynamics : Captures temporal momentum and volatility clustering within numerical market indicators;\\nNews‚ÄìNews Coherence : Models semantic coherence and the propagation of narratives across the news landscape;\\nQuantitative‚ÄìNews Alignment : Aligns market reactions with contemporaneous news events. This cross-modal sub-hypergraph learns how current price patterns co-occur with specific news narratives, contextualizing stock movements within the textual information;\\nNews‚ÄìQuantitative Anticipation : Represents the inverse relationship‚Äîhow news content anticipates market movements. This component captures predictive cues embedded in texts that may not yet manifest in price dynamics.\\nThis symmetric design explicitly differentiates between the market‚Äôs reactive and anticipatory responses to news information.\\nAlthough these sub-hypergraphs encode complementary aspects of market behavior, their relative importance fluctuates with changing market regimes. Rather than treating all relationships uniformly, the LCH employs an adaptive multi-hypergraph fusion mechanism:\\nThis fusion network dynamically weights and integrates the intra- and cross-modal sub-hypergraphs in accordance with the prevailing market context. Subsequently, z-score normalization is applied to each element within the hyperedges, followed by a column-wise softmax operation to ensure that each hyperedge constitutes a probability distribution over nodes, resulting in the unified incidence matrix ùêá l \\u200b o \\u200b c \\u200b a \\u200b l \\\\mathbf{H}_{local} .\\nHowever, not all hyperedges contribute equally to modeling market structure. Some effectively encode distinctive and insightful dependencies, whereas others capture redundant or noisy patterns. To highlight the most informative structures, we introduce an information-theoretic hyperedge weighting scheme based on the Jensen‚ÄìShannon Divergence (JSD). For each pair of hyperedges i i and j j :\\nwhere ùê° i \\\\mathbf{h}_{i} denotes the node distribution of the i i -th hyperedge, ùê¶ i \\u200b j = 1 2 \\u200b ( ùê° i + ùê° j ) \\\\mathbf{m}_{ij}=\\\\frac{1}{2}(\\\\mathbf{h}_{i}+\\\\mathbf{h}_{j}) is their mean distribution, and KL is the Kullback‚ÄìLeibler Divergence. Hyperedges with higher average JSD scores capture more unique relational structures and thus receive larger weights in the diagonal matrix ùêñ ùüè ‚àà ‚Ñù E 1 √ó E 1 \\\\mathbf{W_{1}}\\\\in\\\\mathbb{R}^{E_{1}\\\\times E_{1}} . This adaptive weighting encourages the model to emphasize structurally informative hyperedges while suppressing redundant ones.\\nFinally, information is propagated through the weighted hypergraph structure to model higher-order interactions among stock‚Äìtime instances. For each modality, hypergraph convolution is performed using the shared local hypergraph ùêá l \\u200b o \\u200b c \\u200b a \\u200b l \\\\mathbf{H}^{local} :\\nwhere ùöØ l \\u200b o \\u200b c \\u200b a \\u200b l ( m ) ‚àà ‚Ñù D √ó D \\\\boldsymbol{\\\\Theta}^{(m)}_{local}\\\\in\\\\mathbb{R}^{D\\\\times D} is a learnable modality-specific transformation matrix, and œÉ \\\\sigma denotes a nonlinear activation. This operation facilitates high-order cross-temporal, and cross-modal interactions, enabling implicit alignment between modalities. Consequently, the model learns temporal lead‚Äìlag relationships and cross-stock spillover effects, uncovering latent connections between quantitative market dynamics and textual narratives.\\nThe resulting features ùêô ‚Ä≤ L \\u200b C \\u200b H ( m ) \\\\mathbf{Z^{\\\\prime}}^{(m)}_{LCH} are then reshaped to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} to yield ùêô L \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{LCH} , embedding sophisticated dependencies.\\nWhile the Local Context Hypergraph (LCH) captures fine-grained spatiotemporal patterns, the Global Context Hypergraph (GCH) models persistent structural relationships‚Äîsuch as sector affiliations, supply chain dependencies, and competitive dynamics‚Äîthat span the entire temporal horizon. This global perspective facilitates the identification of stable sectoral structures and long-term dependencies through higher-order group interactions.\\nThe GCH adopts an architecture analogous to that of the LCH but operates at the stock level rather than the instance level, thereby modeling persistent inter-stock relationships across time. The features are first flattened into stock-level representations of shape ‚Ñù N √ó ( T ‚ãÖ D ) \\\\mathbb{R}^{N\\\\times(T\\\\cdot D)} . Subsequently, multi-head self- and cross-attention mechanisms, sub-hypergraph construction, adaptive fusion, and the derivation of the global hypergraph incidence matrix ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} are performed, followed by the JSD-based weighting mechanism and hypergraph convolution to derive enhanced global representations for both modalities. These representations are then reshaped back to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} , yielding ùêô G \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{GCH} . Comprehensive formulations of these processes are provided in Appendix B .\\nThe GCH captures industry-wide trends, sectoral correlations, and market-wide sentiment flows, thereby complementing the micro-level temporal patterns learned by the LCH. Working in concert, these two hypergraphs yield a multi-context representation: the LCH effectively models short-term responses‚Äîsuch as how individual stocks react to technical indicators or news‚Äîwhile the GCH contextualizes these reactions within overarching market dynamics. Collectively, this hypergraph architecture forms a robust foundation for comprehensive market understanding, enabling rich multi-modal and multi-scale representations.',\n",
       "      'subsections': [{'title': '3.2.1. Local Context Hypergraph (LCH)',\n",
       "        'paragraphs': 'The Local Context Hypergraph (LCH) is designed to model the intricate spatiotemporal dependencies inherent in financial markets, where individual stock movements are jointly influenced by immediate behavioral patterns and market narratives. Unlike conventional methods relying on fixed temporal windows or predefined, static correlations, the LCH flexibly discovers dynamic group relationships of stock‚Äìtime quantitative and textual instances that exhibit coordinated behaviors by representing each stock at each timestamp as a distinct node within a hypergraph. This formulation preserves fine-grained temporal resolution while leveraging hyperedges to model group-wise dependencies that evolve over time.\\nTo achieve unified processing across temporal and spatial dimensions, we flatten the stock and time dimensions to obtain ùêô f \\u200b l \\u200b a \\u200b t ( m ) ‚àà ‚Ñù N ‚ãÖ T √ó D \\\\mathbf{Z}^{(m)}_{flat}\\\\in\\\\mathbb{R}^{N\\\\cdot T\\\\times D} for each modality m ‚àà { quantitative , news } m\\\\in\\\\{\\\\text{quantitative},\\\\text{news}\\\\} , where each row represents a unique stock‚Äìtime instance. The core innovation lies in constructing multimodal sub-hypergraphs that capture distinct types of dependencies: temporal correlations within numerical indicators, semantic coherence across news narratives, and the bidirectional interplay between quantitative and textual modalities. This design explicitly acknowledges that market behaviors are governed by fundamentally heterogeneous forms of relationships and modalities.\\nFor each modality pair ( m i , m j ) (m_{i},m_{j}) (quantitative or news), a specialized sub-hypergraph is learned via adaptive projection:\\nwhere FFN l \\u200b o \\u200b c \\u200b a \\u200b l ( m i , m j ) \\\\text{FFN}_{local}^{(m_{i},m_{j})} is a modality-pair-specific network that learns to identify E 1 E_{1} latent hyperedges, each representing a set of stock‚Äìtime instances exhibiting coordinated behaviors. This formulation yields four distinct sub-hypergraphs corresponding to intra- and inter-modal relationships intertwined with critical market dynamics:\\nQuantitative‚ÄìQuantitative Dynamics : Captures temporal momentum and volatility clustering within numerical market indicators;\\nNews‚ÄìNews Coherence : Models semantic coherence and the propagation of narratives across the news landscape;\\nQuantitative‚ÄìNews Alignment : Aligns market reactions with contemporaneous news events. This cross-modal sub-hypergraph learns how current price patterns co-occur with specific news narratives, contextualizing stock movements within the textual information;\\nNews‚ÄìQuantitative Anticipation : Represents the inverse relationship‚Äîhow news content anticipates market movements. This component captures predictive cues embedded in texts that may not yet manifest in price dynamics.\\nThis symmetric design explicitly differentiates between the market‚Äôs reactive and anticipatory responses to news information.\\nAlthough these sub-hypergraphs encode complementary aspects of market behavior, their relative importance fluctuates with changing market regimes. Rather than treating all relationships uniformly, the LCH employs an adaptive multi-hypergraph fusion mechanism:\\nThis fusion network dynamically weights and integrates the intra- and cross-modal sub-hypergraphs in accordance with the prevailing market context. Subsequently, z-score normalization is applied to each element within the hyperedges, followed by a column-wise softmax operation to ensure that each hyperedge constitutes a probability distribution over nodes, resulting in the unified incidence matrix ùêá l \\u200b o \\u200b c \\u200b a \\u200b l \\\\mathbf{H}_{local} .\\nHowever, not all hyperedges contribute equally to modeling market structure. Some effectively encode distinctive and insightful dependencies, whereas others capture redundant or noisy patterns. To highlight the most informative structures, we introduce an information-theoretic hyperedge weighting scheme based on the Jensen‚ÄìShannon Divergence (JSD). For each pair of hyperedges i i and j j :\\nwhere ùê° i \\\\mathbf{h}_{i} denotes the node distribution of the i i -th hyperedge, ùê¶ i \\u200b j = 1 2 \\u200b ( ùê° i + ùê° j ) \\\\mathbf{m}_{ij}=\\\\frac{1}{2}(\\\\mathbf{h}_{i}+\\\\mathbf{h}_{j}) is their mean distribution, and KL is the Kullback‚ÄìLeibler Divergence. Hyperedges with higher average JSD scores capture more unique relational structures and thus receive larger weights in the diagonal matrix ùêñ ùüè ‚àà ‚Ñù E 1 √ó E 1 \\\\mathbf{W_{1}}\\\\in\\\\mathbb{R}^{E_{1}\\\\times E_{1}} . This adaptive weighting encourages the model to emphasize structurally informative hyperedges while suppressing redundant ones.\\nFinally, information is propagated through the weighted hypergraph structure to model higher-order interactions among stock‚Äìtime instances. For each modality, hypergraph convolution is performed using the shared local hypergraph ùêá l \\u200b o \\u200b c \\u200b a \\u200b l \\\\mathbf{H}^{local} :\\nwhere ùöØ l \\u200b o \\u200b c \\u200b a \\u200b l ( m ) ‚àà ‚Ñù D √ó D \\\\boldsymbol{\\\\Theta}^{(m)}_{local}\\\\in\\\\mathbb{R}^{D\\\\times D} is a learnable modality-specific transformation matrix, and œÉ \\\\sigma denotes a nonlinear activation. This operation facilitates high-order cross-temporal, and cross-modal interactions, enabling implicit alignment between modalities. Consequently, the model learns temporal lead‚Äìlag relationships and cross-stock spillover effects, uncovering latent connections between quantitative market dynamics and textual narratives.\\nThe resulting features ùêô ‚Ä≤ L \\u200b C \\u200b H ( m ) \\\\mathbf{Z^{\\\\prime}}^{(m)}_{LCH} are then reshaped to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} to yield ùêô L \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{LCH} , embedding sophisticated dependencies.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.2.2. Global Context Hypergraph (GCH)',\n",
       "        'paragraphs': 'While the Local Context Hypergraph (LCH) captures fine-grained spatiotemporal patterns, the Global Context Hypergraph (GCH) models persistent structural relationships‚Äîsuch as sector affiliations, supply chain dependencies, and competitive dynamics‚Äîthat span the entire temporal horizon. This global perspective facilitates the identification of stable sectoral structures and long-term dependencies through higher-order group interactions.\\nThe GCH adopts an architecture analogous to that of the LCH but operates at the stock level rather than the instance level, thereby modeling persistent inter-stock relationships across time. The features are first flattened into stock-level representations of shape ‚Ñù N √ó ( T ‚ãÖ D ) \\\\mathbb{R}^{N\\\\times(T\\\\cdot D)} . Subsequently, multi-head self- and cross-attention mechanisms, sub-hypergraph construction, adaptive fusion, and the derivation of the global hypergraph incidence matrix ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} are performed, followed by the JSD-based weighting mechanism and hypergraph convolution to derive enhanced global representations for both modalities. These representations are then reshaped back to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} , yielding ùêô G \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{GCH} . Comprehensive formulations of these processes are provided in Appendix B .\\nThe GCH captures industry-wide trends, sectoral correlations, and market-wide sentiment flows, thereby complementing the micro-level temporal patterns learned by the LCH. Working in concert, these two hypergraphs yield a multi-context representation: the LCH effectively models short-term responses‚Äîsuch as how individual stocks react to technical indicators or news‚Äîwhile the GCH contextualizes these reactions within overarching market dynamics. Collectively, this hypergraph architecture forms a robust foundation for comprehensive market understanding, enabling rich multi-modal and multi-scale representations.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.3. LLM for Semantic Enhancement and Multimodal Reasoning',\n",
       "      'paragraphs': 'Following hypergraph processing, we incorporate a frozen Large Language Model (LLM) to enrich semantic representations and facilitate advanced multimodal reasoning across the aligned numerical and textual modalities. Specifically, the quantitative and news embeddings ùêô G \\u200b C \\u200b H quant , ùêô G \\u200b C \\u200b H news ‚àà ‚Ñù N √ó T √ó D \\\\mathbf{Z}^{\\\\text{quant}}_{GCH},\\\\mathbf{Z}^{\\\\text{news}}_{GCH}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D} produced by the GCH are further refined through an LLM-based reasoning layer.\\nWe employ the frozen Llama-3.2-1B model, chosen for its favorable trade-off between semantic reasoning capacity and computational efficiency. Freezing the LLM parameters preserves its extensive linguistic and financial domain-specific knowledge acquired during pre-training, while lightweight adapter layers are utilized to perform modality alignment and feature fusion without imposing significant training costs.\\nFirst, the quantitative and news features are concatenated along the feature dimension:\\nThe concatenated representations are then projected into the LLM input space via a multimodal fusion network:\\nwhere D LLM = 2048 D_{\\\\text{LLM}}=2048 denotes the hidden dimension of the Llama-3.2-1B model. The fused embeddings are subsequently processed by the frozen LLM to yield high-level semantic representations:\\nThis design enables the framework to exploit the LLM‚Äôs pre-trained understanding of finance, market structures, and contextual semantics while maintaining computational efficiency through parameter freezing. The LLM serves as a semantic reasoning engine that enhances multimodal feature representations with deep linguistic and financial knowledge, thereby augmenting the model‚Äôs capacity for nuanced and context-aware market prediction.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.4. Style-Structured Mixture of Experts (SSMoEs)',\n",
       "      'paragraphs': 'Stock markets exhibit heterogeneous behaviors across multiple scales‚Äîfrom global sentiment shifts to sector-specific momentum. We propose a Style-Structured Mixture of Experts (SSMoEs) architecture comprising two complementary expert pools: Shared Market Experts, which model overarching market regimes, and Industry-specialized Experts, which capture sector-level dynamics. Leveraging sparse activation, only the most relevant experts are dynamically selected based on market context and industry characteristics, ensuring high model expressiveness with efficient computation.\\nA central innovation of this module lies in the parametric style space: each expert incorporates learnable style parameters that enable distinct predictive strategies.\\nDuring training, these parameters naturally differentiate across experts‚Äîyielding diverse strategic orientations such as bullish versus bearish or conservative versus aggressive. This diversity fosters a broad spectrum of adaptive trading perspectives among the experts.\\nIndividual stock dynamics are strongly conditioned by the broader market environment. Distinct market regimes, such as bullish, bearish, or high-volatility phases, exhibit systematic patterns that collectively shape asset behavior. The Shared Market Experts module is designed to infer the prevailing market regime and to adapt its specialization accordingly.\\nTo capture the global market state, we first flatten ùêô L \\u200b L \\u200b M ‚àà ‚Ñù N √ó T √ó D L \\u200b L \\u200b M \\\\mathbf{Z}_{LLM}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D_{LLM}} into ‚Ñù N √ó T ‚ãÖ D L \\u200b L \\u200b M \\\\mathbb{R}^{N\\\\times T\\\\cdot D_{LLM}} , yielding ùêô f \\u200b l \\u200b a \\u200b t \\\\mathbf{Z}_{flat} . This representation is then projected into a lower-dimensional space ‚Ñù N √ó D \\\\mathbb{R}^{N\\\\times D} to reduce dimensionality and computational overhead. Next, we derive the market state by aggregating information across all stocks:\\nwhere W r ‚àà ‚Ñù D √ó D m W_{r}\\\\in\\\\mathbb{R}^{D\\\\times D_{m}} projects features into a market-representative subspace, and W l ‚àà ‚Ñù 1 √ó N W_{l}\\\\in\\\\mathbb{R}^{1\\\\times N} performs cross-stock aggregation. These projections allow the model to learn both which features are most informative for market-state inference and how to optimally aggregate asset-level information.\\nThe resulting market state is then concatenated with individual stock features to construct an augmented representation:\\nensuring that each stock‚Äôs routing decision reflects both local characteristics and global market context.\\nEach Shared Market Expert j j is parameterized by a learnable style vector ùê¨ j m \\u200b k \\u200b t ‚àà ‚Ñù D s \\\\mathbf{s}_{j}^{mkt}\\\\in\\\\mathbb{R}^{D_{s}} , which defines its regime-specific specialization:\\nThrough training, these style vectors evolve into distinct market archetypes‚Äîsuch as bullish, bearish, or neutral‚Äîenabling the ensemble to capture a diverse and adaptive set of prediction and trading behaviors across varying market conditions.\\nBeyond market-level influences, sector-specific forces often drive synchronized movements within industries, shaped by shared fundamentals or supply-chain dependencies. The Industry-specialized Experts complements the market view by modeling these intra-sector dynamics through experts specialized in distinct industry behaviors.\\nThis module leverages the higher-order sectoral relationships ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} learned by the Global Context Hypergraph (GCH) to guide industry-aware routing:\\nThe transformation extracts industry embeddings that capture latent cross-sector dependencies and evolving sectoral relationships. Each stock‚Äôs representation is then augmented as:\\nwhere ùêà i \\\\mathbf{I}_{i} is the i i -th row of ùêà \\\\mathbf{I} , representing the industry-level embedding associated with stock i i . This design provides the routing mechanism with both asset-specific and industrial context. For instance, a semiconductor stock might be routed to experts specializing simultaneously in technology momentum, global supply-chain shifts, and cyclical manufacturing patterns.\\nEach Industry-specialized Expert k k also maintains a learnable style vector ùê¨ k i \\u200b n \\u200b d ‚àà ‚Ñù D s \\\\mathbf{s}_{k}^{ind}\\\\in\\\\mathbb{R}^{D_{s}} :\\nThese style vectors encourage differentiation among experts, promoting the emergence of specialized sectoral archetypes‚Äîsome focusing on defensive industries, others on growth-oriented technology or cyclical manufacturing sectors.\\nFinally, both expert modules employ sparse gating with top-K selection to aggregate outputs from the most relevant experts in their respective pools, yielding ùê° i m \\u200b k \\u200b t , ùê° i i \\u200b n \\u200b d \\\\mathbf{h}^{mkt}_{i},\\\\mathbf{h}^{ind}_{i} for each stock i i , respectively. Detailed formulations of the routing and weighted aggregation processes are provided in Appendix C .\\nThe SSMoEs module integrates complementary insights from both global market and industrial perspectives via an flexible aggregation mechanism. This layer coordinates expert selection and aggregation across both pools, producing final predictions that adapt to multi-scale market structures.\\nSpecifically, the final integration stage adaptively combines outputs from both expert pools through a learnable nonlinear fusion:\\nwhere W mkt , W ind ‚àà ‚Ñù d √ó d W_{\\\\text{mkt}},W_{\\\\text{ind}}\\\\in\\\\mathbb{R}^{d\\\\times d} are learnable weights controlling the relative influence of market and industry signals, and œÉ \\u200b ( ‚ãÖ ) \\\\sigma(\\\\cdot) denotes a nonlinear activation. By integrating broad and granular insights, the SSMoEs captures multi-scale dependencies, yielding representations that reflect each stock‚Äôs unique position within the evolving market ecosystem.',\n",
       "      'subsections': [{'title': '3.4.1. Shared Market Experts',\n",
       "        'paragraphs': 'Individual stock dynamics are strongly conditioned by the broader market environment. Distinct market regimes, such as bullish, bearish, or high-volatility phases, exhibit systematic patterns that collectively shape asset behavior. The Shared Market Experts module is designed to infer the prevailing market regime and to adapt its specialization accordingly.\\nTo capture the global market state, we first flatten ùêô L \\u200b L \\u200b M ‚àà ‚Ñù N √ó T √ó D L \\u200b L \\u200b M \\\\mathbf{Z}_{LLM}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D_{LLM}} into ‚Ñù N √ó T ‚ãÖ D L \\u200b L \\u200b M \\\\mathbb{R}^{N\\\\times T\\\\cdot D_{LLM}} , yielding ùêô f \\u200b l \\u200b a \\u200b t \\\\mathbf{Z}_{flat} . This representation is then projected into a lower-dimensional space ‚Ñù N √ó D \\\\mathbb{R}^{N\\\\times D} to reduce dimensionality and computational overhead. Next, we derive the market state by aggregating information across all stocks:\\nwhere W r ‚àà ‚Ñù D √ó D m W_{r}\\\\in\\\\mathbb{R}^{D\\\\times D_{m}} projects features into a market-representative subspace, and W l ‚àà ‚Ñù 1 √ó N W_{l}\\\\in\\\\mathbb{R}^{1\\\\times N} performs cross-stock aggregation. These projections allow the model to learn both which features are most informative for market-state inference and how to optimally aggregate asset-level information.\\nThe resulting market state is then concatenated with individual stock features to construct an augmented representation:\\nensuring that each stock‚Äôs routing decision reflects both local characteristics and global market context.\\nEach Shared Market Expert j j is parameterized by a learnable style vector ùê¨ j m \\u200b k \\u200b t ‚àà ‚Ñù D s \\\\mathbf{s}_{j}^{mkt}\\\\in\\\\mathbb{R}^{D_{s}} , which defines its regime-specific specialization:\\nThrough training, these style vectors evolve into distinct market archetypes‚Äîsuch as bullish, bearish, or neutral‚Äîenabling the ensemble to capture a diverse and adaptive set of prediction and trading behaviors across varying market conditions.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.4.2. Industry-specialized Experts',\n",
       "        'paragraphs': 'Beyond market-level influences, sector-specific forces often drive synchronized movements within industries, shaped by shared fundamentals or supply-chain dependencies. The Industry-specialized Experts complements the market view by modeling these intra-sector dynamics through experts specialized in distinct industry behaviors.\\nThis module leverages the higher-order sectoral relationships ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} learned by the Global Context Hypergraph (GCH) to guide industry-aware routing:\\nThe transformation extracts industry embeddings that capture latent cross-sector dependencies and evolving sectoral relationships. Each stock‚Äôs representation is then augmented as:\\nwhere ùêà i \\\\mathbf{I}_{i} is the i i -th row of ùêà \\\\mathbf{I} , representing the industry-level embedding associated with stock i i . This design provides the routing mechanism with both asset-specific and industrial context. For instance, a semiconductor stock might be routed to experts specializing simultaneously in technology momentum, global supply-chain shifts, and cyclical manufacturing patterns.\\nEach Industry-specialized Expert k k also maintains a learnable style vector ùê¨ k i \\u200b n \\u200b d ‚àà ‚Ñù D s \\\\mathbf{s}_{k}^{ind}\\\\in\\\\mathbb{R}^{D_{s}} :\\nThese style vectors encourage differentiation among experts, promoting the emergence of specialized sectoral archetypes‚Äîsome focusing on defensive industries, others on growth-oriented technology or cyclical manufacturing sectors.\\nFinally, both expert modules employ sparse gating with top-K selection to aggregate outputs from the most relevant experts in their respective pools, yielding ùê° i m \\u200b k \\u200b t , ùê° i i \\u200b n \\u200b d \\\\mathbf{h}^{mkt}_{i},\\\\mathbf{h}^{ind}_{i} for each stock i i , respectively. Detailed formulations of the routing and weighted aggregation processes are provided in Appendix C .',\n",
       "        'subsections': []},\n",
       "       {'title': '3.4.3. Expert Pool Aggregation Layer',\n",
       "        'paragraphs': 'The SSMoEs module integrates complementary insights from both global market and industrial perspectives via an flexible aggregation mechanism. This layer coordinates expert selection and aggregation across both pools, producing final predictions that adapt to multi-scale market structures.\\nSpecifically, the final integration stage adaptively combines outputs from both expert pools through a learnable nonlinear fusion:\\nwhere W mkt , W ind ‚àà ‚Ñù d √ó d W_{\\\\text{mkt}},W_{\\\\text{ind}}\\\\in\\\\mathbb{R}^{d\\\\times d} are learnable weights controlling the relative influence of market and industry signals, and œÉ \\u200b ( ‚ãÖ ) \\\\sigma(\\\\cdot) denotes a nonlinear activation. By integrating broad and granular insights, the SSMoEs captures multi-scale dependencies, yielding representations that reflect each stock‚Äôs unique position within the evolving market ecosystem.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.5. Loss Function',\n",
       "      'paragraphs': 'The fused representation ùê≥ i \\\\mathbf{z}_{i} is finally passed through a FFN followed by softmax to generate the binary classification probabilities ùê≤ ^ i = [ y ^ i , 0 , y ^ i , 1 ] \\\\mathbf{\\\\hat{y}}_{i}=[\\\\hat{y}_{i,0},\\\\hat{y}_{i,1}] for each stock i i , indicating the likelihood of an upward price movement d d days ahead.\\nTo optimize the model, we employ a composite loss function that combines the classification objective with two auxiliary losses for balanced expert utilization. The classification component adopts the cross-entropy loss:\\nwhere y i ‚àà { 0 , 1 } y_{i}\\\\in\\\\{0,1\\\\} denotes the ground-truth label for the i i -th stock.\\nTo encourage balanced expert utilization within both expert pools, we incorporate sequence-wise auxiliary losses (Liu et\\xa0al., 2024a ) . For each module, the auxiliary loss is formulated as:\\nwhere f i f_{i} represents the fraction of stocks routed to expert i i , P i P_{i} denotes the average routing probability assigned to expert i i , and N e N_{e} is the total number of experts. This term promotes uniform expert utilization while preserving specialization among experts.\\nThe overall loss function combines all components as:\\nwhere Œ± \\\\alpha and Œ≤ \\\\beta are the balance factors.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4. Experiment',\n",
       "    'paragraphs': 'We obtained historical stock data from Yahoo Finance 1 1 1 https://ranaroussi.github.io/yfinance/ , comprising five basic attributes: close, high, low, open, and volume. To enrich the features, we utilized Qlib (Yang et\\xa0al., 2020 ) to compute the Alpha158 and Alpha360 technical indicators. After removing features containing missing values, these indicators were integrated with the basic attributes to construct an enriched dataset. To prevent information leakage and promote stable model training, z-score normalization was applied independently within each data split.\\nTo complement the quantitative features with textual information that captures market sentiment and analytical insights, we employed the Finrobot (Yang et\\xa0al., 2024 ) to generate daily news for each stock. The agent synthesizes multiple data sources, including recent financial news, company fundamentals, and market dynamics, to produce structured textual content. This approach ensures consistent and high-quality textual data across all stocks and time periods, effectively addressing the issue of incomplete or missing news coverage that often affects smaller firms or periods of low market activity. By integrating these rich textual features with quantitative data, we provide the model with comprehensive multimodal inputs that enhance its ability to predict stock movements accurately.\\nWe evaluated our method on three major stock indices: DJIA, NASDAQ 100, and S&P 100, using data from January 1, 2020 to August 31, 2025. The dataset was split 7:1:2 into training, validation (for hyperparameter tuning), and testing (for final evaluation). See Table 1 for detailed statistics.\\nWe evaluate H3M-SSMoEs against 15 baselines, spanning 4 categories:\\nStock Prediction Models (6): SFM (Zhang et\\xa0al., 2017 ) , Adv-ALSTM (Feng et\\xa0al., 2018 ) , DTML (Yoo et\\xa0al., 2021 ) , ESTIMATE (Huynh et\\xa0al., 2023 ) , StockMixer (Fan and Shen, 2024 ) , MASTER (Li et\\xa0al., 2024b ) ;\\nTime Series Models (3): DLinear (Zeng et\\xa0al., 2023 ) , iTransformer (Liu et\\xa0al., 2023 ) , TimeMixer (Wang et\\xa0al., 2024b ) ;\\nGraph Models (3): GCN (Kipf, 2016 ) , GraphSAGE (Hamilton et\\xa0al., 2017 ) , GAT (Veliƒçkoviƒá et\\xa0al., 2017 ) ;\\nTime Series LLM & Foundation Model (3): GPT4TS (Zhou et\\xa0al., 2023 ) ,\\naLLM4TS (Bian et\\xa0al., 2024 ) ,\\nTime-LLM (Jin et\\xa0al., 2024 ) .\\nDetailed experiment settings of our model are presented in Appendix D , and descriptions of all baselines are provided in Appendix E .\\nModel performance is evaluated using both portfolio backtesting and classification-based metrics to comprehensively assess investment returns and predictive accuracy.\\nBacktesting : Annual Return (AR), Sharpe Ratio (SR, applying a 2% risk-free rate), Calmar Ratio (CR), and Maximum Drawdown (MDD) are employed to evaluate the profitability and risk of the model within simulated investment scenarios.\\nPrediction : Accuracy (ACC) and Precision (PRE) are used to measure the quality of the model‚Äôs classification performance, where Precision denotes the proportion of stocks predicted and purchased as ‚Äúrising‚Äù that actually increased in closing price during the holding period.\\nDetailed definitions and formulations of evaluation metrics are provided in Appendix F .\\nWe evaluate each model using a dynamic d d -day trading strategy with adaptive portfolio construction and stop-loss mechanisms, initialized with a capital of 1 , 000 , 000 1,000,000 and assumes a transaction cost of 0.25 % 0.25\\\\% . Detailed descriptions of the backtesting methodology and hyperparameter configurations are provided in the Appendix G .\\nTable 2 presents the evaluation results on the DJIA. Our model achieves 57.47% accuracy and 62.01% precision (second-best after DTML‚Äôs 62.44%), demonstrating strong reliability in identifying upward price movements.\\nIn backtesting, H3M-SSMoEs achieves an outstanding annual return of 50.00%, which is 57.7% higher than the second-best model, MASTER (31.70%).\\nRisk-adjusted performance metrics further corroborate this superiority, with the highest Sharpe ratio (1.585), the best Calmar ratio (3.377, exceeding the second-best by 66.8%), and the lowest maximum drawdown (14.81%).\\nThese results highlight the efficacy of the H3M-SSMoEs in achieving high returns with controlled risk exposure.\\nTable 3 presents the evaluation results for the NASDAQ 100 dataset. H3M-SSMoEs achieves the highest accuracy (58.60%) and precision (69.97%), indicating superior predictive capability in this highly volatile, technology-driven index.\\nIn backtesting, our model delivers a strong annual return of 70.80%, second only to MASTER (71.75%), while demonstrating exceptional risk management. It achieves the best Sharpe ratio (2.100) and the highest Calmar ratio (4.380), significantly surpassing SFM (4.088) and MASTER (3.021). Furthermore, with the lowest maximum drawdown (16.17%), H3M-SSMoEs exhibits superior ability to generate substantial returns with controlled downside risk.\\nTable 4 presents the evaluation results on the S&P 100 dataset. Our model achieves the highest accuracy (56.91%) and precision (66.04%), outperforming the second-best TimeMixer (56.74%) and SFM (65.59%), respectively, demonstrating robust predictive performance on this diversified index.\\nIn backtesting, the H3M-SSMoEs delivers competitive returns of 29.62% (second only to aLLM4TS‚Äôs 30.62%) while excelling in risk management. We achieve the best Sharpe ratio (1.351), highest Calmar ratio (2.075, surpassing aLLM4TS‚Äôs 1.986 by 4.5%), and lowest maximum drawdown (14.27%). This combination of near-optimal returns with superior risk-adjusted metrics validates the robustness of our multimodal hypergraph architecture in handling the S&P 100‚Äôs diverse constituents.\\nFigure 2 visualizes our model‚Äôs trading performance with 10-day rebalancing periods. Each row corresponds to one index, with three panels illustrating complementary aspects of portfolio behavior. The left panels display portfolio value evolution over testing period, demonstrating consistent upward trajectories. The H3M-SSMoEs achieves the highest terminal value at approximately 1.75x initial capital on NASDAQ 100, followed by DJIA at 1.5x and S&P 100 at 1.3x. The center panels present daily returns distributions, revealing positively skewed profiles with mean daily returns. The concentration of returns near zero with extended positive tails indicates our model‚Äôs ability to capture upside opportunities while limiting downside exposure. The right panels depict drawdown dynamics over time, with maximum drawdowns constrained to around 15‚Äì17%, indicating a reasonable risk‚Äìreturn balance. The shaded regions reveal that all three portfolios experience moderate short-term losses, demonstrating the model‚Äôs resilience and rapid recovery capabilities. Overall, these visualizations corroborate the superior risk-adjusted performance metrics reported in Tables 2 , 3 and 4 , demonstrating that H3M-SSMoEs generalizes well across different market compositions and provides consistent positive returns.\\nIn conclusion, H3M-SSMoEs demonstrates consistent superiority across DJIA, NASDAQ 100, and S&P 100 indices, achieving the highest Sharpe ratios (1.585, 2.100, 1.351) and Calmar ratios (3.377, 4.380, 2.075) with the lowest maximum drawdowns (14.81%, 16.17%, 14.27%). Combined with competitive or best-in-class returns (50.00%, 70.80%, 29.62%) and high precision (62.01%, 69.97%, 66.04%), these results validate our architectural innovations: multi-context multimodal hypergraph for pattern capture, LLM integration for semantic enhancement, and style-structured MoEs for adaptive specialization. The synergistic combination of these components successfully addresses financial market challenges, achieving superior risk-adjusted returns.\\nTo assess the contribution of each architectural component, we conducted ablation studies by removing key modules from H3M-SSMoEs. Table 5 summarizes the comprehensive results across the three datasets.\\nThe results demonstrate that each component is indispensable, as its removal leads to substantial performance degradation across all metrics.\\nAmong the variants,\\nremoving the Local Context Hypergraph module ( w/o LCH ) yields the most severe deterioration, with annual returns plummeting from 50.00% to 16.47% on DJIA and from 70.80% to 7.40% on NASDAQ 100, alongside a reduction in the Calmar Ratio from 4.380 to 0.331 on NASDAQ 100.\\nEliminating the frozen LLM semantic reasoning layer ( w/o LLM ) produces a similarly adverse effect, reducing annual returns to 9.78% on NASDAQ 100 (versus 70.80% for the full model) and decreasing the Sharpe Ratio from 2.100 to 0.451. Likewise, replacing the SSMoEs with a standard feedforward network ( w/o SSMoEs ) causes notable degradation, with returns declining to 16.52% (DJIA) and 12.20% (NASDAQ 100), compared to 50.00% and 70.80% achieved by the complete model.',\n",
       "    'subsections': [{'title': '4.1. Features',\n",
       "      'paragraphs': 'We obtained historical stock data from Yahoo Finance 1 1 1 https://ranaroussi.github.io/yfinance/ , comprising five basic attributes: close, high, low, open, and volume. To enrich the features, we utilized Qlib (Yang et\\xa0al., 2020 ) to compute the Alpha158 and Alpha360 technical indicators. After removing features containing missing values, these indicators were integrated with the basic attributes to construct an enriched dataset. To prevent information leakage and promote stable model training, z-score normalization was applied independently within each data split.\\nTo complement the quantitative features with textual information that captures market sentiment and analytical insights, we employed the Finrobot (Yang et\\xa0al., 2024 ) to generate daily news for each stock. The agent synthesizes multiple data sources, including recent financial news, company fundamentals, and market dynamics, to produce structured textual content. This approach ensures consistent and high-quality textual data across all stocks and time periods, effectively addressing the issue of incomplete or missing news coverage that often affects smaller firms or periods of low market activity. By integrating these rich textual features with quantitative data, we provide the model with comprehensive multimodal inputs that enhance its ability to predict stock movements accurately.',\n",
       "      'subsections': [{'title': '4.1.1. Quantitative Data',\n",
       "        'paragraphs': 'We obtained historical stock data from Yahoo Finance 1 1 1 https://ranaroussi.github.io/yfinance/ , comprising five basic attributes: close, high, low, open, and volume. To enrich the features, we utilized Qlib (Yang et\\xa0al., 2020 ) to compute the Alpha158 and Alpha360 technical indicators. After removing features containing missing values, these indicators were integrated with the basic attributes to construct an enriched dataset. To prevent information leakage and promote stable model training, z-score normalization was applied independently within each data split.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.1.2. News Data',\n",
       "        'paragraphs': 'To complement the quantitative features with textual information that captures market sentiment and analytical insights, we employed the Finrobot (Yang et\\xa0al., 2024 ) to generate daily news for each stock. The agent synthesizes multiple data sources, including recent financial news, company fundamentals, and market dynamics, to produce structured textual content. This approach ensures consistent and high-quality textual data across all stocks and time periods, effectively addressing the issue of incomplete or missing news coverage that often affects smaller firms or periods of low market activity. By integrating these rich textual features with quantitative data, we provide the model with comprehensive multimodal inputs that enhance its ability to predict stock movements accurately.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.2. Datasets',\n",
       "      'paragraphs': 'We evaluated our method on three major stock indices: DJIA, NASDAQ 100, and S&P 100, using data from January 1, 2020 to August 31, 2025. The dataset was split 7:1:2 into training, validation (for hyperparameter tuning), and testing (for final evaluation). See Table 1 for detailed statistics.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3. Baselines',\n",
       "      'paragraphs': 'We evaluate H3M-SSMoEs against 15 baselines, spanning 4 categories:\\nStock Prediction Models (6): SFM (Zhang et\\xa0al., 2017 ) , Adv-ALSTM (Feng et\\xa0al., 2018 ) , DTML (Yoo et\\xa0al., 2021 ) , ESTIMATE (Huynh et\\xa0al., 2023 ) , StockMixer (Fan and Shen, 2024 ) , MASTER (Li et\\xa0al., 2024b ) ;\\nTime Series Models (3): DLinear (Zeng et\\xa0al., 2023 ) , iTransformer (Liu et\\xa0al., 2023 ) , TimeMixer (Wang et\\xa0al., 2024b ) ;\\nGraph Models (3): GCN (Kipf, 2016 ) , GraphSAGE (Hamilton et\\xa0al., 2017 ) , GAT (Veliƒçkoviƒá et\\xa0al., 2017 ) ;\\nTime Series LLM & Foundation Model (3): GPT4TS (Zhou et\\xa0al., 2023 ) ,\\naLLM4TS (Bian et\\xa0al., 2024 ) ,\\nTime-LLM (Jin et\\xa0al., 2024 ) .\\nDetailed experiment settings of our model are presented in Appendix D , and descriptions of all baselines are provided in Appendix E .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.4. Evaluation Metrics',\n",
       "      'paragraphs': 'Model performance is evaluated using both portfolio backtesting and classification-based metrics to comprehensively assess investment returns and predictive accuracy.\\nBacktesting : Annual Return (AR), Sharpe Ratio (SR, applying a 2% risk-free rate), Calmar Ratio (CR), and Maximum Drawdown (MDD) are employed to evaluate the profitability and risk of the model within simulated investment scenarios.\\nPrediction : Accuracy (ACC) and Precision (PRE) are used to measure the quality of the model‚Äôs classification performance, where Precision denotes the proportion of stocks predicted and purchased as ‚Äúrising‚Äù that actually increased in closing price during the holding period.\\nDetailed definitions and formulations of evaluation metrics are provided in Appendix F .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.5. Backtesting & Prediction Results',\n",
       "      'paragraphs': 'We evaluate each model using a dynamic d d -day trading strategy with adaptive portfolio construction and stop-loss mechanisms, initialized with a capital of 1 , 000 , 000 1,000,000 and assumes a transaction cost of 0.25 % 0.25\\\\% . Detailed descriptions of the backtesting methodology and hyperparameter configurations are provided in the Appendix G .\\nTable 2 presents the evaluation results on the DJIA. Our model achieves 57.47% accuracy and 62.01% precision (second-best after DTML‚Äôs 62.44%), demonstrating strong reliability in identifying upward price movements.\\nIn backtesting, H3M-SSMoEs achieves an outstanding annual return of 50.00%, which is 57.7% higher than the second-best model, MASTER (31.70%).\\nRisk-adjusted performance metrics further corroborate this superiority, with the highest Sharpe ratio (1.585), the best Calmar ratio (3.377, exceeding the second-best by 66.8%), and the lowest maximum drawdown (14.81%).\\nThese results highlight the efficacy of the H3M-SSMoEs in achieving high returns with controlled risk exposure.\\nTable 3 presents the evaluation results for the NASDAQ 100 dataset. H3M-SSMoEs achieves the highest accuracy (58.60%) and precision (69.97%), indicating superior predictive capability in this highly volatile, technology-driven index.\\nIn backtesting, our model delivers a strong annual return of 70.80%, second only to MASTER (71.75%), while demonstrating exceptional risk management. It achieves the best Sharpe ratio (2.100) and the highest Calmar ratio (4.380), significantly surpassing SFM (4.088) and MASTER (3.021). Furthermore, with the lowest maximum drawdown (16.17%), H3M-SSMoEs exhibits superior ability to generate substantial returns with controlled downside risk.\\nTable 4 presents the evaluation results on the S&P 100 dataset. Our model achieves the highest accuracy (56.91%) and precision (66.04%), outperforming the second-best TimeMixer (56.74%) and SFM (65.59%), respectively, demonstrating robust predictive performance on this diversified index.\\nIn backtesting, the H3M-SSMoEs delivers competitive returns of 29.62% (second only to aLLM4TS‚Äôs 30.62%) while excelling in risk management. We achieve the best Sharpe ratio (1.351), highest Calmar ratio (2.075, surpassing aLLM4TS‚Äôs 1.986 by 4.5%), and lowest maximum drawdown (14.27%). This combination of near-optimal returns with superior risk-adjusted metrics validates the robustness of our multimodal hypergraph architecture in handling the S&P 100‚Äôs diverse constituents.\\nFigure 2 visualizes our model‚Äôs trading performance with 10-day rebalancing periods. Each row corresponds to one index, with three panels illustrating complementary aspects of portfolio behavior. The left panels display portfolio value evolution over testing period, demonstrating consistent upward trajectories. The H3M-SSMoEs achieves the highest terminal value at approximately 1.75x initial capital on NASDAQ 100, followed by DJIA at 1.5x and S&P 100 at 1.3x. The center panels present daily returns distributions, revealing positively skewed profiles with mean daily returns. The concentration of returns near zero with extended positive tails indicates our model‚Äôs ability to capture upside opportunities while limiting downside exposure. The right panels depict drawdown dynamics over time, with maximum drawdowns constrained to around 15‚Äì17%, indicating a reasonable risk‚Äìreturn balance. The shaded regions reveal that all three portfolios experience moderate short-term losses, demonstrating the model‚Äôs resilience and rapid recovery capabilities. Overall, these visualizations corroborate the superior risk-adjusted performance metrics reported in Tables 2 , 3 and 4 , demonstrating that H3M-SSMoEs generalizes well across different market compositions and provides consistent positive returns.\\nIn conclusion, H3M-SSMoEs demonstrates consistent superiority across DJIA, NASDAQ 100, and S&P 100 indices, achieving the highest Sharpe ratios (1.585, 2.100, 1.351) and Calmar ratios (3.377, 4.380, 2.075) with the lowest maximum drawdowns (14.81%, 16.17%, 14.27%). Combined with competitive or best-in-class returns (50.00%, 70.80%, 29.62%) and high precision (62.01%, 69.97%, 66.04%), these results validate our architectural innovations: multi-context multimodal hypergraph for pattern capture, LLM integration for semantic enhancement, and style-structured MoEs for adaptive specialization. The synergistic combination of these components successfully addresses financial market challenges, achieving superior risk-adjusted returns.',\n",
       "      'subsections': [{'title': '4.5.1. Results for DJIA',\n",
       "        'paragraphs': 'Table 2 presents the evaluation results on the DJIA. Our model achieves 57.47% accuracy and 62.01% precision (second-best after DTML‚Äôs 62.44%), demonstrating strong reliability in identifying upward price movements.\\nIn backtesting, H3M-SSMoEs achieves an outstanding annual return of 50.00%, which is 57.7% higher than the second-best model, MASTER (31.70%).\\nRisk-adjusted performance metrics further corroborate this superiority, with the highest Sharpe ratio (1.585), the best Calmar ratio (3.377, exceeding the second-best by 66.8%), and the lowest maximum drawdown (14.81%).\\nThese results highlight the efficacy of the H3M-SSMoEs in achieving high returns with controlled risk exposure.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.5.2. Results for NASDAQ 100',\n",
       "        'paragraphs': 'Table 3 presents the evaluation results for the NASDAQ 100 dataset. H3M-SSMoEs achieves the highest accuracy (58.60%) and precision (69.97%), indicating superior predictive capability in this highly volatile, technology-driven index.\\nIn backtesting, our model delivers a strong annual return of 70.80%, second only to MASTER (71.75%), while demonstrating exceptional risk management. It achieves the best Sharpe ratio (2.100) and the highest Calmar ratio (4.380), significantly surpassing SFM (4.088) and MASTER (3.021). Furthermore, with the lowest maximum drawdown (16.17%), H3M-SSMoEs exhibits superior ability to generate substantial returns with controlled downside risk.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.5.3. Results for S&P 100',\n",
       "        'paragraphs': 'Table 4 presents the evaluation results on the S&P 100 dataset. Our model achieves the highest accuracy (56.91%) and precision (66.04%), outperforming the second-best TimeMixer (56.74%) and SFM (65.59%), respectively, demonstrating robust predictive performance on this diversified index.\\nIn backtesting, the H3M-SSMoEs delivers competitive returns of 29.62% (second only to aLLM4TS‚Äôs 30.62%) while excelling in risk management. We achieve the best Sharpe ratio (1.351), highest Calmar ratio (2.075, surpassing aLLM4TS‚Äôs 1.986 by 4.5%), and lowest maximum drawdown (14.27%). This combination of near-optimal returns with superior risk-adjusted metrics validates the robustness of our multimodal hypergraph architecture in handling the S&P 100‚Äôs diverse constituents.',\n",
       "        'subsections': []},\n",
       "       {'title': '4.5.4. Result Visualization',\n",
       "        'paragraphs': 'Figure 2 visualizes our model‚Äôs trading performance with 10-day rebalancing periods. Each row corresponds to one index, with three panels illustrating complementary aspects of portfolio behavior. The left panels display portfolio value evolution over testing period, demonstrating consistent upward trajectories. The H3M-SSMoEs achieves the highest terminal value at approximately 1.75x initial capital on NASDAQ 100, followed by DJIA at 1.5x and S&P 100 at 1.3x. The center panels present daily returns distributions, revealing positively skewed profiles with mean daily returns. The concentration of returns near zero with extended positive tails indicates our model‚Äôs ability to capture upside opportunities while limiting downside exposure. The right panels depict drawdown dynamics over time, with maximum drawdowns constrained to around 15‚Äì17%, indicating a reasonable risk‚Äìreturn balance. The shaded regions reveal that all three portfolios experience moderate short-term losses, demonstrating the model‚Äôs resilience and rapid recovery capabilities. Overall, these visualizations corroborate the superior risk-adjusted performance metrics reported in Tables 2 , 3 and 4 , demonstrating that H3M-SSMoEs generalizes well across different market compositions and provides consistent positive returns.\\nIn conclusion, H3M-SSMoEs demonstrates consistent superiority across DJIA, NASDAQ 100, and S&P 100 indices, achieving the highest Sharpe ratios (1.585, 2.100, 1.351) and Calmar ratios (3.377, 4.380, 2.075) with the lowest maximum drawdowns (14.81%, 16.17%, 14.27%). Combined with competitive or best-in-class returns (50.00%, 70.80%, 29.62%) and high precision (62.01%, 69.97%, 66.04%), these results validate our architectural innovations: multi-context multimodal hypergraph for pattern capture, LLM integration for semantic enhancement, and style-structured MoEs for adaptive specialization. The synergistic combination of these components successfully addresses financial market challenges, achieving superior risk-adjusted returns.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.6. Ablation Studies',\n",
       "      'paragraphs': 'To assess the contribution of each architectural component, we conducted ablation studies by removing key modules from H3M-SSMoEs. Table 5 summarizes the comprehensive results across the three datasets.\\nThe results demonstrate that each component is indispensable, as its removal leads to substantial performance degradation across all metrics.\\nAmong the variants,\\nremoving the Local Context Hypergraph module ( w/o LCH ) yields the most severe deterioration, with annual returns plummeting from 50.00% to 16.47% on DJIA and from 70.80% to 7.40% on NASDAQ 100, alongside a reduction in the Calmar Ratio from 4.380 to 0.331 on NASDAQ 100.\\nEliminating the frozen LLM semantic reasoning layer ( w/o LLM ) produces a similarly adverse effect, reducing annual returns to 9.78% on NASDAQ 100 (versus 70.80% for the full model) and decreasing the Sharpe Ratio from 2.100 to 0.451. Likewise, replacing the SSMoEs with a standard feedforward network ( w/o SSMoEs ) causes notable degradation, with returns declining to 16.52% (DJIA) and 12.20% (NASDAQ 100), compared to 50.00% and 70.80% achieved by the complete model.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5. Conclusion',\n",
       "    'paragraphs': 'We proposed the H3M-SSMoEs, a comprehensive multi-modal framework that synergistically integrates multi-context hypergraph modeling, LLM-enhanced semantic reasoning, and a Style-Structured Mixture of Experts (SSMoEs) for stock prediction. By unifying structural, semantic, and stylistic dimensions of market information across quantitative and textual modalities, H3M-SSMoEs effectively captures both fine-grained temporal dependencies and long-term inter-stock relationships while maintaining computational efficiency through lightweight LLM and sparse expert routing. Extensive experiments on DJIA, NASDAQ 100, and S&P 100 indices show consistent improvements in both predictive accuracy and risk-adjusted returns, achieving state-of-the-art Sharpe and Calmar ratios with significantly reduced drawdowns, validating the robustness and practical applicability of the architecture. Ablation studies further confirm the essential roles of each component. The results demonstrate that integrating hypergraph representations and alignment, LLM reasoning, and adaptive style-structured expert specialization provides a robust foundation for multimodal financial forecasting.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Problem Definition',\n",
       "    'paragraphs': 'We formulate the d d -day‚Äìahead stock movement prediction task as a binary classification problem. Let ùíÆ = { s 1 , ‚Ä¶ , s N } \\\\mathcal{S}=\\\\{s_{1},\\\\dots,s_{N}\\\\} denote a universe of N N stocks. For each stock s i s_{i} , we consider its historical quantitative feature over a lookback horizon of T T trading days, comprising F F financial indicators. These numerical features form a matrix ùêó i q \\u200b u \\u200b a \\u200b n \\u200b t ‚àà ‚Ñù T √ó F \\\\mathbf{X}_{i}^{quant}\\\\in\\\\mathbb{R}^{T\\\\times F} , and stacking all stock-level matrices yields ùêó q \\u200b u \\u200b a \\u200b n \\u200b t ‚àà ‚Ñù N √ó T √ó F \\\\mathbf{X}^{quant}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times F} . The ground truth is defined by comparing the closing price on day t t with that on day t + d t+d :\\nwhere p i ( t ) p_{i}^{(t)} denotes the closing price of stock s i s_{i} on day t t .\\nIn addition to quantitative modality, we incorporate two other complementary modalities at each time step:\\nDaily news (textual modality) ùêó news ‚àà ‚Ñù N √ó T √ó D news \\\\mathbf{X}^{\\\\text{news}}\\\\in\\\\mathbb{R}^{N\\\\times T\\\\times D_{\\\\text{news}}} : Each stock is paired with one news item per day, which is encoded using a frozen Llama-3.2-1B model (Dubey et\\xa0al., 2024 ) .\\nTimestamp embeddings (temporal modality for positional encoding) ùêó time ‚àà ‚Ñù T √ó D time \\\\mathbf{X}^{\\\\text{time}}\\\\in\\\\mathbb{R}^{T\\\\times D_{\\\\text{time}}} : Each date string is embedded using the same frozen LLM, and the resulting representations are broadcast-added to each stock‚Äôs quantitative features at the corresponding time step.\\nBoth news and timestamp embeddings are extracted from the LLM‚Äôs last hidden state corresponding to the end-of-sequence token ‚ü® EOS ‚ü© \\\\langle\\\\mathrm{EOS}\\\\rangle (Liu et\\xa0al., 2024b ) . These embeddings are pre-computed prior to model training to ensure computational efficiency while capturing rich semantic and temporal dependencies.\\nLet f \\u200b ( ‚ãÖ ; ùöØ ) f(\\\\cdot;\\\\boldsymbol{\\\\Theta}) denote the predictive function parameterized by ùöØ \\\\boldsymbol{\\\\Theta} . Given the triplet of input sequences ( ùêó q \\u200b u \\u200b a \\u200b n \\u200b t , ùêó news , ùêó time ) (\\\\mathbf{X}^{quant},\\\\mathbf{X}^{\\\\text{news}},\\\\mathbf{X}^{\\\\text{time}}) over the lookback window, the model estimates the probability of an upward price movement for each stock:',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Global Context Hypergraph (GCH)',\n",
       "    'paragraphs': 'Following Local Context Hypergraph (LCH) processing, we transform the features into stock-level representations:\\nwhere each row encapsulates the complete temporal evolution of an individual stock. This format facilitates the modeling of long-term, cross-stock relationships that span multiple temporal scales.\\nUnlike conventional approaches that depend on predefined or static relational structures, financial market interactions are inherently dynamic, continuously forming, evolving, and dissolving as market conditions change. To capture these evolving dependencies, we employ a multi-head attention mechanism that integrates both self- and cross-attention, enabling the model to adaptively learn intra- and inter-modal relationships:\\nThis mechanism yields four complementary attention matrices, each emphasizing distinct facets of global market dynamics. Analogous to the four sub-hypergraphs in the LCH, these matrices represent quantitative‚Äìquantitative interactions, news‚Äìnews coherence, and bidirectional cross-modal dependencies that connect market behavior with textual narratives.\\nFinancial markets exhibit pronounced collective dynamics, where stocks in the same industry often evolve coherently. To move beyond pairwise attention-based graphs and capture such higher-order interactions, we transform the dyadic attention weights into hypergraph representations:\\nwhere E 2 E_{2} denotes the number of global hyperedges, interpretable as latent industry factors. The projection networks FFN g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l ( m i , m j ) \\\\text{FFN}^{(m_{i},m_{j})}_{global} serve as factorization modules, decomposing dense attention matrices into group membership structures. Each sub-hypergraph is standardized via z-score normalization followed by softmax to ensure valid probabilistic incidence matrices, yielding ùêá ~ g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l ( m i , m j ) \\\\tilde{\\\\mathbf{H}}^{(m_{i},m_{j})}_{global} .\\nThe normalized sub-hypergraphs are subsequently integrated through an adaptive fusion network that synthesizes a unified representation:\\nThis fusion module learns non-linear combinations that amplify synergistic relationships while suppressing redundancy. The fused structure is again standardized via z-score normalization and column-wise softmax to produce the final global incidence matrix:\\nWe also employ JSD-based adaptive weighting to construct a diagonal weight matrix ùêñ ùüê ‚àà ‚Ñù E 2 √ó E 2 \\\\mathbf{W_{2}}\\\\in\\\\mathbb{R}^{E_{2}\\\\times E_{2}} , which assigns adaptive weights to the hyperedges. Global hypergraph convolutions are then applied to both modalities using the shared incidence matrix ùêá g \\u200b l \\u200b o \\u200b b \\u200b a \\u200b l \\\\mathbf{H}_{global} , enabling consistent propagation of information across all stocks:\\nwhere m ‚àà { quantitative , news } m\\\\in\\\\{\\\\text{quantitative},\\\\text{news}\\\\} . Unlike the hypergraph convolution in the LCH operating at individual time steps, the GCH convolutions aggregate information across the entire temporal span, thereby capturing persistent patterns in industry behaviors. The resulting features ùêô ‚Ä≤ G \\u200b C \\u200b H ( m ) \\\\mathbf{Z^{\\\\prime}}^{(m)}_{GCH} are reshaped to ‚Ñù N √ó T √ó D \\\\mathbb{R}^{N\\\\times T\\\\times D} , yielding ùêô G \\u200b C \\u200b H ( m ) \\\\mathbf{Z}^{(m)}_{GCH} , encapsulates rich, temporally invariant global contextual information.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Expert Routing and Aggregation',\n",
       "    'paragraphs': 'For the Shared Market Experts, the routing mechanism determines which experts to activate based on the augmented market representation ùê≥ i m \\u200b k \\u200b t \\\\mathbf{z}^{mkt}_{i} . The procedure begins by computing routing logits for all market experts:\\nwhere ùêñ r \\u200b o \\u200b u \\u200b t \\u200b e ‚àà ‚Ñù ( D + d m ) √ó N m \\u200b k \\u200b t \\\\mathbf{W}_{route}\\\\in\\\\mathbb{R}^{(D+d_{m})\\\\times N_{mkt}} denotes the routing matrix that learns to project the concatenated stock feature and market state onto expert relevance scores. To balance computational efficiency with model expressiveness, we employ sparse activation by selecting only the top K m K_{m} experts:\\nThe sparse gating mechanism subsequently constructs masks, where only the selected experts retain their corresponding activation values while the remainder are masked out:\\nThese sparse logits are normalized via a softmax to yield the gating weights:\\nThe aggregated market-level output is then computed as a weighted combination of the selected experts‚Äô predictions:\\nwhere Top-K \\u200b ( i ) \\\\text{Top-K}(i) represents the subset of K K experts activated for stock i i . This adaptive routing design enables the model to dynamically adjust to evolving market conditions by selectively engaging experts most relevant to the prevailing regime.\\nIn parallel, the industry-specialized experts undergo an analogous routing process, utilizing the industry-augmented representation ùê≥ i ind \\\\mathbf{z}^{\\\\text{ind}}_{i} . The routing logits are computed as:\\nwhere ùêñ r \\u200b o \\u200b u \\u200b t \\u200b e ind ‚àà ‚Ñù ( D + E 2 ) √ó N ind \\\\mathbf{W}^{\\\\text{ind}}_{route}\\\\in\\\\mathbb{R}^{(D+E_{2})\\\\times N_{\\\\text{ind}}} denotes the industry routing matrix that maps stock features, augmented with learned sectoral embeddings, to industry expert relevance scores. Following the same sparse selection and gating procedure, the aggregated output from industry experts is expressed as:',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Experiment Settings',\n",
       "    'paragraphs': 'Our model was implemented in PyTorch and optimized using the cross-entropy loss. Training was conducted for 40 epochs with the AdamW optimizer (learning rate = 1 √ó 10 ‚àí 4 =1\\\\times 10^{-4} , weight decay = 0.05 =0.05 ). We applied linear warmup for the first 10% of training steps, followed by a linear decay schedule.\\nThe main hyperparameters were configured as follows:\\nfeature embedding dimension D = 256 D=256 ,\\ndropout rate = 0.1,\\nattention heads = 2 (for GCH),\\nmarket state dimension = 16,\\nexpert style dimension = 16,\\ntop- K = 2 K=2 (for both expert pools in SSMoEs),\\nauxiliary loss balance factors Œ± , Œ≤ = 0.1 \\\\alpha,\\\\beta=0.1 .\\nFor the LLM backbone, we adopted a frozen Llama-3.2-1B model with a hidden dimension of 2048.\\nThe lookback window was set to T = 20 T=20 trading days, with a prediction horizon of d = 10 d=10 days. We tuned several structural hyperparameters through grid search to maximize validation accuracy, including:\\nnumber of hyperedges for both LCH and GCH, E 1 , E 2 ‚àà { 32 , 64 , 128 } E_{1},E_{2}\\\\in\\\\{32,64,128\\\\} ,\\nnumber of Shared Market Experts N m \\u200b k \\u200b t ‚àà { 3 , 4 , 5 } N_{mkt}\\\\in\\\\{3,4,5\\\\} ,\\nand number of Industry-specialized Experts N i \\u200b n \\u200b d ‚àà { 6 , 8 , 10 } N_{ind}\\\\in\\\\{6,8,10\\\\} , and the final settings are reported in Table 6 .',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix E Baseline Descriptions',\n",
       "    'paragraphs': 'To evaluate the effectiveness of the H3M-SSMoEs, we compare it against 15 baselines with several state-of-the-art baselines from 4 different categories. These models provide a diverse set of benchmarks to evaluate our method‚Äôs performance.\\n1. Stock Prediction Models (6):\\nSFM (Zhang et\\xa0al., 2017 ) : State Frequency Memory networks that model price fluctuations across multiple frequencies using frequency-based decomposition.\\nAdv-ALSTM (Feng et\\xa0al., 2018 ) : Attentive LSTM with adversarial training for improved robustness against stochastic price movements.\\nDTML (Yoo et\\xa0al., 2021 ) : Transformer architecture capturing dynamic inter-stock correlations through multi-level contexts.\\nESTIMATE (Huynh et\\xa0al., 2023 ) : Combines wavelet-based hypergraph convolution with memory-enhanced LSTM for non-pairwise stock correlations.\\nStockMixer (Fan and Shen, 2024 ) : MLP-based model that sequentially mixes indicators, temporal patterns, and market correlations.\\nMASTER (Li et\\xa0al., 2024b ) : Integrates intra/inter-stock attention with market-guided gating for dynamic correlation capture.\\n2. Time Series Models (3):\\nDLinear (Zeng et\\xa0al., 2023 ) : One-layer linear model that directly models temporal relations for long-term forecasting.\\niTransformer (Liu et\\xa0al., 2023 ) : Inverted Transformer applying attention across variates rather than time steps.\\nTimeMixer (Wang et\\xa0al., 2024b ) : MLP-based model using multiscale mixing to disentangle temporal variations.\\n3. Graph Models (3):\\nGCN (Graph Convolutional Network) (Kipf, 2016 ) : Uses first-order spectral graph convolutions for efficient node embedding learning.\\nGraphSAGE (Hamilton et\\xa0al., 2017 ) : Inductive framework generating embeddings via neighborhood sampling and aggregation.\\nGAT (Graph Attention Network) (Veliƒçkoviƒá et\\xa0al., 2017 ) : Employs masked self-attention to assign weights to neighbors for flexible node embedding.\\n4. Time Series LLM & Foundation Model (3):\\nGPT4TS (Zhou et\\xa0al., 2023 ) : Builds on a frozen GPT-2, fine-tuning only input embeddings, normalization, and output layers, using instance normalization and patching to construct a cross-modality framework for time-series representation.\\naLLM4TS (Bian et\\xa0al., 2024 ) : Employs a two-stage architecture (causal next-patch pre-training and multi-patch fine-tuning) with a patch-wise decoder to enable localized temporal modeling and representation learning within LLMs.\\nTime-LLM (Jin et\\xa0al., 2024 ) : Features a three-part framework consisting of input reprogramming, a frozen LLM backbone, and output projection, where time-series patches are mapped into text prototype embeddings and guided by Prompt-as-Prefix (PaP) prompts for modality alignment.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix F Metric Definitions',\n",
       "    'paragraphs': 'where:\\nTP (True Positives): Correctly predicted positive cases;\\nTN (True Negatives): Correctly predicted negative cases;\\nFP (False Positives): Incorrectly predicted as positive;\\nFN (False Negatives): Incorrectly predicted as negative.\\nwhere r t r_{t} = return for day t, T = number of trading days, 252 = typical number of trading days per year.\\nwhere R p R_{p} = annualized portfolio return, R f R_{f} = 0.02 (2% risk-free rate), œÉ p \\\\sigma_{p} = annualized standard deviation = œÉ d \\u200b a \\u200b i \\u200b l \\u200b y √ó 252 \\\\sigma_{daily}\\\\times\\\\sqrt{252} .\\nwhere P t P_{t} = portfolio value at day t.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix G Backtesting Setting',\n",
       "    'paragraphs': 'This section presents a comprehensive description of the dynamic d d -day stock trading strategy and the corresponding hyperparameter configurations set for backtesting.\\nEach backtest begins with an initial capital of 1,000,000, incorporating a transaction cost rate of 0.25% per trade to reflect realistic market frictions. The trading universe comprises all constituent stocks within each index. The core design follows a dynamic d d -day trading cycle with adaptive portfolio construction and a stop-loss mechanism, where d d denotes both the prediction horizon and the rebalancing frequency. The pseudocode for the Dynamic d d -Day Trading Strategy is presented in Algorithm 1 . All transaction adjustments incorporate transaction costs.\\nPrediction Generation : The model outputs the probability of price rise d d days ahead for all stocks, which we use to rank them.\\nPortfolio Construction with Stop-Loss Mechanism : We define a portfolio selection proportion p p (where 0 < p ‚â§ 1 0<p\\\\leq 1 ). On each rebalancing day:\\nIf the number of stocks predicted to rise (with probability > > 0.5) d d days ahead is at least p √ó N p\\\\times N , we purchase the top p √ó N p\\\\times N stocks;\\nIf the number of rising predictions falls into an intermediate zone, specifically p √ó N √ó q ‚â§ M < p √ó N p\\\\times N\\\\times q\\\\leq M<p\\\\times N (where q q is a stop-loss threshold hyperparameter with 0 < q < 1 0<q<1 ), then we adopt a conservative approach: only buy the top r √ó M r\\\\times M predicted rising stocks (with 0 ‚â§ r ‚â§ 1 0\\\\leq r\\\\leq 1 );\\nIf the number of rising predictions is below p √ó N √ó q p\\\\times N\\\\times q , we do not buy new positions and liquidate all current holdings to avoid downside exposure.\\nPortfolio Reconstitution : Positions excluded from the new targets are liquidated with proceeds credited to cash. New target stocks are then purchased with equal capital allocation, subject to current available cash.\\nPortfolio Rebalancing : To maintain equal-capital allocations, we adjust positions‚Äîselling excess holdings that exceed target allocation and purchasing additional shares for under-allocated positions.\\nHold Period : Between rebalancing days, all positions are held constant without any trading activity. Portfolio values are recorded daily for performance tracking, but no transactions occur until the next rebalancing day.\\nThis backtesting strategy facilitates direct evaluation of model predictions and profitability within a realistic trading environment. For our framework, we set d = 10 d=10 days, balancing prediction reliability with reduced transaction costs from less frequent rebalancing.\\nFor each model‚Äìdataset pair, we perform grid search on the validation set across three hyperparameters in the trading strategy:\\nPortfolio Selection Ratio p ‚àà { 0.05 , 0.10 , ‚Ä¶ , 1.0 } p\\\\in\\\\{0.05,0.10,\\\\ldots,1.0\\\\} ,\\nStop-Loss Threshold q ‚àà { 0.05 , 0.10 , ‚Ä¶ , 0.95 } q\\\\in\\\\{0.05,0.10,\\\\ldots,0.95\\\\} , and\\nRising Ratio for Partial Entry r ‚àà { 0.0 , 0.05 , ‚Ä¶ , 1.0 } r\\\\in\\\\{0.0,0.05,\\\\ldots,1.0\\\\} .\\nThe optimal combination yielding the highest Sharpe ratio on the validation set is applied to the test set for final evaluation. The selected hyperparameters for each dataset of our model are shown in Table 7 .',\n",
       "    'subsections': [{'title': 'G.1. Backtesting Configurations',\n",
       "      'paragraphs': 'For each model‚Äìdataset pair, we perform grid search on the validation set across three hyperparameters in the trading strategy:\\nPortfolio Selection Ratio p ‚àà { 0.05 , 0.10 , ‚Ä¶ , 1.0 } p\\\\in\\\\{0.05,0.10,\\\\ldots,1.0\\\\} ,\\nStop-Loss Threshold q ‚àà { 0.05 , 0.10 , ‚Ä¶ , 0.95 } q\\\\in\\\\{0.05,0.10,\\\\ldots,0.95\\\\} , and\\nRising Ratio for Partial Entry r ‚àà { 0.0 , 0.05 , ‚Ä¶ , 1.0 } r\\\\in\\\\{0.0,0.05,\\\\ldots,1.0\\\\} .\\nThe optimal combination yielding the highest Sharpe ratio on the validation set is applied to the test set for final evaluation. The selected hyperparameters for each dataset of our model are shown in Table 7 .',\n",
       "      'subsections': []}]}],\n",
       "  'abstract': 'Stock movement prediction remains fundamentally challenging due to complex temporal dependencies, heterogeneous modalities, and dynamically evolving inter-stock relationships. Existing approaches often fail to unify structural, semantic, and regime-adaptive modeling within a scalable framework. This work introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with LLM reasoning and Style-Structured Mixture of Experts, integrating three key innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph (LCH) and persistent inter-stock dependencies through a Global Context Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen‚ÄìShannon Divergence weighting mechanism for adaptive relational learning and cross-modal alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities, enriching representations with domain-specific financial knowledge; and (3) a Style-Structured Mixture of Experts (SSMoEs) that combines shared market experts and industry-specialized experts, each parameterized by learnable style vectors enabling regime-aware specialization under sparse activation. Extensive experiments on three major stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in both superior predictive accuracy and investment performance, while exhibiting effective risk control. Datasets, source code, and model weights are available at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.'},\n",
       " '2510.25518v1': {'content': [{'title': 'I Introduction',\n",
       "    'paragraphs': 'Retrieval Augmented Generation (RAG) combines large language models (LLMs) with external document retrieval, allowing models to access information beyond their training data. These systems have shown impressive results in general-purpose applications such as technical support [ 1 ] , coding assistants [ 2 ] , and document summarization [ 3 ] , especially when operating as proof-of-concepts on publicly available or loosely structured data.\\nHowever, deploying such systems at scale in highly specialized and tightly regulated domains such as financial technology is far from straightforward [ 4 , 5 ] .\\nFintech-specific use cases often involve structured-unstructured data (or tightly regulated free text), such as proprietary knowledge bases, role-based access to information, and stringent compliance constraints [ 6 ] , all of which make it difficult to repurpose existing tools and methodologies designed for open-domain settings or Software-as-a-Service (SaaS)-based environments. These complexities are both technical and organizational. Regulatory restrictions prohibit data from leaving organisational boundaries, making cloud-hosted APIs or third-party evaluation platforms unsuitable for most practical deployments. In addition, often fintech firms follow a compartmentalized structure with clear divisions of responsibility between product managers, analysts, compliance officers, and engineers. This further complicates how knowledge is captured, retrieved, and applied.\\nFor example, consider a product manager in Mastercard analyzing feature alignment across offerings such as MDES (Mastercard Digital Enablement Service) 1 1 1 https://www.mastercard.com/newsroom/press/news-briefs/mastercard-digital-enablement-service-mdes , Accessed: 2025-06-25 , M4M (Mastercard for Merchants) [ 7 ] , and SwitchCore (Mastercard‚Äôs transaction switching platform) [ 8 ] .\\nInternal knowledge sources‚Äîranging from note-taking apps, product management platforms and architecture decks to regulatory compliance PDFs‚Äîoften use acronyms inconsistently or assume implicit context based on team-specific language. This forces the user to manually piece together fragmented information to understand interoperability constraints, roadmap alignment, or API-level integration guidelines. A standard RAG system typically underperforms in such scenarios, frequently misinterpreting short forms like ‚ÄúCMA‚Äù (which could mean ‚ÄúConsumer Management Application‚Äù or ‚ÄúCardholder Management Architecture‚Äù depending on the context), or retrieving documents based on keyword overlap rather than intent.\\nAnother challenge with such domain-specific use-cases for large enterprises is evaluation. Traditional RAG benchmarks assume public datasets, crowd-sourced relevance judgments, or static ground truth‚Äîall of which are infeasible in fintech. Finding subject-matter experts to annotate queries at scale is both costly and time-consuming. Moreover, regulations around confidentiality and data residency prevent the use of crowd platforms for human evaluation.\\nTherefore, designing on-prem, domain-specific RAG applications requires several considerations and not limited to ‚Äî a careful rethinking of architecture, privacy guarantees, a structured retrieval and reasoning process capable of adapting to ambiguity (especially when the source content is internal, fragmented, and often acronym-heavy). In this work, we focus on some of these core challenges and propose an agent design tailored to domain-specific retrieval and reasoning. We also present evaluation methodologies that are secure, reproducible, and feasible at scale within enterprise constraints.\\nWe propose a hierarchical agent framework, where an Orchestrator Agent delegates tasks to specialized sub-agents responsible for acronym resolution, domain-aware query rewriting, and cross-encoder-based re-ranking. This modular structure allows the system to encode organizational idiosyncrasies such as internal taxonomies, access controls, and documentation practices, significantly improving the quality of answers retrieved. In addition, we develop a semi-automated evaluation strategy that combines LLM-as-a-judge [ 9 ] paradigms with constrained prompt templates and human-in-the-loop validation. Inspired by recent work on synthetic evaluation datasets [ 9 , 10 ] , we leverage internal knowledge bases to generate realistic query-answer pairs, which are then assessed using a mix of LLM scoring and manual spot-checking. This allows for a secure, efficient, and consistent evaluation process that scales across teams and data silos.\\nTo summarize, our primary contributions include:\\nA Fintech-focused Agentic Design: We develop an Orchestrator Agent that coordinates specialized agents for acronym resolution, iterative sub-query generation using key phrases, and context refinement through a cross-encoder re-ranking method.\\nAutomatic Query Enhancement with Continuous Feedback: Our system proactively identifies and resolves domain-specific acronyms within user queries and retrieved content, enhancing retrieval accuracy and answer completeness.\\nThorough Evaluation Methodology: We construct an evaluation dataset from an enterprise knowledge base, leveraging LLM-driven synthetic data generation and manual validation to ensure high-quality, contextually relevant questions.\\nEmpirical Comparative Analysis: We quantitatively and qualitatively compare our agentic approach against a standard RAG baseline, measuring effectiveness through metrics such as retrieval accuracy and answer relevance.\\nThe remainder of this paper is organized as follows. Section II reviews relevant work in RAG, multi-agent pipelines, and domain-specific retrieval systems. Section III describes our methodology, including the design of the baseline and agentic RAG architectures. Section IV details the knowledge base preparation and the construction of the evaluation dataset. Section V presents the experimental setup and results, comparing retrieval accuracy and latency across configurations. Section VI provides a qualitative error analysis and discussion of observed performance patterns. Finally, Section VII concludes the paper and outlines directions for future work.',\n",
       "    'subsections': []},\n",
       "   {'title': 'II Related Work',\n",
       "    'paragraphs': 'Recent work has increasingly explored enhancing Retrieval-Augmented Generation (RAG) systems with structured, multi-agent architectures for complex information tasks. Agentic RAG integrates autonomous AI agents into the pipeline, enabling dynamic query decomposition and iterative reasoning. Singh et al. introduce the concept of Agentic RAG, where agents perform planning, reflection, and tool use to refine context and retrieval [ 11 ] . Similarly, Nguyen et al. propose MA-RAG, a modular, training-free framework where agents (e.g., Planner, Extractor, QA) collaboratively process questions using chain-of-thought reasoning [ 12 ] . Their results show that this structure improves answer quality and resolves ambiguities without fine-tuning. The Pathway team further observes that domain-adaptive agents enhance performance by assigning each agent task-relevant expertise [ 13 , 12 ] .\\nDeploying RAG in specialized domains such as finance, healthcare, and law requires grounding retrieval in domain knowledge. Recent studies emphasize using structured ontologies to enhance LLM performance. Barron et al. introduce the SMART-SLIC framework, combining knowledge graphs (KGs) and vector stores customized for a domain to support retrieval [ 14 ] . They demonstrate that referencing KG ontologies improves QA by aligning retrieval with relevant subgraph structures. Their approach fuses structured (KG) and unstructured (text) sources to reduce hallucinations and improve citation fidelity [ 14 ] . However, they also highlight the resource demands of curating such domain-specific infrastructure. Other studies similarly confirm that structured knowledge sources boost retrieval precision in specialized settings such as legal and medical QA [ 14 , 11 ] .\\nLarge language models have found wide adoption in fintech, powering analytics and automation across customer support, fraud detection, risk modeling, and compliance. Daiya surveys how LLMs monitor financial risks by processing vast unstructured data, detecting patterns, and forecasting threats from logs, news, and communications [ 15 ] . Broughton highlights their role in fraud detection, credit scoring, and automating regulatory tasks like document parsing and report generation [ 16 ] . These studies agree that LLMs extract actionable insights from diverse financial sources, whether through sentiment analysis, trading signal identification, or customer feedback mining [ 16 , 15 ] . These applications underscore the need for domain-aware RAG to enhance factual accuracy in high-stakes financial contexts.\\nWithin fintech, RAG systems are being prototyped for data-driven assistants and analytics tools. Lumenova AI reports growing adoption of RAG-backed chatbots that integrate live market and account data to improve customer interaction [ 17 ] . A notable example is Bank of America‚Äôs ‚ÄúErica,‚Äù which uses LLMs and real-time retrieval to support over a billion user interactions [ 17 ] . Hernandez Leal demonstrates a RAG-powered assistant trained on SEC filings to answer investor questions from 10-K and 10-Q reports [ 18 ] . These examples show how RAG can anchor LLM outputs in authoritative financial sources, enhancing both precision and citation. While still in early stages, these systems suggest that real-time, curated retrieval pipelines significantly boost LLM utility and trustworthiness in financial applications [ 17 , 18 ] .',\n",
       "    'subsections': [{'title': 'Agentic RAG',\n",
       "      'paragraphs': 'Recent work has increasingly explored enhancing Retrieval-Augmented Generation (RAG) systems with structured, multi-agent architectures for complex information tasks. Agentic RAG integrates autonomous AI agents into the pipeline, enabling dynamic query decomposition and iterative reasoning. Singh et al. introduce the concept of Agentic RAG, where agents perform planning, reflection, and tool use to refine context and retrieval [ 11 ] . Similarly, Nguyen et al. propose MA-RAG, a modular, training-free framework where agents (e.g., Planner, Extractor, QA) collaboratively process questions using chain-of-thought reasoning [ 12 ] . Their results show that this structure improves answer quality and resolves ambiguities without fine-tuning. The Pathway team further observes that domain-adaptive agents enhance performance by assigning each agent task-relevant expertise [ 13 , 12 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': 'Domain-specific RAG',\n",
       "      'paragraphs': 'Deploying RAG in specialized domains such as finance, healthcare, and law requires grounding retrieval in domain knowledge. Recent studies emphasize using structured ontologies to enhance LLM performance. Barron et al. introduce the SMART-SLIC framework, combining knowledge graphs (KGs) and vector stores customized for a domain to support retrieval [ 14 ] . They demonstrate that referencing KG ontologies improves QA by aligning retrieval with relevant subgraph structures. Their approach fuses structured (KG) and unstructured (text) sources to reduce hallucinations and improve citation fidelity [ 14 ] . However, they also highlight the resource demands of curating such domain-specific infrastructure. Other studies similarly confirm that structured knowledge sources boost retrieval precision in specialized settings such as legal and medical QA [ 14 , 11 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': 'LLM Applications in Fintech',\n",
       "      'paragraphs': 'Large language models have found wide adoption in fintech, powering analytics and automation across customer support, fraud detection, risk modeling, and compliance. Daiya surveys how LLMs monitor financial risks by processing vast unstructured data, detecting patterns, and forecasting threats from logs, news, and communications [ 15 ] . Broughton highlights their role in fraud detection, credit scoring, and automating regulatory tasks like document parsing and report generation [ 16 ] . These studies agree that LLMs extract actionable insights from diverse financial sources, whether through sentiment analysis, trading signal identification, or customer feedback mining [ 16 , 15 ] . These applications underscore the need for domain-aware RAG to enhance factual accuracy in high-stakes financial contexts.',\n",
       "      'subsections': []},\n",
       "     {'title': 'RAG for Fintech',\n",
       "      'paragraphs': 'Within fintech, RAG systems are being prototyped for data-driven assistants and analytics tools. Lumenova AI reports growing adoption of RAG-backed chatbots that integrate live market and account data to improve customer interaction [ 17 ] . A notable example is Bank of America‚Äôs ‚ÄúErica,‚Äù which uses LLMs and real-time retrieval to support over a billion user interactions [ 17 ] . Hernandez Leal demonstrates a RAG-powered assistant trained on SEC filings to answer investor questions from 10-K and 10-Q reports [ 18 ] . These examples show how RAG can anchor LLM outputs in authoritative financial sources, enhancing both precision and citation. While still in early stages, these systems suggest that real-time, curated retrieval pipelines significantly boost LLM utility and trustworthiness in financial applications [ 17 , 18 ] .',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'III Agent Design',\n",
       "    'paragraphs': 'Building on the challenges introduced in Section I , this section details how we first implemented a Baseline RAG pipeline to explore retrieval limitations in fintech settings, and subsequently refined the design into a modular agentic architecture.\\nThe Baseline RAG system, which will be referred to as \"B-RAG\" for the remainder of this work, represents a standard retrieval pipeline that serves as a comparison point for our agentic architecture. This approach follows a sequential process, as illustrated in Figure 2 .\\nThe B-RAG system, illustrated in Figure 2 , consists of a sequence of specialized agents structured to enable stepwise information retrieval and synthesis. The process begins with the user‚Äôs initial input, which serves as the contextual foundation for subsequent components. This input is then propagated through the following agents:\\nQuery Reformulator Agent : Takes the user‚Äôs question and refines it into a concise, keyword-focused query optimized for retrieval. It uses predefined prompt templates to detect if the query is a continuation of previous interactions or a standalone new topic.\\nRetriever Agent : Executes a single-pass retrieval from the knowledge base with cosine similarity between embeddings to locate relevant document chunks.\\nSummary Agent : Aggregates retrieved document chunks into a coherent answer. It is explicitly instructed to avoid generating information not present in the retrieved context, clearly indicating if the retrieved content lacks sufficient information.\\nFinally, the output of the summary agent is presented as a coherent response, supported by the reference links.\\nThe B-RAG pipeline serves as a reference due to its simplified architecture, which allows us to conduct clear and interpretable performance benchmarking against the more complex agentic system. However, this simplicity imposes several notable limitations:\\nLack of multi-pass retrieval logic : The system performs a single retrieval pass without iterative refinement, thereby constraining its capacity to effectively address complex queries that require deeper exploration.\\nAbsence of sub-query decomposition : The pipeline is unable to partition broad or ambiguous queries into multiple targeted sub-queries. This limitation results into diminished precision when questions span multiple document fragments.\\nNo acronym resolution mechanism : The system does not resolve domain-specific acronyms frequently encountered in enterprise context, which may adversely affect the clarity and accuracy of retrieved information.\\nAbsence of document re-ranking : Retrieved results are utilized without further subsequent validation or refinement through cross-encoder-based re-ranking. This potentially may compromise the relevance of the final responses.\\nDespite these limitations, the simple design of the baseline pipeline offers some advantages, including ease of deployment, lower computational overhead, and enhanced interpretability. These attributes make it an effective benchmark for systematically evaluating the incremental improvements introduced by each advanced component of the proposed agentic architecture.\\nThe Agentic RAG system, which will be referred to as \"A-RAG\" throughout this paper, is designed to address the limitations inherent in the B-RAG system by integrating modular intelligence and task-specific specialization. Central to A-RAG is the Orchestrator Agent , which coordinates a suite of specialized agents, each tasked with a distinct retrieval or synthesis function. This design is inspired by an investigative research workflow. It initiates with a direct attempt to address the user query, subsequently evaluates the output, and when necessary, activates more focused and iterative exploration.\\nTo address challenges such as ambiguous terminology, fragmented sources, and answer confidence, A-RAG incorporates several key capabilities, including acronym resolution, sub-query generation, parallel retrieval, re-ranking, and quality assessment. The quality of generated answers is evaluated by a dedicated QA agent whose confidence score determines whether further iterative refinement is required. A comparison of the hybrid pipeline architectures for B-RAG and A-RAG workflows is shown in Figure 3 . It highlights the differences in processing flow and iterative mechanisms.\\nThe A-RAG system comprises a set of lightweight, modular agents orchestrated to support iterative retrieval and reasoning. The following subsections describe the system‚Äôs core agent components and the operational workflow using a representative fintech query example.\\nThe orchestrator coordinates eight specialized agents, each responsible for a distinct stage in the retrieval‚Äìreasoning cycle:\\nIntent Classifier ‚Äì determines whether the user input requires (i) retrieval , which involves fetching new context from the knowledge base, or (ii) summary , which compresses existing conversational history.\\nQuery Reformulator ‚Äì transforms the raw query into a dense, keyword-optimized search string by removing function words, expanding recognized acronyms, and injecting domain-specific synonyms to maximize embedding-based recall.\\nRetriever Manager & Retriever Agent ‚Äì launch one or more vector store queries‚Äîexecuted in parallel if sub-queries exist‚Äîand return the top- k k most relevant chunks.\\nSub-Query Generator ‚Äì in cases of low initial retrieval scores, identifies 2‚Äì3 key entities or phrases to construct targeted follow-up queries.\\nRe-Ranker Agent ‚Äì reorders the retrieved chunks using a cross-encoder to prioritize those with higher semantic alignment to the query.\\nSummary Agent ‚Äì fuses the ranked context snippets into a concise, citation-style answer composed strictly from the retrieved content.\\nQA Agent ‚Äì evaluates the synthesized answer on a 0‚Äì10 scale, assessing relevance and support from context; the score determines whether the pipeline concludes or proceeds with refinement.\\nAcronym Resolution Logic ‚Äì manages a local glossary and, when invoked, appends in-line definitions to prevent downstream agents from misinterpreting shorthand expressions.\\nThe operational workflow integrates the agent components into a cohesive, adaptive pipeline, illustrated with the example query: ‚ÄúHow is CVaR calculated in the IRRBB framework?‚Äù The process begins with query reformulation and acronym expansion, ensuring clarity and disambiguation of terms like CVaR and IRRBB . A first-pass vector retrieval fetches relevant document chunks, which are synthesized into an initial answer. If the QA agent assigns a low confidence score, the system triggers refinement: sub-queries such as ‚ÄúCVaR formula‚Äù or ‚ÄúIRRBB risk quantification‚Äù are generated, results are re-ranked, and a revised synthesis is attempted. Should this prove inadequate, a broader retrieval sweep is conducted to increase coverage. In the absence of a sufficiently confident answer, the system transparently communicates its uncertainty to the user. This adaptive pipeline dynamically balances computational efficiency with retrieval depth based on answer confidence.',\n",
       "    'subsections': [{'title': 'III-A Baseline RAG System',\n",
       "      'paragraphs': 'The Baseline RAG system, which will be referred to as \"B-RAG\" for the remainder of this work, represents a standard retrieval pipeline that serves as a comparison point for our agentic architecture. This approach follows a sequential process, as illustrated in Figure 2 .\\nThe B-RAG system, illustrated in Figure 2 , consists of a sequence of specialized agents structured to enable stepwise information retrieval and synthesis. The process begins with the user‚Äôs initial input, which serves as the contextual foundation for subsequent components. This input is then propagated through the following agents:\\nQuery Reformulator Agent : Takes the user‚Äôs question and refines it into a concise, keyword-focused query optimized for retrieval. It uses predefined prompt templates to detect if the query is a continuation of previous interactions or a standalone new topic.\\nRetriever Agent : Executes a single-pass retrieval from the knowledge base with cosine similarity between embeddings to locate relevant document chunks.\\nSummary Agent : Aggregates retrieved document chunks into a coherent answer. It is explicitly instructed to avoid generating information not present in the retrieved context, clearly indicating if the retrieved content lacks sufficient information.\\nFinally, the output of the summary agent is presented as a coherent response, supported by the reference links.\\nThe B-RAG pipeline serves as a reference due to its simplified architecture, which allows us to conduct clear and interpretable performance benchmarking against the more complex agentic system. However, this simplicity imposes several notable limitations:\\nLack of multi-pass retrieval logic : The system performs a single retrieval pass without iterative refinement, thereby constraining its capacity to effectively address complex queries that require deeper exploration.\\nAbsence of sub-query decomposition : The pipeline is unable to partition broad or ambiguous queries into multiple targeted sub-queries. This limitation results into diminished precision when questions span multiple document fragments.\\nNo acronym resolution mechanism : The system does not resolve domain-specific acronyms frequently encountered in enterprise context, which may adversely affect the clarity and accuracy of retrieved information.\\nAbsence of document re-ranking : Retrieved results are utilized without further subsequent validation or refinement through cross-encoder-based re-ranking. This potentially may compromise the relevance of the final responses.\\nDespite these limitations, the simple design of the baseline pipeline offers some advantages, including ease of deployment, lower computational overhead, and enhanced interpretability. These attributes make it an effective benchmark for systematically evaluating the incremental improvements introduced by each advanced component of the proposed agentic architecture.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-B Proposed Refined Design',\n",
       "      'paragraphs': 'The Agentic RAG system, which will be referred to as \"A-RAG\" throughout this paper, is designed to address the limitations inherent in the B-RAG system by integrating modular intelligence and task-specific specialization. Central to A-RAG is the Orchestrator Agent , which coordinates a suite of specialized agents, each tasked with a distinct retrieval or synthesis function. This design is inspired by an investigative research workflow. It initiates with a direct attempt to address the user query, subsequently evaluates the output, and when necessary, activates more focused and iterative exploration.\\nTo address challenges such as ambiguous terminology, fragmented sources, and answer confidence, A-RAG incorporates several key capabilities, including acronym resolution, sub-query generation, parallel retrieval, re-ranking, and quality assessment. The quality of generated answers is evaluated by a dedicated QA agent whose confidence score determines whether further iterative refinement is required. A comparison of the hybrid pipeline architectures for B-RAG and A-RAG workflows is shown in Figure 3 . It highlights the differences in processing flow and iterative mechanisms.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-C System Architecture and Workflow',\n",
       "      'paragraphs': 'The A-RAG system comprises a set of lightweight, modular agents orchestrated to support iterative retrieval and reasoning. The following subsections describe the system‚Äôs core agent components and the operational workflow using a representative fintech query example.\\nThe orchestrator coordinates eight specialized agents, each responsible for a distinct stage in the retrieval‚Äìreasoning cycle:\\nIntent Classifier ‚Äì determines whether the user input requires (i) retrieval , which involves fetching new context from the knowledge base, or (ii) summary , which compresses existing conversational history.\\nQuery Reformulator ‚Äì transforms the raw query into a dense, keyword-optimized search string by removing function words, expanding recognized acronyms, and injecting domain-specific synonyms to maximize embedding-based recall.\\nRetriever Manager & Retriever Agent ‚Äì launch one or more vector store queries‚Äîexecuted in parallel if sub-queries exist‚Äîand return the top- k k most relevant chunks.\\nSub-Query Generator ‚Äì in cases of low initial retrieval scores, identifies 2‚Äì3 key entities or phrases to construct targeted follow-up queries.\\nRe-Ranker Agent ‚Äì reorders the retrieved chunks using a cross-encoder to prioritize those with higher semantic alignment to the query.\\nSummary Agent ‚Äì fuses the ranked context snippets into a concise, citation-style answer composed strictly from the retrieved content.\\nQA Agent ‚Äì evaluates the synthesized answer on a 0‚Äì10 scale, assessing relevance and support from context; the score determines whether the pipeline concludes or proceeds with refinement.\\nAcronym Resolution Logic ‚Äì manages a local glossary and, when invoked, appends in-line definitions to prevent downstream agents from misinterpreting shorthand expressions.\\nThe operational workflow integrates the agent components into a cohesive, adaptive pipeline, illustrated with the example query: ‚ÄúHow is CVaR calculated in the IRRBB framework?‚Äù The process begins with query reformulation and acronym expansion, ensuring clarity and disambiguation of terms like CVaR and IRRBB . A first-pass vector retrieval fetches relevant document chunks, which are synthesized into an initial answer. If the QA agent assigns a low confidence score, the system triggers refinement: sub-queries such as ‚ÄúCVaR formula‚Äù or ‚ÄúIRRBB risk quantification‚Äù are generated, results are re-ranked, and a revised synthesis is attempted. Should this prove inadequate, a broader retrieval sweep is conducted to increase coverage. In the absence of a sufficiently confident answer, the system transparently communicates its uncertainty to the user. This adaptive pipeline dynamically balances computational efficiency with retrieval depth based on answer confidence.',\n",
       "      'subsections': [{'title': 'Core Agent Components',\n",
       "        'paragraphs': 'The orchestrator coordinates eight specialized agents, each responsible for a distinct stage in the retrieval‚Äìreasoning cycle:\\nIntent Classifier ‚Äì determines whether the user input requires (i) retrieval , which involves fetching new context from the knowledge base, or (ii) summary , which compresses existing conversational history.\\nQuery Reformulator ‚Äì transforms the raw query into a dense, keyword-optimized search string by removing function words, expanding recognized acronyms, and injecting domain-specific synonyms to maximize embedding-based recall.\\nRetriever Manager & Retriever Agent ‚Äì launch one or more vector store queries‚Äîexecuted in parallel if sub-queries exist‚Äîand return the top- k k most relevant chunks.\\nSub-Query Generator ‚Äì in cases of low initial retrieval scores, identifies 2‚Äì3 key entities or phrases to construct targeted follow-up queries.\\nRe-Ranker Agent ‚Äì reorders the retrieved chunks using a cross-encoder to prioritize those with higher semantic alignment to the query.\\nSummary Agent ‚Äì fuses the ranked context snippets into a concise, citation-style answer composed strictly from the retrieved content.\\nQA Agent ‚Äì evaluates the synthesized answer on a 0‚Äì10 scale, assessing relevance and support from context; the score determines whether the pipeline concludes or proceeds with refinement.\\nAcronym Resolution Logic ‚Äì manages a local glossary and, when invoked, appends in-line definitions to prevent downstream agents from misinterpreting shorthand expressions.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Workflow Overview',\n",
       "        'paragraphs': 'The operational workflow integrates the agent components into a cohesive, adaptive pipeline, illustrated with the example query: ‚ÄúHow is CVaR calculated in the IRRBB framework?‚Äù The process begins with query reformulation and acronym expansion, ensuring clarity and disambiguation of terms like CVaR and IRRBB . A first-pass vector retrieval fetches relevant document chunks, which are synthesized into an initial answer. If the QA agent assigns a low confidence score, the system triggers refinement: sub-queries such as ‚ÄúCVaR formula‚Äù or ‚ÄúIRRBB risk quantification‚Äù are generated, results are re-ranked, and a revised synthesis is attempted. Should this prove inadequate, a broader retrieval sweep is conducted to increase coverage. In the absence of a sufficiently confident answer, the system transparently communicates its uncertainty to the user. This adaptive pipeline dynamically balances computational efficiency with retrieval depth based on answer confidence.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': 'IV Data and Evaluation Strategy',\n",
       "    'paragraphs': 'In Section I , we described the challenge of designing retrieval systems that can operate effectively over fragmented and acronym-heavy enterprise knowledge bases. This section details how we constructed the knowledge corpus and generated an evaluation set tailored to this context.\\nThe knowledge base was derived from proprietary internal documents exported in structured markup format. A custom pipeline was developed to preprocess the data. This process focused on:\\nConverting structured elements such as tables and code blocks into linear, plain-text representations while preserving semantic relationships.\\nRemoving formatting noise and markup artifacts to produce clean, model-ready text.\\nTo enable retrieval, the documents were segmented into overlapping chunks (to preserve contextual coherence across boundaries), embedded with a publicly available sentence transformer, and stored in a vector index.\\nThe resulting corpus consisted of over 30,000 text chunks representing 1,624 unique documents. On average, each document was split into approximately 19 chunks. Figure 4 shows the distribution of chunk lengths measured by word count.\\nTo further illustrate the dataset composition, Figure 1 and Figure 5 provide complementary views of term frequency. Both highlight the prevalence of domain-specific language in the internal fintech corpus, particularly terms such as ‚Äúfeature,‚Äù ‚ÄúAPI,‚Äù and ‚Äúmodel,‚Äù reflecting its emphasis on product, technical, and compliance-related content.\\nInspired by the LLM-as-a-judge paradigm [ 9 ] , we designed a multi-phase, model-assisted pipeline to create a high-quality evaluation set. This approach is motivated by the constraints outlined in Section I , particularly the impracticality of relying solely on manual annotation within confidential enterprise environments.\\nFirst, we randomly sampled a subset of chunks from the curated corpus. For each chunk, a language model was prompted to generate a question-answer pair grounded in the provided context. Two distinct prompt templates were used; one tailored to narrative text and the other to structured elements such as tables. This distinction is made to ensure both content diversity and fidelity to the source material.\\nSubsequently, each generated question‚Äìanswer pair underwent a quality control phase in which a language model assessed contextual alignment, factual accuracy, and coherence. To standardize this evaluation, we defined three criteria that each pair was required to meet:\\nSpecificity: Is the question clearly and narrowly scoped to a particular aspect of the context?\\nFaithfulness: Is the answer grounded exclusively in the provided source text, without introducing external information?\\nCompleteness: Does the answer fully and directly address the question posed?\\nOnly candidate pairs that satisfied all three criteria were retained. These pairs then underwent an additional round of manual review to eliminate any remaining edge cases or ambiguous examples. Finally, the resulting evaluation set consisted of 85 validated question-answer pairs, each traceable to a unique document chunk. This dataset served as the foundation for the quantitative and qualitative evaluations presented in Sections V-B , V-C .\\nIn addition to the automated evaluation set described above, we constructed a manually curated benchmark designed to stress-test both retrieval fidelity and answer ranking. The benchmark construction process involved the following key steps:\\nCandidate questions were extracted from routine activities within the internal knowledge base , including onboarding runbooks, intern reports, and project or product wikis.\\nFor each question, human annotators identified all relevant documentation pages and selected one representative chunk per page as a potential answer. Among these, the most comprehensive chunk was designated as the ground truth. This setup reflects a ‚Äúone correct, many plausible‚Äù retrieval scenario.\\nEach query was labeled according to an intent taxonomy‚Äî Procedural , Definitional , or Acronym Expansion ‚Äîto support stratified analysis, as discussed in Section V-C .\\nThe final benchmark comprises 27 question‚Äìanswer pairs, each corresponding to a distinct query with multiple associated ground-truth answers: seven procedural, fourteen definitional, and six acronym-expansion examples.\\nThis human-verified dataset was used to (i) compute strict retrieval metrics ( Hit@k ), and (ii) support error analysis in the presence of multiple valid answers. For each item, we log the following fields: Question , Category , Ground-truth Answer(s) , Ground-truth Source(s) , Generated Answer , and Retrieved Source(s) .',\n",
       "    'subsections': [{'title': 'IV-A Knowledge Base Preparation',\n",
       "      'paragraphs': 'The knowledge base was derived from proprietary internal documents exported in structured markup format. A custom pipeline was developed to preprocess the data. This process focused on:\\nConverting structured elements such as tables and code blocks into linear, plain-text representations while preserving semantic relationships.\\nRemoving formatting noise and markup artifacts to produce clean, model-ready text.\\nTo enable retrieval, the documents were segmented into overlapping chunks (to preserve contextual coherence across boundaries), embedded with a publicly available sentence transformer, and stored in a vector index.\\nThe resulting corpus consisted of over 30,000 text chunks representing 1,624 unique documents. On average, each document was split into approximately 19 chunks. Figure 4 shows the distribution of chunk lengths measured by word count.\\nTo further illustrate the dataset composition, Figure 1 and Figure 5 provide complementary views of term frequency. Both highlight the prevalence of domain-specific language in the internal fintech corpus, particularly terms such as ‚Äúfeature,‚Äù ‚ÄúAPI,‚Äù and ‚Äúmodel,‚Äù reflecting its emphasis on product, technical, and compliance-related content.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-B Evaluation Set Generation',\n",
       "      'paragraphs': 'Inspired by the LLM-as-a-judge paradigm [ 9 ] , we designed a multi-phase, model-assisted pipeline to create a high-quality evaluation set. This approach is motivated by the constraints outlined in Section I , particularly the impracticality of relying solely on manual annotation within confidential enterprise environments.\\nFirst, we randomly sampled a subset of chunks from the curated corpus. For each chunk, a language model was prompted to generate a question-answer pair grounded in the provided context. Two distinct prompt templates were used; one tailored to narrative text and the other to structured elements such as tables. This distinction is made to ensure both content diversity and fidelity to the source material.\\nSubsequently, each generated question‚Äìanswer pair underwent a quality control phase in which a language model assessed contextual alignment, factual accuracy, and coherence. To standardize this evaluation, we defined three criteria that each pair was required to meet:\\nSpecificity: Is the question clearly and narrowly scoped to a particular aspect of the context?\\nFaithfulness: Is the answer grounded exclusively in the provided source text, without introducing external information?\\nCompleteness: Does the answer fully and directly address the question posed?\\nOnly candidate pairs that satisfied all three criteria were retained. These pairs then underwent an additional round of manual review to eliminate any remaining edge cases or ambiguous examples. Finally, the resulting evaluation set consisted of 85 validated question-answer pairs, each traceable to a unique document chunk. This dataset served as the foundation for the quantitative and qualitative evaluations presented in Sections V-B , V-C .',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-C Human-Verified Prompt‚ÄìAnswer Benchmark',\n",
       "      'paragraphs': 'In addition to the automated evaluation set described above, we constructed a manually curated benchmark designed to stress-test both retrieval fidelity and answer ranking. The benchmark construction process involved the following key steps:\\nCandidate questions were extracted from routine activities within the internal knowledge base , including onboarding runbooks, intern reports, and project or product wikis.\\nFor each question, human annotators identified all relevant documentation pages and selected one representative chunk per page as a potential answer. Among these, the most comprehensive chunk was designated as the ground truth. This setup reflects a ‚Äúone correct, many plausible‚Äù retrieval scenario.\\nEach query was labeled according to an intent taxonomy‚Äî Procedural , Definitional , or Acronym Expansion ‚Äîto support stratified analysis, as discussed in Section V-C .\\nThe final benchmark comprises 27 question‚Äìanswer pairs, each corresponding to a distinct query with multiple associated ground-truth answers: seven procedural, fourteen definitional, and six acronym-expansion examples.\\nThis human-verified dataset was used to (i) compute strict retrieval metrics ( Hit@k ), and (ii) support error analysis in the presence of multiple valid answers. For each item, we log the following fields: Question , Category , Ground-truth Answer(s) , Ground-truth Source(s) , Generated Answer , and Retrieved Source(s) .',\n",
       "      'subsections': [{'title': 'Question sourcing',\n",
       "        'paragraphs': 'Candidate questions were extracted from routine activities within the internal knowledge base , including onboarding runbooks, intern reports, and project or product wikis.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Human validation of answers',\n",
       "        'paragraphs': 'For each question, human annotators identified all relevant documentation pages and selected one representative chunk per page as a potential answer. Among these, the most comprehensive chunk was designated as the ground truth. This setup reflects a ‚Äúone correct, many plausible‚Äù retrieval scenario.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Intent categorization',\n",
       "        'paragraphs': 'Each query was labeled according to an intent taxonomy‚Äî Procedural , Definitional , or Acronym Expansion ‚Äîto support stratified analysis, as discussed in Section V-C .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Dataset composition',\n",
       "        'paragraphs': 'The final benchmark comprises 27 question‚Äìanswer pairs, each corresponding to a distinct query with multiple associated ground-truth answers: seven procedural, fourteen definitional, and six acronym-expansion examples.\\nThis human-verified dataset was used to (i) compute strict retrieval metrics ( Hit@k ), and (ii) support error analysis in the presence of multiple valid answers. For each item, we log the following fields: Question , Category , Ground-truth Answer(s) , Ground-truth Source(s) , Generated Answer , and Retrieved Source(s) .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': 'V Experiments and Results',\n",
       "    'paragraphs': 'We compared B-RAG and A-RAG pipelines on a specialized internal fintech knowledge base. Both pipelines used the same LLM backend, Llama-3.1-8B-Instruct , served via a vLLM endpoint. Document embeddings were generated using the all-MiniLM-L6-v2 model and stored in ChromaDB. We evaluated both systems using two key metrics: (1) Retrieval Accuracy (Hit Rate @5), defined as the percentage of questions for which the correct document source appeared among the top five retrieved links; and (2) Average Latency, defined as the time from user query submission to the final system response.\\nTable I summarizes the results. The A-RAG system achieved a strict retrieval accuracy of 62.35%, outperforming the Baseline‚Äôs 54.12%. This performance gain can be attributed to the system‚Äôs specialized agents for acronym resolution and sub-query expansion, which were designed to address the fragmented and semantically sparse nature of enterprise knowledge described in Section I . As expected, A-RAG‚Äôs average query latency was 5.02 seconds, significantly higher than B-RAG‚Äôs 0.79 seconds, due to multi-stage processing and iterative document re-ranking.\\nAlthough retrieval was evaluated using exact link matches, manual inspection identified several queries where the system returned correct content from semantically equivalent but non-identical documents. Specifically, A-RAG retrieved valid answers from alternate sources in six additional cases, and B-RAG in three. Incorporating these, the adjusted retrieval accuracy increases to 69.41% for A-RAG and 58.82% for B-RAG. These cases reinforce the core challenge introduced in Section I ; that enterprise knowledge in fintech domains is often fragmented, with key information distributed across related documents. Unlike B-RAG, which often fails when the exact match is missing, A-RAG‚Äôs sub-query generation and iterative re-ranking modules better synthesize partial context across sources, producing correct responses even when the originating chunk is not directly retrieved. In the next section we evaluate answer quality using semantic accuracy , defined as the mean LLM-judge score measuring semantic equivalence between a system‚Äôs answer and the human ground-truth answer, independent of surface lexical overlap.\\nBuilding on the retrieval metrics in Table I (Section V-B ), we next measure how faithfully each pipeline answers user questions. Rather than lexical overlap, we target semantic agreement with the ground-truth answers created in Section IV .\\nWe define the following metric to calculate the semantic accuracy. Let N N denote the total number of evaluated questions (here N = 85 N=85 ). For every question q i q_{i} , an external vLLM-hosted judge ( Llama-3.1-8B ) assigns an integer score s i ‚àà { 1 , ‚Ä¶ , 10 } s_{i}\\\\in\\\\{1,\\\\ldots,10\\\\} using the rubric in Table II . The semantic-accuracy metric is the mean judge score:\\nApplying this metric to our evaluation set reveals that the A-RAG pipeline outperforms the baseline B-RAG. A-RAG achieves a mean score of s ¬Ø = 7.04 \\\\bar{s}=7.04 , compared to s ¬Ø = 6.35 \\\\bar{s}=6.35 for B-RAG, yielding a performance gain of Œî \\u200b s ¬Ø ‚âà 0.68 \\\\Delta\\\\bar{s}\\\\approx 0.68 .\\nAs shown in Figure 6 , A-RAG notably reduces the frequency of low-quality answers ( s < 5 s{<}5 ) and increases the proportion of responses in the top rubric tier ( s ‚â• 9 s\\\\geq 9 ). The per-question score deltas (Figure 7 ) show that A-RAG is preferred in 64\\u2009% of cases, ties in 25\\u2009% and is outperformed by B-RAG in only 11\\u2009%. Even in those rare cases, the largest drop is just 3 points, indicating that A-RAG rarely degrades answer quality.\\nWe further evaluate both systems using a human-curated benchmark consisting of 17 questions, each designed to allow multiple plausible answers. The benchmark comprises 9 definitional questions, 4 procedural questions, and 4 acronym-based questions. Collectively, these questions are associated with 33 distinct ground-truth source links. In addition to assessing semantic accuracy, we introduce coverage as a complementary metric:\\nwhere ùí¢ \\\\mathcal{G} is the set of 33 distinct ground-truth source links and ‚Ñõ \\\\mathcal{R} the subset retrieved within the top‚Äì5 across all questions. B-RAG achieves 66.67% (22/33), while A-RAG attains 69.70% (23/33). This result indicates a modest advantage in aggregating distributed evidence.',\n",
       "    'subsections': [{'title': 'V-A Experimental Setup',\n",
       "      'paragraphs': 'We compared B-RAG and A-RAG pipelines on a specialized internal fintech knowledge base. Both pipelines used the same LLM backend, Llama-3.1-8B-Instruct , served via a vLLM endpoint. Document embeddings were generated using the all-MiniLM-L6-v2 model and stored in ChromaDB. We evaluated both systems using two key metrics: (1) Retrieval Accuracy (Hit Rate @5), defined as the percentage of questions for which the correct document source appeared among the top five retrieved links; and (2) Average Latency, defined as the time from user query submission to the final system response.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-B Quantitative Results',\n",
       "      'paragraphs': 'Table I summarizes the results. The A-RAG system achieved a strict retrieval accuracy of 62.35%, outperforming the Baseline‚Äôs 54.12%. This performance gain can be attributed to the system‚Äôs specialized agents for acronym resolution and sub-query expansion, which were designed to address the fragmented and semantically sparse nature of enterprise knowledge described in Section I . As expected, A-RAG‚Äôs average query latency was 5.02 seconds, significantly higher than B-RAG‚Äôs 0.79 seconds, due to multi-stage processing and iterative document re-ranking.\\nAlthough retrieval was evaluated using exact link matches, manual inspection identified several queries where the system returned correct content from semantically equivalent but non-identical documents. Specifically, A-RAG retrieved valid answers from alternate sources in six additional cases, and B-RAG in three. Incorporating these, the adjusted retrieval accuracy increases to 69.41% for A-RAG and 58.82% for B-RAG. These cases reinforce the core challenge introduced in Section I ; that enterprise knowledge in fintech domains is often fragmented, with key information distributed across related documents. Unlike B-RAG, which often fails when the exact match is missing, A-RAG‚Äôs sub-query generation and iterative re-ranking modules better synthesize partial context across sources, producing correct responses even when the originating chunk is not directly retrieved. In the next section we evaluate answer quality using semantic accuracy , defined as the mean LLM-judge score measuring semantic equivalence between a system‚Äôs answer and the human ground-truth answer, independent of surface lexical overlap.',\n",
       "      'subsections': [{'title': 'Adjusted Retrieval Interpretation',\n",
       "        'paragraphs': 'Although retrieval was evaluated using exact link matches, manual inspection identified several queries where the system returned correct content from semantically equivalent but non-identical documents. Specifically, A-RAG retrieved valid answers from alternate sources in six additional cases, and B-RAG in three. Incorporating these, the adjusted retrieval accuracy increases to 69.41% for A-RAG and 58.82% for B-RAG. These cases reinforce the core challenge introduced in Section I ; that enterprise knowledge in fintech domains is often fragmented, with key information distributed across related documents. Unlike B-RAG, which often fails when the exact match is missing, A-RAG‚Äôs sub-query generation and iterative re-ranking modules better synthesize partial context across sources, producing correct responses even when the originating chunk is not directly retrieved. In the next section we evaluate answer quality using semantic accuracy , defined as the mean LLM-judge score measuring semantic equivalence between a system‚Äôs answer and the human ground-truth answer, independent of surface lexical overlap.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'V-C Answer-Quality Evaluation (Semantic Accuracy)',\n",
       "      'paragraphs': 'Building on the retrieval metrics in Table I (Section V-B ), we next measure how faithfully each pipeline answers user questions. Rather than lexical overlap, we target semantic agreement with the ground-truth answers created in Section IV .\\nWe define the following metric to calculate the semantic accuracy. Let N N denote the total number of evaluated questions (here N = 85 N=85 ). For every question q i q_{i} , an external vLLM-hosted judge ( Llama-3.1-8B ) assigns an integer score s i ‚àà { 1 , ‚Ä¶ , 10 } s_{i}\\\\in\\\\{1,\\\\ldots,10\\\\} using the rubric in Table II . The semantic-accuracy metric is the mean judge score:\\nApplying this metric to our evaluation set reveals that the A-RAG pipeline outperforms the baseline B-RAG. A-RAG achieves a mean score of s ¬Ø = 7.04 \\\\bar{s}=7.04 , compared to s ¬Ø = 6.35 \\\\bar{s}=6.35 for B-RAG, yielding a performance gain of Œî \\u200b s ¬Ø ‚âà 0.68 \\\\Delta\\\\bar{s}\\\\approx 0.68 .\\nAs shown in Figure 6 , A-RAG notably reduces the frequency of low-quality answers ( s < 5 s{<}5 ) and increases the proportion of responses in the top rubric tier ( s ‚â• 9 s\\\\geq 9 ). The per-question score deltas (Figure 7 ) show that A-RAG is preferred in 64\\u2009% of cases, ties in 25\\u2009% and is outperformed by B-RAG in only 11\\u2009%. Even in those rare cases, the largest drop is just 3 points, indicating that A-RAG rarely degrades answer quality.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-D Human-Curated Benchmark',\n",
       "      'paragraphs': 'We further evaluate both systems using a human-curated benchmark consisting of 17 questions, each designed to allow multiple plausible answers. The benchmark comprises 9 definitional questions, 4 procedural questions, and 4 acronym-based questions. Collectively, these questions are associated with 33 distinct ground-truth source links. In addition to assessing semantic accuracy, we introduce coverage as a complementary metric:\\nwhere ùí¢ \\\\mathcal{G} is the set of 33 distinct ground-truth source links and ‚Ñõ \\\\mathcal{R} the subset retrieved within the top‚Äì5 across all questions. B-RAG achieves 66.67% (22/33), while A-RAG attains 69.70% (23/33). This result indicates a modest advantage in aggregating distributed evidence.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'VI Discussion',\n",
       "    'paragraphs': 'The results indicate that the A-RAG system achieved measurable improvements in retrieval accuracy over the Baseline, confirming our hypothesis that agentic decomposition strategies are beneficial in fragmented, domain-specific settings. The strict accuracy gain (62.35% vs. 54.12%) is further supported by manual review, where A-RAG often retrieved semantically correct information from alternative sources not marked as ground truth. These observations align with the core challenge described in Section I: enterprise fintech documents are semantically sparse and often distribute relevant information across multiple partially redundant pages.\\nQualitative analysis revealed that the most effective agentic component was sub-query generation, which enabled targeted exploration of edge cases where the initial query lacked specificity. Acronym resolution proved more error-prone: in cases of ambiguous or undefined acronyms, the agent occasionally surfaced overly generic sources. The cross-encoder re-ranking improved relevance when multiple near-matches were retrieved but introduced latency. These trade-offs confirm that while A-RAG design introduces computational overhead, its modular structure is more robust to domain-specific ambiguity than the static B-RAG pipeline.\\nAs shown in the human-curated benchmark (Table III ), A-RAG achieves higher overall coverage (69.70% vs. 66.67%) and a modest improvement in semantic accuracy (8.06 vs. 7.88), indicating enhanced consolidation of dispersed evidence. The gains are most evident in procedural queries, where coverage reaches 100% and semantic quality rises to 8.25. This suggests that iterative sub-query expansion is particularly effective when task steps are implicit. In contrast, acronym and definitional queries do not show consistent improvements‚Äîacronym coverage declines (57.14% to 42.85%), and definitional coverage decreases slightly. These outcomes imply that acronym resolution and re-ranking mechanisms may occasionally over-filter or misprioritize near-duplicate sources. Nevertheless, the identical acronym semantic scores (8.25) suggest that A-RAG can synthesize sufficient context from partial retrieval. Overall, the ‚Äúmany plausible answers‚Äù setting underscores both the robustness advantages of agentic orchestration and specific limitations (e.g., acronym recall) that warrant further investigation.\\nThe A-RAG pipeline explicitly handled two challenges central to fintech ontologies: acronym disambiguation and context fragmentation. By integrating an acronym helper agent and embedding definitions where available, the system was able to expand domain-specific abbreviations during both query reformulation and document re-ranking. However, this component showed limitations in edge cases where acronyms were neither defined nor contextually grounded in the document. In contrast, B-RAG relied entirely on surface-level keyword overlap, often missing results that required any interpretive inference.\\nSub-query phrasing also improved navigation through the ontology by decomposing vague or overloaded user queries into focused, domain-aligned sub-tasks. This was impactful for queries that implicitly referenced process hierarchies or tools‚Äîcommon in enterprise documentation. Overall, the A-RAG system demonstrated a stronger ability to operate within the constraints of a specialized ontology, especially when semantic cues in the documents were minimal or inconsistently structured.\\nThe study is constrained by several factors. Firstly, the evaluation dataset is relatively small (85 questions) and may not reflect the full spectrum of real-world queries or document structures in fintech. Secondly, all results are grounded in the use of a single LLM ( Llama-3.1-8B-Instruct ) and embedding model, limiting generalization across architectures. Thirdly, certain agents (e.g., acronym resolver) rely on heuristic definitions and simple regex-based expansion, which may under perform in more complex acronym usage. Lastly, while adjusted accuracy was discussed, it is not backed by a formal metric; a more rigorous semantic equivalence measure should be incorporated in future work to better capture retrieval performance in fragmented corpora.',\n",
       "    'subsections': [{'title': 'VI-A Interpretation of Results',\n",
       "      'paragraphs': 'The results indicate that the A-RAG system achieved measurable improvements in retrieval accuracy over the Baseline, confirming our hypothesis that agentic decomposition strategies are beneficial in fragmented, domain-specific settings. The strict accuracy gain (62.35% vs. 54.12%) is further supported by manual review, where A-RAG often retrieved semantically correct information from alternative sources not marked as ground truth. These observations align with the core challenge described in Section I: enterprise fintech documents are semantically sparse and often distribute relevant information across multiple partially redundant pages.\\nQualitative analysis revealed that the most effective agentic component was sub-query generation, which enabled targeted exploration of edge cases where the initial query lacked specificity. Acronym resolution proved more error-prone: in cases of ambiguous or undefined acronyms, the agent occasionally surfaced overly generic sources. The cross-encoder re-ranking improved relevance when multiple near-matches were retrieved but introduced latency. These trade-offs confirm that while A-RAG design introduces computational overhead, its modular structure is more robust to domain-specific ambiguity than the static B-RAG pipeline.\\nAs shown in the human-curated benchmark (Table III ), A-RAG achieves higher overall coverage (69.70% vs. 66.67%) and a modest improvement in semantic accuracy (8.06 vs. 7.88), indicating enhanced consolidation of dispersed evidence. The gains are most evident in procedural queries, where coverage reaches 100% and semantic quality rises to 8.25. This suggests that iterative sub-query expansion is particularly effective when task steps are implicit. In contrast, acronym and definitional queries do not show consistent improvements‚Äîacronym coverage declines (57.14% to 42.85%), and definitional coverage decreases slightly. These outcomes imply that acronym resolution and re-ranking mechanisms may occasionally over-filter or misprioritize near-duplicate sources. Nevertheless, the identical acronym semantic scores (8.25) suggest that A-RAG can synthesize sufficient context from partial retrieval. Overall, the ‚Äúmany plausible answers‚Äù setting underscores both the robustness advantages of agentic orchestration and specific limitations (e.g., acronym recall) that warrant further investigation.',\n",
       "      'subsections': []},\n",
       "     {'title': 'VI-B Addressing Domain-Specific Ontology',\n",
       "      'paragraphs': 'The A-RAG pipeline explicitly handled two challenges central to fintech ontologies: acronym disambiguation and context fragmentation. By integrating an acronym helper agent and embedding definitions where available, the system was able to expand domain-specific abbreviations during both query reformulation and document re-ranking. However, this component showed limitations in edge cases where acronyms were neither defined nor contextually grounded in the document. In contrast, B-RAG relied entirely on surface-level keyword overlap, often missing results that required any interpretive inference.\\nSub-query phrasing also improved navigation through the ontology by decomposing vague or overloaded user queries into focused, domain-aligned sub-tasks. This was impactful for queries that implicitly referenced process hierarchies or tools‚Äîcommon in enterprise documentation. Overall, the A-RAG system demonstrated a stronger ability to operate within the constraints of a specialized ontology, especially when semantic cues in the documents were minimal or inconsistently structured.',\n",
       "      'subsections': []},\n",
       "     {'title': 'VI-C Limitations of the Current Study',\n",
       "      'paragraphs': 'The study is constrained by several factors. Firstly, the evaluation dataset is relatively small (85 questions) and may not reflect the full spectrum of real-world queries or document structures in fintech. Secondly, all results are grounded in the use of a single LLM ( Llama-3.1-8B-Instruct ) and embedding model, limiting generalization across architectures. Thirdly, certain agents (e.g., acronym resolver) rely on heuristic definitions and simple regex-based expansion, which may under perform in more complex acronym usage. Lastly, while adjusted accuracy was discussed, it is not backed by a formal metric; a more rigorous semantic equivalence measure should be incorporated in future work to better capture retrieval performance in fragmented corpora.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'VII Conclusion and Future Work',\n",
       "    'paragraphs': 'This study investigated whether a modular, agent-driven RAG pipeline could more effectively navigate fragmented enterprise knowledge bases in the fintech domain. The proposed A-RAG system incorporated acronym resolution, subquery decomposition, and document reranking agents to enable more adaptive retrieval strategies, yielding a measurable improvement in strict retrieval accuracy (62.35% vs. 54.12%) over a standard baseline, and rising to 69.41% when accounting for semantically relevant but non-ground-truth sources.\\nLooking ahead, future work could explore more principled design strategies for agent coordination, such as reinforcement learning or meta-controller frameworks that dynamically adapt the agent composition based on query type or retrieval feedback. Additionally, integrating stronger context-awareness‚Äîsuch as discourse-level tracking, temporal grounding, or multi-turn memory could help align answers more closely with user intent. Enhancing the reasoning capabilities of the pipeline remains an open challenge, particularly in producing not just faithful but helpful responses that satisfy nuanced informational needs. Techniques such as agent self-critique, counterfactual retrieval, or reflection-based loops may offer promising paths forward. We expect this work contribute a meaningful step toward more robust and interpretable retrieval-augmented systems in high-stakes, domain-specific applications.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialised agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question‚Äìanswer‚Äìreference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.'},\n",
       " '2510.24832v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Advancing the complex reasoning capabilities of Large Language Models (LLMs) remains a significant challenge, particularly in domains like mathematical problem-solving. Reinforcement Learning with Verifiable Reward (RLVR) (Gao et\\xa0al., 2024 ; DeepSeek-AI et\\xa0al., 2025 ) , especially through policy optimization methods like GRPO (Shao et\\xa0al., 2024 ) , has emerged as a powerful paradigm to address this challenge.\\nAs shown in Figure 1 (a), in this framework, the space of potential solution paths for a query can be modeled as a specific ‚ÄòReasoning Tree‚Äô (Wang et\\xa0al., 2025 ; Yang et\\xa0al., 2025 ) , where each node represents an intermediate reasoning step and each path represents a potential solution trajectory.\\nFrom this perspective, RLVR operates as a dynamic ‚Äònode-editing‚Äô process of the reasoning tree: by rewarding correct paths and penalizing incorrect ones, the model iteratively refines its decision policy at each tree node.\\nThis optimization process gradually prunes branches that lead to low-quality or incorrect solutions, thereby improving overall reasoning accuracy.\\nIn this paradigm, data scheduling plays a critical role in model performance (Hu et\\xa0al., 2025 ; Yu et\\xa0al., 2025 ; Li et\\xa0al., 2025 ) .\\nThe concept of data scheduling originates from curriculum learning (Bengio et\\xa0al., 2009 ) ,\\nwhich posits that models learn more effectively when training examples (queries) are organized in a meaningful sequence. Existing data scheduling strategies typically pre-define a‚Äòdifficulty‚Äô metric for queries, and and schedule them from easy to hard to improve data efficiency and final performance (Xi et\\xa0al., 2024 ; Chen et\\xa0al., 2025b ; a ; Dai et\\xa0al., 2025 ) However, from a reasoning tree perspective, current difficulty measure strategies exhibits a critical limitation: current methods estimate difficulty primarily via final solution accuracy, overlooking richer query-level characteristics such as the structural complexity of the reasoning tree. Accuracy alone is insufficient ‚Äî low accuracy does not necessarily indicate that a query is inherently hard, and high accuracy does not guarantee ease of optimization. This inconsistency can undermine the efficacy of accuracy-based scheduling approaches. We illustrate this issue with the following examples.\\nTo illustrate, consider two representative queries, q1 and q2 , whose reasoning trees are shown in Figure 1 (a). As depicted in Figure 1 (b), LLMs may exhibit low initial accuracy on q1 , due to the presence of many incorrect solution trajectories (reasoning paths). However, its simple tree structure means that modifying a few key decision nodes can yield substantial accuracy gains, indicating high learning efficiency despite the poor initial performance. In contrast, q2 achieves higher initial accuracy, with roughly half of its trajectories being correct, yet these correct paths are scattered across disparate subtrees. This fragmented structure requires more extensive edits across numerous tree nodes, typically resulting in higher training difficulty and lower learning efficiency.\\nCritically, existing path-based metrics will misinterpret q1 ‚Äôs low accuracy as high difficulty, thus assigning it a lower training weight, while incorrectly prioritizing the more difficult q2 . Such path-based metrics may lead to a less efficient training process.\\nThis motivates our central research question: How can we move beyond path-based metrics to directly quantify a query‚Äôs true learning difficulty from its reasoning-tree structure?\\nTo address this question, we introduce the Reasoning Score (r-score), a novel metric that quantifies a query‚Äôs learning potential based on its reasoning tree structure. We formalize this by framing the reinforcement learning training process as an optimization problem under a finite ‚Äônode editing budget‚Äô, which we define as a fixed number of node editing operations. Consequently, a query‚Äôs r-score is its maximum potential accuracy gain achievable within this limited editing budget. This metric clearly explains the discrepancy in our example: q1 , with its ‚Äòconcentrated‚Äô error structure, yields a high r-score because a small budget (e.g., two edits) produces a massive accuracy gain (+75%). Conversely, q2 ‚Äôs ‚Äòdiffuse‚Äô structure results in a low r-score, as the same budget only yields marginal improvement (+25%). Therefore, a higher r-score signifies a more tractable reasoning structure and greater learning efficiency, offering a more comprehensive assessment of difficulty than path-based metrics.\\nBuilding on the Reasoning Score, we propose the R easoning Tr e e Schedule (Re-Schedule) , a novel data scheduling algorithm designed to guide RLVR training more efficiently. Our method consists of three main stages. First, an offline approximation of each query‚Äôs reasoning tree is constructed by sampling multiple solution trajectories from a base model. Second, this approximated reasoning tree is used to calculate each query‚Äôs reasoning score by simulating the editing process. Finally, we integrate the r-score as a dynamic weight into the RLVR loss function to form a schedule. This schedule prioritizes high-scoring (simple) queries in the initial training phases to accelerate convergence on simple queries. As training progresses, the weighting gradually shifts to lower-scoring (difficult) queries, enabling the model to master more challenging problems.\\nIn summary, the main contributions of this paper are:\\nWe introduce the Reasoning Score (r-score), a new tree-based metric that measures a query‚Äôs learning efficiency rather than its path-based solution accuracy.\\nWe propose Re-Schedule, a data scheduling algorithm that uses the r-score to create an effective, easy-to-hard curriculum for RLVR.\\nAs shown in Figure 1 (c), we empirically demonstrate that our approach significantly improves average accuracy, achieving gains of up to 3.2%, on complex reasoning tasks.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Reinforcement learning with verifiable reward (RLVR), where the reward is computed by a rule-based verification function, has been shown to be effective in improving the reasoning capabilities of LLMs (Gao et\\xa0al., 2024 ; DeepSeek-AI et\\xa0al., 2025 ; Kimi et\\xa0al., 2025 ; Zeng et\\xa0al., 2025 ; Wen et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) .\\nTypically, RLVR frameworks assign a binary reward by comparing the model‚Äôs generated output against a ground-truth solution, indicating whether it is correct or incorrect. This reward design obviates the need for complex outcome-based or process-based reward models, offering a straightforward yet potent approach. Recent advancements in policy optimization algorithms, such as PPO and GRPO, have further refined this paradigm (Schulman et\\xa0al., 2017 ; Kazemnejad et\\xa0al., 2024 ; Yuan et\\xa0al., 2025 ; Yue et\\xa0al., 2025 ; Shao et\\xa0al., 2024 ; Yu et\\xa0al., 2025 ; Liu et\\xa0al., 2025 ; Zhang et\\xa0al., 2025 ; Hu, 2025 ) .\\nIn contrast to these studies, which focus on algorithmic improvements, our work builds upon the standard GRPO framework with a primary focus on designing a more effective training data schedule.\\nVarious data scheduling strategies have been proposed to enhance the reasoning capabilities in LLM Reinforcement Learning. These can be broadly categorized into static selection and dynamic adjustment methods.\\nRepresentative of static selection is LIMR (Li et\\xa0al., 2025 ) , which selected 1.4k examples from an 8.5k set for RLVR to match the performance of using the full set.\\nIn contrast, dynamic strategies make real-time adjustments during training.\\nFor instance, R 3 R^{3} employs reverse curriculum reinforcement learning to simplify the model‚Äôs exploration space (Xi et\\xa0al., 2024 ) .\\nLPPO dynamically adjusts data weights based on model accuracy and provides online ‚Äùprompts‚Äù for challenging problems to guide exploration (Chen et\\xa0al., 2025b ) .\\nSEED-GRPO focuses on model uncertainty (Chen et\\xa0al., 2025a ) , using token entropy to dynamically re-weight data.\\nFurthermore, DELT leverages training gradients to measure the quality and learnability of data (Dai et\\xa0al., 2025 ) , subsequently adjusting sample weights.\\nHowever, these existing methods primarily rely on metrics such as accuracy and diversity, which overlook the structural relationships among data samples.\\nIn contrast, our work aims to quantify the learning potential of a query by analyzing its reasoning tree.',\n",
       "    'subsections': [{'title': '2.1 Reinforcement learning with verifiable rewards in LLMs',\n",
       "      'paragraphs': 'Reinforcement learning with verifiable reward (RLVR), where the reward is computed by a rule-based verification function, has been shown to be effective in improving the reasoning capabilities of LLMs (Gao et\\xa0al., 2024 ; DeepSeek-AI et\\xa0al., 2025 ; Kimi et\\xa0al., 2025 ; Zeng et\\xa0al., 2025 ; Wen et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) .\\nTypically, RLVR frameworks assign a binary reward by comparing the model‚Äôs generated output against a ground-truth solution, indicating whether it is correct or incorrect. This reward design obviates the need for complex outcome-based or process-based reward models, offering a straightforward yet potent approach. Recent advancements in policy optimization algorithms, such as PPO and GRPO, have further refined this paradigm (Schulman et\\xa0al., 2017 ; Kazemnejad et\\xa0al., 2024 ; Yuan et\\xa0al., 2025 ; Yue et\\xa0al., 2025 ; Shao et\\xa0al., 2024 ; Yu et\\xa0al., 2025 ; Liu et\\xa0al., 2025 ; Zhang et\\xa0al., 2025 ; Hu, 2025 ) .\\nIn contrast to these studies, which focus on algorithmic improvements, our work builds upon the standard GRPO framework with a primary focus on designing a more effective training data schedule.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Data Scheduling Algorithm in LLM Reinforcement Learning',\n",
       "      'paragraphs': 'Various data scheduling strategies have been proposed to enhance the reasoning capabilities in LLM Reinforcement Learning. These can be broadly categorized into static selection and dynamic adjustment methods.\\nRepresentative of static selection is LIMR (Li et\\xa0al., 2025 ) , which selected 1.4k examples from an 8.5k set for RLVR to match the performance of using the full set.\\nIn contrast, dynamic strategies make real-time adjustments during training.\\nFor instance, R 3 R^{3} employs reverse curriculum reinforcement learning to simplify the model‚Äôs exploration space (Xi et\\xa0al., 2024 ) .\\nLPPO dynamically adjusts data weights based on model accuracy and provides online ‚Äùprompts‚Äù for challenging problems to guide exploration (Chen et\\xa0al., 2025b ) .\\nSEED-GRPO focuses on model uncertainty (Chen et\\xa0al., 2025a ) , using token entropy to dynamically re-weight data.\\nFurthermore, DELT leverages training gradients to measure the quality and learnability of data (Dai et\\xa0al., 2025 ) , subsequently adjusting sample weights.\\nHowever, these existing methods primarily rely on metrics such as accuracy and diversity, which overlook the structural relationships among data samples.\\nIn contrast, our work aims to quantify the learning potential of a query by analyzing its reasoning tree.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Preliminaries',\n",
       "    'paragraphs': 'The objective of the GRPO algorithm is to optimize a policy œÄ Œ∏ \\\\pi_{\\\\theta} based on a group of generated responses (Shao et\\xa0al., 2024 ; Yu et\\xa0al., 2025 ) . For a query q q from a dataset ùíü \\\\mathcal{D} , the policy generates G G responses { o i } i = 1 G \\\\{o_{i}\\\\}_{i=1}^{G} . The token-level objective function is formulated as:\\nwhere r i , t = œÄ Œ∏ \\u200b ( o i , t | q , o i , < t ) œÄ old \\u200b ( o i , t | q , o i , < t ) r_{i,t}=\\\\frac{\\\\pi_{\\\\theta}(o_{i,t}|q,o_{i,<t})}{\\\\pi_{\\\\text{old}}(o_{i,t}|q,o_{i,<t})} is the probability ratio of the token o i , t o_{i,t} between the current and old policies. The advantage term A i , t A_{i,t} is constant for all tokens within a single response and is calculated by normalizing the response‚Äôs reward R i R_{i} relative to the other responses in the group:\\nwhere Œ¥ \\\\delta is a small constant for numerical stability.\\nData scheduling algorithms can be formulated by introducing a weighting function œâ \\u200b ( q , t ) \\\\omega(q,t) that modulates the contribution of each query q ‚àà ùíü q\\\\in\\\\mathcal{D} and current epoch t t to the overall objective.\\nSpecifically, the objective in Equation 1 is modified as follows:\\nNote: In the equations above, we have abbreviated the full objective for clarity.\\nFor example, in an accuracy-based curriculum learning, the training weight œâ \\\\omega is formulated as a function of the query‚Äôs accuracy ACC \\u200b ( q ) \\\\text{ACC}(q) and current epoch t t :\\nHere, œâ max \\\\omega_{\\\\text{max}} and œâ min \\\\omega_{\\\\text{min}} are hyperparameters that define the maximum and minimum training weights (e.g., œâ max = 0.8 \\\\omega_{\\\\text{max}}=0.8 , œâ min = 0.2 \\\\omega_{\\\\text{min}}=0.2 );\\nAnd rank \\u200b ( Œ± ) \\\\text{rank}(\\\\alpha) means calculating the reverse order of Œ± \\\\alpha in the entire dataset.\\nThe term Œ≥ \\u200b ( t ) \\\\gamma(t) is a scheduling function that progresses over time.\\nCommon choices for Œ≥ \\u200b ( t ) \\\\gamma(t) include a linear mapping, Œ≥ \\u200b ( t ) = t / T \\\\gamma(t)=t/T , or a sigmoid function, Œ≥ \\u200b ( t ) = œÉ \\u200b ( ( t T ‚àí 0.5 ) ) \\\\gamma(t)=\\\\sigma\\\\left(\\\\left(\\\\frac{t}{T}-0.5\\\\right)\\\\right) , œÉ \\u200b ( x ) = ( 1 + e ‚àí x ) ‚àí 1 \\\\sigma(x)=(1+e^{-x})^{-1} , where T T is the total number of epochs.\\nFor complex reasoning tasks, the process of generating a solution can be conceptualized as traversing a ‚ÄòReasoning Tree‚Äô. In this context, the root of the tree is the initial prompt, and each node represents a partial solution or an intermediate reasoning step. The branches extending from a node correspond to the possible next tokens or thought segments that the LLM can generate.\\nDue to the combinatorial explosion of possible solution paths, the complete reasoning tree is typically computationally intractable. Therefore, analysis often relies on a structured approximation (e.g., a fixed-structure k-ary reasoning tree). Formally, an approximated reasoning tree is defined as a triplet T = ( ùí© , ‚Ñ∞ , ‚Ñõ ) T=(\\\\mathcal{N},\\\\mathcal{E},\\\\mathcal{R}) , where ùí© \\\\mathcal{N} is the set of nodes, ‚Ñ∞ \\\\mathcal{E} is the set of edges, and ‚Ñõ \\\\mathcal{R} defines the parent-child relationships.\\nThe components of the tree are described using the following notation: ùí© leaf ‚äÇ ùí© \\\\mathcal{N}_{\\\\text{leaf}}\\\\subset\\\\mathcal{N} is the set of leaf nodes;\\nFor a given node n i ‚àà ùí© n_{i}\\\\in\\\\mathcal{N} , C \\u200b ( n i ) C(n_{i}) denotes the set of its immediate children and ‚Ñí \\u200b ( n i ) \\\\mathcal{L}(n_{i}) denotes the set of its leaf descendants.\\nIf n i n_{i} is a leaf node, then ‚Ñí \\u200b ( n i ) = { n i } \\\\mathcal{L}(n_{i})=\\\\{n_{i}\\\\} .\\nWithin this framework, each non-leaf node n i ‚àà ùí© ‚àñ ùí© leaf n_{i}\\\\in\\\\mathcal{N}\\\\setminus\\\\mathcal{N}_{\\\\text{leaf}} represents a partial reasoning path, while a complete path to a leaf node n j ‚àà ùí© leaf n_{j}\\\\in\\\\mathcal{N}_{\\\\text{leaf}} corresponds to a full solution trajectory.\\nFrom this perspective, the RLVR optimization process is a dynamic ‚Äònode editing‚Äô of this reasoning tree.\\nBy rewarding correct paths and penalizing incorrect ones, the policy optimization algorithm adjusts the token probabilities at each node, effectively strengthening the branches that lead to correct answers and weakening those that lead to errors.\\nThe structure of this tree‚Äîthe distribution of correct and incorrect paths‚Äîis intrinsic to each problem sample and, as we will argue, is a key clue to its learning dynamics.',\n",
       "    'subsections': [{'title': '3.1 Group Relative Policy Optimization',\n",
       "      'paragraphs': 'The objective of the GRPO algorithm is to optimize a policy œÄ Œ∏ \\\\pi_{\\\\theta} based on a group of generated responses (Shao et\\xa0al., 2024 ; Yu et\\xa0al., 2025 ) . For a query q q from a dataset ùíü \\\\mathcal{D} , the policy generates G G responses { o i } i = 1 G \\\\{o_{i}\\\\}_{i=1}^{G} . The token-level objective function is formulated as:\\nwhere r i , t = œÄ Œ∏ \\u200b ( o i , t | q , o i , < t ) œÄ old \\u200b ( o i , t | q , o i , < t ) r_{i,t}=\\\\frac{\\\\pi_{\\\\theta}(o_{i,t}|q,o_{i,<t})}{\\\\pi_{\\\\text{old}}(o_{i,t}|q,o_{i,<t})} is the probability ratio of the token o i , t o_{i,t} between the current and old policies. The advantage term A i , t A_{i,t} is constant for all tokens within a single response and is calculated by normalizing the response‚Äôs reward R i R_{i} relative to the other responses in the group:\\nwhere Œ¥ \\\\delta is a small constant for numerical stability.\\nData scheduling algorithms can be formulated by introducing a weighting function œâ \\u200b ( q , t ) \\\\omega(q,t) that modulates the contribution of each query q ‚àà ùíü q\\\\in\\\\mathcal{D} and current epoch t t to the overall objective.\\nSpecifically, the objective in Equation 1 is modified as follows:\\nNote: In the equations above, we have abbreviated the full objective for clarity.\\nFor example, in an accuracy-based curriculum learning, the training weight œâ \\\\omega is formulated as a function of the query‚Äôs accuracy ACC \\u200b ( q ) \\\\text{ACC}(q) and current epoch t t :\\nHere, œâ max \\\\omega_{\\\\text{max}} and œâ min \\\\omega_{\\\\text{min}} are hyperparameters that define the maximum and minimum training weights (e.g., œâ max = 0.8 \\\\omega_{\\\\text{max}}=0.8 , œâ min = 0.2 \\\\omega_{\\\\text{min}}=0.2 );\\nAnd rank \\u200b ( Œ± ) \\\\text{rank}(\\\\alpha) means calculating the reverse order of Œ± \\\\alpha in the entire dataset.\\nThe term Œ≥ \\u200b ( t ) \\\\gamma(t) is a scheduling function that progresses over time.\\nCommon choices for Œ≥ \\u200b ( t ) \\\\gamma(t) include a linear mapping, Œ≥ \\u200b ( t ) = t / T \\\\gamma(t)=t/T , or a sigmoid function, Œ≥ \\u200b ( t ) = œÉ \\u200b ( ( t T ‚àí 0.5 ) ) \\\\gamma(t)=\\\\sigma\\\\left(\\\\left(\\\\frac{t}{T}-0.5\\\\right)\\\\right) , œÉ \\u200b ( x ) = ( 1 + e ‚àí x ) ‚àí 1 \\\\sigma(x)=(1+e^{-x})^{-1} , where T T is the total number of epochs.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Reasoning Tree',\n",
       "      'paragraphs': 'For complex reasoning tasks, the process of generating a solution can be conceptualized as traversing a ‚ÄòReasoning Tree‚Äô. In this context, the root of the tree is the initial prompt, and each node represents a partial solution or an intermediate reasoning step. The branches extending from a node correspond to the possible next tokens or thought segments that the LLM can generate.\\nDue to the combinatorial explosion of possible solution paths, the complete reasoning tree is typically computationally intractable. Therefore, analysis often relies on a structured approximation (e.g., a fixed-structure k-ary reasoning tree). Formally, an approximated reasoning tree is defined as a triplet T = ( ùí© , ‚Ñ∞ , ‚Ñõ ) T=(\\\\mathcal{N},\\\\mathcal{E},\\\\mathcal{R}) , where ùí© \\\\mathcal{N} is the set of nodes, ‚Ñ∞ \\\\mathcal{E} is the set of edges, and ‚Ñõ \\\\mathcal{R} defines the parent-child relationships.\\nThe components of the tree are described using the following notation: ùí© leaf ‚äÇ ùí© \\\\mathcal{N}_{\\\\text{leaf}}\\\\subset\\\\mathcal{N} is the set of leaf nodes;\\nFor a given node n i ‚àà ùí© n_{i}\\\\in\\\\mathcal{N} , C \\u200b ( n i ) C(n_{i}) denotes the set of its immediate children and ‚Ñí \\u200b ( n i ) \\\\mathcal{L}(n_{i}) denotes the set of its leaf descendants.\\nIf n i n_{i} is a leaf node, then ‚Ñí \\u200b ( n i ) = { n i } \\\\mathcal{L}(n_{i})=\\\\{n_{i}\\\\} .\\nWithin this framework, each non-leaf node n i ‚àà ùí© ‚àñ ùí© leaf n_{i}\\\\in\\\\mathcal{N}\\\\setminus\\\\mathcal{N}_{\\\\text{leaf}} represents a partial reasoning path, while a complete path to a leaf node n j ‚àà ùí© leaf n_{j}\\\\in\\\\mathcal{N}_{\\\\text{leaf}} corresponds to a full solution trajectory.\\nFrom this perspective, the RLVR optimization process is a dynamic ‚Äònode editing‚Äô of this reasoning tree.\\nBy rewarding correct paths and penalizing incorrect ones, the policy optimization algorithm adjusts the token probabilities at each node, effectively strengthening the branches that lead to correct answers and weakening those that lead to errors.\\nThe structure of this tree‚Äîthe distribution of correct and incorrect paths‚Äîis intrinsic to each problem sample and, as we will argue, is a key clue to its learning dynamics.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Motivation',\n",
       "    'paragraphs': 'The premise of this work is that path-based metrics such as accuracy are poor indicators of a query‚Äôs true learning difficulty. To illustrate our point, we supplement the example from the introduction with an experiment.\\nAs shown in Figure 2 , we selected two distinct sets of 100 queries each from the DAPA-Math-17K dataset, using the Qwen2.5-Math-7B model.\\nThe blue line represents queries that have high initial accuracy but require extensive training to show further improvement.\\nIn contrast, the red line, standing for queries with simple tree structures, begins with low accuracy but improves rapidly.\\nAlthough queries for the blue line appear ‚Äòeasier‚Äô based on their initial accuracy, queries for the red line demonstrate greater potential.\\nThis discrepancy highlights that path-based metrics, like accuracy, are biased measurements for learning difficulty. This finding motivates us to design a new metric based on the structure of the reasoning tree.',\n",
       "    'subsections': []},\n",
       "   {'title': '5 Method',\n",
       "    'paragraphs': 'As illustrated in Figure 3 , the Reasoning Tree Schedule (Re-Schedule) enhances reinforcement learning performance by creating a curriculum based on our novel metric, the Reasoning Score (r-score) .\\nThe r-score quantifies a query‚Äôs learning difficulty a priori based on the structure of its reasoning tree.\\nNext, we will introduce the specific implementation details.\\nAs the entire reasoning tree is computationally intractable, we construct a manageable, fixed-structure k k -ary approximation for each query q q . The structure of this tree, ùíØ \\\\mathcal{T} , is defined by a branching factor k k , a maximum depth d d , and a token interval l l (e.g., k = 4 , d = 4 , l = 200 k=4,d=4,l=200 ).\\nThe construction process begins at the root node (the query q q ) and proceeds via a periodic branching strategy during response generation. Specifically, a branch is triggered immediately at the beginning of the response and subsequently at every l l -token interval. As shown in Figure 3 (a), at each trigger, the current path splits into k k independent sub-paths that continue to generate in parallel. This recursive branching process continues until a predefined maximum depth d d is reached. To minimize computational overhead from this multi-path sampling, we use the Key-Value (KV) Cache, as all sibling branches share the same prefix.\\nIn RLVR tasks, a solution‚Äôs quality is determined by the correctness of its final answer, which corresponds to a leaf node in our framework. Therefore, we define the quality of any intermediate node n i n_{i} as the average accuracy of its leaf descendants, ‚Ñí \\u200b ( n i ) \\\\mathcal{L}(n_{i}) . This is quantified using an accuracy function:\\nwhere S S is a set of leaf nodes and ùïÄ \\u200b ( ‚ãÖ ) \\\\mathbb{I}(\\\\cdot) is the indicator function. This allows us to assess quality at different levels: the quality of a reasoning segment via ACC \\u200b ( ‚Ñí \\u200b ( n i ) ) \\\\text{ACC}(\\\\mathcal{L}(n_{i})) and the model‚Äôs aggregate performance on the query via ACC \\u200b ( ùí© leaf ) \\\\text{ACC}(\\\\mathcal{N}_{\\\\text{leaf}}) .\\nThe r-score quantifies the learning potential of a node or query by measuring the maximum achievable accuracy gain under a limited policy refining cost, like a limited node editing budget.\\nGiven this idea, for any non-leaf node n i n_{i} , we define its r-score, R \\u200b ( n i ) R(n_{i}) , as the maximal accuracy gain achievable by selecting its single best child branch and pruning all others. This is formulated as:\\nThe overall r-score for a query, R \\u200b ( q ) R(q) , estimates the total accuracy gain achievable under a budget that limits modifications to a maximum of M M nodes. It is the maximum sum of r-scores from any set of M M non-conflicting nodes (e.g., for a budget of M = 4 M=4 ):\\nTwo nodes are considered conflicting if one is located in a subtree that is implicitly pruned by the optimal branch selection of the other.\\nIntuitively, solving Equation ( 7 ) represents the evaluation process of the sub-tree‚Äôs structure, while a simpler structure of reasoning tree starting from n i n_{i} yields a higher R \\u200b ( n i ) R(n_{i}) . Combining the evaluation R \\u200b ( n i ) R(n_{i}) of each node n i n_{i} under a limited budget M M , solving Equation ( 8 ) is to find the maximum achievable accuracy gain over the reasoning tree, like exploring possible combinations of M M nodes and picking the best combination. Thus, a higher R \\u200b ( q ) R(q) indicates that substantial accuracy improvements can be made by correcting just a few critical reasoning steps, signifying a structurally simple and efficient-to-learn query.\\nTo strike a balance between data diversity and data scheduling, we propose a weighted scheduling framework that dynamically adjusts data prioritization.\\nSpecifically, queries are assigned adaptive weights determined by both training step t t and r-score R R . Specifically, when it is an early training stage, higher weights are assigned to samples with higher r-scores (indicating lower learning difficulty), stabilizing the reinforcement learning. When RL training meets the later training phase, queries‚Äô weights will be redistributed gradually towards lower-r-score samples (higher learning difficulty) to enhance model generalization.\\nMotivated by this, the training weight œâ \\\\omega of each query is formulated as\\nwhere t t is the current epoch; œâ max \\\\omega_{\\\\text{max}} and œâ min \\\\omega_{\\\\text{min}} are hyperparameters that define the maximum and minimum training weights;\\nAnd rank \\u200b ( Œ± ) \\\\text{rank}(\\\\alpha) means calculating the reverse order of Œ± \\\\alpha in the entire dataset; Œ≥ \\u200b ( t ) \\\\gamma(t) can be either linear mapping Œ≥ \\u200b ( t ) = t T \\\\gamma(t)=\\\\frac{t}{T} or sigmoid Œ≥ \\u200b ( t ) = œÉ \\u200b ( ( t T ‚àí 0.5 ) ) \\\\gamma(t)=\\\\sigma\\\\left(\\\\left(\\\\frac{t}{T}-0.5\\\\right)\\\\right) .\\nThe Œ± \\u200b ( R \\u200b ( q ) , t ) \\\\alpha(R(q),t) is a monotonically varying function that down‚Äêweights high-scoring (simple) queries over time while up‚Äêweighting lower-scoring (difficult) ones.\\nThis scheduling approach balances exploitation of easily learnable patterns and exploration of challenging instances, mitigating catastrophic forgetting of underrepresented data distributions.',\n",
       "    'subsections': [{'title': '5.1 Tree Construction',\n",
       "      'paragraphs': 'As the entire reasoning tree is computationally intractable, we construct a manageable, fixed-structure k k -ary approximation for each query q q . The structure of this tree, ùíØ \\\\mathcal{T} , is defined by a branching factor k k , a maximum depth d d , and a token interval l l (e.g., k = 4 , d = 4 , l = 200 k=4,d=4,l=200 ).\\nThe construction process begins at the root node (the query q q ) and proceeds via a periodic branching strategy during response generation. Specifically, a branch is triggered immediately at the beginning of the response and subsequently at every l l -token interval. As shown in Figure 3 (a), at each trigger, the current path splits into k k independent sub-paths that continue to generate in parallel. This recursive branching process continues until a predefined maximum depth d d is reached. To minimize computational overhead from this multi-path sampling, we use the Key-Value (KV) Cache, as all sibling branches share the same prefix.\\nIn RLVR tasks, a solution‚Äôs quality is determined by the correctness of its final answer, which corresponds to a leaf node in our framework. Therefore, we define the quality of any intermediate node n i n_{i} as the average accuracy of its leaf descendants, ‚Ñí \\u200b ( n i ) \\\\mathcal{L}(n_{i}) . This is quantified using an accuracy function:\\nwhere S S is a set of leaf nodes and ùïÄ \\u200b ( ‚ãÖ ) \\\\mathbb{I}(\\\\cdot) is the indicator function. This allows us to assess quality at different levels: the quality of a reasoning segment via ACC \\u200b ( ‚Ñí \\u200b ( n i ) ) \\\\text{ACC}(\\\\mathcal{L}(n_{i})) and the model‚Äôs aggregate performance on the query via ACC \\u200b ( ùí© leaf ) \\\\text{ACC}(\\\\mathcal{N}_{\\\\text{leaf}}) .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 R-Score Calculation',\n",
       "      'paragraphs': 'The r-score quantifies the learning potential of a node or query by measuring the maximum achievable accuracy gain under a limited policy refining cost, like a limited node editing budget.\\nGiven this idea, for any non-leaf node n i n_{i} , we define its r-score, R \\u200b ( n i ) R(n_{i}) , as the maximal accuracy gain achievable by selecting its single best child branch and pruning all others. This is formulated as:\\nThe overall r-score for a query, R \\u200b ( q ) R(q) , estimates the total accuracy gain achievable under a budget that limits modifications to a maximum of M M nodes. It is the maximum sum of r-scores from any set of M M non-conflicting nodes (e.g., for a budget of M = 4 M=4 ):\\nTwo nodes are considered conflicting if one is located in a subtree that is implicitly pruned by the optimal branch selection of the other.\\nIntuitively, solving Equation ( 7 ) represents the evaluation process of the sub-tree‚Äôs structure, while a simpler structure of reasoning tree starting from n i n_{i} yields a higher R \\u200b ( n i ) R(n_{i}) . Combining the evaluation R \\u200b ( n i ) R(n_{i}) of each node n i n_{i} under a limited budget M M , solving Equation ( 8 ) is to find the maximum achievable accuracy gain over the reasoning tree, like exploring possible combinations of M M nodes and picking the best combination. Thus, a higher R \\u200b ( q ) R(q) indicates that substantial accuracy improvements can be made by correcting just a few critical reasoning steps, signifying a structurally simple and efficient-to-learn query.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Dynamic Weighting',\n",
       "      'paragraphs': 'To strike a balance between data diversity and data scheduling, we propose a weighted scheduling framework that dynamically adjusts data prioritization.\\nSpecifically, queries are assigned adaptive weights determined by both training step t t and r-score R R . Specifically, when it is an early training stage, higher weights are assigned to samples with higher r-scores (indicating lower learning difficulty), stabilizing the reinforcement learning. When RL training meets the later training phase, queries‚Äô weights will be redistributed gradually towards lower-r-score samples (higher learning difficulty) to enhance model generalization.\\nMotivated by this, the training weight œâ \\\\omega of each query is formulated as\\nwhere t t is the current epoch; œâ max \\\\omega_{\\\\text{max}} and œâ min \\\\omega_{\\\\text{min}} are hyperparameters that define the maximum and minimum training weights;\\nAnd rank \\u200b ( Œ± ) \\\\text{rank}(\\\\alpha) means calculating the reverse order of Œ± \\\\alpha in the entire dataset; Œ≥ \\u200b ( t ) \\\\gamma(t) can be either linear mapping Œ≥ \\u200b ( t ) = t T \\\\gamma(t)=\\\\frac{t}{T} or sigmoid Œ≥ \\u200b ( t ) = œÉ \\u200b ( ( t T ‚àí 0.5 ) ) \\\\gamma(t)=\\\\sigma\\\\left(\\\\left(\\\\frac{t}{T}-0.5\\\\right)\\\\right) .\\nThe Œ± \\u200b ( R \\u200b ( q ) , t ) \\\\alpha(R(q),t) is a monotonically varying function that down‚Äêweights high-scoring (simple) queries over time while up‚Äêweighting lower-scoring (difficult) ones.\\nThis scheduling approach balances exploitation of easily learnable patterns and exploration of challenging instances, mitigating catastrophic forgetting of underrepresented data distributions.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Analysis',\n",
       "    'paragraphs': 'To empirically validate that the training process is optimizing reasoning trees, we conducted an experiment centered on a new metric: the Minimum Corrective Nodes (MCN).\\nThis metric is defined as the minimum number of node modifications required for the reasoning tree to achieve a specified target accuracy.\\nA single node modification is counted as one token change; thus, a lower MCN signifies a well-structured reasoning tree.\\nIn our experiment, we tracked the MCN on the DAPA-Math-17K training set during the training of Qwen2.5-Math-7B, excluding queries where the base model‚Äôs accuracy was below 10%.\\nAs shown in Figure 4 (a), the average MCN across the training set exhibits a consistent downward trend as training progresses, regardless of the target accuracy. This result demonstrates that the reinforcement learning process effectively refines the model‚Äôs policy at critical decision nodes, thereby validating our central assumption that training is a process of reasoning tree optimization.\\nIn this experiment, we want to see which metric best identifies valuable queries for early-stage training. The process is as follows: First, we use each metric to select the top one-third of the data, creating several distinct subsets. Second, we train a separate model on each of these subsets for a single epoch. Finally, we evaluate the resulting models on both the training and test sets.\\nAs shown in Figure 4 (b), the subset selected by the ACC-based method initially shows higher average accuracy on the training set, as expected from its selection criteria.\\nHowever, as training progresses, the model trained on the r-score-selected subset quickly surpasses it. This indicates that the r-score is more effective at identifying queries with low learning difficulty, rather than just initial accuracy.\\nThe advantage of r-score is even more evident on the test set, as shown in Figure 4 (c).\\nHere, the model trained on the r-score-selected queries consistently outperforms both the ACC-based selection and a baseline with random query selection (GRPO).\\nThis confirms that the queries identified by the r-score provide the most effective learning signal, leading to better performance improvement and validating its capability in identifying the real difficulty of queries.',\n",
       "    'subsections': [{'title': '6.1 Training as Reasoning Tree Optimization',\n",
       "      'paragraphs': 'To empirically validate that the training process is optimizing reasoning trees, we conducted an experiment centered on a new metric: the Minimum Corrective Nodes (MCN).\\nThis metric is defined as the minimum number of node modifications required for the reasoning tree to achieve a specified target accuracy.\\nA single node modification is counted as one token change; thus, a lower MCN signifies a well-structured reasoning tree.\\nIn our experiment, we tracked the MCN on the DAPA-Math-17K training set during the training of Qwen2.5-Math-7B, excluding queries where the base model‚Äôs accuracy was below 10%.\\nAs shown in Figure 4 (a), the average MCN across the training set exhibits a consistent downward trend as training progresses, regardless of the target accuracy. This result demonstrates that the reinforcement learning process effectively refines the model‚Äôs policy at critical decision nodes, thereby validating our central assumption that training is a process of reasoning tree optimization.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.2 The relationship between R-Score and Learning Difficulty',\n",
       "      'paragraphs': 'In this experiment, we want to see which metric best identifies valuable queries for early-stage training. The process is as follows: First, we use each metric to select the top one-third of the data, creating several distinct subsets. Second, we train a separate model on each of these subsets for a single epoch. Finally, we evaluate the resulting models on both the training and test sets.\\nAs shown in Figure 4 (b), the subset selected by the ACC-based method initially shows higher average accuracy on the training set, as expected from its selection criteria.\\nHowever, as training progresses, the model trained on the r-score-selected subset quickly surpasses it. This indicates that the r-score is more effective at identifying queries with low learning difficulty, rather than just initial accuracy.\\nThe advantage of r-score is even more evident on the test set, as shown in Figure 4 (c).\\nHere, the model trained on the r-score-selected queries consistently outperforms both the ACC-based selection and a baseline with random query selection (GRPO).\\nThis confirms that the queries identified by the r-score provide the most effective learning signal, leading to better performance improvement and validating its capability in identifying the real difficulty of queries.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 Experiment',\n",
       "    'paragraphs': 'Training setting We conduct experiments on two different models, including Qwen2.5-Math-7B and Qwen2.5-7B.\\nWe adapt our training codebase from verl (Sheng et\\xa0al., 2025 ) and follow the training recipe of standard GRPO.\\nOur training data is DAPO-Math-17k (Yu et\\xa0al., 2025 ) , containing only math problems with integer ground-truth answers.\\nBoth the KL-divergence and entropy loss terms are removed in our experiments.\\nGeneration batch size is set to 512 512 .\\nTraining is performed with top-p value of 1.0 1.0 and temperature = 1.0 =1.0 .\\nEvulation We evaluated our models and baselines on six widely used mathematical reasoning benchmarks: AIME24, AIME25, AMC23 (Li et\\xa0al., 2024 ) , MATH-500 (Hendrycks et\\xa0al., 2021 ) , Minerva Math (Lewkowycz et\\xa0al., 2022 ) , and OlympiadBench (He et\\xa0al., 2024 ) .\\nValidation is performed with a top-p value of 0.7 0.7 and temperature = 1.0 =1.0 across all models and test sets.\\nWe use Math-Verify for training, validation, and final evaluation.\\nBaselines For the throughout comparison, we compare our method against 7 baselines, including standard GRPO (Shao et\\xa0al., 2024 ) , SimpleRL-Zoo (Zeng et\\xa0al., 2025 ) , Eurus-PRIME (Cui et\\xa0al., 2025 ) , OPO (Hao et\\xa0al., 2025 ) ,\\nACC (curriculum learning based on accuracy, using sigmoid weighting), LPPO (Chen et\\xa0al., 2025b ) , and Seed-GRPO (Chen et\\xa0al., 2025a ) .\\nOur Methods Re-Schedule is implemented with two weighting schemes: ‚Äòlinear‚Äô and ‚Äòsigmoid‚Äô. Unless otherwise specified, the reasoning trees in our experiments are constructed with a branching factor of k = 4 k=4 , a maximum depth of d = 4 d=4 , and a token interval of l = 200 l=200 . The weighting schemes are defined as follows:\\n1. The ‚Äòlinear‚Äô scheme uses Œ≥ \\u200b ( t ) = t / T \\\\gamma(t)=t/T ;\\n2. The ‚Äòsigmoid‚Äô scheme uses Œ≥ \\u200b ( t ) = œÉ \\u200b ( ( t T ‚àí 0.5 ) ) \\\\gamma(t)=\\\\sigma\\\\left(\\\\left(\\\\frac{t}{T}-0.5\\\\right)\\\\right) .\\nFor both, we set the total number of epochs T = 10 T=10 .\\nDetails of the training setup can be found in the Appendix B .\\nAs shown in Tables 1 and 2 , our Re-Schedule method consistently sets a new state-of-the-art, achieving average scores of 48.5 on Qwen2.5-Math-7B and 44.5 on Qwen2.5-7B. It significantly outperforms both scheduling baselines like ACC s \\u200b i \\u200b g \\u200b m \\u200b o \\u200b i \\u200b d \\\\text{ACC}_{sigmoid} (by up to 3.2%) and classical RLVR methods like OPO/GRPO (by up to 3.8%). These results validate our central claim: that the reasoning tree‚Äôs structure, captured by our r-score, is a more effective way to measure the real learning difficulty of a query than path-based metrics like accuracy.\\nWe investigate the impact of the reasoning tree‚Äôs structure by varying the branching factor k k and maximum depth d d . The choice of these parameters determines the fidelity of the approximated reasoning tree. While larger values for k k and d d theoretically provide a more accurate approximation and thus a more effective r-score, they also introduce a significant computational overhead. As shown in Table 3 , our default configuration of k = 4 k=4 and d = 4 d=4 yields the best average performance (48.3%).\\nWe analyze the sensitivity of our method to the minimum œâ min \\\\omega_{\\\\text{min}} and maximum œâ max \\\\omega_{\\\\text{max}} weight hyperparameters, which control the dynamic range of the curriculum. Results in Table 4 show that our default setting of œâ min = 0.5 \\\\omega_{\\\\text{min}}=0.5 and œâ max = 2.0 \\\\omega_{\\\\text{max}}=2.0 achieves the highest average score (48.5). Decreasing the dynamic range by either reducing œâ max \\\\omega_{\\\\text{max}} (to 1.5) or increasing œâ min \\\\omega_{\\\\text{min}} (to 0.8) leads to performance degradation. This indicates that a sufficiently large weighting range is crucial for the curriculum to effectively differentiate between easy and hard samples.\\nConversely, an overly extreme range (e.g., œâ min = 0.2 \\\\omega_{\\\\text{min}}=0.2 ) also degrades performance, possibly because the curriculum excessively under-weights difficult queries. By assigning them a minimal weight for a prolonged period, the model is prevented from learning difficult queries.\\nFurthermore, for additional experiments on the design choices for the r-score calculation, please see Appendix C .',\n",
       "    'subsections': [{'title': '7.1 RL training Setups',\n",
       "      'paragraphs': 'Training setting We conduct experiments on two different models, including Qwen2.5-Math-7B and Qwen2.5-7B.\\nWe adapt our training codebase from verl (Sheng et\\xa0al., 2025 ) and follow the training recipe of standard GRPO.\\nOur training data is DAPO-Math-17k (Yu et\\xa0al., 2025 ) , containing only math problems with integer ground-truth answers.\\nBoth the KL-divergence and entropy loss terms are removed in our experiments.\\nGeneration batch size is set to 512 512 .\\nTraining is performed with top-p value of 1.0 1.0 and temperature = 1.0 =1.0 .\\nEvulation We evaluated our models and baselines on six widely used mathematical reasoning benchmarks: AIME24, AIME25, AMC23 (Li et\\xa0al., 2024 ) , MATH-500 (Hendrycks et\\xa0al., 2021 ) , Minerva Math (Lewkowycz et\\xa0al., 2022 ) , and OlympiadBench (He et\\xa0al., 2024 ) .\\nValidation is performed with a top-p value of 0.7 0.7 and temperature = 1.0 =1.0 across all models and test sets.\\nWe use Math-Verify for training, validation, and final evaluation.\\nBaselines For the throughout comparison, we compare our method against 7 baselines, including standard GRPO (Shao et\\xa0al., 2024 ) , SimpleRL-Zoo (Zeng et\\xa0al., 2025 ) , Eurus-PRIME (Cui et\\xa0al., 2025 ) , OPO (Hao et\\xa0al., 2025 ) ,\\nACC (curriculum learning based on accuracy, using sigmoid weighting), LPPO (Chen et\\xa0al., 2025b ) , and Seed-GRPO (Chen et\\xa0al., 2025a ) .\\nOur Methods Re-Schedule is implemented with two weighting schemes: ‚Äòlinear‚Äô and ‚Äòsigmoid‚Äô. Unless otherwise specified, the reasoning trees in our experiments are constructed with a branching factor of k = 4 k=4 , a maximum depth of d = 4 d=4 , and a token interval of l = 200 l=200 . The weighting schemes are defined as follows:\\n1. The ‚Äòlinear‚Äô scheme uses Œ≥ \\u200b ( t ) = t / T \\\\gamma(t)=t/T ;\\n2. The ‚Äòsigmoid‚Äô scheme uses Œ≥ \\u200b ( t ) = œÉ \\u200b ( ( t T ‚àí 0.5 ) ) \\\\gamma(t)=\\\\sigma\\\\left(\\\\left(\\\\frac{t}{T}-0.5\\\\right)\\\\right) .\\nFor both, we set the total number of epochs T = 10 T=10 .\\nDetails of the training setup can be found in the Appendix B .',\n",
       "      'subsections': []},\n",
       "     {'title': '7.2 Main Experiment',\n",
       "      'paragraphs': 'As shown in Tables 1 and 2 , our Re-Schedule method consistently sets a new state-of-the-art, achieving average scores of 48.5 on Qwen2.5-Math-7B and 44.5 on Qwen2.5-7B. It significantly outperforms both scheduling baselines like ACC s \\u200b i \\u200b g \\u200b m \\u200b o \\u200b i \\u200b d \\\\text{ACC}_{sigmoid} (by up to 3.2%) and classical RLVR methods like OPO/GRPO (by up to 3.8%). These results validate our central claim: that the reasoning tree‚Äôs structure, captured by our r-score, is a more effective way to measure the real learning difficulty of a query than path-based metrics like accuracy.',\n",
       "      'subsections': []},\n",
       "     {'title': '7.3 Ablation Experiment',\n",
       "      'paragraphs': 'We investigate the impact of the reasoning tree‚Äôs structure by varying the branching factor k k and maximum depth d d . The choice of these parameters determines the fidelity of the approximated reasoning tree. While larger values for k k and d d theoretically provide a more accurate approximation and thus a more effective r-score, they also introduce a significant computational overhead. As shown in Table 3 , our default configuration of k = 4 k=4 and d = 4 d=4 yields the best average performance (48.3%).\\nWe analyze the sensitivity of our method to the minimum œâ min \\\\omega_{\\\\text{min}} and maximum œâ max \\\\omega_{\\\\text{max}} weight hyperparameters, which control the dynamic range of the curriculum. Results in Table 4 show that our default setting of œâ min = 0.5 \\\\omega_{\\\\text{min}}=0.5 and œâ max = 2.0 \\\\omega_{\\\\text{max}}=2.0 achieves the highest average score (48.5). Decreasing the dynamic range by either reducing œâ max \\\\omega_{\\\\text{max}} (to 1.5) or increasing œâ min \\\\omega_{\\\\text{min}} (to 0.8) leads to performance degradation. This indicates that a sufficiently large weighting range is crucial for the curriculum to effectively differentiate between easy and hard samples.\\nConversely, an overly extreme range (e.g., œâ min = 0.2 \\\\omega_{\\\\text{min}}=0.2 ) also degrades performance, possibly because the curriculum excessively under-weights difficult queries. By assigning them a minimal weight for a prolonged period, the model is prevented from learning difficult queries.\\nFurthermore, for additional experiments on the design choices for the r-score calculation, please see Appendix C .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '8 Conclusions',\n",
       "    'paragraphs': 'In this work, we challenged the reliance on path-based metrics for RLVR data scheduling. We introduced the r-score, a novel metric that quantifies learnability based on the structure of a query‚Äôs reasoning tree, and proposed Re-Schedule, a curriculum learning algorithm built upon it. Extensive experiments demonstrated that Re-Schedule consistently outperforms classical RLVR and existing scheduling methods, validating that r-score is a more effective proxy for learnability than path-based accuracy. Our findings establish that a structural understanding of the reasoning process provides a more powerful and principled foundation for creating efficient training curricula in RLVR.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Ethics statement',\n",
       "    'paragraphs': 'We have manually reevaluated the dataset we created to ensure it is free of any potential for discrimination, human rights violations, bias, exploitation, and any other ethical concerns.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Reproducibility statement',\n",
       "    'paragraphs': 'To ensure the reproducibility of our findings, all source code and datasets used in our experiments are included in the supplementary material. The provided materials are sufficient to replicate the main results presented in this paper.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Usage of LLMs',\n",
       "    'paragraphs': 'Throughout the preparation of this manuscript, Large Language Models (LLMs) were utilized as a writing and editing tool. Specifically, we employed LLMs to improve the clarity and readability of the text, refine sentence structures, and correct grammatical errors. All final content, including the core scientific claims, experimental design, and conclusions, was conceived and written by us, and we take full responsibility for the final version of this paper.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Details of Experimental Setup',\n",
       "    'paragraphs': 'All algorithms are implemented based on the official GRPO codebase within the VeRL framework.\\nWe use a learning rate of 1e-6 without warm-up across all experiments.\\nAt each rollout step, we generate 8 8 answers for each of 512 512 sampled questions, then split the data into 16 16 mini-batches and train the policy network for 16 gradient steps.\\nModels are trained for at most 150 150 rollout steps.\\nUnless otherwise specified, we follow GRPO‚Äôs default design choices with token-level loss normalization without dynamic sampling and KL regularization.\\nFor all models, the maximum input length is 1024 and the minimum input length is 3072.\\nAll the experiments were conducted on H20 GPUs.\\nNote: The authors of Eurus-PRIME only published results from training on Qwen2.5-Math-7B. Therefore, we do not include results for the Qwen2.5-7B model in our comparison.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Supplementary Experiment: Effect of Metric Selection',\n",
       "    'paragraphs': 'We validate our core design choice for the r-score calculation. Our proposed method (‚ÄòFix‚Äô) defines an ‚Äòedit‚Äô as a single node modification. We compare this against an alternative (‚ÄòPruning‚Äô), where an ‚Äòedit‚Äô is defined as pruning an entire sub-branch from a decision point.\\nTable 5 shows that the ‚ÄòFix‚Äô method consistently outperforms ‚ÄòPruning‚Äô for both linear (47.1% vs. 46.9%) and sigmoid (48.3% vs. 47.4%) schedules.\\nThis result shows that compared with the branch ‚ÄòPruning‚Äô, the node ‚ÄòFix‚Äô is more consistent with the training process of reinforcement learning.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large Language Models (LLMs) can be conceptualized as progressively editing a query‚Äôs ‚ÄòReasoning Tree‚Äô. This process involves exploring nodes (tokens) and dynamically modifying the model‚Äôs policy at each node. When combined with data scheduling, this process yields further gains in data efficiency and accuracy.\\n However, existing RLVR data scheduling methods typically rely on path-based metrics to rank queries, overlooking the reasoning tree structures of these queries.\\n In this paper, we introduce a novel metric, namely Reasoning Score (r-score), which measures the query‚Äôs learning difficulty based on the structure of its reasoning tree.\\n Based on the r-score, we propose the Reasoning Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a curriculum progressing from structurally simple (high r-score) to complex (low r-score) queries.\\n Experiments on six math-reasoning benchmarks show that Re-Schedule significantly improves average accuracy, achieving gains of up to 3.2%.\\n These strong results validate our approach and demonstrate that a structural understanding of the reasoning tree provides a more powerful and principled foundation for RLVR data scheduling 111Our code is available at https://github.com/zz-haooo/Re-Schedule..'},\n",
       " '2510.25504v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Multi-objective Search (MOS) problems are pervasive in real-world settings where decision makers must balance several, often conflicting, objectives.\\nFor example, in route finding applications, we are interested in simultaneously minimizing both travel time and fuel consumption, or distance and toll costs.\\nIn many such cases, improvements in one objective cannot be achieved without hinderring another objective, making the search for well-balanced solutions both challenging and essential.\\nWhen a decision maker can articulate how much loss in one objective is acceptable for a given gain in another, all objectives can be turned into one scalar value by, e.g., optimising a weighted sum or another order-preserving (monotone) aggregation.\\nThen, the resulting problem can be solved by any standard single-objective algorithm.\\nThis aggregation approach, however, presupposes reliable apriori information about acceptable trade-off for the decision maker, which is often not available to the algorithm.\\nAn alternative approach to addressing the multidimensional trade-off is to use MOS algorithms that compute the best attainable trade-offs wherein no objective can be improved without degrading at least one other objective. This set can then be presented to the decision maker for an a posteriori preference articulation and final choice.\\nWhile being a decades-old problem (Vincke 1976 ; Hansen 1980 ; Cl√≠maco and Pascoal 2012 ; Current and Marsh 1993 ; Skriver 2000 ; Tarapata 2007 ; Ulungu and Teghem 1991 ) , in recent years, the study of MOS has attracted growing attention across multiple research communities.\\nDedicated workshops and tutorials addressing complex, often conflicting, objectives\\nhave been featured in mainstream AI\\nvenues\\n(e.g.,\\nAAAI\\xa0[ 2024 ],\\nIJCAI\\xa0[ 2023 ; 2025 ],\\nAAMAS\\xa0[ 2024 ],\\nICAPS\\xa0[ 2024 ],\\nECAI\\xa0[ 2025 ]\\nand\\nSoCS\\xa0[ 2023 ]),\\nand in robotics and machine-learning venues\\n(e.g., RSS\\xa0[ 2025 ]\\nand\\nNeurIPS\\xa0[ 2024 ]).\\nRelated developments are also emerging in operations research (OR), transportation science, and evolutionary computation, where multi-objective optimization has a long tradition but is now being revisited with modern heuristic search, reinforcement learning, and hybrid approaches. This convergence of interests reflects a shared recognition that real-world decision-making rarely optimizes a single criterion, and that principled multi-objective reasoning is essential for building intelligent, robust, and adaptable systems.\\nIn this paper, we highlight recent advances in the field in terms of problem variants, algorithms, applications and emerging directions. It is by no means a comprehensive literature review but an attempt to provide an accessible starting point for any researcher interested in the field.\\nHere, we focus on the setting of multi-objective search . However, we also highlight extensions and variants such as those that include uncertainty.\\nImportantly, due to lack of space, we maintain a high-level description of approaches and refer the reader to (Salzman et\\xa0al. 2023 ) for a technical overview of recent MOS advances.',\n",
       "    'subsections': [{'title': 'Scope.',\n",
       "      'paragraphs': 'In this paper, we highlight recent advances in the field in terms of problem variants, algorithms, applications and emerging directions. It is by no means a comprehensive literature review but an attempt to provide an accessible starting point for any researcher interested in the field.\\nHere, we focus on the setting of multi-objective search . However, we also highlight extensions and variants such as those that include uncertainty.\\nImportantly, due to lack of space, we maintain a high-level description of approaches and refer the reader to (Salzman et\\xa0al. 2023 ) for a technical overview of recent MOS advances.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '2 Problem Setting & Variants',\n",
       "    'paragraphs': 'Boldface font indicates vectors, lower-case and upper-case symbols indicate elements and sets, respectively. The notation p i p_{i} will be used to denote the i i ‚Äôth component of ùê© \\\\mathbf{p} .\\nThe addition of two d d -dimensional vectors ùê© \\\\mathbf{p} and ùê™ \\\\mathbf{q} and the multiplication\\nof a real-valued scalar k k and a d d -dimensional vector ùê© \\\\mathbf{p} are defined as ùê© + ùê™ = ( p 1 + q 1 , ‚Ä¶ , p d + q d ) \\\\mathbf{p}+\\\\mathbf{q}=(p_{1}+q_{1},\\\\ldots,p_{d}+q_{d}) and k \\u200b ùê© = ( k \\u200b p 1 , ‚Ä¶ , k \\u200b p d ) k\\\\mathbf{p}=(kp_{1},\\\\ldots,kp_{d}) , respectively.\\nLet ùê© \\\\mathbf{p} and ùê™ \\\\mathbf{q} be d d -dimensional vectors.\\nFor a minimization problem, we say that ùê© \\\\mathbf{p} dominates ùê™ \\\\mathbf{q} and denote this as ùê© ‚™Ø ùê™ \\\\mathbf{p}\\\\preceq\\\\mathbf{q} if ‚àÄ i , p i ‚â§ q i \\\\forall i,p_{i}\\\\leq q_{i} .\\nWe say that ùê© \\\\mathbf{p} is lexicographically smaller than ùê™ \\\\mathbf{q} and denote this as ùê© ‚â∫ lex ùê™ \\\\mathbf{p}\\\\prec_{\\\\rm{lex}}\\\\mathbf{q} if p k < q k {p}_{k}<{q}_{k} for the first index k k s.t. p k ‚â† q k {p}_{k}\\\\neq{q}_{k} .\\nFinally,\\nlet ùê© \\\\mathbf{p} and ùê™ \\\\mathbf{q} be two d d -dimensional vectors\\nand let ùú∫ \\\\boldsymbol{\\\\varepsilon} be another d d -dimensional vector such that ‚àÄ i \\u200b Œµ i ‚â• 0 \\\\forall i~\\\\varepsilon_{i}\\\\geq 0 .\\nWe say that ùê© \\\\mathbf{p} approximately dominates ùê™ \\\\mathbf{q} with an approximation factor ùú∫ \\\\boldsymbol{\\\\varepsilon} and denote this as ùê© ‚™Ø ùú∫ ùê™ \\\\mathbf{p}\\\\preceq_{\\\\boldsymbol{\\\\varepsilon}}\\\\mathbf{q} if ‚àÄ i , p i ‚â§ ( 1 + Œµ i ) ‚ãÖ q i \\\\forall i,p_{i}\\\\leq(1+\\\\varepsilon_{i})\\\\cdot q_{i} .\\nIn most variants of a MOS problem\\na directed graph G = ( V , E ) G=(V,E) where each edge e ‚àà E e\\\\in E has a nonnegative cost vector ùêú \\u200b ( e ) ‚àà ‚Ñù d \\\\mathbf{c}(e)\\\\in\\\\mathbb{R}^{d} where d > 0 d>0 is the number of objectives. For the specific cases where d = 1 d=1 and d = 2 d=2 , we refer to the problem as single-objective and bi-objective , respectively.\\nFor a path œÄ = ‚ü® v 1 , ‚Ä¶ , v k ‚ü© \\\\pi=\\\\langle v_{1},\\\\ldots,v_{k}\\\\rangle where ( v i , v i + 1 ) ‚àà E (v_{i},v_{i+1})\\\\in E , its cost ùêú \\u200b ( p ) \\\\mathbf{c}(p) is the sum of the edge costs. That is, ùêú \\u200b ( œÄ ) = ‚àë i ùêú \\u200b ( v i , v i + 1 ) \\\\mathbf{c}(\\\\pi)=\\\\sum_{i}\\\\mathbf{c}(v_{i},v_{i+1}) .\\nGiven start and target vertices s , t ‚àà V s,t\\\\in V ,\\na path from s s to t t is called a solution .\\nA solution is Pareto-optimal iff its cost is not dominated by any other solution.\\nSee Fig. 1 .\\nIn the basic MOS problem, we are given vertices s , t ‚àà V s,t\\\\in V and the goal is to compute the set Œ† ‚ãÜ \\\\Pi^{\\\\star} of Pareto-optimal solutions, also known as the Pareto front (PF) (Salzman et\\xa0al. 2023 ) .\\nImportantly, computing Œ† ‚ãÜ \\\\Pi^{\\\\star} is ùñ≠ùñØ \\\\mathsf{NP} -hard (Serafini 1987 ) as its cardinality may be exponential in | V | |V| (Ehrgott 2005 ; Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) . Even\\ndetermining whether a path belongs to Œ† ‚ãÜ \\\\Pi^{\\\\star} is ùñ≠ùñØ \\\\mathsf{NP} -hard (Papadimitriou and Yannakakis 2000 ) .\\nIn certain settings, we would like to compute the PF from a source s ‚àà V s\\\\in V to every other vertex v ‚àà V v\\\\in V (see, e.g., (Martins 1984 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2021 ; Kurbanov, Cuch√Ω, and Vokr√≠nek 2023 ) )\\nor from any vertex u ‚àà V u\\\\in V to every other vertex v ‚àà V v\\\\in V (see, e.g., (Zhang et\\xa0al. 2023b ; Cuch√Ω, Vokr√≠nek, and Jakob 2024 ) ).\\nIn real-world settings, we are often not interested in the entire PF as it may be too large to present to decision makers (for example, there may be thousands of solutions in the PF of large road networks (Ren et\\xa0al. 2025 ) ).\\nThus,\\nwe are often interested in computing a bounded approximation of Œ† ‚ãÜ \\\\Pi^{\\\\star} .\\nHere, we are given an approximation factor Œµ \\\\mathbf{\\\\varepsilon} .\\nThe Œµ \\\\mathbf{\\\\varepsilon} -approximate PF Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} is a set of solutions such that ‚àÄ œÄ ‚àà Œ† ‚ãÜ , ‚àÉ œÄ ‚Ä≤ ‚àà Œ† Œµ ‚ãÜ \\u200b s . t . ùêú \\u200b ( œÄ ‚Ä≤ ) ‚™Ø ùú∫ ùêú \\u200b ( œÄ ) \\\\forall\\\\pi\\\\in\\\\Pi^{\\\\star},\\\\exists\\\\pi^{\\\\prime}\\\\in\\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}}~s.t.~\\\\mathbf{c}(\\\\pi^{\\\\prime})\\\\preceq_{\\\\boldsymbol{\\\\varepsilon}}\\\\mathbf{c}(\\\\pi) .\\nNamely, every solution in Œ† ‚ãÜ \\\\Pi^{\\\\star} is approximately dominated by some solution in in Œ† Œµ ‚àó \\\\Pi^{*}_{\\\\mathbf{\\\\varepsilon}} .\\nImportantly,\\n(i)\\xa0the Œµ \\\\mathbf{\\\\varepsilon} -approximate Pareto-optimal solution set is not necessarily unique\\nand\\n(ii)\\xa0some variants of this definition require that Œ† Œµ ‚ãÜ ‚äÜ Œ† ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}}\\\\subseteq\\\\Pi^{\\\\star} while others don‚Äôt (See Fig. 1 ).\\nAlternatively, some problem formulations seek\\na small representative set of solutions in Œ† ‚ãÜ \\\\Pi^{\\\\star} (Rivera, Baier, and Hern√°ndez 2022 ) (without any formal definition of ‚Äúsmall‚Äù).\\nMany applications benefit from obtaining a subset Œ† ~ ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star} of Œ† ‚ãÜ \\\\Pi^{\\\\star} as fast as possible. As more time is available to the algorithm, additional solutions from Œ† ‚ãÜ ‚àñ Œ† ~ ‚ãÜ \\\\Pi^{\\\\star}\\\\setminus\\\\tilde{\\\\Pi}^{\\\\star} are added to Œ† ~ ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star} . The algorithm terminates when either (i)\\xa0the decision maker or the algorithm that uses the solutions terminates the algorithm or (ii) the entire PF has been returned (i.e., Œ† ~ ‚ãÜ = Œ† ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star}=\\\\Pi^{\\\\star} ).\\nFormally, we define the dominance factor of a solution œÄ \\\\pi over another solution œÄ ‚Ä≤ \\\\pi^{\\\\prime} as\\nwhich measures how ‚Äúgood‚Äù œÄ \\\\pi approximates œÄ ‚Ä≤ \\\\pi^{\\\\prime} . Df ( œÄ , œÄ ‚Ä≤ ) (\\\\pi,\\\\pi^{\\\\prime}) encodes the smallest Œµ \\\\varepsilon -value that satisfies œÄ ‚™Ø ( Œµ , ‚Ä¶ , Œµ ) œÄ ‚Ä≤ \\\\pi\\\\preceq_{(\\\\varepsilon,\\\\ldots,\\\\varepsilon)}\\\\pi^{\\\\prime} (See Fig. 1 ).\\nFor a set of solutions Œ† \\\\Pi , we define the approximation error of Œ† \\\\Pi as\\nwhich, roughly speaking, measure the solution in Œ† ‚ãÜ \\\\Pi^{\\\\star} that is ‚Äúleast‚Äù approximated by any solution in Œ† \\\\Pi .\\nNow, to measure the performance of an anytime MOS algorithm, we typically wish to minimize the Area Under the Curve (AUC) of the approximation error formally defined as Auc = ‚à´ 0 t limit Err \\u200b ( Œ† \\u200b ( t ) ) \\\\textsc{Auc}=\\\\int_{0}^{t^{\\\\text{limit}}}\\\\textsc{Err}(\\\\Pi(t)) , where t limit t^{\\\\text{limit}} is the runtime limit and ( Œ† \\u200b ( t ) ) (\\\\Pi(t)) is solution set returned at time t t .\\nWhen the MOS problem is applied in an online fashion (i.e., planning is interleaved with taking actions) and the query is updated (either because the target is updated or because the environment‚Äôs representation is updated), one may want to avoid calling an algorithm from scratch and instead reuse previous search efforts.\\nIncremental multi-objective graph search algorithms (see, e.g., (Ren et\\xa0al. 2022b ) reuse previous searches to speed up subsequent exact or approximate MOS searches.\\nSimilarly, in MOS applications such as flight planning, dynamic traffic roadmaps, and telecommunication and data networks, the underlying graph changes over time since either its structure (edges, nodes) or the cost functions (weights, travel times, risks, etc.) evolve.\\nIn contrast to the incremental setting, here we are given the dynamics before planning begins and need to account for the temporal changes. This was only recently formulated by de\\xa0las Casas et\\xa0al. ( 2021 ) . For additional details, see also the recent work by Shovan, Khanda, and Das ( 2025 ) .\\nWhile MOS assumes a deterministic model, many real problems demand richer models. To capture stochasticity, or general optimization beyond paths, several extensions have been studied.\\nEach extends MOS along a different axis while keeping Pareto optimality central. We briefly describe each model, highlighting the similarities and differences compared to MOS.\\nRecall that a MOS problem is defined using a graph G = ( V , E ) G=(V,E) together with edge costs ùêú \\\\mathbf{c} which present a deterministic model.\\nA Multi-objective Stochastic Shortest Path (MOSSP) problem extends the MOS framework by introducing probabilistic transitions between states (Roijers and Whiteson 2017 ) .\\nFormally, we are given a graph G = ( V , E ) G=(V,E) together with edge costs ùêú \\\\mathbf{c} and a transition probability distribution over successor vertices.\\nA policy Œº \\\\mu maps vertices to successor edges, inducing a distribution over paths from the start s s to the target t t .\\nThe cost of a policy is defined as the expected cumulative cost vector across objectives.\\nAs in MOS, policies are compared using dominance: a policy Œº 1 \\\\mu_{1} dominates Œº 2 \\\\mu_{2} if it has no worse expected cost in every objective and is strictly better in at least one.\\nThe goal is to compute a coverage PF of policies.\\nA Multi-objective Markov Decision Process (MOMDP) generalizes MOSSP by adopting an MDP formalism. In contrast to MOSSP which focuses on reaching a target in a stochastic graph with vector costs, MOMDP allow arbitrary horizon settings (e.g., finite, infinite, discounted) and sequential decision-making under uncertainty, not just reaching a target vertex.\\nMulti-objective Reinforcement Learning (MORL) extends the MOS framework to settings where the agent interacts with an environment through repeated trial-and-error learning rather than having an explicit model of the state-transition dynamics (Hayes et\\xa0al. 2022 ; Felten, Talbi, and Danoy 2024 ) .\\nFormally, MORL is defined over the same structure as an MOMDP, M = ( S , A , P , ùêú ) M=(S,A,P,\\\\mathbf{c}) , with state space S S , action space A A , transition function P P , and vector-valued cost or reward function ùêú \\u200b ( s , a ) ‚àà ‚Ñù d \\\\mathbf{c}(s,a)\\\\in\\\\mathbb{R}^{d} .\\nHowever, unlike MOMDPs, in MORL the transition probabilities and reward distributions are not assumed to be known a priori.\\nInstead, the agent learns a policy Œº : S ‚Üí A \\\\mu:S\\\\to A through experience, typically by interacting with the environment and receiving vector-valued feedback.\\nMORL generalizes MOS in that it seeks Pareto-optimal solutions across multiple objectives, but unlike MOS, it does not assume a static graph with deterministic edges.\\nRelative to MOSSP and MOMDP, MORL replaces planning with learning: instead of computing Pareto-optimal policies from a known model of uncertainty, the agent must discover them through exploration and approximation.\\nThus, MORL inherits the challenges of both reinforcement learning (e.g., exploration-exploitation trade-offs, function approximation) and MOS (e.g., dominance checks).\\nThe goal in MORL remains to approximate the Pareto set of policies, but learning algorithms must balance sample efficiency, preference sensitivity, and scalability in high-dimensional state and objective spaces.\\nIn this paper, we focus on MOS which can be seen as an instance of the more general multi-objective optimization (MOO) problem, (see, e.g., (Branke et\\xa0al. 2008 ; Miettinen 2012 ; Roijers and Whiteson 2017 ; Hwang and Masud 2012 ; Emmerich and Deutz 2018 ) ).\\nIt is important to highlight the similarities and differences between the two fields.\\nMOO is the most general formulation of problems in which several, possibly conflicting, objectives must be optimized simultaneously.\\nFormally, given a feasible set ùí≥ \\\\mathcal{X} and objective functions f i : ùí≥ ‚Üí ‚Ñù f_{i}:\\\\mathcal{X}\\\\to\\\\mathbb{R} for i = 1 , ‚Ä¶ , d i=1,\\\\ldots,d , the problem is to find the set of non-dominated solutions\\nwhere ùêü \\u200b ( x ) = ( f 1 \\u200b ( x ) , ‚Ä¶ , f d \\u200b ( x ) ) \\\\mathbf{f}(x)=(f_{1}(x),\\\\ldots,f_{d}(x)) .\\nHere, ùí≥ ‚ãÜ \\\\mathcal{X}^{\\\\star} corresponds to the Pareto set whose image in ‚Ñù d \\\\mathbb{R}^{d} is the PF.\\nRelative to MOS, MOO generalizes the underlying domain: whereas MOS is defined over paths in a deterministic graph with additive vector costs, MOO is agnostic to the structure of the feasible set and can capture continuous, combinatorial, or black-box domains.\\nCompared to stochastic settings such as MOSSP and MOMDP, MOO does not necessarily assume probabilistic dynamics or sequential decision processes; instead, it focuses purely on the optimization of static or offline-defined objectives.\\nIn contrast to MORL, MOO assumes direct access to the objective functions rather than learning them through interaction.\\nIn this sense, MOO serves as the broad umbrella under which MOS, MOSSP, MOMDP, and MORL can be seen as structured subclasses with additional constraints on the representation of ùí≥ \\\\mathcal{X} , the dynamics of decision-making, and the information available to the algorithm.\\nFrom an algorithmic point of view, in contrast to MOS, which builds upon search algorithms,\\nMOO typically builds upon local and global optimization methods such as\\ngenetic algorithms (Deb et\\xa0al. 2002 ; Deb and Jain 2013 ; Zhang and Li 2007 ) ,\\nparticle swarm optimization (Coello and Lechuga 2002 ) ,\\nand\\nsimulated annealing (Li and Landa-Silva 2011 ) .',\n",
       "    'subsections': [{'title': '2.1 Notation',\n",
       "      'paragraphs': 'Boldface font indicates vectors, lower-case and upper-case symbols indicate elements and sets, respectively. The notation p i p_{i} will be used to denote the i i ‚Äôth component of ùê© \\\\mathbf{p} .\\nThe addition of two d d -dimensional vectors ùê© \\\\mathbf{p} and ùê™ \\\\mathbf{q} and the multiplication\\nof a real-valued scalar k k and a d d -dimensional vector ùê© \\\\mathbf{p} are defined as ùê© + ùê™ = ( p 1 + q 1 , ‚Ä¶ , p d + q d ) \\\\mathbf{p}+\\\\mathbf{q}=(p_{1}+q_{1},\\\\ldots,p_{d}+q_{d}) and k \\u200b ùê© = ( k \\u200b p 1 , ‚Ä¶ , k \\u200b p d ) k\\\\mathbf{p}=(kp_{1},\\\\ldots,kp_{d}) , respectively.\\nLet ùê© \\\\mathbf{p} and ùê™ \\\\mathbf{q} be d d -dimensional vectors.\\nFor a minimization problem, we say that ùê© \\\\mathbf{p} dominates ùê™ \\\\mathbf{q} and denote this as ùê© ‚™Ø ùê™ \\\\mathbf{p}\\\\preceq\\\\mathbf{q} if ‚àÄ i , p i ‚â§ q i \\\\forall i,p_{i}\\\\leq q_{i} .\\nWe say that ùê© \\\\mathbf{p} is lexicographically smaller than ùê™ \\\\mathbf{q} and denote this as ùê© ‚â∫ lex ùê™ \\\\mathbf{p}\\\\prec_{\\\\rm{lex}}\\\\mathbf{q} if p k < q k {p}_{k}<{q}_{k} for the first index k k s.t. p k ‚â† q k {p}_{k}\\\\neq{q}_{k} .\\nFinally,\\nlet ùê© \\\\mathbf{p} and ùê™ \\\\mathbf{q} be two d d -dimensional vectors\\nand let ùú∫ \\\\boldsymbol{\\\\varepsilon} be another d d -dimensional vector such that ‚àÄ i \\u200b Œµ i ‚â• 0 \\\\forall i~\\\\varepsilon_{i}\\\\geq 0 .\\nWe say that ùê© \\\\mathbf{p} approximately dominates ùê™ \\\\mathbf{q} with an approximation factor ùú∫ \\\\boldsymbol{\\\\varepsilon} and denote this as ùê© ‚™Ø ùú∫ ùê™ \\\\mathbf{p}\\\\preceq_{\\\\boldsymbol{\\\\varepsilon}}\\\\mathbf{q} if ‚àÄ i , p i ‚â§ ( 1 + Œµ i ) ‚ãÖ q i \\\\forall i,p_{i}\\\\leq(1+\\\\varepsilon_{i})\\\\cdot q_{i} .',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 MOS‚ÄîProblem definition and variants',\n",
       "      'paragraphs': 'In most variants of a MOS problem\\na directed graph G = ( V , E ) G=(V,E) where each edge e ‚àà E e\\\\in E has a nonnegative cost vector ùêú \\u200b ( e ) ‚àà ‚Ñù d \\\\mathbf{c}(e)\\\\in\\\\mathbb{R}^{d} where d > 0 d>0 is the number of objectives. For the specific cases where d = 1 d=1 and d = 2 d=2 , we refer to the problem as single-objective and bi-objective , respectively.\\nFor a path œÄ = ‚ü® v 1 , ‚Ä¶ , v k ‚ü© \\\\pi=\\\\langle v_{1},\\\\ldots,v_{k}\\\\rangle where ( v i , v i + 1 ) ‚àà E (v_{i},v_{i+1})\\\\in E , its cost ùêú \\u200b ( p ) \\\\mathbf{c}(p) is the sum of the edge costs. That is, ùêú \\u200b ( œÄ ) = ‚àë i ùêú \\u200b ( v i , v i + 1 ) \\\\mathbf{c}(\\\\pi)=\\\\sum_{i}\\\\mathbf{c}(v_{i},v_{i+1}) .\\nGiven start and target vertices s , t ‚àà V s,t\\\\in V ,\\na path from s s to t t is called a solution .\\nA solution is Pareto-optimal iff its cost is not dominated by any other solution.\\nSee Fig. 1 .\\nIn the basic MOS problem, we are given vertices s , t ‚àà V s,t\\\\in V and the goal is to compute the set Œ† ‚ãÜ \\\\Pi^{\\\\star} of Pareto-optimal solutions, also known as the Pareto front (PF) (Salzman et\\xa0al. 2023 ) .\\nImportantly, computing Œ† ‚ãÜ \\\\Pi^{\\\\star} is ùñ≠ùñØ \\\\mathsf{NP} -hard (Serafini 1987 ) as its cardinality may be exponential in | V | |V| (Ehrgott 2005 ; Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) . Even\\ndetermining whether a path belongs to Œ† ‚ãÜ \\\\Pi^{\\\\star} is ùñ≠ùñØ \\\\mathsf{NP} -hard (Papadimitriou and Yannakakis 2000 ) .\\nIn certain settings, we would like to compute the PF from a source s ‚àà V s\\\\in V to every other vertex v ‚àà V v\\\\in V (see, e.g., (Martins 1984 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2021 ; Kurbanov, Cuch√Ω, and Vokr√≠nek 2023 ) )\\nor from any vertex u ‚àà V u\\\\in V to every other vertex v ‚àà V v\\\\in V (see, e.g., (Zhang et\\xa0al. 2023b ; Cuch√Ω, Vokr√≠nek, and Jakob 2024 ) ).\\nIn real-world settings, we are often not interested in the entire PF as it may be too large to present to decision makers (for example, there may be thousands of solutions in the PF of large road networks (Ren et\\xa0al. 2025 ) ).\\nThus,\\nwe are often interested in computing a bounded approximation of Œ† ‚ãÜ \\\\Pi^{\\\\star} .\\nHere, we are given an approximation factor Œµ \\\\mathbf{\\\\varepsilon} .\\nThe Œµ \\\\mathbf{\\\\varepsilon} -approximate PF Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} is a set of solutions such that ‚àÄ œÄ ‚àà Œ† ‚ãÜ , ‚àÉ œÄ ‚Ä≤ ‚àà Œ† Œµ ‚ãÜ \\u200b s . t . ùêú \\u200b ( œÄ ‚Ä≤ ) ‚™Ø ùú∫ ùêú \\u200b ( œÄ ) \\\\forall\\\\pi\\\\in\\\\Pi^{\\\\star},\\\\exists\\\\pi^{\\\\prime}\\\\in\\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}}~s.t.~\\\\mathbf{c}(\\\\pi^{\\\\prime})\\\\preceq_{\\\\boldsymbol{\\\\varepsilon}}\\\\mathbf{c}(\\\\pi) .\\nNamely, every solution in Œ† ‚ãÜ \\\\Pi^{\\\\star} is approximately dominated by some solution in in Œ† Œµ ‚àó \\\\Pi^{*}_{\\\\mathbf{\\\\varepsilon}} .\\nImportantly,\\n(i)\\xa0the Œµ \\\\mathbf{\\\\varepsilon} -approximate Pareto-optimal solution set is not necessarily unique\\nand\\n(ii)\\xa0some variants of this definition require that Œ† Œµ ‚ãÜ ‚äÜ Œ† ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}}\\\\subseteq\\\\Pi^{\\\\star} while others don‚Äôt (See Fig. 1 ).\\nAlternatively, some problem formulations seek\\na small representative set of solutions in Œ† ‚ãÜ \\\\Pi^{\\\\star} (Rivera, Baier, and Hern√°ndez 2022 ) (without any formal definition of ‚Äúsmall‚Äù).\\nMany applications benefit from obtaining a subset Œ† ~ ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star} of Œ† ‚ãÜ \\\\Pi^{\\\\star} as fast as possible. As more time is available to the algorithm, additional solutions from Œ† ‚ãÜ ‚àñ Œ† ~ ‚ãÜ \\\\Pi^{\\\\star}\\\\setminus\\\\tilde{\\\\Pi}^{\\\\star} are added to Œ† ~ ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star} . The algorithm terminates when either (i)\\xa0the decision maker or the algorithm that uses the solutions terminates the algorithm or (ii) the entire PF has been returned (i.e., Œ† ~ ‚ãÜ = Œ† ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star}=\\\\Pi^{\\\\star} ).\\nFormally, we define the dominance factor of a solution œÄ \\\\pi over another solution œÄ ‚Ä≤ \\\\pi^{\\\\prime} as\\nwhich measures how ‚Äúgood‚Äù œÄ \\\\pi approximates œÄ ‚Ä≤ \\\\pi^{\\\\prime} . Df ( œÄ , œÄ ‚Ä≤ ) (\\\\pi,\\\\pi^{\\\\prime}) encodes the smallest Œµ \\\\varepsilon -value that satisfies œÄ ‚™Ø ( Œµ , ‚Ä¶ , Œµ ) œÄ ‚Ä≤ \\\\pi\\\\preceq_{(\\\\varepsilon,\\\\ldots,\\\\varepsilon)}\\\\pi^{\\\\prime} (See Fig. 1 ).\\nFor a set of solutions Œ† \\\\Pi , we define the approximation error of Œ† \\\\Pi as\\nwhich, roughly speaking, measure the solution in Œ† ‚ãÜ \\\\Pi^{\\\\star} that is ‚Äúleast‚Äù approximated by any solution in Œ† \\\\Pi .\\nNow, to measure the performance of an anytime MOS algorithm, we typically wish to minimize the Area Under the Curve (AUC) of the approximation error formally defined as Auc = ‚à´ 0 t limit Err \\u200b ( Œ† \\u200b ( t ) ) \\\\textsc{Auc}=\\\\int_{0}^{t^{\\\\text{limit}}}\\\\textsc{Err}(\\\\Pi(t)) , where t limit t^{\\\\text{limit}} is the runtime limit and ( Œ† \\u200b ( t ) ) (\\\\Pi(t)) is solution set returned at time t t .\\nWhen the MOS problem is applied in an online fashion (i.e., planning is interleaved with taking actions) and the query is updated (either because the target is updated or because the environment‚Äôs representation is updated), one may want to avoid calling an algorithm from scratch and instead reuse previous search efforts.\\nIncremental multi-objective graph search algorithms (see, e.g., (Ren et\\xa0al. 2022b ) reuse previous searches to speed up subsequent exact or approximate MOS searches.\\nSimilarly, in MOS applications such as flight planning, dynamic traffic roadmaps, and telecommunication and data networks, the underlying graph changes over time since either its structure (edges, nodes) or the cost functions (weights, travel times, risks, etc.) evolve.\\nIn contrast to the incremental setting, here we are given the dynamics before planning begins and need to account for the temporal changes. This was only recently formulated by de\\xa0las Casas et\\xa0al. ( 2021 ) . For additional details, see also the recent work by Shovan, Khanda, and Das ( 2025 ) .',\n",
       "      'subsections': [{'title': 'Exact MOS.',\n",
       "        'paragraphs': 'In the basic MOS problem, we are given vertices s , t ‚àà V s,t\\\\in V and the goal is to compute the set Œ† ‚ãÜ \\\\Pi^{\\\\star} of Pareto-optimal solutions, also known as the Pareto front (PF) (Salzman et\\xa0al. 2023 ) .\\nImportantly, computing Œ† ‚ãÜ \\\\Pi^{\\\\star} is ùñ≠ùñØ \\\\mathsf{NP} -hard (Serafini 1987 ) as its cardinality may be exponential in | V | |V| (Ehrgott 2005 ; Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) . Even\\ndetermining whether a path belongs to Œ† ‚ãÜ \\\\Pi^{\\\\star} is ùñ≠ùñØ \\\\mathsf{NP} -hard (Papadimitriou and Yannakakis 2000 ) .\\nIn certain settings, we would like to compute the PF from a source s ‚àà V s\\\\in V to every other vertex v ‚àà V v\\\\in V (see, e.g., (Martins 1984 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2021 ; Kurbanov, Cuch√Ω, and Vokr√≠nek 2023 ) )\\nor from any vertex u ‚àà V u\\\\in V to every other vertex v ‚àà V v\\\\in V (see, e.g., (Zhang et\\xa0al. 2023b ; Cuch√Ω, Vokr√≠nek, and Jakob 2024 ) ).',\n",
       "        'subsections': []},\n",
       "       {'title': 'Approximate MOS.',\n",
       "        'paragraphs': 'In real-world settings, we are often not interested in the entire PF as it may be too large to present to decision makers (for example, there may be thousands of solutions in the PF of large road networks (Ren et\\xa0al. 2025 ) ).\\nThus,\\nwe are often interested in computing a bounded approximation of Œ† ‚ãÜ \\\\Pi^{\\\\star} .\\nHere, we are given an approximation factor Œµ \\\\mathbf{\\\\varepsilon} .\\nThe Œµ \\\\mathbf{\\\\varepsilon} -approximate PF Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} is a set of solutions such that ‚àÄ œÄ ‚àà Œ† ‚ãÜ , ‚àÉ œÄ ‚Ä≤ ‚àà Œ† Œµ ‚ãÜ \\u200b s . t . ùêú \\u200b ( œÄ ‚Ä≤ ) ‚™Ø ùú∫ ùêú \\u200b ( œÄ ) \\\\forall\\\\pi\\\\in\\\\Pi^{\\\\star},\\\\exists\\\\pi^{\\\\prime}\\\\in\\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}}~s.t.~\\\\mathbf{c}(\\\\pi^{\\\\prime})\\\\preceq_{\\\\boldsymbol{\\\\varepsilon}}\\\\mathbf{c}(\\\\pi) .\\nNamely, every solution in Œ† ‚ãÜ \\\\Pi^{\\\\star} is approximately dominated by some solution in in Œ† Œµ ‚àó \\\\Pi^{*}_{\\\\mathbf{\\\\varepsilon}} .\\nImportantly,\\n(i)\\xa0the Œµ \\\\mathbf{\\\\varepsilon} -approximate Pareto-optimal solution set is not necessarily unique\\nand\\n(ii)\\xa0some variants of this definition require that Œ† Œµ ‚ãÜ ‚äÜ Œ† ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}}\\\\subseteq\\\\Pi^{\\\\star} while others don‚Äôt (See Fig. 1 ).\\nAlternatively, some problem formulations seek\\na small representative set of solutions in Œ† ‚ãÜ \\\\Pi^{\\\\star} (Rivera, Baier, and Hern√°ndez 2022 ) (without any formal definition of ‚Äúsmall‚Äù).',\n",
       "        'subsections': []},\n",
       "       {'title': 'Anytime MOS.',\n",
       "        'paragraphs': 'Many applications benefit from obtaining a subset Œ† ~ ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star} of Œ† ‚ãÜ \\\\Pi^{\\\\star} as fast as possible. As more time is available to the algorithm, additional solutions from Œ† ‚ãÜ ‚àñ Œ† ~ ‚ãÜ \\\\Pi^{\\\\star}\\\\setminus\\\\tilde{\\\\Pi}^{\\\\star} are added to Œ† ~ ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star} . The algorithm terminates when either (i)\\xa0the decision maker or the algorithm that uses the solutions terminates the algorithm or (ii) the entire PF has been returned (i.e., Œ† ~ ‚ãÜ = Œ† ‚ãÜ \\\\tilde{\\\\Pi}^{\\\\star}=\\\\Pi^{\\\\star} ).\\nFormally, we define the dominance factor of a solution œÄ \\\\pi over another solution œÄ ‚Ä≤ \\\\pi^{\\\\prime} as\\nwhich measures how ‚Äúgood‚Äù œÄ \\\\pi approximates œÄ ‚Ä≤ \\\\pi^{\\\\prime} . Df ( œÄ , œÄ ‚Ä≤ ) (\\\\pi,\\\\pi^{\\\\prime}) encodes the smallest Œµ \\\\varepsilon -value that satisfies œÄ ‚™Ø ( Œµ , ‚Ä¶ , Œµ ) œÄ ‚Ä≤ \\\\pi\\\\preceq_{(\\\\varepsilon,\\\\ldots,\\\\varepsilon)}\\\\pi^{\\\\prime} (See Fig. 1 ).\\nFor a set of solutions Œ† \\\\Pi , we define the approximation error of Œ† \\\\Pi as\\nwhich, roughly speaking, measure the solution in Œ† ‚ãÜ \\\\Pi^{\\\\star} that is ‚Äúleast‚Äù approximated by any solution in Œ† \\\\Pi .\\nNow, to measure the performance of an anytime MOS algorithm, we typically wish to minimize the Area Under the Curve (AUC) of the approximation error formally defined as Auc = ‚à´ 0 t limit Err \\u200b ( Œ† \\u200b ( t ) ) \\\\textsc{Auc}=\\\\int_{0}^{t^{\\\\text{limit}}}\\\\textsc{Err}(\\\\Pi(t)) , where t limit t^{\\\\text{limit}} is the runtime limit and ( Œ† \\u200b ( t ) ) (\\\\Pi(t)) is solution set returned at time t t .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Incremental & Dynamic MOS.',\n",
       "        'paragraphs': 'When the MOS problem is applied in an online fashion (i.e., planning is interleaved with taking actions) and the query is updated (either because the target is updated or because the environment‚Äôs representation is updated), one may want to avoid calling an algorithm from scratch and instead reuse previous search efforts.\\nIncremental multi-objective graph search algorithms (see, e.g., (Ren et\\xa0al. 2022b ) reuse previous searches to speed up subsequent exact or approximate MOS searches.\\nSimilarly, in MOS applications such as flight planning, dynamic traffic roadmaps, and telecommunication and data networks, the underlying graph changes over time since either its structure (edges, nodes) or the cost functions (weights, travel times, risks, etc.) evolve.\\nIn contrast to the incremental setting, here we are given the dynamics before planning begins and need to account for the temporal changes. This was only recently formulated by de\\xa0las Casas et\\xa0al. ( 2021 ) . For additional details, see also the recent work by Shovan, Khanda, and Das ( 2025 ) .',\n",
       "        'subsections': []}]},\n",
       "     {'title': '2.3 Beyond MOS',\n",
       "      'paragraphs': 'While MOS assumes a deterministic model, many real problems demand richer models. To capture stochasticity, or general optimization beyond paths, several extensions have been studied.\\nEach extends MOS along a different axis while keeping Pareto optimality central. We briefly describe each model, highlighting the similarities and differences compared to MOS.\\nRecall that a MOS problem is defined using a graph G = ( V , E ) G=(V,E) together with edge costs ùêú \\\\mathbf{c} which present a deterministic model.\\nA Multi-objective Stochastic Shortest Path (MOSSP) problem extends the MOS framework by introducing probabilistic transitions between states (Roijers and Whiteson 2017 ) .\\nFormally, we are given a graph G = ( V , E ) G=(V,E) together with edge costs ùêú \\\\mathbf{c} and a transition probability distribution over successor vertices.\\nA policy Œº \\\\mu maps vertices to successor edges, inducing a distribution over paths from the start s s to the target t t .\\nThe cost of a policy is defined as the expected cumulative cost vector across objectives.\\nAs in MOS, policies are compared using dominance: a policy Œº 1 \\\\mu_{1} dominates Œº 2 \\\\mu_{2} if it has no worse expected cost in every objective and is strictly better in at least one.\\nThe goal is to compute a coverage PF of policies.\\nA Multi-objective Markov Decision Process (MOMDP) generalizes MOSSP by adopting an MDP formalism. In contrast to MOSSP which focuses on reaching a target in a stochastic graph with vector costs, MOMDP allow arbitrary horizon settings (e.g., finite, infinite, discounted) and sequential decision-making under uncertainty, not just reaching a target vertex.\\nMulti-objective Reinforcement Learning (MORL) extends the MOS framework to settings where the agent interacts with an environment through repeated trial-and-error learning rather than having an explicit model of the state-transition dynamics (Hayes et\\xa0al. 2022 ; Felten, Talbi, and Danoy 2024 ) .\\nFormally, MORL is defined over the same structure as an MOMDP, M = ( S , A , P , ùêú ) M=(S,A,P,\\\\mathbf{c}) , with state space S S , action space A A , transition function P P , and vector-valued cost or reward function ùêú \\u200b ( s , a ) ‚àà ‚Ñù d \\\\mathbf{c}(s,a)\\\\in\\\\mathbb{R}^{d} .\\nHowever, unlike MOMDPs, in MORL the transition probabilities and reward distributions are not assumed to be known a priori.\\nInstead, the agent learns a policy Œº : S ‚Üí A \\\\mu:S\\\\to A through experience, typically by interacting with the environment and receiving vector-valued feedback.\\nMORL generalizes MOS in that it seeks Pareto-optimal solutions across multiple objectives, but unlike MOS, it does not assume a static graph with deterministic edges.\\nRelative to MOSSP and MOMDP, MORL replaces planning with learning: instead of computing Pareto-optimal policies from a known model of uncertainty, the agent must discover them through exploration and approximation.\\nThus, MORL inherits the challenges of both reinforcement learning (e.g., exploration-exploitation trade-offs, function approximation) and MOS (e.g., dominance checks).\\nThe goal in MORL remains to approximate the Pareto set of policies, but learning algorithms must balance sample efficiency, preference sensitivity, and scalability in high-dimensional state and objective spaces.\\nIn this paper, we focus on MOS which can be seen as an instance of the more general multi-objective optimization (MOO) problem, (see, e.g., (Branke et\\xa0al. 2008 ; Miettinen 2012 ; Roijers and Whiteson 2017 ; Hwang and Masud 2012 ; Emmerich and Deutz 2018 ) ).\\nIt is important to highlight the similarities and differences between the two fields.\\nMOO is the most general formulation of problems in which several, possibly conflicting, objectives must be optimized simultaneously.\\nFormally, given a feasible set ùí≥ \\\\mathcal{X} and objective functions f i : ùí≥ ‚Üí ‚Ñù f_{i}:\\\\mathcal{X}\\\\to\\\\mathbb{R} for i = 1 , ‚Ä¶ , d i=1,\\\\ldots,d , the problem is to find the set of non-dominated solutions\\nwhere ùêü \\u200b ( x ) = ( f 1 \\u200b ( x ) , ‚Ä¶ , f d \\u200b ( x ) ) \\\\mathbf{f}(x)=(f_{1}(x),\\\\ldots,f_{d}(x)) .\\nHere, ùí≥ ‚ãÜ \\\\mathcal{X}^{\\\\star} corresponds to the Pareto set whose image in ‚Ñù d \\\\mathbb{R}^{d} is the PF.\\nRelative to MOS, MOO generalizes the underlying domain: whereas MOS is defined over paths in a deterministic graph with additive vector costs, MOO is agnostic to the structure of the feasible set and can capture continuous, combinatorial, or black-box domains.\\nCompared to stochastic settings such as MOSSP and MOMDP, MOO does not necessarily assume probabilistic dynamics or sequential decision processes; instead, it focuses purely on the optimization of static or offline-defined objectives.\\nIn contrast to MORL, MOO assumes direct access to the objective functions rather than learning them through interaction.\\nIn this sense, MOO serves as the broad umbrella under which MOS, MOSSP, MOMDP, and MORL can be seen as structured subclasses with additional constraints on the representation of ùí≥ \\\\mathcal{X} , the dynamics of decision-making, and the information available to the algorithm.\\nFrom an algorithmic point of view, in contrast to MOS, which builds upon search algorithms,\\nMOO typically builds upon local and global optimization methods such as\\ngenetic algorithms (Deb et\\xa0al. 2002 ; Deb and Jain 2013 ; Zhang and Li 2007 ) ,\\nparticle swarm optimization (Coello and Lechuga 2002 ) ,\\nand\\nsimulated annealing (Li and Landa-Silva 2011 ) .',\n",
       "      'subsections': [{'title': 'Handling uncertainty.',\n",
       "        'paragraphs': 'Recall that a MOS problem is defined using a graph G = ( V , E ) G=(V,E) together with edge costs ùêú \\\\mathbf{c} which present a deterministic model.\\nA Multi-objective Stochastic Shortest Path (MOSSP) problem extends the MOS framework by introducing probabilistic transitions between states (Roijers and Whiteson 2017 ) .\\nFormally, we are given a graph G = ( V , E ) G=(V,E) together with edge costs ùêú \\\\mathbf{c} and a transition probability distribution over successor vertices.\\nA policy Œº \\\\mu maps vertices to successor edges, inducing a distribution over paths from the start s s to the target t t .\\nThe cost of a policy is defined as the expected cumulative cost vector across objectives.\\nAs in MOS, policies are compared using dominance: a policy Œº 1 \\\\mu_{1} dominates Œº 2 \\\\mu_{2} if it has no worse expected cost in every objective and is strictly better in at least one.\\nThe goal is to compute a coverage PF of policies.\\nA Multi-objective Markov Decision Process (MOMDP) generalizes MOSSP by adopting an MDP formalism. In contrast to MOSSP which focuses on reaching a target in a stochastic graph with vector costs, MOMDP allow arbitrary horizon settings (e.g., finite, infinite, discounted) and sequential decision-making under uncertainty, not just reaching a target vertex.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Learning.',\n",
       "        'paragraphs': 'Multi-objective Reinforcement Learning (MORL) extends the MOS framework to settings where the agent interacts with an environment through repeated trial-and-error learning rather than having an explicit model of the state-transition dynamics (Hayes et\\xa0al. 2022 ; Felten, Talbi, and Danoy 2024 ) .\\nFormally, MORL is defined over the same structure as an MOMDP, M = ( S , A , P , ùêú ) M=(S,A,P,\\\\mathbf{c}) , with state space S S , action space A A , transition function P P , and vector-valued cost or reward function ùêú \\u200b ( s , a ) ‚àà ‚Ñù d \\\\mathbf{c}(s,a)\\\\in\\\\mathbb{R}^{d} .\\nHowever, unlike MOMDPs, in MORL the transition probabilities and reward distributions are not assumed to be known a priori.\\nInstead, the agent learns a policy Œº : S ‚Üí A \\\\mu:S\\\\to A through experience, typically by interacting with the environment and receiving vector-valued feedback.\\nMORL generalizes MOS in that it seeks Pareto-optimal solutions across multiple objectives, but unlike MOS, it does not assume a static graph with deterministic edges.\\nRelative to MOSSP and MOMDP, MORL replaces planning with learning: instead of computing Pareto-optimal policies from a known model of uncertainty, the agent must discover them through exploration and approximation.\\nThus, MORL inherits the challenges of both reinforcement learning (e.g., exploration-exploitation trade-offs, function approximation) and MOS (e.g., dominance checks).\\nThe goal in MORL remains to approximate the Pareto set of policies, but learning algorithms must balance sample efficiency, preference sensitivity, and scalability in high-dimensional state and objective spaces.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Multi-objective Optimization.',\n",
       "        'paragraphs': 'In this paper, we focus on MOS which can be seen as an instance of the more general multi-objective optimization (MOO) problem, (see, e.g., (Branke et\\xa0al. 2008 ; Miettinen 2012 ; Roijers and Whiteson 2017 ; Hwang and Masud 2012 ; Emmerich and Deutz 2018 ) ).\\nIt is important to highlight the similarities and differences between the two fields.\\nMOO is the most general formulation of problems in which several, possibly conflicting, objectives must be optimized simultaneously.\\nFormally, given a feasible set ùí≥ \\\\mathcal{X} and objective functions f i : ùí≥ ‚Üí ‚Ñù f_{i}:\\\\mathcal{X}\\\\to\\\\mathbb{R} for i = 1 , ‚Ä¶ , d i=1,\\\\ldots,d , the problem is to find the set of non-dominated solutions\\nwhere ùêü \\u200b ( x ) = ( f 1 \\u200b ( x ) , ‚Ä¶ , f d \\u200b ( x ) ) \\\\mathbf{f}(x)=(f_{1}(x),\\\\ldots,f_{d}(x)) .\\nHere, ùí≥ ‚ãÜ \\\\mathcal{X}^{\\\\star} corresponds to the Pareto set whose image in ‚Ñù d \\\\mathbb{R}^{d} is the PF.\\nRelative to MOS, MOO generalizes the underlying domain: whereas MOS is defined over paths in a deterministic graph with additive vector costs, MOO is agnostic to the structure of the feasible set and can capture continuous, combinatorial, or black-box domains.\\nCompared to stochastic settings such as MOSSP and MOMDP, MOO does not necessarily assume probabilistic dynamics or sequential decision processes; instead, it focuses purely on the optimization of static or offline-defined objectives.\\nIn contrast to MORL, MOO assumes direct access to the objective functions rather than learning them through interaction.\\nIn this sense, MOO serves as the broad umbrella under which MOS, MOSSP, MOMDP, and MORL can be seen as structured subclasses with additional constraints on the representation of ùí≥ \\\\mathcal{X} , the dynamics of decision-making, and the information available to the algorithm.\\nFrom an algorithmic point of view, in contrast to MOS, which builds upon search algorithms,\\nMOO typically builds upon local and global optimization methods such as\\ngenetic algorithms (Deb et\\xa0al. 2002 ; Deb and Jain 2013 ; Zhang and Li 2007 ) ,\\nparticle swarm optimization (Coello and Lechuga 2002 ) ,\\nand\\nsimulated annealing (Li and Landa-Silva 2011 ) .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '3 Algorithmic Advances',\n",
       "    'paragraphs': 'In this section we outline recent algorithmic advances in MOS. We start with a brief historical overview and continue to outline tools, techniques and algorithms that advances the state-of-the-art in MOS. We conclude with a brief description of advances in generalizations of MOS (Sec. 2.3 ) that have close ties to MOS algorithms\\nEarly work on MOS established the algorithmic framework which is the basis of most modern algorithms (Hansen 1980 ; Martins 1984 ; Warburton 1987 ) .\\nFor efficiently computing Œ† ‚ãÜ \\\\Pi^{\\\\star} , two notable approaches emerged.\\nThe first generalizes the label-correcting paradigm to the multi-objective setting (Guerriero and Musmanno 2001 ) . Label-correcting is an iterative shortest-path method that repeatedly updates tentative distance labels of vertices whenever a shorter path is found, allowing multiple updates per vertex until no further improvements are possible.\\nThe second generalizes the celebrated A ‚àó algorithm (Hart, Nilsson, and Raphael 1968 ) which we detail next as most of the recent advancements fall under this category.\\nA notable contribution was the work by Stewart et al.\\xa0( 1991 ), who introduced\\nMulti-Objective A* ( MOA ‚àó ). MOA ‚àó served as the foundation to multiple extensions (see, e.g., (Mandow and De\\xa0La\\xa0Cruz 2005 , 2010 ) ) which differ in\\nwhich information is contained in the nodes,\\nhow nodes are ordered in the priority queue\\nand\\nhow dominance checks are implemented and when they are performed (upon generation or upon expansion).\\nA key insight that dramatically improved the efficiency of these algorithms was to order the nodes in the priority queue in increasing lexicographic order and apply the notion of dimensionality reduction (Pulido, Mandow, and P√©rez-de-la Cruz 2015 ) . See (Salzman et\\xa0al. 2023 ) for an overview of the approach.\\nThe resulting algorithm based on this idea was termed NAMOA-dr 1 1 1 Here, ‚Äòdr‚Äô stands for dimensionality reduction. .\\nEarly approaches (Warburton 1987 ; Perny and Spanjaard 2008 ; Tsaggouris and Zaroliagis 2009 ; Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) to approximating Œ† ‚ãÜ \\\\Pi^{\\\\star} focused on Fully Polynomial Time Approximation Schemes 2 2 2 An FPTAS is an approximation scheme whose time complexity is polynomial in the input size and also polynomial in 1 / Œµ 1/\\\\varepsilon where Œµ \\\\varepsilon is the approximation factor. (FPTAS) (Vazirani 2001 ) .\\nUnfortunately,\\nrunning these approaches on moderately-sized graphs (i.e., with roughly 10 , 000 10,000 vertices) is often impractical (Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) .\\nIn recent years, several algorithms dramatically improved the efficiency of exact MOS algorithms (see, e.g., (de\\xa0las Casas et\\xa0al. 2023 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2021 ; Ahmadi et\\xa0al. 2021 ; Ren et\\xa0al. 2025 ) ).\\nNotable examples that reduce the computational complexity of key operations include\\n(i) BOA ‚àó (Hern√°ndez et\\xa0al. 2023 ) which adapted and simplified NAMOA-dr for the bi-objective setting performing dominance checks in O \\u200b ( 1 ) O(1) , and\\n(ii)\\xa0recent work (Zhang et\\xa0al. 2024b ; Ren et\\xa0al. 2022a ) which improves node indexing and data structures for dominance checks of two and three objectives to yield dramatic speedups.\\nFinally, recent work (Ahmadi et\\xa0al. 2024 ) considered the more general set of graphs with negative edges.\\nGoldin and Salzman ( 2021 ) suggested PPA ‚àó , an extension of BOA ‚àó that introduced new pruning techniques to efficiently compute an Œµ \\\\mathbf{\\\\varepsilon} -approximate PF Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for the bi-objective setting. PPA ‚àó was later generalized by Zhang et\\xa0al. ( 2022 ) who suggested the A ‚àó pex algorithm which allows to compute Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for any number of objectives. The efficiency of A ‚àó pex stems from the observation that paths whose cost is very similar can be grouped in an efficient manner allowing to dramatically prune the PF. A ‚àó pex was later used to exploit correlation of edge costs (Halle et\\xa0al. 2025 ) ,\\ndevelop anytime MOS algorithms (Zhang et\\xa0al. 2024c ) and\\nmore (Zhang et\\xa0al. 2024a ) ,\\nsee also Sec. 4 .\\nWhile there has been some research on parallelizing MOS algorithms (see, e.g., (Sanders and Mandow 2013 ; Erb, Kobitzsch, and Sanders 2014 ; Medrano and Church 2015 ) , this research direction has been largely unexplored (Salzman et\\xa0al. 2023 ) .\\nTwo notable exceptions include\\n(i)\\xa0The work by Ahmadi et\\xa0al. ( 2025 ) who explored permutations of objective orderings in parallel while sharing bounds to collapse subproblems, achieving near-linear speedups on many-core systems\\nand\\n(ii)\\xa0the work by Hern√°ndez et\\xa0al. ( 2024 ) who suggest an approach to compute set dominance checks or SDC (a key procedure, which dominates the running time of many state-of-the-art MOS algorithms) in parallel.\\nThey exploit vectorized the operations offered by ‚ÄúSingle Instruction/Multiple Data‚Äù (SIMD) instructions to perform SDC on ubiquitous consumer CPUs thereby dramatically improving the runtime of existing MOS algorithms.\\nThere have not been many recent theoretical advances. de\\xa0las Casas et\\xa0al. ( 2021 ) suggested an FPTAS for the new setting of dynamic MOS in which edges cost can change online. Skyler et\\xa0al. ( 2024 ) extended theory from single-objective search (SOS) to MOS which characterizes the set of vertices and search nodes that any unidirectional search algorithm must expand to prove the optimality of the solution.\\nSpecifically, they introduce a classification of vertices into must-expand , maybe-expand , and never-expand categories.\\nThe notable difference between SOS and MOS is that vertices must be expanded to\\n(i)\\xa0prove that any path in a PF is Pareto-optimal (these are called optimality vertices)\\nand\\n(ii)\\xa0ensure that there are no more solution costs that are not represented in the PF (these are called completness vertices).\\nCompleteness vertices have no analogy in SOS.\\nKey to the success of heuristic search in general, and heuristic MOS in particular, is the ability to incorporate domain knowledge using heuristics that guide the algorithm.\\nAlmost all MOS algorithms use the ‚Äúideal point heuristic‚Äù ùê° ideal \\\\mathbf{h_{\\\\rm ideal}} , which combines a set of d d single-objective heuristics h 1 , ‚Ä¶ , h d h_{1},\\\\ldots,h_{d} .\\nHere, h i : V ‚Üí ‚Ñù ‚â• 0 h_{i}:V\\\\rightarrow\\\\mathbb{R}_{\\\\geq 0} corresponds to the shortest path from each vertex according to the i i ‚Äôth objective and ‚àÄ v ‚àà V \\u200b ùê° ideal \\u200b ( v ) := ( h 1 \\u200b ( v ) , ‚Ä¶ , h d \\u200b ( v ) ) \\\\forall v\\\\in V~\\\\mathbf{h_{\\\\rm ideal}}(v):=(h_{1}(v),\\\\ldots,h_{d}(v)) .\\nHowever, in contrast to SOS, in the general case of MOS, the heuristic value of a vertex v v is not a single cost vector, but a set of cost vectors (see (Mandow and De\\xa0La\\xa0Cruz 2010 ) for original definition and (Salzman et\\xa0al. 2023 ) for an in-depth discussion).\\nWhile such heuristics, called Multi-Value Heuristics (MVH) are much more informative, the overhead of computing and using MVHs in MOS algorithms can be large and the total runtime is often larger than when using ùê° ideal \\\\mathbf{h_{\\\\rm ideal}} (Gei√üer et\\xa0al. 2022 ) .\\nA notable example where MVHs are used is the recent work by Zhang et\\xa0al. ( 2023a ) , which generalize Differential Heuristics (DHs) (Goldberg and Harrelson 2005 ) , a class of memory-based heuristics for SOS, to bi-objective search, resulting in Bi-Objective Differential Heuristics ( BO-DHs ). They propose several techniques to reduce the memory usage and computational overhead of BO-DHs , demonstrating reductions in runtime of a bi-objective search algorithm by up to an order of magnitude.\\nWhile there has been many advances in MOSSP, MOMDP and MOO algorithms, which are not the focus of this paper, here we mention work that is closely related to MOS.\\nRecent work by Chen, Trevizan, and Thi√©baux ( 2023 ) suggests adapting heuristic-search algorithms (which are the foundation of MOS algorithms) for MOSSP. This is done by extending (single-objective) stochastic shortest-path algorithms, such as LAO* (Hansen and Zilberstein 2001 ) and LRTDP (Bonet and Geffner 2001 ) , to the multi-objective setting.\\nThey also study how to guide their algorithms with domain-independent heuristics to account for the\\nprobabilistic and multi-objective features of the problem.',\n",
       "    'subsections': [{'title': '3.1 Brief historical overview',\n",
       "      'paragraphs': 'Early work on MOS established the algorithmic framework which is the basis of most modern algorithms (Hansen 1980 ; Martins 1984 ; Warburton 1987 ) .\\nFor efficiently computing Œ† ‚ãÜ \\\\Pi^{\\\\star} , two notable approaches emerged.\\nThe first generalizes the label-correcting paradigm to the multi-objective setting (Guerriero and Musmanno 2001 ) . Label-correcting is an iterative shortest-path method that repeatedly updates tentative distance labels of vertices whenever a shorter path is found, allowing multiple updates per vertex until no further improvements are possible.\\nThe second generalizes the celebrated A ‚àó algorithm (Hart, Nilsson, and Raphael 1968 ) which we detail next as most of the recent advancements fall under this category.\\nA notable contribution was the work by Stewart et al.\\xa0( 1991 ), who introduced\\nMulti-Objective A* ( MOA ‚àó ). MOA ‚àó served as the foundation to multiple extensions (see, e.g., (Mandow and De\\xa0La\\xa0Cruz 2005 , 2010 ) ) which differ in\\nwhich information is contained in the nodes,\\nhow nodes are ordered in the priority queue\\nand\\nhow dominance checks are implemented and when they are performed (upon generation or upon expansion).\\nA key insight that dramatically improved the efficiency of these algorithms was to order the nodes in the priority queue in increasing lexicographic order and apply the notion of dimensionality reduction (Pulido, Mandow, and P√©rez-de-la Cruz 2015 ) . See (Salzman et\\xa0al. 2023 ) for an overview of the approach.\\nThe resulting algorithm based on this idea was termed NAMOA-dr 1 1 1 Here, ‚Äòdr‚Äô stands for dimensionality reduction. .\\nEarly approaches (Warburton 1987 ; Perny and Spanjaard 2008 ; Tsaggouris and Zaroliagis 2009 ; Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) to approximating Œ† ‚ãÜ \\\\Pi^{\\\\star} focused on Fully Polynomial Time Approximation Schemes 2 2 2 An FPTAS is an approximation scheme whose time complexity is polynomial in the input size and also polynomial in 1 / Œµ 1/\\\\varepsilon where Œµ \\\\varepsilon is the approximation factor. (FPTAS) (Vazirani 2001 ) .\\nUnfortunately,\\nrunning these approaches on moderately-sized graphs (i.e., with roughly 10 , 000 10,000 vertices) is often impractical (Breugem, Dollevoet, and van\\xa0den Heuvel 2017 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Algorithmic advances in MOS',\n",
       "      'paragraphs': 'In recent years, several algorithms dramatically improved the efficiency of exact MOS algorithms (see, e.g., (de\\xa0las Casas et\\xa0al. 2023 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2021 ; Ahmadi et\\xa0al. 2021 ; Ren et\\xa0al. 2025 ) ).\\nNotable examples that reduce the computational complexity of key operations include\\n(i) BOA ‚àó (Hern√°ndez et\\xa0al. 2023 ) which adapted and simplified NAMOA-dr for the bi-objective setting performing dominance checks in O \\u200b ( 1 ) O(1) , and\\n(ii)\\xa0recent work (Zhang et\\xa0al. 2024b ; Ren et\\xa0al. 2022a ) which improves node indexing and data structures for dominance checks of two and three objectives to yield dramatic speedups.\\nFinally, recent work (Ahmadi et\\xa0al. 2024 ) considered the more general set of graphs with negative edges.\\nGoldin and Salzman ( 2021 ) suggested PPA ‚àó , an extension of BOA ‚àó that introduced new pruning techniques to efficiently compute an Œµ \\\\mathbf{\\\\varepsilon} -approximate PF Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for the bi-objective setting. PPA ‚àó was later generalized by Zhang et\\xa0al. ( 2022 ) who suggested the A ‚àó pex algorithm which allows to compute Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for any number of objectives. The efficiency of A ‚àó pex stems from the observation that paths whose cost is very similar can be grouped in an efficient manner allowing to dramatically prune the PF. A ‚àó pex was later used to exploit correlation of edge costs (Halle et\\xa0al. 2025 ) ,\\ndevelop anytime MOS algorithms (Zhang et\\xa0al. 2024c ) and\\nmore (Zhang et\\xa0al. 2024a ) ,\\nsee also Sec. 4 .\\nWhile there has been some research on parallelizing MOS algorithms (see, e.g., (Sanders and Mandow 2013 ; Erb, Kobitzsch, and Sanders 2014 ; Medrano and Church 2015 ) , this research direction has been largely unexplored (Salzman et\\xa0al. 2023 ) .\\nTwo notable exceptions include\\n(i)\\xa0The work by Ahmadi et\\xa0al. ( 2025 ) who explored permutations of objective orderings in parallel while sharing bounds to collapse subproblems, achieving near-linear speedups on many-core systems\\nand\\n(ii)\\xa0the work by Hern√°ndez et\\xa0al. ( 2024 ) who suggest an approach to compute set dominance checks or SDC (a key procedure, which dominates the running time of many state-of-the-art MOS algorithms) in parallel.\\nThey exploit vectorized the operations offered by ‚ÄúSingle Instruction/Multiple Data‚Äù (SIMD) instructions to perform SDC on ubiquitous consumer CPUs thereby dramatically improving the runtime of existing MOS algorithms.\\nThere have not been many recent theoretical advances. de\\xa0las Casas et\\xa0al. ( 2021 ) suggested an FPTAS for the new setting of dynamic MOS in which edges cost can change online. Skyler et\\xa0al. ( 2024 ) extended theory from single-objective search (SOS) to MOS which characterizes the set of vertices and search nodes that any unidirectional search algorithm must expand to prove the optimality of the solution.\\nSpecifically, they introduce a classification of vertices into must-expand , maybe-expand , and never-expand categories.\\nThe notable difference between SOS and MOS is that vertices must be expanded to\\n(i)\\xa0prove that any path in a PF is Pareto-optimal (these are called optimality vertices)\\nand\\n(ii)\\xa0ensure that there are no more solution costs that are not represented in the PF (these are called completness vertices).\\nCompleteness vertices have no analogy in SOS.\\nKey to the success of heuristic search in general, and heuristic MOS in particular, is the ability to incorporate domain knowledge using heuristics that guide the algorithm.\\nAlmost all MOS algorithms use the ‚Äúideal point heuristic‚Äù ùê° ideal \\\\mathbf{h_{\\\\rm ideal}} , which combines a set of d d single-objective heuristics h 1 , ‚Ä¶ , h d h_{1},\\\\ldots,h_{d} .\\nHere, h i : V ‚Üí ‚Ñù ‚â• 0 h_{i}:V\\\\rightarrow\\\\mathbb{R}_{\\\\geq 0} corresponds to the shortest path from each vertex according to the i i ‚Äôth objective and ‚àÄ v ‚àà V \\u200b ùê° ideal \\u200b ( v ) := ( h 1 \\u200b ( v ) , ‚Ä¶ , h d \\u200b ( v ) ) \\\\forall v\\\\in V~\\\\mathbf{h_{\\\\rm ideal}}(v):=(h_{1}(v),\\\\ldots,h_{d}(v)) .\\nHowever, in contrast to SOS, in the general case of MOS, the heuristic value of a vertex v v is not a single cost vector, but a set of cost vectors (see (Mandow and De\\xa0La\\xa0Cruz 2010 ) for original definition and (Salzman et\\xa0al. 2023 ) for an in-depth discussion).\\nWhile such heuristics, called Multi-Value Heuristics (MVH) are much more informative, the overhead of computing and using MVHs in MOS algorithms can be large and the total runtime is often larger than when using ùê° ideal \\\\mathbf{h_{\\\\rm ideal}} (Gei√üer et\\xa0al. 2022 ) .\\nA notable example where MVHs are used is the recent work by Zhang et\\xa0al. ( 2023a ) , which generalize Differential Heuristics (DHs) (Goldberg and Harrelson 2005 ) , a class of memory-based heuristics for SOS, to bi-objective search, resulting in Bi-Objective Differential Heuristics ( BO-DHs ). They propose several techniques to reduce the memory usage and computational overhead of BO-DHs , demonstrating reductions in runtime of a bi-objective search algorithm by up to an order of magnitude.',\n",
       "      'subsections': [{'title': 'Exact approaches.',\n",
       "        'paragraphs': 'In recent years, several algorithms dramatically improved the efficiency of exact MOS algorithms (see, e.g., (de\\xa0las Casas et\\xa0al. 2023 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2021 ; Ahmadi et\\xa0al. 2021 ; Ren et\\xa0al. 2025 ) ).\\nNotable examples that reduce the computational complexity of key operations include\\n(i) BOA ‚àó (Hern√°ndez et\\xa0al. 2023 ) which adapted and simplified NAMOA-dr for the bi-objective setting performing dominance checks in O \\u200b ( 1 ) O(1) , and\\n(ii)\\xa0recent work (Zhang et\\xa0al. 2024b ; Ren et\\xa0al. 2022a ) which improves node indexing and data structures for dominance checks of two and three objectives to yield dramatic speedups.\\nFinally, recent work (Ahmadi et\\xa0al. 2024 ) considered the more general set of graphs with negative edges.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Approximate approaches.',\n",
       "        'paragraphs': 'Goldin and Salzman ( 2021 ) suggested PPA ‚àó , an extension of BOA ‚àó that introduced new pruning techniques to efficiently compute an Œµ \\\\mathbf{\\\\varepsilon} -approximate PF Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for the bi-objective setting. PPA ‚àó was later generalized by Zhang et\\xa0al. ( 2022 ) who suggested the A ‚àó pex algorithm which allows to compute Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for any number of objectives. The efficiency of A ‚àó pex stems from the observation that paths whose cost is very similar can be grouped in an efficient manner allowing to dramatically prune the PF. A ‚àó pex was later used to exploit correlation of edge costs (Halle et\\xa0al. 2025 ) ,\\ndevelop anytime MOS algorithms (Zhang et\\xa0al. 2024c ) and\\nmore (Zhang et\\xa0al. 2024a ) ,\\nsee also Sec. 4 .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Parallelization.',\n",
       "        'paragraphs': 'While there has been some research on parallelizing MOS algorithms (see, e.g., (Sanders and Mandow 2013 ; Erb, Kobitzsch, and Sanders 2014 ; Medrano and Church 2015 ) , this research direction has been largely unexplored (Salzman et\\xa0al. 2023 ) .\\nTwo notable exceptions include\\n(i)\\xa0The work by Ahmadi et\\xa0al. ( 2025 ) who explored permutations of objective orderings in parallel while sharing bounds to collapse subproblems, achieving near-linear speedups on many-core systems\\nand\\n(ii)\\xa0the work by Hern√°ndez et\\xa0al. ( 2024 ) who suggest an approach to compute set dominance checks or SDC (a key procedure, which dominates the running time of many state-of-the-art MOS algorithms) in parallel.\\nThey exploit vectorized the operations offered by ‚ÄúSingle Instruction/Multiple Data‚Äù (SIMD) instructions to perform SDC on ubiquitous consumer CPUs thereby dramatically improving the runtime of existing MOS algorithms.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Theory.',\n",
       "        'paragraphs': 'There have not been many recent theoretical advances. de\\xa0las Casas et\\xa0al. ( 2021 ) suggested an FPTAS for the new setting of dynamic MOS in which edges cost can change online. Skyler et\\xa0al. ( 2024 ) extended theory from single-objective search (SOS) to MOS which characterizes the set of vertices and search nodes that any unidirectional search algorithm must expand to prove the optimality of the solution.\\nSpecifically, they introduce a classification of vertices into must-expand , maybe-expand , and never-expand categories.\\nThe notable difference between SOS and MOS is that vertices must be expanded to\\n(i)\\xa0prove that any path in a PF is Pareto-optimal (these are called optimality vertices)\\nand\\n(ii)\\xa0ensure that there are no more solution costs that are not represented in the PF (these are called completness vertices).\\nCompleteness vertices have no analogy in SOS.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Heuristics.',\n",
       "        'paragraphs': 'Key to the success of heuristic search in general, and heuristic MOS in particular, is the ability to incorporate domain knowledge using heuristics that guide the algorithm.\\nAlmost all MOS algorithms use the ‚Äúideal point heuristic‚Äù ùê° ideal \\\\mathbf{h_{\\\\rm ideal}} , which combines a set of d d single-objective heuristics h 1 , ‚Ä¶ , h d h_{1},\\\\ldots,h_{d} .\\nHere, h i : V ‚Üí ‚Ñù ‚â• 0 h_{i}:V\\\\rightarrow\\\\mathbb{R}_{\\\\geq 0} corresponds to the shortest path from each vertex according to the i i ‚Äôth objective and ‚àÄ v ‚àà V \\u200b ùê° ideal \\u200b ( v ) := ( h 1 \\u200b ( v ) , ‚Ä¶ , h d \\u200b ( v ) ) \\\\forall v\\\\in V~\\\\mathbf{h_{\\\\rm ideal}}(v):=(h_{1}(v),\\\\ldots,h_{d}(v)) .\\nHowever, in contrast to SOS, in the general case of MOS, the heuristic value of a vertex v v is not a single cost vector, but a set of cost vectors (see (Mandow and De\\xa0La\\xa0Cruz 2010 ) for original definition and (Salzman et\\xa0al. 2023 ) for an in-depth discussion).\\nWhile such heuristics, called Multi-Value Heuristics (MVH) are much more informative, the overhead of computing and using MVHs in MOS algorithms can be large and the total runtime is often larger than when using ùê° ideal \\\\mathbf{h_{\\\\rm ideal}} (Gei√üer et\\xa0al. 2022 ) .\\nA notable example where MVHs are used is the recent work by Zhang et\\xa0al. ( 2023a ) , which generalize Differential Heuristics (DHs) (Goldberg and Harrelson 2005 ) , a class of memory-based heuristics for SOS, to bi-objective search, resulting in Bi-Objective Differential Heuristics ( BO-DHs ). They propose several techniques to reduce the memory usage and computational overhead of BO-DHs , demonstrating reductions in runtime of a bi-objective search algorithm by up to an order of magnitude.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.3 Algorithmic advances in MOS extensions',\n",
       "      'paragraphs': 'While there has been many advances in MOSSP, MOMDP and MOO algorithms, which are not the focus of this paper, here we mention work that is closely related to MOS.\\nRecent work by Chen, Trevizan, and Thi√©baux ( 2023 ) suggests adapting heuristic-search algorithms (which are the foundation of MOS algorithms) for MOSSP. This is done by extending (single-objective) stochastic shortest-path algorithms, such as LAO* (Hansen and Zilberstein 2001 ) and LRTDP (Bonet and Geffner 2001 ) , to the multi-objective setting.\\nThey also study how to guide their algorithms with domain-independent heuristics to account for the\\nprobabilistic and multi-objective features of the problem.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 MOS as an Algorithmic Toolbox',\n",
       "    'paragraphs': 'Recently the algorithmic toolbox developed for MOS has also proven useful in other domains.\\nSome, are new variants of MOS while others are seemingly unrelated optimization problems where MOS approaches have been useful.\\nThe Multi-Objective Minimum Spanning Tree (MO-MST) problem generalizes the classical MST problem to settings where edges are labeled with cost vectors. Instead of a single spanning tree with minimal total weight, the goal is to identify a Pareto set of spanning trees that represent undominated trade-offs among objectives. However, unlike MST, for which there are polynomial time\\nalgorithms that solve it, MO-MST is NP-hard (Fernandes et\\xa0al. 2020 ) .\\nMO-MST is important for communication networks, where spanning trees must balance latency, bandwidth and resilience, and in transport and logistics, where constructing infrastructure with multiple cost criteria is essential (see, e.g., (Levin and Nuriakhmetov 2011 ) ).\\nMO-MST algorithms borrow heavily from MOS techniques (see, e.g., (Sourd and Spanjaard 2008 ; Fernandes et\\xa0al. 2020 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2025 ) ).\\nIn many real-world problems with multiple objectives, the objectives interact in a complex manner, leading to problem formulations that do not allow out-of-the-box usage of MOS algorithms (Fu et\\xa0al. 2023 ; Slutsky et\\xa0al. 2021 ; Axelrod, Kaelbling, and Lozano-P√©rez 2018 ) .\\nRoughly speaking, this is because the search algorithms needs to treat differently paths that are and that are not solutions.\\nFor example, in robot inspection planning (Fu et\\xa0al. 2023 ; Alpert et\\xa0al. 2025 ) , a robot is required to view as many points of interest (POI) as possible using an on-board sensor while minimizing path length.\\nThe two objectives which define a solution œÄ \\\\pi are\\nthe number of POIs viewed along œÄ \\\\pi and the length of œÄ \\\\pi .\\nHowever, every path that is not a solution must keep track of which POI was viewed, essentially defining a binary objective for each POI. This is because two paths to the same vertex that viewed different POIs cannot dominate one another as their final bi-objective cost depends on which POIs will be viewed in the future.\\nThis creates a mismatch between objectives at intermediate nodes, which we term hidden objectives , and objectives at solution nodes, which we term solution objectives .\\nThe relation between solution objectives and hidden objectives is captured via some method of objective aggregation (Peer et\\xa0al. 2025 ) .\\nReturning to our inspection-planning example,\\nthere is one hidden objective that corresponds to each POI as well as one for path length\\nand there are two solution objectives corresponding to number of POIs viewed and path length.\\nHere objective aggregation is done by adding all (binary) cost values of POI hidden objectives.\\nWe call such problems MOS with objective aggregation (MOS-OA).\\nImportantly, MOS-OA algorithms can naturally employ the MOS algorithmic toolbox. Indeed, early versions of A ‚àó pex were developed in the context of MOS-OA (Fu et\\xa0al. 2023 ) .\\nThe Multi-Agent Path Finding (MAPF) problem (Stern et\\xa0al. 2019 ) involves finding non-colliding paths for multiple agents from their start locations to their respective target locations in a shared environment. The primary goal is to optimize a metric such as the sum of travel time of all agents or the makespan (i.e., task completion time).\\nThe Multi-Objective MAPF (MO-MAPF) problem extends the MAPF problem to multiple, often conflicting, optimization criteria such as makespan, energy consumption, safety margin, or fairness among agents.\\nThe result is not a single plan but a PF of MAPF plans, each representing a different trade-off.\\nRecent algorithms (see, e.g., (Ren, Rathinam, and Choset 2021b , a , 2023 ; Wang et\\xa0al. 2024 ) ) integrate MAPF and MOS to obtain scalable algorithms for this purpose.\\nIn the Constrained Shortest-Path problem (CSP) (Storandt 2012 ) we are interested in computing a shortest path subject to some constraints (e.g., limited energy consumption for an autonomous agent).\\nThis setting was generalized by Skyler et\\xa0al. ( 2022 ) who consider the setting where we need to find a solution which belongs to Œ† ‚ãÜ \\\\Pi^{\\\\star} whose costs are below given upper bounds on each objective.\\nLater Zhang et\\xa0al. ( 2024a ) considered a similar setting but where we need to find a solution which belongs to Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for some Œµ > 0 \\\\mathbf{\\\\varepsilon}>0 .\\nIn the k k -Shortest Simple Path ( k k -SSP) problem, we are given a graph G = ( V , E ) G=(V,E) with regular (scalar) edge costs. Given start and target vertices s , t ‚àà V s,t\\\\in V and a parameter k k , we are tasked to compute the k k shortest paths between s s and t t .\\nWhile this is a single-objective problem, recently de\\xa0las Casas et\\xa0al. ( 2025 ) have shown that the 2 2 -SSP can be solved by a reduction to a bi-objective search problem.',\n",
       "    'subsections': [{'title': 'Multi-objective minimum spanning tree.',\n",
       "      'paragraphs': 'The Multi-Objective Minimum Spanning Tree (MO-MST) problem generalizes the classical MST problem to settings where edges are labeled with cost vectors. Instead of a single spanning tree with minimal total weight, the goal is to identify a Pareto set of spanning trees that represent undominated trade-offs among objectives. However, unlike MST, for which there are polynomial time\\nalgorithms that solve it, MO-MST is NP-hard (Fernandes et\\xa0al. 2020 ) .\\nMO-MST is important for communication networks, where spanning trees must balance latency, bandwidth and resilience, and in transport and logistics, where constructing infrastructure with multiple cost criteria is essential (see, e.g., (Levin and Nuriakhmetov 2011 ) ).\\nMO-MST algorithms borrow heavily from MOS techniques (see, e.g., (Sourd and Spanjaard 2008 ; Fernandes et\\xa0al. 2020 ; de\\xa0las Casas, Sede√±o-Noda, and Bornd√∂rfer 2025 ) ).',\n",
       "      'subsections': []},\n",
       "     {'title': 'MOS with objective aggregation.',\n",
       "      'paragraphs': 'In many real-world problems with multiple objectives, the objectives interact in a complex manner, leading to problem formulations that do not allow out-of-the-box usage of MOS algorithms (Fu et\\xa0al. 2023 ; Slutsky et\\xa0al. 2021 ; Axelrod, Kaelbling, and Lozano-P√©rez 2018 ) .\\nRoughly speaking, this is because the search algorithms needs to treat differently paths that are and that are not solutions.\\nFor example, in robot inspection planning (Fu et\\xa0al. 2023 ; Alpert et\\xa0al. 2025 ) , a robot is required to view as many points of interest (POI) as possible using an on-board sensor while minimizing path length.\\nThe two objectives which define a solution œÄ \\\\pi are\\nthe number of POIs viewed along œÄ \\\\pi and the length of œÄ \\\\pi .\\nHowever, every path that is not a solution must keep track of which POI was viewed, essentially defining a binary objective for each POI. This is because two paths to the same vertex that viewed different POIs cannot dominate one another as their final bi-objective cost depends on which POIs will be viewed in the future.\\nThis creates a mismatch between objectives at intermediate nodes, which we term hidden objectives , and objectives at solution nodes, which we term solution objectives .\\nThe relation between solution objectives and hidden objectives is captured via some method of objective aggregation (Peer et\\xa0al. 2025 ) .\\nReturning to our inspection-planning example,\\nthere is one hidden objective that corresponds to each POI as well as one for path length\\nand there are two solution objectives corresponding to number of POIs viewed and path length.\\nHere objective aggregation is done by adding all (binary) cost values of POI hidden objectives.\\nWe call such problems MOS with objective aggregation (MOS-OA).\\nImportantly, MOS-OA algorithms can naturally employ the MOS algorithmic toolbox. Indeed, early versions of A ‚àó pex were developed in the context of MOS-OA (Fu et\\xa0al. 2023 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': 'Multi-objective Multi-Agent Path Finding.',\n",
       "      'paragraphs': 'The Multi-Agent Path Finding (MAPF) problem (Stern et\\xa0al. 2019 ) involves finding non-colliding paths for multiple agents from their start locations to their respective target locations in a shared environment. The primary goal is to optimize a metric such as the sum of travel time of all agents or the makespan (i.e., task completion time).\\nThe Multi-Objective MAPF (MO-MAPF) problem extends the MAPF problem to multiple, often conflicting, optimization criteria such as makespan, energy consumption, safety margin, or fairness among agents.\\nThe result is not a single plan but a PF of MAPF plans, each representing a different trade-off.\\nRecent algorithms (see, e.g., (Ren, Rathinam, and Choset 2021b , a , 2023 ; Wang et\\xa0al. 2024 ) ) integrate MAPF and MOS to obtain scalable algorithms for this purpose.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Constrained shortest path.',\n",
       "      'paragraphs': 'In the Constrained Shortest-Path problem (CSP) (Storandt 2012 ) we are interested in computing a shortest path subject to some constraints (e.g., limited energy consumption for an autonomous agent).\\nThis setting was generalized by Skyler et\\xa0al. ( 2022 ) who consider the setting where we need to find a solution which belongs to Œ† ‚ãÜ \\\\Pi^{\\\\star} whose costs are below given upper bounds on each objective.\\nLater Zhang et\\xa0al. ( 2024a ) considered a similar setting but where we need to find a solution which belongs to Œ† Œµ ‚ãÜ \\\\Pi^{\\\\star}_{\\\\mathbf{\\\\varepsilon}} for some Œµ > 0 \\\\mathbf{\\\\varepsilon}>0 .',\n",
       "      'subsections': []},\n",
       "     {'title': 'kk-Shortest simple path.',\n",
       "      'paragraphs': 'In the k k -Shortest Simple Path ( k k -SSP) problem, we are given a graph G = ( V , E ) G=(V,E) with regular (scalar) edge costs. Given start and target vertices s , t ‚àà V s,t\\\\in V and a parameter k k , we are tasked to compute the k k shortest paths between s s and t t .\\nWhile this is a single-objective problem, recently de\\xa0las Casas et\\xa0al. ( 2025 ) have shown that the 2 2 -SSP can be solved by a reduction to a bi-objective search problem.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Emerging Applications',\n",
       "    'paragraphs': 'We briefly review several diverse domains where MOS and its variants have been recently used. This showcases the applicability of MOS despite its relative simplicity when compared to the richer models reviewed in Sec. 2.3 .\\nMOS has been applied to design problems in chemistry, biology, and engineering.\\nOne example is retrosynthesis planning in computational chemistry, which is the problem of finding reaction sequences that produce a target molecule. Lai et\\xa0al. ( 2025 ) consider multiple objectives and, by searching for un-dominated synthesis routes, they were able to present several candidate pathways to a human chemist to evaluate and choose from.\\nSimilarly, MOS found applications in drug discovery and generative design. For example, Southiratn et\\xa0al. ( 2025 ) suggested a bi-objective search algorithm for generating molecular structures that balance affinity to two proteins while also satisfying drug-like property constraints The result is a set of novel\\nmolecular candidates with high predicted efficacy and acceptable pharmacological profiles. which traditional single-objective or scalarized approaches would have likely missed.\\nMulti-modal journey planning determines routes combining different transport modes (Bast et\\xa0al. 2016 ) , which inherently involves multi-objective optimization such as time, cost and comfort.\\nThese methods (see e.g., (Potthoff and Sauer 2022b , a ) ) build upon MOS algorithms to make queries tractable at metropolitan scale.\\nMany real-world uses of such algorithms have recently been documented.\\nFor example, OpenTripPlanner 2 is an open-source multi-modal journey planner for public transportation in combination with bicycling, walking, and mobility services such as bike share and ride hailing. It has been deployed nationwide in Norway and Finland. In Portland (Oregon), it provides about 40,000 trip plans on a typical weekday (OTP 2025 ) .\\nIn robotics, multiple objectives often need to be simultaneously balanced (e.g., cost, energy and safety)\\nIn Sec. 4 we discussed robot inspection planning in the context of MOS-OA.\\nAnother example is autonomous vehicle (AV) planning using rulebooks (Slutsky et\\xa0al. 2021 ; Censi et\\xa0al. 2019 ; Halder and Althoff 2025 ; Penlington, Zanardi, and Frazzoli 2024 ) , where the system must generate a trajectory that complies with a set of potentially conflicting traffic rules. Consider, for instance, Singapore‚Äôs Final Theory of Driving that requires\\n(i) maintaining at least a one-meter gap when passing a parked vehicle and (ii)\\xa0prohibits crossing a solid double white lane divider. When an AV encounters a car improperly parked along such a divider, it may be impossible to satisfy both requirements simultaneously.\\nFortunately, requirements often form in a hierarchy‚Äîe.g., avoiding a collision is more important than keeping safety margin from parked vehicles and than maintaining lane.\\nRulebooks are a systematic way to address such settings.\\nHere, a rule corresponds to an objective and a rulebook defines a hierarchy that induces a partial order. For example rule r 1 r_{1} (avoiding collision) is more important than rules r 2 , r 3 r_{2},r_{3} (maintaining safety distance and lane) but rules r 2 , r 3 r_{2},r_{3} are incomparable.\\nThis generalizes MOS which is a ‚Äúflat‚Äù hierarchy where no objective (rule) is more critical than any other one.',\n",
       "    'subsections': [{'title': 'Automated design & synthesis.',\n",
       "      'paragraphs': 'MOS has been applied to design problems in chemistry, biology, and engineering.\\nOne example is retrosynthesis planning in computational chemistry, which is the problem of finding reaction sequences that produce a target molecule. Lai et\\xa0al. ( 2025 ) consider multiple objectives and, by searching for un-dominated synthesis routes, they were able to present several candidate pathways to a human chemist to evaluate and choose from.\\nSimilarly, MOS found applications in drug discovery and generative design. For example, Southiratn et\\xa0al. ( 2025 ) suggested a bi-objective search algorithm for generating molecular structures that balance affinity to two proteins while also satisfying drug-like property constraints The result is a set of novel\\nmolecular candidates with high predicted efficacy and acceptable pharmacological profiles. which traditional single-objective or scalarized approaches would have likely missed.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Multi-modal journey planning.',\n",
       "      'paragraphs': 'Multi-modal journey planning determines routes combining different transport modes (Bast et\\xa0al. 2016 ) , which inherently involves multi-objective optimization such as time, cost and comfort.\\nThese methods (see e.g., (Potthoff and Sauer 2022b , a ) ) build upon MOS algorithms to make queries tractable at metropolitan scale.\\nMany real-world uses of such algorithms have recently been documented.\\nFor example, OpenTripPlanner 2 is an open-source multi-modal journey planner for public transportation in combination with bicycling, walking, and mobility services such as bike share and ride hailing. It has been deployed nationwide in Norway and Finland. In Portland (Oregon), it provides about 40,000 trip plans on a typical weekday (OTP 2025 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': 'Robotics.',\n",
       "      'paragraphs': 'In robotics, multiple objectives often need to be simultaneously balanced (e.g., cost, energy and safety)\\nIn Sec. 4 we discussed robot inspection planning in the context of MOS-OA.\\nAnother example is autonomous vehicle (AV) planning using rulebooks (Slutsky et\\xa0al. 2021 ; Censi et\\xa0al. 2019 ; Halder and Althoff 2025 ; Penlington, Zanardi, and Frazzoli 2024 ) , where the system must generate a trajectory that complies with a set of potentially conflicting traffic rules. Consider, for instance, Singapore‚Äôs Final Theory of Driving that requires\\n(i) maintaining at least a one-meter gap when passing a parked vehicle and (ii)\\xa0prohibits crossing a solid double white lane divider. When an AV encounters a car improperly parked along such a divider, it may be impossible to satisfy both requirements simultaneously.\\nFortunately, requirements often form in a hierarchy‚Äîe.g., avoiding a collision is more important than keeping safety margin from parked vehicles and than maintaining lane.\\nRulebooks are a systematic way to address such settings.\\nHere, a rule corresponds to an objective and a rulebook defines a hierarchy that induces a partial order. For example rule r 1 r_{1} (avoiding collision) is more important than rules r 2 , r 3 r_{2},r_{3} (maintaining safety distance and lane) but rules r 2 , r 3 r_{2},r_{3} are incomparable.\\nThis generalizes MOS which is a ‚Äúflat‚Äù hierarchy where no objective (rule) is more critical than any other one.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Open Challenges and Opportunities',\n",
       "    'paragraphs': 'Despite the progress reviewed in this paper, several fundamental challenges remain open.\\nIn contrast to Salzman et\\xa0al. ( 2023 ) who discuss technical challenges that are the foundations for advancing MOS algorithms, here we focus on challenges and opportunities that will increase the impact of MOS.\\nMost existing algorithms scale poorly when the number of objectives grows beyond two or three. Approximate and bounded suboptimal MOS algorithms partially address this issue, but there is no consensus on how to effectively navigate high-dimensional cost spaces which may be essential in real-world applications.\\nReal-world deployment increasingly requires algorithms that adapt to changing graphs or stochastic models. While recent works study dynamic MOS, MOSSP and MOMDP, current algorithms mostly remain theoretical or are limited to small instance sizes. Developing practical, general-purpose dynamic MOS algorithms is an interesting research opportunity.\\nIn many applications, decision makers cannot provide trade-offs upfront. Integrating preference elicitation into the search process‚Äîby interactively presenting Pareto-optimal candidates and learning from user choices‚Äîremains an underexplored yet impactful research direction. Combining MOS algorithms with methods from preference learning and human-in-the-loop AI is another research opportunity.\\nImportant opportunities exist at the interface of MOS and other AI subfields. In reinforcement learning, MORL is rediscovering many algorithmic ideas from MOS; conversely, MOS can benefit from policy-gradient and distributional methods.\\nIn many domains such as robotics, large-scale transport systems and OR, multiple objectives are prevalent but existing MOS formulations need to be adapted to be applied effectively.\\nClassical MOS benchmarks focus on road networks and grid worlds, while robotics emphasizes motion-planning roadmaps, and reinforcement learning relies on synthetic MO-MDPs. This fragmentation hampers comparison across research communities.\\nA standardized benchmark suite that spans classical MOS, stochastic and dynamic settings, and application-inspired domains (such as transportation, robotics and chemistry) would be a major step forward. Beyond static datasets, benchmarks should include interactive tasks for preference elicitation and evaluation metrics that reflect both efficiency and effectiveness. The community would benefit from a shared repository of graphs, environments, and evaluation protocols to foster reproducibility and comparability.',\n",
       "    'subsections': [{'title': 'Scalability and dimensionality.',\n",
       "      'paragraphs': 'Most existing algorithms scale poorly when the number of objectives grows beyond two or three. Approximate and bounded suboptimal MOS algorithms partially address this issue, but there is no consensus on how to effectively navigate high-dimensional cost spaces which may be essential in real-world applications.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Dynamic and uncertain environments.',\n",
       "      'paragraphs': 'Real-world deployment increasingly requires algorithms that adapt to changing graphs or stochastic models. While recent works study dynamic MOS, MOSSP and MOMDP, current algorithms mostly remain theoretical or are limited to small instance sizes. Developing practical, general-purpose dynamic MOS algorithms is an interesting research opportunity.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Preference elicitation and user modeling.',\n",
       "      'paragraphs': 'In many applications, decision makers cannot provide trade-offs upfront. Integrating preference elicitation into the search process‚Äîby interactively presenting Pareto-optimal candidates and learning from user choices‚Äîremains an underexplored yet impactful research direction. Combining MOS algorithms with methods from preference learning and human-in-the-loop AI is another research opportunity.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Cross-fertilization between research communities.',\n",
       "      'paragraphs': 'Important opportunities exist at the interface of MOS and other AI subfields. In reinforcement learning, MORL is rediscovering many algorithmic ideas from MOS; conversely, MOS can benefit from policy-gradient and distributional methods.\\nIn many domains such as robotics, large-scale transport systems and OR, multiple objectives are prevalent but existing MOS formulations need to be adapted to be applied effectively.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Benchmarks.',\n",
       "      'paragraphs': 'Classical MOS benchmarks focus on road networks and grid worlds, while robotics emphasizes motion-planning roadmaps, and reinforcement learning relies on synthetic MO-MDPs. This fragmentation hampers comparison across research communities.\\nA standardized benchmark suite that spans classical MOS, stochastic and dynamic settings, and application-inspired domains (such as transportation, robotics and chemistry) would be a major step forward. Beyond static datasets, benchmarks should include interactive tasks for preference elicitation and evaluation metrics that reflect both efficiency and effectiveness. The community would benefit from a shared repository of graphs, environments, and evaluation protocols to foster reproducibility and comparability.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 Conclusion',\n",
       "    'paragraphs': 'MOS has rapidly expanded from a niche research topic to a broad principle that influences many disciplines and applications and studied by multiple communities. On the algorithmic side, there have been significant improvements in exact search, new approximate and parallel algorithms, and theoretical insights.\\nOn the applications side, numerous communities have started to formulate their problems in terms of trade-offs between different metrics and adopt MOS\\nmethods to handle these trade-offs.\\nWhile MOS was the focus of this paper, there should be more cross fertilization between different multi-objective optimization approaches.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgments',\n",
       "    'paragraphs': 'This research was supported by Grant No. 2021643 from the United States-Israel Binational Science Foundation (BSF).',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Multi-objective search (MOS) has emerged as a unifying framework for planning and decision-making problems where multiple, often conflicting, criteria must be balanced. While the problem has been studied for decades, recent years have seen renewed interest in the topic across AI applications such as robotics, transportation, and operations research, reflecting the reality that real-world systems rarely optimize a single measure.\\n This paper surveys developments in MOS while highlighting cross-disciplinary opportunities, and outlines open challenges that define the emerging frontier of MOS research.'},\n",
       " '2510.25731v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Partial differential equations (PDEs) appear in nearly every scientific discipline, from fluid dynamics to quantitative finance. This ubiquity makes the development of reliable and efficient PDE solvers a central challenge in the field of computational science. In particular, the goal is to find specific solutions of PDEs that satisfy additional initial and/or boundary conditions; this setting is commonly referred to as the initial-boundary value problem (IBVP). While classical numerical methods, such as finite difference and finite element techniques [ 21 ] , can effectively address these problems, their computational cost increases dramatically with the dimensionality of the problem. In particular, most classical methods require a domain discretization, which allows the evaluation of the function at a finite set of predetermined grid points only. As a result, high-dimensional PDEs present particularly daunting challenges that necessitate innovative solution strategies as outlined in [ 17 ] .\\nTo this end, various approaches have been developed that employ machine learning techniques to tackle these problems. Early approaches to solving differential equations with neural networks date back to the pioneering work by Lagaris et al. [ 19 ] , who proposed feedforward architectures that satisfy boundary and initial conditions by construction. Furthermore, various machine learning techniques have been explored to address these challenges, such as the work by [ 38 ] , which aims to recover analytical closed-form solutions through genetic programming.\\nThe introduction of physics-informed neural networks (PINNs) [ 32 ] in 2017 revitalized interest in the field of physics-informed machine learning and the application of machine learning-based techniques for solving PDEs. Similar to the Deep Galerkin Method [ 35 ] , PINNs utilize multiple loss terms corresponding to the PDE and the boundary and/or initial conditions to approximate solutions by minimizing the loss. Hence, the network is guided ‚Äì but not guaranteed ‚Äì to respect the physics. These methods have demonstrated notable success across various applications, by no means limited to physics, in solving IBVPs [ 17 ] . A large body of follow-up work has sought to improve training stability and scalability using a plethora of approaches, including domain decomposition [ 15 ] , adaptive loss balancing [ 42 ] , improved optimizers [ 39 , 33 ] , resampling [ 44 , 20 , 25 ] and multifidelity extensions [ 23 ] . Although promising, PINNs often suffer from unstable convergence [ 41 , 18 ] . In practice, networks can reach very small PDE residuals while failing to approximate the true solution well. The reason is that the PDE loss term used in PINNs is not monotone with respect to the solution error. As a result, the training loss does not necessarily provide predictive power: low loss does not automatically imply high accuracy. This issue limits the reliability of PINNs as general-purpose PDE solvers. Despite numerous enhancements for PINNs, the aforementioned issue is inherent to the intrinsic structure of PINNs and their underlying optimization scheme.\\nIn general, there are several ways to incorporate physics into a machine learning model: (a) by informing the data, (b) through the loss function, or (c) by modifying the model‚Äôs architecture [ 17 ] . Whereas classical PINNs inform the model solely through the loss function (b), there are also hybrid approaches, such as hard-constraint PINNs [ 22 ] , which utilize a PDE-informed loss function in conjunction with a boundary-conditions-informed architecture. In addition, there are several approaches where physics is baked directly into the model architecture. Examples include convolutional networks that hard-code translation symmetry [ 8 ] , equivariant networks for three-dimensional particle systems [ 37 ] , and structure-preserving models such as Hamiltonian [ 13 ] or Lagrangian neural networks [ 9 ] . These architectures ensure physical consistency by design, but are usually specialized to certain physical aspects.\\nIn the realm of physics, symmetries play a dominant role [ 43 ] , inspiring diverse machine learning approaches which are ‚Äúsymmetry-informed‚Äù. Examples include symmetry-informed data augmentation for neural PDE solvers [ 5 ] , self-supervised pretraining based on Lie symmetry transformations [ 24 ] , and symmetry-regularized PINNs [ 1 ] , which all exploit the invariance structure of the underlying equations to improve generalization and sample efficiency. Similarly, [ 47 ] leverages Lie symmetry groups to generate data for supervised learning, providing an alternative to physics-informed training. Other methods embed invariant surface conditions into the loss [ 48 ] to enhance the PINN method. Yet another recent line of work [ 45 , 46 ] utilizes Lie symmetries to discover them from data and integrate them into learning architectures, improving long-term predictive accuracy and facilitating practical applications such as equation discovery.\\nIn our approach, we leverage the inherent symmetries of PDEs, namely their Lie symmetries, by integrating them directly into the model structure. This method is in principle possible for any PDE or any corresponding Lie symmetry, as these symmetries inherently generate parametrized and expressive solutions by themselves. Our work integrates predetermined parametrized Lie symmetries as learnable building blocks, enabling the model to compose symmetry transformations to solve PDEs while provably remaining in the space of solutions of the underlying PDE. Consequently, for every set of model parameters, the resulting model exactly satisfies the PDE. As a result, the optimization problem reduces to fitting the initial-boundary values only. This makes the optimization significantly more controllable and eliminates the need for difficult-to-handle PDE loss term used in PINNs, which in our opinion represents the greatest challenge in the PINN method.\\nConsequently, in our model, a decrease of the loss function directly corresponds to an enhancement of the accuracy of the prediction. Hence, in contrast to PINNs, the loss function serves as a reliable metric for evaluating training progress. In addition, for well-posed IBVPs, one can derive rigorous error bounds that scale with the test loss. Moreover, this approach allows us to design significantly smaller models (in terms of the number of parameters), while preserving a high degree of expressiveness. As a result, this enables more robust optimization leading to more accurate approximations in significantly less time compared to PINNs.\\nThe article is structured as follows:\\nWe begin by introducing the underlying problem: IBVPs ( section 2.2 ) with their associated PDEs ( section 2.1 ). These concepts are introduced in a way that is useful for the study of Lie symmetries, which are reviewed in section 2.3 . We complement the discussion of Lie symmetries with more details in section A.1 . In section 3 we propose our new method LieSolver , which is specialized to linear homogeneous PDEs. section 4 presents our experiments, testing the scope of LieSolver and comparing it with PINNs, demonstrating that our approach leads to more accurate solutions that can be computed significantly faster.\\nTo summarize the contributions of our work, we provide:\\nThe new machine learning framework LieSolver based on Lie symmetries, which aims to solve IBVPs. This method will enforce PDE exactly, by the use of a PDE-informed architecture. Learning happens only on the boundary domain and requires no data, as arbitrary data points can be sampled as needed. Our approach is currently limited to linear homogeneous PDEs.\\nAn illustration of how to derive rigorous error bounds from the test loss in well-posed problems.\\nThe path towards theoretical convergence guarantees. We do not prove any bounds rigorously.\\nBenchmark experiments demonstrating higher solution accuracy and faster compute times compared to vanilla PINNs.\\nAn improved interpretability through access to a symbolic decomposition of the solution.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Theoretical Background',\n",
       "    'paragraphs': 'The main focus of this article is to solve initial-boundary-value problems (IBVPs) effectively by using machine learning techniques. We will introduce IBVPs in section 2.2 along with their associated partial differential equations (PDEs) ( section 2.1 ), while placing a particular emphasis on their symmetries in section 2.3 . For readers seeking a quick overview, we recommend focusing on section 2.3 . A more comprehensive description of the topics discussed will be compiled in section A.1 . For a detailed exploration of these subjects, we refer to [ 31 ] as an extensive standard source, [ 30 , 3 , 2 ] for alternative presentations, [ 10 ] for a focus on Lie groups, and [ 27 ] for an overview, including a historical perspective.\\nIn a nutshell, PDEs are just equations containing derivatives. When studying symmetries of PDEs, it is often convenient to think of their solutions as parametrized surfaces, instead of functions, which enables a geometric treatment. In the following, we develop this idea rigorously and introduce the key concepts. This summary is oriented towards the description in [ 31 ] . For a more general perspective on PDEs we recommend [ 34 , 16 , 11 , 29 ] .\\nConsider n n independent variables x = ( x 1 , ‚Ä¶ , x n ) ‚àà X = ‚Ñù n x=(x_{1},\\\\ldots,x_{n})\\\\in X=\\\\mathbb{R}^{n} and m m dependent variables u = ( u 1 , ‚Ä¶ , u m ) ‚àà ùí∞ = ‚Ñù m u=(u^{1},\\\\ldots,u^{m})\\\\in\\\\mathcal{U}=\\\\mathbb{R}^{m} . This terminology reflects that we will later treat solutions as maps from X X to ùí∞ \\\\mathcal{U} . In addition to the dependent and independent variables, the PDE also contains derivatives. Therefore, we denote by d ‚àà ‚Ñï d\\\\in\\\\mathbb{N} the order of the considered PDE, i.e., the order of its highest derivative. For n n independent variables, the number of distinct partial derivatives up to order d d is given 1 1 1 We will always assume smooth functions, such that derivatives will commute. The number of partial derivatives follows from the combinatorial identity ‚àë i = 0 d ( n + i ‚àí 1 i ) = ( n + d d ) \\\\sum_{i=0}^{d}\\\\binom{n+i-1}{i}=\\\\binom{n+d}{d} for d ‚â• 0 d\\\\geq 0 and n > 0 n>0 , which can be easily shown by induction. by ( n + d d ) \\\\binom{n+d}{d} . We prolong the space ùí∞ \\\\mathcal{U} to include all partial derivatives up to order d d , which we denote by ùí∞ ( d ) := ‚Ñù m \\u200b ( n + d d ) \\\\mathcal{U}^{(d)}:=\\\\mathbb{R}^{m\\\\binom{n+d}{d}} . By these definitions, a system of p p PDEs of order d d is given as a set of equations\\nwhere we assume ùíü : X √ó ùí∞ ( d ) ‚Üí ‚Ñù p \\\\mathcal{D}:X\\\\times\\\\mathcal{U}^{(d)}\\\\to\\\\mathbb{R}^{p} to be smooth. Accordingly, we describe PDEs as functions of the independent variables and the prolonged dependent variables.\\nIn order to connect the placeholder variable u ( d ) u^{(d)} to the actual derivatives, we introduce the prolongation of functions. For a sufficiently smooth function f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} with Œ© ‚äÜ X \\\\Omega\\\\subseteq X open, its d d -th prolongation pr ( d ) \\u2061 f : Œ© ‚Üí ùí∞ ( d ) \\\\operatorname{pr}^{(d)}f:\\\\Omega\\\\to\\\\mathcal{U}^{(d)} is defined as the tuple\\nwhere [ n ] d := { œÉ ‚äÜ { 1 , ‚Ä¶ , n } : | œÉ | ‚â§ d } [n]^{d}:=\\\\big\\\\{\\\\sigma\\\\subseteq\\\\{1,\\\\ldots,n\\\\}:|\\\\sigma|\\\\leq d\\\\big\\\\} contains all subsets of [ n ] [n] with at most d d elements (including the empty set), and we write for a set J = { j 1 , ‚Ä¶ , j k } J=\\\\{j_{1},\\\\ldots,j_{k}\\\\}\\nfor brevity. Thus, the prolongation explicitly lists all derivatives up to order d d . For instance, for a function f : ‚Ñù 2 ‚Üí ‚Ñù f:\\\\mathbb{R}^{2}\\\\to\\\\mathbb{R} we get pr ( 2 ) f ( x , y ) = ( f , f x , f y , \\\\operatorname{pr}^{(2)}f(x,y)=\\\\bigl(f,f_{x},f_{y}, f x \\u200b x , f x \\u200b y , f y \\u200b y ) ‚àà ùí∞ ( 2 ) ‚âÖ ‚Ñù 6 f_{xx},f_{xy},f_{yy}\\\\bigr)\\\\in\\\\mathcal{U}^{(2)}\\\\cong\\\\mathbb{R}^{6} .\\nA sufficiently smooth function f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} , with Œ© ‚äÜ X \\\\Omega\\\\subseteq X open, is called a solution of the PDE system ùíü \\\\mathcal{D} if\\nIt is important to note, that there is no general theory to find solutions of PDEs. However, for the further treatment, it is useful to classify PDEs with respect to their properties. We call a PDE (system) ùíü \\\\mathcal{D} linear if we can write it in the form\\nfor some functions h , a J : X ‚Üí ‚Ñù p h,a_{J}:X\\\\to\\\\mathbb{R}^{p} . If in addition h \\u200b ( x ) ‚â° 0 h(x)\\\\equiv 0 , we call the PDE (system) linear homogeneous . For any linear homogeneous PDEs, we have that if f 1 f_{1} and f 2 f_{2} are two solutions of the PDE, then so is Œ± \\u200b f 1 + Œ≤ \\u200b f 2 \\\\alpha f_{1}+\\\\beta f_{2} for any Œ± , Œ≤ ‚àà ‚Ñù \\\\alpha,\\\\beta\\\\in\\\\mathbb{R} . In particular, it holds that f \\u200b ( x ) ‚â° 0 f(x)\\\\equiv 0 is always a solution for any linear homogeneous PDE. If a linear homogeneous PDE does not contain a term depending on u u , it additionally is true that every constant function solves this PDE.\\nSince PDEs usually admit infinitely many solutions, we impose additional constraints in the form of initial-boundary conditions (IBC) to ensure uniqueness. To simplify the notation, we will not explicitly distinguish between time and spatial coordinates. Hence, initial and boundary conditions get the same form. Let Œ© ‚äÇ X \\\\Omega\\\\subset X be a bounded domain, where its boundary ‚àÇ Œ© \\\\partial\\\\Omega contains smooth, connected components Œì 1 , ‚Ä¶ , Œì r ‚äÜ ‚àÇ Œ© \\\\Gamma_{1},\\\\ldots,\\\\Gamma_{r}\\\\subseteq\\\\partial\\\\Omega . For each Œì k \\\\Gamma_{k} , we introduce a condition of the form\\nwith ‚Ñ¨ k : Œì k √ó ùí∞ √ó ùí∞ ‚Üí ‚Ñù \\\\mathcal{B}_{k}:\\\\Gamma_{k}\\\\times\\\\mathcal{U}\\\\times\\\\mathcal{U}\\\\to\\\\mathbb{R} . The symbol u ùíè u_{\\\\bm{n}} is a placeholder, which will later be associated with the normal derivative. A PDE system ùíü \\\\mathcal{D} together with associated IBCs ‚Ñ¨ 1 , ‚Ä¶ , ‚Ñ¨ r \\\\mathcal{B}_{1},\\\\ldots,\\\\mathcal{B}_{r} defines an initial-boundary value problem (IBVP). A sufficiently smooth function f : Œ© ¬Ø ‚Üí ùí∞ f:\\\\overline{\\\\Omega}\\\\to\\\\mathcal{U} is called a solution of the IBVP if\\nwhere d \\u200b f d \\u200b ùíè \\u200b ( x ) = ‚àá f \\u200b ( x ) ‚ãÖ n ^ \\u200b ( x ) \\\\frac{\\\\mathop{}\\\\!\\\\mathrm{d}f}{\\\\mathop{}\\\\!\\\\mathrm{d}\\\\bm{n}}(x)=\\\\nabla f(x)\\\\cdot\\\\hat{n}(x) is the normal outward derivative on Œì k \\\\Gamma_{k} .\\nIn the following, we will always focus on well-posed IBVPs, meaning that (i) the IBVP has a solution, (ii) the solution is unique, and (iii) the solution depends continuously on the prescribed initial and boundary data. The precise definition of these three properties depends on the specific problem being considered, and there is no commonly agreed definition, as noted in [ 16 ] . Typically, we restrict the space of possible solutions to smooth functions and may add further restrictions, e.g., certain assumptions on the growth at infinity, in order to specify uniqueness. To illustrate the last condition in practice, we give the following example.\\nConsider the two IBVPs of the so-called reverse heat equation in the domain x ‚àà [ 0 , 2 \\u200b œÄ ) x\\\\in[0,2\\\\pi) , t > 0 t>0 with\\nwith œµ > 0 \\\\epsilon>0 . It is not hard to verify the two solutions u \\u200b ( x , t ) ‚â° 0 u(x,t)\\\\equiv 0 and v \\u200b ( x , t ) = œµ \\u200b sin \\u2061 ( x ) \\u200b e t v(x,t)=\\\\epsilon\\\\sin(x)e^{t} . These two problems only differ in the initial condition, having the distance ‚Äñ u \\u200b ( 0 ) ‚àí v \\u200b ( 0 ) ‚Äñ ‚àû = œµ \\\\left\\\\|u(0)-v(0)\\\\right\\\\|_{\\\\infty}=\\\\epsilon , with the norm ‚Äñ f \\u200b ( t ) ‚Äñ ‚àû := max x ‚àà [ 0 , 2 \\u200b œÄ ) \\u2061 | f \\u200b ( x , t ) | \\\\left\\\\|f(t)\\\\right\\\\|_{\\\\infty}:=\\\\max_{x\\\\in[0,2\\\\pi)}|f(x,t)| . Hence, the initial conditions are arbitrary close for small œµ \\\\epsilon . However, the distance between solutions is given by ‚Äñ u \\u200b ( t ) ‚àí v \\u200b ( t ) ‚Äñ ‚àû = œµ \\u200b e t \\\\left\\\\|u(t)-v(t)\\\\right\\\\|_{\\\\infty}=\\\\epsilon\\\\ e^{t} , which is not bounded on the domain. Hence, the solution will not depend continuously on the initial data, and this problem is ill-posed.\\nIn comparison, consider a slightly modified version of the problem (heat equation)\\nwhich results in the solutions u \\u200b ( x , t ) ‚â° 0 u(x,t)\\\\equiv 0 and v \\u200b ( x , t ) = œµ \\u200b sin \\u2061 ( x ) \\u200b e ‚àí t v(x,t)=\\\\epsilon\\\\sin(x)e^{-t} . In that case, the distance between the solutions is given by ‚Äñ u \\u200b ( t ) ‚àí v \\u200b ( t ) ‚Äñ ‚àû = œµ \\u200b e ‚àí t ‚â§ œµ \\\\left\\\\|u(t)-v(t)\\\\right\\\\|_{\\\\infty}=\\\\epsilon\\\\ e^{-t}\\\\leq\\\\epsilon . Hence, the map from initial data to solutions is (locally) Lipschitz continuous\\nwith Lipschitz constant C = 1 C=1 . Hence, we confirmed the well-posedness of the heat equation with Dirichlet conditions.\\nPDEs can be invariant under certain symmetries, and so their set of solutions has to respect the same symmetries. Let us illustrate this behaviour on the differential equation u x \\u200b x = 0 u_{xx}=0 . Obviously, if u = f \\u200b ( x ) u=f(x) is a solution, then so is u = f \\u200b ( x ) + œë u=f(x)+\\\\vartheta , as well as u = e œë \\u200b f \\u200b ( x ) u=e^{\\\\vartheta}f(x) for any œë ‚àà ‚Ñù \\\\vartheta\\\\in\\\\mathbb{R} . Hence, the underlying PDE comes with a translation and scaling invariance\\nMoreover, these symmetry transformations will generate a group structure, i.e., we have a neutral element T 0 i = e T^{i}_{0}=e , inversions ( T œë i ) ‚àí 1 = T ‚àí œë i \\\\big(T^{i}_{\\\\vartheta}\\\\big)^{-1}=T^{i}_{-\\\\vartheta} and T œë 1 i ‚àò T œë 2 i = T œë 1 + œë 2 i T^{i}_{\\\\vartheta_{1}}\\\\circ T^{i}_{\\\\vartheta_{2}}=T^{i}_{\\\\vartheta_{1}+\\\\vartheta_{2}} . Thus, we obtain a two-dimensional group, which is spanned by two one-parameter groups.\\nThe rigorous study of these symmetry groups naturally leads to the framework of Lie groups [ 10 ] , which provide the mathematical structure for continuous transformation groups. A Lie group is a smooth, finite-dimensional manifold endowed with a group structure 2 2 2 A group ( G , ‚ãÖ ) (G,\\\\cdot) is a set G G equipped with a binary operation ‚ãÖ : G √ó G ‚Üí G \\\\cdot:G\\\\times G\\\\to G that is associative, has a unique identity element e ‚àà G e\\\\in G , and assigns to each g ‚àà G g\\\\in G a unique inverse g ‚àí 1 g^{-1} . such that both the multiplication and inversion maps are smooth.\\nSymmetry transformations can be described as actions of the Lie groups on manifolds. For a Lie group G G and a smooth manifold M M , this action ‚àò : G √ó M ‚Üí M \\\\circ:G\\\\times M\\\\to M is required to be compatible with the group axioms, i.e., we demand g 1 ‚àò ( g 2 ‚àò x ) = ( g 1 ‚ãÖ g 2 ) ‚àò x g_{1}\\\\circ(g_{2}\\\\circ x)=(g_{1}\\\\cdot g_{2})\\\\circ x and e ‚àò x = x e\\\\circ x=x for any g 1 , g 2 ‚àà G g_{1},g_{2}\\\\in G and all x ‚àà M x\\\\in M .\\nDepending on the group G G and the manifold M M , not all of these actions may be defined, which is why we allow restricting these actions to a subset ùí∞ ‚äÜ G √ó M \\\\mathscr{U}\\\\subseteq G\\\\times M containing the identity { e } √ó M ‚äÜ ùí∞ \\\\{e\\\\}\\\\times M\\\\subseteq\\\\mathscr{U} . We will call this restriction a local group of transformations . Note that this restriction to group elements around the identity do not pose any serious limitations, since every element of the full Lie group G G can be written as a finite product of elements which are close to the identity (compare [ 31 , thm. 1.22 and prop. 1.24] ).\\nIn order to connect a PDE solution to a manifold, let f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} be a sufficiently smooth function with Œ© ‚äÜ X \\\\Omega\\\\subseteq X . Its graph is defined as\\na smooth submanifold of X √ó ùí∞ X\\\\times\\\\mathcal{U} . If G G is a (local) group of transformations acting on X √ó ùí∞ X\\\\times\\\\mathcal{U} , then for g ‚àà G g\\\\in G we define the transformed graph as\\nIn the case where we can write the transformed graph g ‚àò Œì f = Œì f ~ g\\\\circ\\\\Gamma_{f}=\\\\Gamma_{\\\\tilde{f}} as a graph of another function f ~ \\\\tilde{f} , we simply write f ~ = g ‚àò f \\\\tilde{f}=g\\\\circ f . Note that for elements g g close to the identity, we always find a (single-valued) function f ~ \\\\tilde{f} with that property.\\nA symmetry group of a PDE ùíü \\\\mathcal{D} is a local group of transformations G G acting on M ‚äÜ X √ó ùí∞ ( d ) M\\\\subseteq X\\\\times\\\\mathcal{U}^{(d)} , such that whenever u = f \\u200b ( x ) u=f(x) is a solution of ùíü \\\\mathcal{D} , and g ‚àò f g\\\\circ f is defined for g ‚àà G g\\\\in G , then u = g ‚àò f \\u200b ( x ) u=g\\\\circ f(x) will be a solution of ùíü \\\\mathcal{D} as well.\\nTo illustrate these concepts, we discuss the symmetry group of the 1 1 -dimensional heat equation. We will provide a discussion on how to actually compute these symmetry groups in examples A.2 , A.3 and A.4 .\\nConsider the heat equation\\nwhere n = 2 n=2 , d = 2 d=2 , m = 1 m=1 and p = 1 p=1 . It admits the following six one-parameter symmetry transformations [ 31 ] :\\nfor any œë ‚àà Œò ‚äÜ ‚Ñù \\\\vartheta\\\\in\\\\Theta\\\\subseteq\\\\mathbb{R} . Note that, we can use different values œë \\\\vartheta for any of these transformations, and that ‚Äì depending on the domain Œ© \\\\Omega of f f ‚Äì we may resctrict the domain for T 6 T^{6} to Œò ‚äÜ ‚Ñù \\\\Theta\\\\subseteq\\\\mathbb{R} , to avoid singularities. In addition, there is an additional symmetry transformation, reflecting the linearity, i.e., for any solution Œ± \\u200b ( x , t ) \\\\alpha(x,t) of equation 17 , we have the symmetry\\nHence, for any solution u = f \\u200b ( x , t ) u=f(x,t) of the heat equation equation 17 , applying any of the aforementioned transformations produces another possible solution. As an example, for the constant solution f \\u200b ( x , t ) ‚â° 1 f(x,t)\\\\equiv 1 and after applying the transformation T œë 6 T^{6}_{\\\\vartheta} , obtains the function u = 1 1 + 4 \\u200b œë \\u200b t \\u200b exp \\u2061 ( ‚àí œë \\u200b x 2 1 + 4 \\u200b œë \\u200b t ) u=\\\\frac{1}{\\\\sqrt{1+4\\\\vartheta t}}\\\\exp\\\\!\\\\left(\\\\frac{-\\\\vartheta x^{2}}{1+4\\\\vartheta t}\\\\right) that also solves the heat equation, whenever this function is defined, i.e. for 1 + 4 \\u200b œë \\u200b t ‚â† 0 1+4\\\\vartheta t\\\\neq 0 .\\nWhile the transformations T 1 , ‚Ä¶ , T 4 T^{1},\\\\ldots,T^{4} in example 2.2 can be found by carefully observing the differential equation equation 17 , T 5 T^{5} and T 6 T^{6} are far from being trivial to be obtained by reading the PDE. However, there exists a concrete recipe for how to compute these transformations, which we will summarize in section A.1 . There are also plenty of implementations for the computation of Lie symmetries, and we refer to [ 28 ] for a current approach, and [ 36 , 14 , 6 , 7 ] for an overview about different implementations.',\n",
       "    'subsections': [{'title': '2.1 Partial Differential Equations (PDEs)',\n",
       "      'paragraphs': 'In a nutshell, PDEs are just equations containing derivatives. When studying symmetries of PDEs, it is often convenient to think of their solutions as parametrized surfaces, instead of functions, which enables a geometric treatment. In the following, we develop this idea rigorously and introduce the key concepts. This summary is oriented towards the description in [ 31 ] . For a more general perspective on PDEs we recommend [ 34 , 16 , 11 , 29 ] .\\nConsider n n independent variables x = ( x 1 , ‚Ä¶ , x n ) ‚àà X = ‚Ñù n x=(x_{1},\\\\ldots,x_{n})\\\\in X=\\\\mathbb{R}^{n} and m m dependent variables u = ( u 1 , ‚Ä¶ , u m ) ‚àà ùí∞ = ‚Ñù m u=(u^{1},\\\\ldots,u^{m})\\\\in\\\\mathcal{U}=\\\\mathbb{R}^{m} . This terminology reflects that we will later treat solutions as maps from X X to ùí∞ \\\\mathcal{U} . In addition to the dependent and independent variables, the PDE also contains derivatives. Therefore, we denote by d ‚àà ‚Ñï d\\\\in\\\\mathbb{N} the order of the considered PDE, i.e., the order of its highest derivative. For n n independent variables, the number of distinct partial derivatives up to order d d is given 1 1 1 We will always assume smooth functions, such that derivatives will commute. The number of partial derivatives follows from the combinatorial identity ‚àë i = 0 d ( n + i ‚àí 1 i ) = ( n + d d ) \\\\sum_{i=0}^{d}\\\\binom{n+i-1}{i}=\\\\binom{n+d}{d} for d ‚â• 0 d\\\\geq 0 and n > 0 n>0 , which can be easily shown by induction. by ( n + d d ) \\\\binom{n+d}{d} . We prolong the space ùí∞ \\\\mathcal{U} to include all partial derivatives up to order d d , which we denote by ùí∞ ( d ) := ‚Ñù m \\u200b ( n + d d ) \\\\mathcal{U}^{(d)}:=\\\\mathbb{R}^{m\\\\binom{n+d}{d}} . By these definitions, a system of p p PDEs of order d d is given as a set of equations\\nwhere we assume ùíü : X √ó ùí∞ ( d ) ‚Üí ‚Ñù p \\\\mathcal{D}:X\\\\times\\\\mathcal{U}^{(d)}\\\\to\\\\mathbb{R}^{p} to be smooth. Accordingly, we describe PDEs as functions of the independent variables and the prolonged dependent variables.\\nIn order to connect the placeholder variable u ( d ) u^{(d)} to the actual derivatives, we introduce the prolongation of functions. For a sufficiently smooth function f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} with Œ© ‚äÜ X \\\\Omega\\\\subseteq X open, its d d -th prolongation pr ( d ) \\u2061 f : Œ© ‚Üí ùí∞ ( d ) \\\\operatorname{pr}^{(d)}f:\\\\Omega\\\\to\\\\mathcal{U}^{(d)} is defined as the tuple\\nwhere [ n ] d := { œÉ ‚äÜ { 1 , ‚Ä¶ , n } : | œÉ | ‚â§ d } [n]^{d}:=\\\\big\\\\{\\\\sigma\\\\subseteq\\\\{1,\\\\ldots,n\\\\}:|\\\\sigma|\\\\leq d\\\\big\\\\} contains all subsets of [ n ] [n] with at most d d elements (including the empty set), and we write for a set J = { j 1 , ‚Ä¶ , j k } J=\\\\{j_{1},\\\\ldots,j_{k}\\\\}\\nfor brevity. Thus, the prolongation explicitly lists all derivatives up to order d d . For instance, for a function f : ‚Ñù 2 ‚Üí ‚Ñù f:\\\\mathbb{R}^{2}\\\\to\\\\mathbb{R} we get pr ( 2 ) f ( x , y ) = ( f , f x , f y , \\\\operatorname{pr}^{(2)}f(x,y)=\\\\bigl(f,f_{x},f_{y}, f x \\u200b x , f x \\u200b y , f y \\u200b y ) ‚àà ùí∞ ( 2 ) ‚âÖ ‚Ñù 6 f_{xx},f_{xy},f_{yy}\\\\bigr)\\\\in\\\\mathcal{U}^{(2)}\\\\cong\\\\mathbb{R}^{6} .\\nA sufficiently smooth function f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} , with Œ© ‚äÜ X \\\\Omega\\\\subseteq X open, is called a solution of the PDE system ùíü \\\\mathcal{D} if\\nIt is important to note, that there is no general theory to find solutions of PDEs. However, for the further treatment, it is useful to classify PDEs with respect to their properties. We call a PDE (system) ùíü \\\\mathcal{D} linear if we can write it in the form\\nfor some functions h , a J : X ‚Üí ‚Ñù p h,a_{J}:X\\\\to\\\\mathbb{R}^{p} . If in addition h \\u200b ( x ) ‚â° 0 h(x)\\\\equiv 0 , we call the PDE (system) linear homogeneous . For any linear homogeneous PDEs, we have that if f 1 f_{1} and f 2 f_{2} are two solutions of the PDE, then so is Œ± \\u200b f 1 + Œ≤ \\u200b f 2 \\\\alpha f_{1}+\\\\beta f_{2} for any Œ± , Œ≤ ‚àà ‚Ñù \\\\alpha,\\\\beta\\\\in\\\\mathbb{R} . In particular, it holds that f \\u200b ( x ) ‚â° 0 f(x)\\\\equiv 0 is always a solution for any linear homogeneous PDE. If a linear homogeneous PDE does not contain a term depending on u u , it additionally is true that every constant function solves this PDE.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Initial-boundary value problems (IBVPs)',\n",
       "      'paragraphs': 'Since PDEs usually admit infinitely many solutions, we impose additional constraints in the form of initial-boundary conditions (IBC) to ensure uniqueness. To simplify the notation, we will not explicitly distinguish between time and spatial coordinates. Hence, initial and boundary conditions get the same form. Let Œ© ‚äÇ X \\\\Omega\\\\subset X be a bounded domain, where its boundary ‚àÇ Œ© \\\\partial\\\\Omega contains smooth, connected components Œì 1 , ‚Ä¶ , Œì r ‚äÜ ‚àÇ Œ© \\\\Gamma_{1},\\\\ldots,\\\\Gamma_{r}\\\\subseteq\\\\partial\\\\Omega . For each Œì k \\\\Gamma_{k} , we introduce a condition of the form\\nwith ‚Ñ¨ k : Œì k √ó ùí∞ √ó ùí∞ ‚Üí ‚Ñù \\\\mathcal{B}_{k}:\\\\Gamma_{k}\\\\times\\\\mathcal{U}\\\\times\\\\mathcal{U}\\\\to\\\\mathbb{R} . The symbol u ùíè u_{\\\\bm{n}} is a placeholder, which will later be associated with the normal derivative. A PDE system ùíü \\\\mathcal{D} together with associated IBCs ‚Ñ¨ 1 , ‚Ä¶ , ‚Ñ¨ r \\\\mathcal{B}_{1},\\\\ldots,\\\\mathcal{B}_{r} defines an initial-boundary value problem (IBVP). A sufficiently smooth function f : Œ© ¬Ø ‚Üí ùí∞ f:\\\\overline{\\\\Omega}\\\\to\\\\mathcal{U} is called a solution of the IBVP if\\nwhere d \\u200b f d \\u200b ùíè \\u200b ( x ) = ‚àá f \\u200b ( x ) ‚ãÖ n ^ \\u200b ( x ) \\\\frac{\\\\mathop{}\\\\!\\\\mathrm{d}f}{\\\\mathop{}\\\\!\\\\mathrm{d}\\\\bm{n}}(x)=\\\\nabla f(x)\\\\cdot\\\\hat{n}(x) is the normal outward derivative on Œì k \\\\Gamma_{k} .\\nIn the following, we will always focus on well-posed IBVPs, meaning that (i) the IBVP has a solution, (ii) the solution is unique, and (iii) the solution depends continuously on the prescribed initial and boundary data. The precise definition of these three properties depends on the specific problem being considered, and there is no commonly agreed definition, as noted in [ 16 ] . Typically, we restrict the space of possible solutions to smooth functions and may add further restrictions, e.g., certain assumptions on the growth at infinity, in order to specify uniqueness. To illustrate the last condition in practice, we give the following example.\\nConsider the two IBVPs of the so-called reverse heat equation in the domain x ‚àà [ 0 , 2 \\u200b œÄ ) x\\\\in[0,2\\\\pi) , t > 0 t>0 with\\nwith œµ > 0 \\\\epsilon>0 . It is not hard to verify the two solutions u \\u200b ( x , t ) ‚â° 0 u(x,t)\\\\equiv 0 and v \\u200b ( x , t ) = œµ \\u200b sin \\u2061 ( x ) \\u200b e t v(x,t)=\\\\epsilon\\\\sin(x)e^{t} . These two problems only differ in the initial condition, having the distance ‚Äñ u \\u200b ( 0 ) ‚àí v \\u200b ( 0 ) ‚Äñ ‚àû = œµ \\\\left\\\\|u(0)-v(0)\\\\right\\\\|_{\\\\infty}=\\\\epsilon , with the norm ‚Äñ f \\u200b ( t ) ‚Äñ ‚àû := max x ‚àà [ 0 , 2 \\u200b œÄ ) \\u2061 | f \\u200b ( x , t ) | \\\\left\\\\|f(t)\\\\right\\\\|_{\\\\infty}:=\\\\max_{x\\\\in[0,2\\\\pi)}|f(x,t)| . Hence, the initial conditions are arbitrary close for small œµ \\\\epsilon . However, the distance between solutions is given by ‚Äñ u \\u200b ( t ) ‚àí v \\u200b ( t ) ‚Äñ ‚àû = œµ \\u200b e t \\\\left\\\\|u(t)-v(t)\\\\right\\\\|_{\\\\infty}=\\\\epsilon\\\\ e^{t} , which is not bounded on the domain. Hence, the solution will not depend continuously on the initial data, and this problem is ill-posed.\\nIn comparison, consider a slightly modified version of the problem (heat equation)\\nwhich results in the solutions u \\u200b ( x , t ) ‚â° 0 u(x,t)\\\\equiv 0 and v \\u200b ( x , t ) = œµ \\u200b sin \\u2061 ( x ) \\u200b e ‚àí t v(x,t)=\\\\epsilon\\\\sin(x)e^{-t} . In that case, the distance between the solutions is given by ‚Äñ u \\u200b ( t ) ‚àí v \\u200b ( t ) ‚Äñ ‚àû = œµ \\u200b e ‚àí t ‚â§ œµ \\\\left\\\\|u(t)-v(t)\\\\right\\\\|_{\\\\infty}=\\\\epsilon\\\\ e^{-t}\\\\leq\\\\epsilon . Hence, the map from initial data to solutions is (locally) Lipschitz continuous\\nwith Lipschitz constant C = 1 C=1 . Hence, we confirmed the well-posedness of the heat equation with Dirichlet conditions.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3 Lie Symmetries',\n",
       "      'paragraphs': 'PDEs can be invariant under certain symmetries, and so their set of solutions has to respect the same symmetries. Let us illustrate this behaviour on the differential equation u x \\u200b x = 0 u_{xx}=0 . Obviously, if u = f \\u200b ( x ) u=f(x) is a solution, then so is u = f \\u200b ( x ) + œë u=f(x)+\\\\vartheta , as well as u = e œë \\u200b f \\u200b ( x ) u=e^{\\\\vartheta}f(x) for any œë ‚àà ‚Ñù \\\\vartheta\\\\in\\\\mathbb{R} . Hence, the underlying PDE comes with a translation and scaling invariance\\nMoreover, these symmetry transformations will generate a group structure, i.e., we have a neutral element T 0 i = e T^{i}_{0}=e , inversions ( T œë i ) ‚àí 1 = T ‚àí œë i \\\\big(T^{i}_{\\\\vartheta}\\\\big)^{-1}=T^{i}_{-\\\\vartheta} and T œë 1 i ‚àò T œë 2 i = T œë 1 + œë 2 i T^{i}_{\\\\vartheta_{1}}\\\\circ T^{i}_{\\\\vartheta_{2}}=T^{i}_{\\\\vartheta_{1}+\\\\vartheta_{2}} . Thus, we obtain a two-dimensional group, which is spanned by two one-parameter groups.\\nThe rigorous study of these symmetry groups naturally leads to the framework of Lie groups [ 10 ] , which provide the mathematical structure for continuous transformation groups. A Lie group is a smooth, finite-dimensional manifold endowed with a group structure 2 2 2 A group ( G , ‚ãÖ ) (G,\\\\cdot) is a set G G equipped with a binary operation ‚ãÖ : G √ó G ‚Üí G \\\\cdot:G\\\\times G\\\\to G that is associative, has a unique identity element e ‚àà G e\\\\in G , and assigns to each g ‚àà G g\\\\in G a unique inverse g ‚àí 1 g^{-1} . such that both the multiplication and inversion maps are smooth.\\nSymmetry transformations can be described as actions of the Lie groups on manifolds. For a Lie group G G and a smooth manifold M M , this action ‚àò : G √ó M ‚Üí M \\\\circ:G\\\\times M\\\\to M is required to be compatible with the group axioms, i.e., we demand g 1 ‚àò ( g 2 ‚àò x ) = ( g 1 ‚ãÖ g 2 ) ‚àò x g_{1}\\\\circ(g_{2}\\\\circ x)=(g_{1}\\\\cdot g_{2})\\\\circ x and e ‚àò x = x e\\\\circ x=x for any g 1 , g 2 ‚àà G g_{1},g_{2}\\\\in G and all x ‚àà M x\\\\in M .\\nDepending on the group G G and the manifold M M , not all of these actions may be defined, which is why we allow restricting these actions to a subset ùí∞ ‚äÜ G √ó M \\\\mathscr{U}\\\\subseteq G\\\\times M containing the identity { e } √ó M ‚äÜ ùí∞ \\\\{e\\\\}\\\\times M\\\\subseteq\\\\mathscr{U} . We will call this restriction a local group of transformations . Note that this restriction to group elements around the identity do not pose any serious limitations, since every element of the full Lie group G G can be written as a finite product of elements which are close to the identity (compare [ 31 , thm. 1.22 and prop. 1.24] ).\\nIn order to connect a PDE solution to a manifold, let f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} be a sufficiently smooth function with Œ© ‚äÜ X \\\\Omega\\\\subseteq X . Its graph is defined as\\na smooth submanifold of X √ó ùí∞ X\\\\times\\\\mathcal{U} . If G G is a (local) group of transformations acting on X √ó ùí∞ X\\\\times\\\\mathcal{U} , then for g ‚àà G g\\\\in G we define the transformed graph as\\nIn the case where we can write the transformed graph g ‚àò Œì f = Œì f ~ g\\\\circ\\\\Gamma_{f}=\\\\Gamma_{\\\\tilde{f}} as a graph of another function f ~ \\\\tilde{f} , we simply write f ~ = g ‚àò f \\\\tilde{f}=g\\\\circ f . Note that for elements g g close to the identity, we always find a (single-valued) function f ~ \\\\tilde{f} with that property.\\nA symmetry group of a PDE ùíü \\\\mathcal{D} is a local group of transformations G G acting on M ‚äÜ X √ó ùí∞ ( d ) M\\\\subseteq X\\\\times\\\\mathcal{U}^{(d)} , such that whenever u = f \\u200b ( x ) u=f(x) is a solution of ùíü \\\\mathcal{D} , and g ‚àò f g\\\\circ f is defined for g ‚àà G g\\\\in G , then u = g ‚àò f \\u200b ( x ) u=g\\\\circ f(x) will be a solution of ùíü \\\\mathcal{D} as well.\\nTo illustrate these concepts, we discuss the symmetry group of the 1 1 -dimensional heat equation. We will provide a discussion on how to actually compute these symmetry groups in examples A.2 , A.3 and A.4 .\\nConsider the heat equation\\nwhere n = 2 n=2 , d = 2 d=2 , m = 1 m=1 and p = 1 p=1 . It admits the following six one-parameter symmetry transformations [ 31 ] :\\nfor any œë ‚àà Œò ‚äÜ ‚Ñù \\\\vartheta\\\\in\\\\Theta\\\\subseteq\\\\mathbb{R} . Note that, we can use different values œë \\\\vartheta for any of these transformations, and that ‚Äì depending on the domain Œ© \\\\Omega of f f ‚Äì we may resctrict the domain for T 6 T^{6} to Œò ‚äÜ ‚Ñù \\\\Theta\\\\subseteq\\\\mathbb{R} , to avoid singularities. In addition, there is an additional symmetry transformation, reflecting the linearity, i.e., for any solution Œ± \\u200b ( x , t ) \\\\alpha(x,t) of equation 17 , we have the symmetry\\nHence, for any solution u = f \\u200b ( x , t ) u=f(x,t) of the heat equation equation 17 , applying any of the aforementioned transformations produces another possible solution. As an example, for the constant solution f \\u200b ( x , t ) ‚â° 1 f(x,t)\\\\equiv 1 and after applying the transformation T œë 6 T^{6}_{\\\\vartheta} , obtains the function u = 1 1 + 4 \\u200b œë \\u200b t \\u200b exp \\u2061 ( ‚àí œë \\u200b x 2 1 + 4 \\u200b œë \\u200b t ) u=\\\\frac{1}{\\\\sqrt{1+4\\\\vartheta t}}\\\\exp\\\\!\\\\left(\\\\frac{-\\\\vartheta x^{2}}{1+4\\\\vartheta t}\\\\right) that also solves the heat equation, whenever this function is defined, i.e. for 1 + 4 \\u200b œë \\u200b t ‚â† 0 1+4\\\\vartheta t\\\\neq 0 .\\nWhile the transformations T 1 , ‚Ä¶ , T 4 T^{1},\\\\ldots,T^{4} in example 2.2 can be found by carefully observing the differential equation equation 17 , T 5 T^{5} and T 6 T^{6} are far from being trivial to be obtained by reading the PDE. However, there exists a concrete recipe for how to compute these transformations, which we will summarize in section A.1 . There are also plenty of implementations for the computation of Lie symmetries, and we refer to [ 28 ] for a current approach, and [ 36 , 14 , 6 , 7 ] for an overview about different implementations.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 LieSolver',\n",
       "    'paragraphs': 'In this section, we introduce LieSolver , a solver for IBVPs of linear homogeneous PDEs that enforces the PDE exactly by construction. The model is a linear combination of functions generated by parameterized Lie-symmetry transformations. Since each term satisfies the PDE, optimization reduces to fitting initial and boundary conditions. We formalize this construction and the IBC loss in section 3.1 , and outline the optimization algorithm, which includes greedy term selection, variable projection for linear coefficients, and nonlinear least squares for transformation parameters, in section 3.2 .\\nLet ùíü \\u200b ( x , u ( d ) ) = 0 \\\\mathcal{D}(x,u^{(d)})=0 denote a linear homogeneous PDE and let f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} be a solution of this PDE. As discussed in section 2.3 , a Lie symmetry T œë T_{\\\\vartheta} of ùíü \\\\mathcal{D} is a smooth, invertible transformation, parametrized by œë ‚àà Œò ‚äÜ ‚Ñù \\\\vartheta\\\\in\\\\Theta\\\\subseteq\\\\mathbb{R} , with the property that transforming the solution with T œë T_{\\\\vartheta} yields another solution of the PDE ùíü \\\\mathcal{D} . In particular, as noted in section 2.1 any linear homogeneous PDE will have the following symmetry:\\nwhere x ‚àà X x\\\\in X , œë ‚àà ‚Ñù \\\\vartheta\\\\in\\\\mathbb{R} and Œ± : Œ© ‚Üí ùí∞ \\\\alpha:\\\\Omega\\\\to\\\\mathcal{U} is an arbitrary solution of the PDE. In different terms, for any linear homogeneous PDE, the linear combination of solutions is a solution as well, which is also known as the superposition principle.\\nSince Lie symmetries form a group, concatenating different Lie symmetries yields another valid Lie symmetry. Thus, given a solution, one can generate a family of new solutions by successive applications of these transformations. Finding an initial solution is often straightforward. For instance, any linear homogeneous PDE admits the trivial f \\u200b ( x ) ‚â° 0 f(x)\\\\equiv 0 solution. In addition, a constant function solves every linear homogeneous PDE without a term containing u u , e.g. f \\u200b ( x ) ‚â° 1 f(x)\\\\equiv 1 is a solution for heat equation, Laplace equation and wave equation. We refer to such solutions as seed solutions , denoted f seed f_{\\\\text{seed}} , and they are typically easy to obtain for a given PDE.\\nHence, when starting with one or several seed solutions and applying various Lie transformations, the resulting functions will still constitute exact solutions of the PDE. This reveals a remarkable fact: a single seed solution, together with the action of a Lie group, can define expressive solution (sub)spaces via superposition. We illustrate this in the following example.\\nConsider the following seed solution of the heat equation equation 17 :\\nApplying the transformation T œë 4 T^{4}_{\\\\vartheta} from example 2.2 with œë = ‚àí ln \\u2061 Œª \\\\vartheta=-\\\\ln\\\\lambda , one obtains the family of solution sin \\u2061 ( Œª \\u200b x ) \\u200b e ‚àí Œª 2 \\u200b t \\\\sin(\\\\lambda x)e^{-\\\\lambda^{2}t} for any Œª > 0 \\\\lambda>0 .\\nIt is a well-known result [ 4 , sec. 10.5] that the IBVP for the heat equation u t = u x \\u200b x u_{t}=u_{xx} on ( 0 , L ) (0,L) with u \\u200b ( 0 , t ) = u \\u200b ( L , t ) = 0 u(0,t)=u(L,t)=0 and u \\u200b ( x , 0 ) = ‚àë n = 1 ‚àû a n \\u200b sin \\u2061 ( n \\u200b œÄ \\u200b x / L ) u(x,0)=\\\\sum_{n=1}^{\\\\infty}a_{n}\\\\sin\\\\!\\\\big(n\\\\pi x/L\\\\big) admits the unique solution u \\u200b ( x , t ) = ‚àë n = 1 ‚àû a n \\u200b sin \\u2061 ( n \\u200b œÄ \\u200b x / L ) \\u200b e ‚àí ( n \\u200b œÄ / L ) 2 \\u200b t u(x,t)=\\\\sum_{n=1}^{\\\\infty}a_{n}\\\\sin\\\\!\\\\big(n\\\\pi x/L\\\\big)\\\\,e^{-(n\\\\pi/L)^{2}t} . Hence, even with one specific Lie transformation, we can approximate a certain type of IBVP arbitrary well.\\nThus, by applying a sequence of Lie symmetries T l T^{l} to a given f seed f_{\\\\text{seed}} , we obtain an expressive function\\nthat remains a solution of the underlying PDE ùíü \\\\mathcal{D} for any value of œë = ( œë 1 , ‚Ä¶ , œë k ) ‚àà ‚Ñù k \\\\bm{\\\\vartheta}=(\\\\vartheta_{1},\\\\ldots,\\\\vartheta_{k})\\\\in\\\\mathbb{R}^{k} , whenever the composed function is defined.\\nIn the approach of LieSolver , we consider a set ùíÆ = { f 1 \\u200b ( x ; œë 1 ) , ‚Ä¶ , f N \\u200b ( x ; œë N ) } \\\\mathcal{S}=\\\\{f^{1}(x;\\\\bm{\\\\vartheta}^{1}),\\\\ldots,f^{N}(x;\\\\bm{\\\\vartheta}^{N})\\\\} of expressive functions, where each function f i f^{i} is constructed from one or several seed solutions in the form of equation 27 . We refer to these functions as base solutions 3 3 3 We borrow this term from the Finite Element Method. However, this set will not necessarily be minimal, but we will span a solution-(sub)space by these ‚Äúbase solutions‚Äù. . We will discuss in section 3.2 more in detail, how to choose ‚Äúgood‚Äù base solutions for a given problem.\\nThe central idea of LieSolver is to choose certain base solution and combine them linearly. In this procedure we can also use the same base solution several times with different parameters, i.e.\\nwhich constitutes a very expressive function covering a major part of the solution space of the PDE. When the parameters œë i j i \\\\bm{\\\\vartheta}_{i}^{j_{i}} are specified to certain values œë ^ i j i \\\\hat{\\\\bm{\\\\vartheta}}_{i}^{j_{i}} , we will call the collection of base solutions appearing in equation 28 the active set ùíú = { f 1 j 1 \\u200b ( x ; œë ^ 1 j 1 ) , ‚Ä¶ , f M j M \\u200b ( x ; œë ^ M j M ) } \\\\mathcal{A}=\\\\{f_{1}^{j_{1}}(x;\\\\hat{\\\\bm{\\\\vartheta}}_{1}^{j_{1}}),\\\\ldots,f_{M}^{j_{M}}(x;\\\\hat{\\\\bm{\\\\vartheta}}_{M}^{j_{M}})\\\\} . Note that each œë i j i \\\\bm{\\\\vartheta}_{i}^{j_{i}} may possibly consist of several parameters as given in equation 27 . To shorten the notation, we write Œ∏ = ( œë 1 j 1 , ‚Ä¶ , œë M j M ) \\\\theta=(\\\\bm{\\\\vartheta}_{1}^{j_{1}},\\\\ldots,\\\\bm{\\\\vartheta}_{M}^{j_{M}}) and ùíÇ = ( a 1 , ‚Ä¶ , a M ) \\\\bm{a}=(a_{1},\\\\ldots,a_{M}) . Since f LS f_{\\\\text{LS}} is a solution of the PDE by construction , the problem of finding a solution to the IBVP is reduced to match given initial and/or boundary conditions ‚Ñ¨ 1 , ‚Ä¶ , ‚Ñ¨ r \\\\mathcal{B}_{1},\\\\ldots,\\\\mathcal{B}_{r} , i.e. finding the optimal set of parameters ùíÇ \\\\bm{a} and Œ∏ \\\\theta . To formalize this, we introduce the loss function:\\nwhere ùí≥ k ‚äÇ Œì k \\\\mathcal{X}_{k}\\\\subset\\\\Gamma_{k} are finite sets of randomly sampled collocation points from the boundary parts Œì k ‚äÇ ‚àÇ Œ© \\\\Gamma_{k}\\\\subset\\\\partial\\\\Omega according to section 2.2 .\\nThe union ùí≥ = ‚ãÉ k = 1 r ùí≥ k \\\\mathcal{X}=\\\\bigcup_{k=1}^{r}\\\\mathcal{X}_{k} forms the training dataset and consists of L L points ùí≥ ‚äÜ ‚Ñù L √ó n \\\\mathcal{X}\\\\subseteq\\\\mathbb{R}^{L\\\\times n} (rows are individual points x ‚àà ‚Ñù n x\\\\in\\\\mathbb{R}^{n} ). Consequently, boundary conditions ‚Ñ¨ 1 , ‚Ä¶ , ‚Ñ¨ r \\\\mathcal{B}_{1},\\\\ldots,\\\\mathcal{B}_{r} define set of target values ùí¥ ‚àà ‚Ñù L √ó m \\\\mathcal{Y}\\\\in\\\\mathbb{R}^{L\\\\times m} at datapoints ùí≥ \\\\mathcal{X} .\\nThe seed solutions, the chosen transformations applied to them, and the number of base solutions are design choices of the model. The amplitudes ùíÇ \\\\bm{a} and transformation parameters Œ∏ \\\\theta serve as the optimizable parameters.\\nLieSolver determines an optimal linear combination f LS \\u200b ( x ; ùíÇ , Œ∏ ) = ‚àë i a i \\u200b f i j i \\u200b ( x ; œë i j i ) f_{\\\\text{LS}}(x;\\\\bm{a},\\\\theta)=\\\\sum_{i}a_{i}\\\\,f^{j_{i}}_{i}(x;\\\\bm{\\\\vartheta}^{j_{i}}_{i}) via two stages. First, LieSolver employs greedy addition of the base solutions f i j i \\u200b ( x ; œë i j i ) f^{j_{i}}_{i}(x;\\\\bm{\\\\vartheta}^{j_{i}}_{i}) ; it appends the active set ùíú \\\\mathcal{A} with the base solution with the largest absolute cosine similarity to the current residual\\nand updates amplitudes ùíÇ \\\\bm{a} with ridge least squares (LS). Here, f LS \\u200b ( ùí≥ ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(\\\\mathcal{X};\\\\bm{a},\\\\theta) is to be understood as the natural extension of equation 28 to ùí≥ \\\\mathcal{X} . Second, it employs variable projection to eliminate the linear amplitudes ùíÇ \\\\bm{a} and periodically refine only the nonlinear parameters Œ∏ \\\\theta with nonlinear least squares (NLLS).\\nBy employing variable projection [ 12 , 26 ] , we eliminate the amplitudes ùíÇ \\\\bm{a} by solving the LS subproblem exactly for fixed Œ∏ \\\\theta ,\\nwhere f LS \\u200b ( ùí≥ ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(\\\\mathcal{X};\\\\bm{a},\\\\theta) is represented as the matrix-vector product with the matrix F \\u200b ( Œ∏ ) = [ f i j i \\u200b ( ùí≥ ; œë i j i ) ] i = 1 M F(\\\\theta)=[\\\\,f^{j_{i}}_{i}(\\\\mathcal{X};\\\\bm{\\\\vartheta}^{j_{i}}_{i})\\\\,]_{i=1}^{M} and Œª > 0 \\\\lambda>0 is the ridge parameter. Substituting ùíÇ ‚ãÜ \\u200b ( Œ∏ ) \\\\bm{a}^{\\\\star}(\\\\theta) back into minimization objective yields the reduced problem:\\nwhich we minimize over allowed values of Œ∏ \\\\theta with NLLS 4 4 4 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html , specifically with bound-constrained trust-region reflective least squares. This reduces the optimization dimension to the nonlinear parameters, improves conditioning by solving the linear part exactly at each step, and stabilizes training.\\nThe initial catalogue of bases ùíÆ \\\\mathcal{S} determines the expressivity of the constructed solution, and thus is crucial for performance. The bases should reproduce a wide range of patterns expected under the given PDE and initial-boundary data. Further, the parameter bounds and the chosen sampling rule (uniform or log-uniform) should cover relevant scales.\\nFor example, for the heat equation, the effective choice includes scaled/shifted Fourier modes from a seed solution like sin \\u2061 ( x ) \\u200b e ‚àí t \\\\sin(x)e^{-t} , Gaussian blobs generated from a constant seed solution by applying diffusion/shift, and a hybrid that combines both behaviours.\\nIn conclusion to the details discussed above, we present the specific algorithm for LieSolver , which depends on the following inputs:\\nTraining data ùí≥ ‚äÇ ‚àÇ Œ© ‚àà ‚Ñù L √ó n \\\\mathcal{X}\\\\subset\\\\partial\\\\Omega\\\\in\\\\mathbb{R}^{L\\\\times n} (rows are points x ‚àà ‚Ñù n x\\\\in\\\\mathbb{R}^{n} ) and targets ùí¥ ‚àà ‚Ñù L √ó m \\\\mathcal{Y}\\\\in\\\\mathbb{R}^{L\\\\times m} on the IBC set.\\nA set of base solutions ùíÆ = { f 1 \\u200b ( x ; œë 1 ) , ‚Ä¶ , f N \\u200b ( x ; œë N ) } \\\\mathcal{S}=\\\\{f^{1}(x;\\\\bm{\\\\vartheta}^{1}),\\\\ldots,f^{N}(x;\\\\bm{\\\\vartheta}^{N})\\\\} . Each f i f^{i} is defined by a seed solution and an ordered list of transforms with parameter bounds and a sampling rule equation 27 . Composing them yields a symbolic expression and a fast numeric evaluator f LS \\u200b ( x ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(x;\\\\bm{a},\\\\theta) .\\nHyperparameters:\\nmse_tol : early stopping threshold on training MSE\\nmax_terms : maximum number of bases in the active set ùíú \\\\mathcal{A}\\nP P : parameter samples per base per addition for candidate scoring\\nR R : number of additions between NLLS refinements\\nŒª \\\\lambda : ridge parameter for ‚Ñì 2 \\\\ell_{2} regularization in least squares\\nsampling seed\\nThe Python implementation is available at https://github.com/oduwancheekee/liesolver .',\n",
       "    'subsections': [{'title': '3.1 Basic structure & motivation behind LieSolver',\n",
       "      'paragraphs': 'Let ùíü \\u200b ( x , u ( d ) ) = 0 \\\\mathcal{D}(x,u^{(d)})=0 denote a linear homogeneous PDE and let f : Œ© ‚Üí ùí∞ f:\\\\Omega\\\\to\\\\mathcal{U} be a solution of this PDE. As discussed in section 2.3 , a Lie symmetry T œë T_{\\\\vartheta} of ùíü \\\\mathcal{D} is a smooth, invertible transformation, parametrized by œë ‚àà Œò ‚äÜ ‚Ñù \\\\vartheta\\\\in\\\\Theta\\\\subseteq\\\\mathbb{R} , with the property that transforming the solution with T œë T_{\\\\vartheta} yields another solution of the PDE ùíü \\\\mathcal{D} . In particular, as noted in section 2.1 any linear homogeneous PDE will have the following symmetry:\\nwhere x ‚àà X x\\\\in X , œë ‚àà ‚Ñù \\\\vartheta\\\\in\\\\mathbb{R} and Œ± : Œ© ‚Üí ùí∞ \\\\alpha:\\\\Omega\\\\to\\\\mathcal{U} is an arbitrary solution of the PDE. In different terms, for any linear homogeneous PDE, the linear combination of solutions is a solution as well, which is also known as the superposition principle.\\nSince Lie symmetries form a group, concatenating different Lie symmetries yields another valid Lie symmetry. Thus, given a solution, one can generate a family of new solutions by successive applications of these transformations. Finding an initial solution is often straightforward. For instance, any linear homogeneous PDE admits the trivial f \\u200b ( x ) ‚â° 0 f(x)\\\\equiv 0 solution. In addition, a constant function solves every linear homogeneous PDE without a term containing u u , e.g. f \\u200b ( x ) ‚â° 1 f(x)\\\\equiv 1 is a solution for heat equation, Laplace equation and wave equation. We refer to such solutions as seed solutions , denoted f seed f_{\\\\text{seed}} , and they are typically easy to obtain for a given PDE.\\nHence, when starting with one or several seed solutions and applying various Lie transformations, the resulting functions will still constitute exact solutions of the PDE. This reveals a remarkable fact: a single seed solution, together with the action of a Lie group, can define expressive solution (sub)spaces via superposition. We illustrate this in the following example.\\nConsider the following seed solution of the heat equation equation 17 :\\nApplying the transformation T œë 4 T^{4}_{\\\\vartheta} from example 2.2 with œë = ‚àí ln \\u2061 Œª \\\\vartheta=-\\\\ln\\\\lambda , one obtains the family of solution sin \\u2061 ( Œª \\u200b x ) \\u200b e ‚àí Œª 2 \\u200b t \\\\sin(\\\\lambda x)e^{-\\\\lambda^{2}t} for any Œª > 0 \\\\lambda>0 .\\nIt is a well-known result [ 4 , sec. 10.5] that the IBVP for the heat equation u t = u x \\u200b x u_{t}=u_{xx} on ( 0 , L ) (0,L) with u \\u200b ( 0 , t ) = u \\u200b ( L , t ) = 0 u(0,t)=u(L,t)=0 and u \\u200b ( x , 0 ) = ‚àë n = 1 ‚àû a n \\u200b sin \\u2061 ( n \\u200b œÄ \\u200b x / L ) u(x,0)=\\\\sum_{n=1}^{\\\\infty}a_{n}\\\\sin\\\\!\\\\big(n\\\\pi x/L\\\\big) admits the unique solution u \\u200b ( x , t ) = ‚àë n = 1 ‚àû a n \\u200b sin \\u2061 ( n \\u200b œÄ \\u200b x / L ) \\u200b e ‚àí ( n \\u200b œÄ / L ) 2 \\u200b t u(x,t)=\\\\sum_{n=1}^{\\\\infty}a_{n}\\\\sin\\\\!\\\\big(n\\\\pi x/L\\\\big)\\\\,e^{-(n\\\\pi/L)^{2}t} . Hence, even with one specific Lie transformation, we can approximate a certain type of IBVP arbitrary well.\\nThus, by applying a sequence of Lie symmetries T l T^{l} to a given f seed f_{\\\\text{seed}} , we obtain an expressive function\\nthat remains a solution of the underlying PDE ùíü \\\\mathcal{D} for any value of œë = ( œë 1 , ‚Ä¶ , œë k ) ‚àà ‚Ñù k \\\\bm{\\\\vartheta}=(\\\\vartheta_{1},\\\\ldots,\\\\vartheta_{k})\\\\in\\\\mathbb{R}^{k} , whenever the composed function is defined.\\nIn the approach of LieSolver , we consider a set ùíÆ = { f 1 \\u200b ( x ; œë 1 ) , ‚Ä¶ , f N \\u200b ( x ; œë N ) } \\\\mathcal{S}=\\\\{f^{1}(x;\\\\bm{\\\\vartheta}^{1}),\\\\ldots,f^{N}(x;\\\\bm{\\\\vartheta}^{N})\\\\} of expressive functions, where each function f i f^{i} is constructed from one or several seed solutions in the form of equation 27 . We refer to these functions as base solutions 3 3 3 We borrow this term from the Finite Element Method. However, this set will not necessarily be minimal, but we will span a solution-(sub)space by these ‚Äúbase solutions‚Äù. . We will discuss in section 3.2 more in detail, how to choose ‚Äúgood‚Äù base solutions for a given problem.\\nThe central idea of LieSolver is to choose certain base solution and combine them linearly. In this procedure we can also use the same base solution several times with different parameters, i.e.\\nwhich constitutes a very expressive function covering a major part of the solution space of the PDE. When the parameters œë i j i \\\\bm{\\\\vartheta}_{i}^{j_{i}} are specified to certain values œë ^ i j i \\\\hat{\\\\bm{\\\\vartheta}}_{i}^{j_{i}} , we will call the collection of base solutions appearing in equation 28 the active set ùíú = { f 1 j 1 \\u200b ( x ; œë ^ 1 j 1 ) , ‚Ä¶ , f M j M \\u200b ( x ; œë ^ M j M ) } \\\\mathcal{A}=\\\\{f_{1}^{j_{1}}(x;\\\\hat{\\\\bm{\\\\vartheta}}_{1}^{j_{1}}),\\\\ldots,f_{M}^{j_{M}}(x;\\\\hat{\\\\bm{\\\\vartheta}}_{M}^{j_{M}})\\\\} . Note that each œë i j i \\\\bm{\\\\vartheta}_{i}^{j_{i}} may possibly consist of several parameters as given in equation 27 . To shorten the notation, we write Œ∏ = ( œë 1 j 1 , ‚Ä¶ , œë M j M ) \\\\theta=(\\\\bm{\\\\vartheta}_{1}^{j_{1}},\\\\ldots,\\\\bm{\\\\vartheta}_{M}^{j_{M}}) and ùíÇ = ( a 1 , ‚Ä¶ , a M ) \\\\bm{a}=(a_{1},\\\\ldots,a_{M}) . Since f LS f_{\\\\text{LS}} is a solution of the PDE by construction , the problem of finding a solution to the IBVP is reduced to match given initial and/or boundary conditions ‚Ñ¨ 1 , ‚Ä¶ , ‚Ñ¨ r \\\\mathcal{B}_{1},\\\\ldots,\\\\mathcal{B}_{r} , i.e. finding the optimal set of parameters ùíÇ \\\\bm{a} and Œ∏ \\\\theta . To formalize this, we introduce the loss function:\\nwhere ùí≥ k ‚äÇ Œì k \\\\mathcal{X}_{k}\\\\subset\\\\Gamma_{k} are finite sets of randomly sampled collocation points from the boundary parts Œì k ‚äÇ ‚àÇ Œ© \\\\Gamma_{k}\\\\subset\\\\partial\\\\Omega according to section 2.2 .\\nThe union ùí≥ = ‚ãÉ k = 1 r ùí≥ k \\\\mathcal{X}=\\\\bigcup_{k=1}^{r}\\\\mathcal{X}_{k} forms the training dataset and consists of L L points ùí≥ ‚äÜ ‚Ñù L √ó n \\\\mathcal{X}\\\\subseteq\\\\mathbb{R}^{L\\\\times n} (rows are individual points x ‚àà ‚Ñù n x\\\\in\\\\mathbb{R}^{n} ). Consequently, boundary conditions ‚Ñ¨ 1 , ‚Ä¶ , ‚Ñ¨ r \\\\mathcal{B}_{1},\\\\ldots,\\\\mathcal{B}_{r} define set of target values ùí¥ ‚àà ‚Ñù L √ó m \\\\mathcal{Y}\\\\in\\\\mathbb{R}^{L\\\\times m} at datapoints ùí≥ \\\\mathcal{X} .\\nThe seed solutions, the chosen transformations applied to them, and the number of base solutions are design choices of the model. The amplitudes ùíÇ \\\\bm{a} and transformation parameters Œ∏ \\\\theta serve as the optimizable parameters.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 The LieSolver optimization algorithm',\n",
       "      'paragraphs': 'LieSolver determines an optimal linear combination f LS \\u200b ( x ; ùíÇ , Œ∏ ) = ‚àë i a i \\u200b f i j i \\u200b ( x ; œë i j i ) f_{\\\\text{LS}}(x;\\\\bm{a},\\\\theta)=\\\\sum_{i}a_{i}\\\\,f^{j_{i}}_{i}(x;\\\\bm{\\\\vartheta}^{j_{i}}_{i}) via two stages. First, LieSolver employs greedy addition of the base solutions f i j i \\u200b ( x ; œë i j i ) f^{j_{i}}_{i}(x;\\\\bm{\\\\vartheta}^{j_{i}}_{i}) ; it appends the active set ùíú \\\\mathcal{A} with the base solution with the largest absolute cosine similarity to the current residual\\nand updates amplitudes ùíÇ \\\\bm{a} with ridge least squares (LS). Here, f LS \\u200b ( ùí≥ ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(\\\\mathcal{X};\\\\bm{a},\\\\theta) is to be understood as the natural extension of equation 28 to ùí≥ \\\\mathcal{X} . Second, it employs variable projection to eliminate the linear amplitudes ùíÇ \\\\bm{a} and periodically refine only the nonlinear parameters Œ∏ \\\\theta with nonlinear least squares (NLLS).\\nBy employing variable projection [ 12 , 26 ] , we eliminate the amplitudes ùíÇ \\\\bm{a} by solving the LS subproblem exactly for fixed Œ∏ \\\\theta ,\\nwhere f LS \\u200b ( ùí≥ ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(\\\\mathcal{X};\\\\bm{a},\\\\theta) is represented as the matrix-vector product with the matrix F \\u200b ( Œ∏ ) = [ f i j i \\u200b ( ùí≥ ; œë i j i ) ] i = 1 M F(\\\\theta)=[\\\\,f^{j_{i}}_{i}(\\\\mathcal{X};\\\\bm{\\\\vartheta}^{j_{i}}_{i})\\\\,]_{i=1}^{M} and Œª > 0 \\\\lambda>0 is the ridge parameter. Substituting ùíÇ ‚ãÜ \\u200b ( Œ∏ ) \\\\bm{a}^{\\\\star}(\\\\theta) back into minimization objective yields the reduced problem:\\nwhich we minimize over allowed values of Œ∏ \\\\theta with NLLS 4 4 4 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html , specifically with bound-constrained trust-region reflective least squares. This reduces the optimization dimension to the nonlinear parameters, improves conditioning by solving the linear part exactly at each step, and stabilizes training.\\nThe initial catalogue of bases ùíÆ \\\\mathcal{S} determines the expressivity of the constructed solution, and thus is crucial for performance. The bases should reproduce a wide range of patterns expected under the given PDE and initial-boundary data. Further, the parameter bounds and the chosen sampling rule (uniform or log-uniform) should cover relevant scales.\\nFor example, for the heat equation, the effective choice includes scaled/shifted Fourier modes from a seed solution like sin \\u2061 ( x ) \\u200b e ‚àí t \\\\sin(x)e^{-t} , Gaussian blobs generated from a constant seed solution by applying diffusion/shift, and a hybrid that combines both behaviours.\\nIn conclusion to the details discussed above, we present the specific algorithm for LieSolver , which depends on the following inputs:\\nTraining data ùí≥ ‚äÇ ‚àÇ Œ© ‚àà ‚Ñù L √ó n \\\\mathcal{X}\\\\subset\\\\partial\\\\Omega\\\\in\\\\mathbb{R}^{L\\\\times n} (rows are points x ‚àà ‚Ñù n x\\\\in\\\\mathbb{R}^{n} ) and targets ùí¥ ‚àà ‚Ñù L √ó m \\\\mathcal{Y}\\\\in\\\\mathbb{R}^{L\\\\times m} on the IBC set.\\nA set of base solutions ùíÆ = { f 1 \\u200b ( x ; œë 1 ) , ‚Ä¶ , f N \\u200b ( x ; œë N ) } \\\\mathcal{S}=\\\\{f^{1}(x;\\\\bm{\\\\vartheta}^{1}),\\\\ldots,f^{N}(x;\\\\bm{\\\\vartheta}^{N})\\\\} . Each f i f^{i} is defined by a seed solution and an ordered list of transforms with parameter bounds and a sampling rule equation 27 . Composing them yields a symbolic expression and a fast numeric evaluator f LS \\u200b ( x ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(x;\\\\bm{a},\\\\theta) .\\nHyperparameters:\\nmse_tol : early stopping threshold on training MSE\\nmax_terms : maximum number of bases in the active set ùíú \\\\mathcal{A}\\nP P : parameter samples per base per addition for candidate scoring\\nR R : number of additions between NLLS refinements\\nŒª \\\\lambda : ridge parameter for ‚Ñì 2 \\\\ell_{2} regularization in least squares\\nsampling seed\\nThe Python implementation is available at https://github.com/oduwancheekee/liesolver .',\n",
       "      'subsections': [{'title': 'Variable projection',\n",
       "        'paragraphs': 'By employing variable projection [ 12 , 26 ] , we eliminate the amplitudes ùíÇ \\\\bm{a} by solving the LS subproblem exactly for fixed Œ∏ \\\\theta ,\\nwhere f LS \\u200b ( ùí≥ ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(\\\\mathcal{X};\\\\bm{a},\\\\theta) is represented as the matrix-vector product with the matrix F \\u200b ( Œ∏ ) = [ f i j i \\u200b ( ùí≥ ; œë i j i ) ] i = 1 M F(\\\\theta)=[\\\\,f^{j_{i}}_{i}(\\\\mathcal{X};\\\\bm{\\\\vartheta}^{j_{i}}_{i})\\\\,]_{i=1}^{M} and Œª > 0 \\\\lambda>0 is the ridge parameter. Substituting ùíÇ ‚ãÜ \\u200b ( Œ∏ ) \\\\bm{a}^{\\\\star}(\\\\theta) back into minimization objective yields the reduced problem:\\nwhich we minimize over allowed values of Œ∏ \\\\theta with NLLS 4 4 4 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html , specifically with bound-constrained trust-region reflective least squares. This reduces the optimization dimension to the nonlinear parameters, improves conditioning by solving the linear part exactly at each step, and stabilizes training.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Educated guess of base functions',\n",
       "        'paragraphs': 'The initial catalogue of bases ùíÆ \\\\mathcal{S} determines the expressivity of the constructed solution, and thus is crucial for performance. The bases should reproduce a wide range of patterns expected under the given PDE and initial-boundary data. Further, the parameter bounds and the chosen sampling rule (uniform or log-uniform) should cover relevant scales.\\nFor example, for the heat equation, the effective choice includes scaled/shifted Fourier modes from a seed solution like sin \\u2061 ( x ) \\u200b e ‚àí t \\\\sin(x)e^{-t} , Gaussian blobs generated from a constant seed solution by applying diffusion/shift, and a hybrid that combines both behaviours.',\n",
       "        'subsections': []},\n",
       "       {'title': 'The LieSolver algorithm',\n",
       "        'paragraphs': 'In conclusion to the details discussed above, we present the specific algorithm for LieSolver , which depends on the following inputs:\\nTraining data ùí≥ ‚äÇ ‚àÇ Œ© ‚àà ‚Ñù L √ó n \\\\mathcal{X}\\\\subset\\\\partial\\\\Omega\\\\in\\\\mathbb{R}^{L\\\\times n} (rows are points x ‚àà ‚Ñù n x\\\\in\\\\mathbb{R}^{n} ) and targets ùí¥ ‚àà ‚Ñù L √ó m \\\\mathcal{Y}\\\\in\\\\mathbb{R}^{L\\\\times m} on the IBC set.\\nA set of base solutions ùíÆ = { f 1 \\u200b ( x ; œë 1 ) , ‚Ä¶ , f N \\u200b ( x ; œë N ) } \\\\mathcal{S}=\\\\{f^{1}(x;\\\\bm{\\\\vartheta}^{1}),\\\\ldots,f^{N}(x;\\\\bm{\\\\vartheta}^{N})\\\\} . Each f i f^{i} is defined by a seed solution and an ordered list of transforms with parameter bounds and a sampling rule equation 27 . Composing them yields a symbolic expression and a fast numeric evaluator f LS \\u200b ( x ; ùíÇ , Œ∏ ) f_{\\\\text{LS}}(x;\\\\bm{a},\\\\theta) .\\nHyperparameters:\\nmse_tol : early stopping threshold on training MSE\\nmax_terms : maximum number of bases in the active set ùíú \\\\mathcal{A}\\nP P : parameter samples per base per addition for candidate scoring\\nR R : number of additions between NLLS refinements\\nŒª \\\\lambda : ridge parameter for ‚Ñì 2 \\\\ell_{2} regularization in least squares\\nsampling seed',\n",
       "        'subsections': []},\n",
       "       {'title': 'Code availability',\n",
       "        'paragraphs': 'The Python implementation is available at https://github.com/oduwancheekee/liesolver .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '4 Experiments and results',\n",
       "    'paragraphs': 'To evaluate performance and demonstrate the capabilities of the model, we apply LieSolver to several IBVPs for the one-dimensional heat equation, and the one-dimensional wave equation and compare its performance with standard PINNs and demonstrate its efficiency. LieSolver outperforms PINNs in our benchmarks regarding accuracy as well efficiency. Since both of these problems are well-posed, we can demonstrate the relation between accuracy and test loss as stated in section 2.2 .\\nIn our experiments, we report the mean squared error (MSE) on the IBC data and the relative L 2 L_{2} error (L2RE) on the space-time grid in the domain Œ© \\\\Omega . Since all targets are on the order of unity in their magnitude, MSEs are comparable across all problem instances. Unless stated otherwise, we use P = 1000 P=1000 , R = 5 R=5 , nfev_global = 4 =4 (number of trust-region reflective steps in global refine), Œª = 10 ‚àí 1 \\\\lambda=10^{-1} , L = 3000 L=3000 , mse_tol = 10 ‚àí 6 =10^{-6} , for a robust, empirically determined configuration in LieSolver .\\nFor the heat equation ùíü \\u200b ( x , t , u ( 2 ) ) = u t ‚àí u x \\u200b x \\\\mathcal{D}(x,t,u^{(2)})=u_{t}-u_{xx} , we use ( x , t ) ‚àà ( 0 , 1 ) √ó ( 0 , 0.1 ) (x,t)\\\\in(0,1)\\\\times(0,0.1) as the domain, and we consider five different profiles polynomial , Gaussian , sine , sine mix , and step for the initial condition u \\u200b ( x , 0 ) = u 0 \\u200b ( x ) u(x,0)=u_{0}(x) , which are shown in figure 1 . The boundary conditions are set to constant values defined by the initial condition u \\u200b ( 0 , t ) = u 0 \\u200b ( 0 ) u(0,t)=u_{0}(0) and u \\u200b ( 1 , t ) = u 0 \\u200b ( 1 ) u(1,t)=u_{0}(1) . Similarly, for the wave equation ùíü \\u200b ( x , t , u ( 2 ) ) = u t \\u200b t ‚àí u x \\u200b x \\\\mathcal{D}(x,t,u^{(2)})=u_{tt}-u_{xx} , we set ( x , t ) ‚àà ( 0 , 1 ) √ó ( 0 , 1 ) (x,t)\\\\in(0,1)\\\\times(0,1) with u \\u200b ( 0 , t ) = u \\u200b ( 1 , t ) = 0 u(0,t)=u(1,t)=0 , u t \\u200b ( x , 0 ) = 0 u_{t}(x,0)=0 and consider the five IC profiles sine , sine mix , Gaussian , Gaussian mix , and step , also depicted in figure 1 .\\nIn order to compare the predictions of LieSolver , we compute a reference ground truth solutions by expanding the initial data in a sine basis according to [ 4 ] . For the heat equation with Dirichlet BC, we set b L = u \\u200b ( 0 , 0 ) b_{L}=u(0,0) and b R = u \\u200b ( 1 , 0 ) b_{R}=u(1,0) and define w \\u200b ( x ) = b L + ( b R ‚àí b L ) \\u200b x w(x)=b_{L}+(b_{R}-b_{L})\\\\,x so that v 0 \\u200b ( x ) = u \\u200b ( x , 0 ) ‚àí w \\u200b ( x ) v_{0}(x)=u(x,0)-w(x) satisfies homogeneous Dirichlet conditions. We sample v 0 v_{0} at the interior points x m = m M + 1 x_{m}=\\\\frac{m}{M+1} for m = 1 , ‚Ä¶ , M m=1,\\\\dots,M and obtain the coefficients A m A_{m} via orthonormal projection onto sin \\u2061 ( m \\u200b œÄ \\u200b x ) \\\\sin(m\\\\pi x) . The solution is\\nFor the wave equation with homogeneous Dirichlet boundaries, we sample u \\u200b ( x , 0 ) u(x,0) and u t \\u200b ( x , 0 ) u_{t}(x,0) at the same interior points x m x_{m} and compute A m A_{m} and B m B_{m} by orthonormal projection onto sin \\u2061 ( m \\u200b œÄ \\u200b x ) \\\\sin(m\\\\pi x) . The solution is then given by\\nLieSolver requires an educated choice of parametrized base solutions f \\u200b ( x ; œë ) f(x;\\\\bm{\\\\vartheta}) (see section 3.2 ). For the one-dimensional heat equation, we can use the constant solution f seed , 1 \\u200b ( x , t ) ‚â° 1 f_{\\\\text{seed},1}(x,t)\\\\equiv 1 and f seed , 2 \\u200b ( x , t ) = e ‚àí t \\u200b sin \\u2061 x f_{\\\\text{seed},2}(x,t)=e^{-t}\\\\sin x as possible seed solutions. Based on these seed solutions, we employ three complementary base solutions generated by Lie symmetries equation 18 ‚Äì equation 23 of the heat equation:\\nf 1 \\u200b ( x , t ; œë 1 ) = T œë 2 4 ‚àò T œë 1 1 ‚àò ( e ‚àí t \\u200b sin \\u2061 x ) f^{1}(x,t;\\\\bm{\\\\vartheta}^{1})=T^{4}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(e^{-t}\\\\sin x\\\\big) : scaled/shifted Fourier mode (variable phase/frequency).\\nf 2 \\u200b ( x , t ; œë 2 ) = T œë 2 1 ‚àò T œë 1 6 ‚àò ( 1 ) f^{2}(x,t;\\\\bm{\\\\vartheta}^{2})=T^{1}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{6}_{\\\\vartheta_{1}}\\\\circ(1) : diffusion-generated Gaussian blob (variable center and width).\\nf 3 \\u200b ( x , t ; œë 3 ) = T œë 4 1 ‚àò T œë 3 6 ‚àò T œë 2 4 ‚àò T œë 1 1 ‚àò ( e ‚àí t \\u200b sin \\u2061 x ) f^{3}(x,t;\\\\bm{\\\\vartheta}^{3})=T^{1}_{\\\\vartheta_{4}}\\\\!\\\\circ T^{6}_{\\\\vartheta_{3}}\\\\!\\\\circ T^{4}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(e^{-t}\\\\sin x\\\\big) : Gaussian blob modulated by a scaled/shifted sine.\\nFigure 2 illustrates the variety of base solutions chosen to solve the heat equation.\\nFor the one-dimensional wave equation, we identify a subgroup of the Lie symmetry group and consider two transformations: spatial translation T 1 T^{1} and spatiotemporal scaling T 2 T^{2} :\\nWe employ them to compose the following two base solutions for the wave equation:\\nf 1 \\u200b ( x , t ; œë 1 ) = T œë 2 2 ‚àò T œë 1 1 ‚àò ( sin \\u2061 x \\u200b cos \\u2061 t ) f^{1}(x,t;\\\\bm{\\\\vartheta}^{1})=T^{2}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(\\\\sin x\\\\cos t\\\\big) : scaled/shifted standing-wave (variable phase/frequency), which is suitable for homogeneous Dirichlet BCs.\\nf 2 \\u200b ( x , t ; œë 2 ) = T œë 2 1 ‚àò T œë 1 2 ‚àò ( e ‚àí ( x ‚àí t ) 2 + e ‚àí ( x + t ) 2 ) f^{2}(x,t;\\\\bm{\\\\vartheta}^{2})=T^{1}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{2}_{\\\\vartheta_{1}}\\\\circ(e^{-(x-t)^{2}}+e^{-(x+t)^{2}}) : Gaussian pattern (variable centre and width). Zero velocity IC enables us to use travelling blob pair instead of just one travelling blob.\\nWe use a standard fully connected neural network f PINN : ‚Ñù 2 ‚Üí ‚Ñù f_{\\\\text{PINN}}:\\\\mathbb{R}^{2}\\\\to\\\\mathbb{R} with input ( x , t ) (x,t) , tanh \\\\tanh activation function and where the number of layers and their width is specified in table 2 in alignment with [ 40 ] . The loss combines the PDE residual, a several weighted IBC terms:\\nHere, ùí≥ r ‚äÇ Œ© \\\\mathcal{X}_{r}\\\\subset\\\\Omega are the collocation points in the domain, while ùí≥ \\\\mathcal{X} and ùí¥ \\\\mathcal{Y} are the same sets of IBC points and targets used for LieSolver. The loss weight w ICBC w_{\\\\mathrm{ICBC}} is tuned to mitigate ill-conditioning and facilitate training (see table 2 ). We warm up with Adam for 1000 iterations and then train with L‚ÄìBFGS until convergence or optimizer stall.\\nTable 1 summarizes representative runs of LieSolver for the considered IBVPs, while table 2 reports the corresponding PINN runs. For all ICs except the step IC, LieSolver reliably attains IBC MSE values of order 10 ‚àí 6 10^{-6} using between 5 5 and 40 40 bases. The step IC requires more bases due to its sharp jump. PINNs match LieSolver in terms of attained MSE only on the easiest cases, but falls short on the more challenging ICs. The sine mix IC requires a larger 6 √ó 100 6\\\\times 100 PINN. Notably, the step IC does not reach the target tolerance even with larger PINN models.\\nLieSolver is about one to two orders of magnitude faster than PINN across the considered cases. However, its runtime 5 5 5 All experiments were run on an Intel Core Ultra 7 165H CPU with 11 cores and 16\\u2009GiB RAM. increases sharply as the number of bases grows. Configurations with fewer than 20 20 bases train within 10 10 s, 40 40 bases ‚Äì within 40 40 to 60 60 s, and around 80 80 bases take 200 200 to 300 300 s. This growth is determined by the global parameter refinement stage. More targeted refinement strategies could mitigate this bottleneck.\\nThe choice of base solutions strongly influences performance. Employing sine and Gaussian bases, LieSolver naturally fits IC profiles with similar sine and Gaussian patterns and reaches the target MSE in under a minute. The step IC remains a challenging stress test and highlights model capability. In the following, we discuss three representative cases that illustrate LieSolver behaviour.\\nFigure 3 shows the evolution of the IBC MSE as a function of the number of added bases. The greedy stage begins with a Gaussian base f 2 f^{2} that already captures most of the IC (MSE ‚àº 10 ‚àí 3 \\\\sim 10^{-3} ), followed by several Fourier-like additions ( f 1 f^{1} and f 3 f^{3} ) that enforce the Dirichlet BCs. The first global refinement stage (after 5 bases) yields a one-order-of-magnitude drop in MSE; the target tolerance ( < 10 ‚àí 6 <10^{-6} ) is met after 11 bases. Details on performance metrics and computation are provided in table 1 . Moreover, we confirm the considerations from section 2.2 that there is an upper bound for the ratio between MSE on the domain and the test MSE on the IBCs as indicated in equation 13 . Hence, we show that the loss serves as a reliable metric for the overall domain accuracy.\\nFigure 4 and figure 5 compare the model prediction with the ground truth in the domain and on the IBCs, respectively. The IBC plots include a decomposition into the weighted base terms, revealing dominant Gaussian-like contributors for the IC profile and additional base terms that fit the BCs.\\nFor reference, a vanilla PINN also fits this smooth case to an IBC MSE < 10 ‚àí 6 <10^{-6} ( figure 6 ), albeit requiring a longer runtime, as indicated in table 2 .\\nThe step IC represents the most challenging case due to the jump in the profile. Figure 7 highlights three regimes of LieSolver optimization: (i) rapid improvement to MSE ‚âà 10 ‚àí 4 \\\\approx 10^{-4} within 15 15 bases, (ii) a stall where additional bases bring limited improvement, and (iii) resumed progress after 40 40 bases culminating in < 10 ‚àí 5 <10^{-5} with 58 bases. Figures 8 and 9 show that the dominant residual is localized near the initial jump. Despite IBC MSE being higher than the target, the domain L2RE reaches 4.6 ‚ãÖ 10 ‚àí 3 4.6\\\\cdot 10^{-3} , which is comparable to other IC cases. The decomposition in figure 9 is less interpretable than in the Gaussian case due to the large number modulated base contributions.\\nA standard PINN plateaus around 10 ‚àí 3 10^{-3} IBC MSE ( figure 10 ) and ‚àº 10 ‚àí 1 \\\\sim 10^{-1} L2RE in the domain. The transition from an extremely sharp IC to a rapidly smoothening solution in the domain is hard for PINNs. Even larger networks fail to capture this dependency and underperform relative to the LieSolver .\\nFigure 11 illustrates effective handling of the challenging wave PDE with a sharp step IC by LieSolver . A Global parameter refinement at 60 and 80 base terms yields pronounced drops in MSE. With a sufficient number of active bases, the optimizer can adjust them effectively and achieve lower error. Figure 13 shows that the solution is primarily composed of Fourier sine modes with localized Gaussian corrections. During optimization, the greedy stage first selects sine bases to fit the dominant high-amplitude oscillatory component. As the remaining residual becomes localized and irregular, the algorithm increasingly adds Gaussian bases to correct for edges and boundary regions.\\nWith Gaussian bases, the residual is fitted in two steps. First, the model picks a positive pulse propagating into the domain. Second, it adds opposite-sign contributions to compensate for positive pulses at the boundaries and to reproduce the reflected wave (see the BC plots, middle and right subplots in figure 13 ). This structure is straightforward for the optimizer, and further MSE reduction is attainable with additional bases and parameters refinement. The domain plot is presented in figure 12 . The model achieves an L2RE of 5.1 ‚ãÖ 10 ‚àí 3 5.1\\\\cdot 10^{-3} on the domain. The PINN handles the step IC better for the wave equation ( Figure 14 ) than for the heat equation ( Figure 10 ). The reason is that the wave PDE lacks parabolic smoothing; there is no abrupt transition from a sharp IC to a rapidly smoothed interior solution. Nevertheless, it does not resolve the sharp features sufficiently well and underperforms relative to LieSolver , achieving only an L2RE of 4.5 ‚ãÖ 10 ‚àí 2 4.5\\\\cdot 10^{-2} in the domain.',\n",
       "    'subsections': [{'title': '4.1 Experimental setup',\n",
       "      'paragraphs': 'In our experiments, we report the mean squared error (MSE) on the IBC data and the relative L 2 L_{2} error (L2RE) on the space-time grid in the domain Œ© \\\\Omega . Since all targets are on the order of unity in their magnitude, MSEs are comparable across all problem instances. Unless stated otherwise, we use P = 1000 P=1000 , R = 5 R=5 , nfev_global = 4 =4 (number of trust-region reflective steps in global refine), Œª = 10 ‚àí 1 \\\\lambda=10^{-1} , L = 3000 L=3000 , mse_tol = 10 ‚àí 6 =10^{-6} , for a robust, empirically determined configuration in LieSolver .\\nFor the heat equation ùíü \\u200b ( x , t , u ( 2 ) ) = u t ‚àí u x \\u200b x \\\\mathcal{D}(x,t,u^{(2)})=u_{t}-u_{xx} , we use ( x , t ) ‚àà ( 0 , 1 ) √ó ( 0 , 0.1 ) (x,t)\\\\in(0,1)\\\\times(0,0.1) as the domain, and we consider five different profiles polynomial , Gaussian , sine , sine mix , and step for the initial condition u \\u200b ( x , 0 ) = u 0 \\u200b ( x ) u(x,0)=u_{0}(x) , which are shown in figure 1 . The boundary conditions are set to constant values defined by the initial condition u \\u200b ( 0 , t ) = u 0 \\u200b ( 0 ) u(0,t)=u_{0}(0) and u \\u200b ( 1 , t ) = u 0 \\u200b ( 1 ) u(1,t)=u_{0}(1) . Similarly, for the wave equation ùíü \\u200b ( x , t , u ( 2 ) ) = u t \\u200b t ‚àí u x \\u200b x \\\\mathcal{D}(x,t,u^{(2)})=u_{tt}-u_{xx} , we set ( x , t ) ‚àà ( 0 , 1 ) √ó ( 0 , 1 ) (x,t)\\\\in(0,1)\\\\times(0,1) with u \\u200b ( 0 , t ) = u \\u200b ( 1 , t ) = 0 u(0,t)=u(1,t)=0 , u t \\u200b ( x , 0 ) = 0 u_{t}(x,0)=0 and consider the five IC profiles sine , sine mix , Gaussian , Gaussian mix , and step , also depicted in figure 1 .\\nIn order to compare the predictions of LieSolver , we compute a reference ground truth solutions by expanding the initial data in a sine basis according to [ 4 ] . For the heat equation with Dirichlet BC, we set b L = u \\u200b ( 0 , 0 ) b_{L}=u(0,0) and b R = u \\u200b ( 1 , 0 ) b_{R}=u(1,0) and define w \\u200b ( x ) = b L + ( b R ‚àí b L ) \\u200b x w(x)=b_{L}+(b_{R}-b_{L})\\\\,x so that v 0 \\u200b ( x ) = u \\u200b ( x , 0 ) ‚àí w \\u200b ( x ) v_{0}(x)=u(x,0)-w(x) satisfies homogeneous Dirichlet conditions. We sample v 0 v_{0} at the interior points x m = m M + 1 x_{m}=\\\\frac{m}{M+1} for m = 1 , ‚Ä¶ , M m=1,\\\\dots,M and obtain the coefficients A m A_{m} via orthonormal projection onto sin \\u2061 ( m \\u200b œÄ \\u200b x ) \\\\sin(m\\\\pi x) . The solution is\\nFor the wave equation with homogeneous Dirichlet boundaries, we sample u \\u200b ( x , 0 ) u(x,0) and u t \\u200b ( x , 0 ) u_{t}(x,0) at the same interior points x m x_{m} and compute A m A_{m} and B m B_{m} by orthonormal projection onto sin \\u2061 ( m \\u200b œÄ \\u200b x ) \\\\sin(m\\\\pi x) . The solution is then given by\\nLieSolver requires an educated choice of parametrized base solutions f \\u200b ( x ; œë ) f(x;\\\\bm{\\\\vartheta}) (see section 3.2 ). For the one-dimensional heat equation, we can use the constant solution f seed , 1 \\u200b ( x , t ) ‚â° 1 f_{\\\\text{seed},1}(x,t)\\\\equiv 1 and f seed , 2 \\u200b ( x , t ) = e ‚àí t \\u200b sin \\u2061 x f_{\\\\text{seed},2}(x,t)=e^{-t}\\\\sin x as possible seed solutions. Based on these seed solutions, we employ three complementary base solutions generated by Lie symmetries equation 18 ‚Äì equation 23 of the heat equation:\\nf 1 \\u200b ( x , t ; œë 1 ) = T œë 2 4 ‚àò T œë 1 1 ‚àò ( e ‚àí t \\u200b sin \\u2061 x ) f^{1}(x,t;\\\\bm{\\\\vartheta}^{1})=T^{4}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(e^{-t}\\\\sin x\\\\big) : scaled/shifted Fourier mode (variable phase/frequency).\\nf 2 \\u200b ( x , t ; œë 2 ) = T œë 2 1 ‚àò T œë 1 6 ‚àò ( 1 ) f^{2}(x,t;\\\\bm{\\\\vartheta}^{2})=T^{1}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{6}_{\\\\vartheta_{1}}\\\\circ(1) : diffusion-generated Gaussian blob (variable center and width).\\nf 3 \\u200b ( x , t ; œë 3 ) = T œë 4 1 ‚àò T œë 3 6 ‚àò T œë 2 4 ‚àò T œë 1 1 ‚àò ( e ‚àí t \\u200b sin \\u2061 x ) f^{3}(x,t;\\\\bm{\\\\vartheta}^{3})=T^{1}_{\\\\vartheta_{4}}\\\\!\\\\circ T^{6}_{\\\\vartheta_{3}}\\\\!\\\\circ T^{4}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(e^{-t}\\\\sin x\\\\big) : Gaussian blob modulated by a scaled/shifted sine.\\nFigure 2 illustrates the variety of base solutions chosen to solve the heat equation.\\nFor the one-dimensional wave equation, we identify a subgroup of the Lie symmetry group and consider two transformations: spatial translation T 1 T^{1} and spatiotemporal scaling T 2 T^{2} :\\nWe employ them to compose the following two base solutions for the wave equation:\\nf 1 \\u200b ( x , t ; œë 1 ) = T œë 2 2 ‚àò T œë 1 1 ‚àò ( sin \\u2061 x \\u200b cos \\u2061 t ) f^{1}(x,t;\\\\bm{\\\\vartheta}^{1})=T^{2}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(\\\\sin x\\\\cos t\\\\big) : scaled/shifted standing-wave (variable phase/frequency), which is suitable for homogeneous Dirichlet BCs.\\nf 2 \\u200b ( x , t ; œë 2 ) = T œë 2 1 ‚àò T œë 1 2 ‚àò ( e ‚àí ( x ‚àí t ) 2 + e ‚àí ( x + t ) 2 ) f^{2}(x,t;\\\\bm{\\\\vartheta}^{2})=T^{1}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{2}_{\\\\vartheta_{1}}\\\\circ(e^{-(x-t)^{2}}+e^{-(x+t)^{2}}) : Gaussian pattern (variable centre and width). Zero velocity IC enables us to use travelling blob pair instead of just one travelling blob.',\n",
       "      'subsections': [{'title': 'Reference solution',\n",
       "        'paragraphs': 'In order to compare the predictions of LieSolver , we compute a reference ground truth solutions by expanding the initial data in a sine basis according to [ 4 ] . For the heat equation with Dirichlet BC, we set b L = u \\u200b ( 0 , 0 ) b_{L}=u(0,0) and b R = u \\u200b ( 1 , 0 ) b_{R}=u(1,0) and define w \\u200b ( x ) = b L + ( b R ‚àí b L ) \\u200b x w(x)=b_{L}+(b_{R}-b_{L})\\\\,x so that v 0 \\u200b ( x ) = u \\u200b ( x , 0 ) ‚àí w \\u200b ( x ) v_{0}(x)=u(x,0)-w(x) satisfies homogeneous Dirichlet conditions. We sample v 0 v_{0} at the interior points x m = m M + 1 x_{m}=\\\\frac{m}{M+1} for m = 1 , ‚Ä¶ , M m=1,\\\\dots,M and obtain the coefficients A m A_{m} via orthonormal projection onto sin \\u2061 ( m \\u200b œÄ \\u200b x ) \\\\sin(m\\\\pi x) . The solution is\\nFor the wave equation with homogeneous Dirichlet boundaries, we sample u \\u200b ( x , 0 ) u(x,0) and u t \\u200b ( x , 0 ) u_{t}(x,0) at the same interior points x m x_{m} and compute A m A_{m} and B m B_{m} by orthonormal projection onto sin \\u2061 ( m \\u200b œÄ \\u200b x ) \\\\sin(m\\\\pi x) . The solution is then given by',\n",
       "        'subsections': []},\n",
       "       {'title': 'Base solutions',\n",
       "        'paragraphs': 'LieSolver requires an educated choice of parametrized base solutions f \\u200b ( x ; œë ) f(x;\\\\bm{\\\\vartheta}) (see section 3.2 ). For the one-dimensional heat equation, we can use the constant solution f seed , 1 \\u200b ( x , t ) ‚â° 1 f_{\\\\text{seed},1}(x,t)\\\\equiv 1 and f seed , 2 \\u200b ( x , t ) = e ‚àí t \\u200b sin \\u2061 x f_{\\\\text{seed},2}(x,t)=e^{-t}\\\\sin x as possible seed solutions. Based on these seed solutions, we employ three complementary base solutions generated by Lie symmetries equation 18 ‚Äì equation 23 of the heat equation:\\nf 1 \\u200b ( x , t ; œë 1 ) = T œë 2 4 ‚àò T œë 1 1 ‚àò ( e ‚àí t \\u200b sin \\u2061 x ) f^{1}(x,t;\\\\bm{\\\\vartheta}^{1})=T^{4}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(e^{-t}\\\\sin x\\\\big) : scaled/shifted Fourier mode (variable phase/frequency).\\nf 2 \\u200b ( x , t ; œë 2 ) = T œë 2 1 ‚àò T œë 1 6 ‚àò ( 1 ) f^{2}(x,t;\\\\bm{\\\\vartheta}^{2})=T^{1}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{6}_{\\\\vartheta_{1}}\\\\circ(1) : diffusion-generated Gaussian blob (variable center and width).\\nf 3 \\u200b ( x , t ; œë 3 ) = T œë 4 1 ‚àò T œë 3 6 ‚àò T œë 2 4 ‚àò T œë 1 1 ‚àò ( e ‚àí t \\u200b sin \\u2061 x ) f^{3}(x,t;\\\\bm{\\\\vartheta}^{3})=T^{1}_{\\\\vartheta_{4}}\\\\!\\\\circ T^{6}_{\\\\vartheta_{3}}\\\\!\\\\circ T^{4}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(e^{-t}\\\\sin x\\\\big) : Gaussian blob modulated by a scaled/shifted sine.\\nFigure 2 illustrates the variety of base solutions chosen to solve the heat equation.\\nFor the one-dimensional wave equation, we identify a subgroup of the Lie symmetry group and consider two transformations: spatial translation T 1 T^{1} and spatiotemporal scaling T 2 T^{2} :\\nWe employ them to compose the following two base solutions for the wave equation:\\nf 1 \\u200b ( x , t ; œë 1 ) = T œë 2 2 ‚àò T œë 1 1 ‚àò ( sin \\u2061 x \\u200b cos \\u2061 t ) f^{1}(x,t;\\\\bm{\\\\vartheta}^{1})=T^{2}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{1}_{\\\\vartheta_{1}}\\\\circ\\\\big(\\\\sin x\\\\cos t\\\\big) : scaled/shifted standing-wave (variable phase/frequency), which is suitable for homogeneous Dirichlet BCs.\\nf 2 \\u200b ( x , t ; œë 2 ) = T œë 2 1 ‚àò T œë 1 2 ‚àò ( e ‚àí ( x ‚àí t ) 2 + e ‚àí ( x + t ) 2 ) f^{2}(x,t;\\\\bm{\\\\vartheta}^{2})=T^{1}_{\\\\vartheta_{2}}\\\\!\\\\circ T^{2}_{\\\\vartheta_{1}}\\\\circ(e^{-(x-t)^{2}}+e^{-(x+t)^{2}}) : Gaussian pattern (variable centre and width). Zero velocity IC enables us to use travelling blob pair instead of just one travelling blob.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.2 PINN Baseline Setup',\n",
       "      'paragraphs': 'We use a standard fully connected neural network f PINN : ‚Ñù 2 ‚Üí ‚Ñù f_{\\\\text{PINN}}:\\\\mathbb{R}^{2}\\\\to\\\\mathbb{R} with input ( x , t ) (x,t) , tanh \\\\tanh activation function and where the number of layers and their width is specified in table 2 in alignment with [ 40 ] . The loss combines the PDE residual, a several weighted IBC terms:\\nHere, ùí≥ r ‚äÇ Œ© \\\\mathcal{X}_{r}\\\\subset\\\\Omega are the collocation points in the domain, while ùí≥ \\\\mathcal{X} and ùí¥ \\\\mathcal{Y} are the same sets of IBC points and targets used for LieSolver. The loss weight w ICBC w_{\\\\mathrm{ICBC}} is tuned to mitigate ill-conditioning and facilitate training (see table 2 ). We warm up with Adam for 1000 iterations and then train with L‚ÄìBFGS until convergence or optimizer stall.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Results',\n",
       "      'paragraphs': 'Table 1 summarizes representative runs of LieSolver for the considered IBVPs, while table 2 reports the corresponding PINN runs. For all ICs except the step IC, LieSolver reliably attains IBC MSE values of order 10 ‚àí 6 10^{-6} using between 5 5 and 40 40 bases. The step IC requires more bases due to its sharp jump. PINNs match LieSolver in terms of attained MSE only on the easiest cases, but falls short on the more challenging ICs. The sine mix IC requires a larger 6 √ó 100 6\\\\times 100 PINN. Notably, the step IC does not reach the target tolerance even with larger PINN models.\\nLieSolver is about one to two orders of magnitude faster than PINN across the considered cases. However, its runtime 5 5 5 All experiments were run on an Intel Core Ultra 7 165H CPU with 11 cores and 16\\u2009GiB RAM. increases sharply as the number of bases grows. Configurations with fewer than 20 20 bases train within 10 10 s, 40 40 bases ‚Äì within 40 40 to 60 60 s, and around 80 80 bases take 200 200 to 300 300 s. This growth is determined by the global parameter refinement stage. More targeted refinement strategies could mitigate this bottleneck.\\nThe choice of base solutions strongly influences performance. Employing sine and Gaussian bases, LieSolver naturally fits IC profiles with similar sine and Gaussian patterns and reaches the target MSE in under a minute. The step IC remains a challenging stress test and highlights model capability. In the following, we discuss three representative cases that illustrate LieSolver behaviour.\\nFigure 3 shows the evolution of the IBC MSE as a function of the number of added bases. The greedy stage begins with a Gaussian base f 2 f^{2} that already captures most of the IC (MSE ‚àº 10 ‚àí 3 \\\\sim 10^{-3} ), followed by several Fourier-like additions ( f 1 f^{1} and f 3 f^{3} ) that enforce the Dirichlet BCs. The first global refinement stage (after 5 bases) yields a one-order-of-magnitude drop in MSE; the target tolerance ( < 10 ‚àí 6 <10^{-6} ) is met after 11 bases. Details on performance metrics and computation are provided in table 1 . Moreover, we confirm the considerations from section 2.2 that there is an upper bound for the ratio between MSE on the domain and the test MSE on the IBCs as indicated in equation 13 . Hence, we show that the loss serves as a reliable metric for the overall domain accuracy.\\nFigure 4 and figure 5 compare the model prediction with the ground truth in the domain and on the IBCs, respectively. The IBC plots include a decomposition into the weighted base terms, revealing dominant Gaussian-like contributors for the IC profile and additional base terms that fit the BCs.\\nFor reference, a vanilla PINN also fits this smooth case to an IBC MSE < 10 ‚àí 6 <10^{-6} ( figure 6 ), albeit requiring a longer runtime, as indicated in table 2 .\\nThe step IC represents the most challenging case due to the jump in the profile. Figure 7 highlights three regimes of LieSolver optimization: (i) rapid improvement to MSE ‚âà 10 ‚àí 4 \\\\approx 10^{-4} within 15 15 bases, (ii) a stall where additional bases bring limited improvement, and (iii) resumed progress after 40 40 bases culminating in < 10 ‚àí 5 <10^{-5} with 58 bases. Figures 8 and 9 show that the dominant residual is localized near the initial jump. Despite IBC MSE being higher than the target, the domain L2RE reaches 4.6 ‚ãÖ 10 ‚àí 3 4.6\\\\cdot 10^{-3} , which is comparable to other IC cases. The decomposition in figure 9 is less interpretable than in the Gaussian case due to the large number modulated base contributions.\\nA standard PINN plateaus around 10 ‚àí 3 10^{-3} IBC MSE ( figure 10 ) and ‚àº 10 ‚àí 1 \\\\sim 10^{-1} L2RE in the domain. The transition from an extremely sharp IC to a rapidly smoothening solution in the domain is hard for PINNs. Even larger networks fail to capture this dependency and underperform relative to the LieSolver .\\nFigure 11 illustrates effective handling of the challenging wave PDE with a sharp step IC by LieSolver . A Global parameter refinement at 60 and 80 base terms yields pronounced drops in MSE. With a sufficient number of active bases, the optimizer can adjust them effectively and achieve lower error. Figure 13 shows that the solution is primarily composed of Fourier sine modes with localized Gaussian corrections. During optimization, the greedy stage first selects sine bases to fit the dominant high-amplitude oscillatory component. As the remaining residual becomes localized and irregular, the algorithm increasingly adds Gaussian bases to correct for edges and boundary regions.\\nWith Gaussian bases, the residual is fitted in two steps. First, the model picks a positive pulse propagating into the domain. Second, it adds opposite-sign contributions to compensate for positive pulses at the boundaries and to reproduce the reflected wave (see the BC plots, middle and right subplots in figure 13 ). This structure is straightforward for the optimizer, and further MSE reduction is attainable with additional bases and parameters refinement. The domain plot is presented in figure 12 . The model achieves an L2RE of 5.1 ‚ãÖ 10 ‚àí 3 5.1\\\\cdot 10^{-3} on the domain. The PINN handles the step IC better for the wave equation ( Figure 14 ) than for the heat equation ( Figure 10 ). The reason is that the wave PDE lacks parabolic smoothing; there is no abrupt transition from a sharp IC to a rapidly smoothed interior solution. Nevertheless, it does not resolve the sharp features sufficiently well and underperforms relative to LieSolver , achieving only an L2RE of 4.5 ‚ãÖ 10 ‚àí 2 4.5\\\\cdot 10^{-2} in the domain.',\n",
       "      'subsections': [{'title': 'Heat equation with Gaussian IC',\n",
       "        'paragraphs': 'Figure 3 shows the evolution of the IBC MSE as a function of the number of added bases. The greedy stage begins with a Gaussian base f 2 f^{2} that already captures most of the IC (MSE ‚àº 10 ‚àí 3 \\\\sim 10^{-3} ), followed by several Fourier-like additions ( f 1 f^{1} and f 3 f^{3} ) that enforce the Dirichlet BCs. The first global refinement stage (after 5 bases) yields a one-order-of-magnitude drop in MSE; the target tolerance ( < 10 ‚àí 6 <10^{-6} ) is met after 11 bases. Details on performance metrics and computation are provided in table 1 . Moreover, we confirm the considerations from section 2.2 that there is an upper bound for the ratio between MSE on the domain and the test MSE on the IBCs as indicated in equation 13 . Hence, we show that the loss serves as a reliable metric for the overall domain accuracy.\\nFigure 4 and figure 5 compare the model prediction with the ground truth in the domain and on the IBCs, respectively. The IBC plots include a decomposition into the weighted base terms, revealing dominant Gaussian-like contributors for the IC profile and additional base terms that fit the BCs.\\nFor reference, a vanilla PINN also fits this smooth case to an IBC MSE < 10 ‚àí 6 <10^{-6} ( figure 6 ), albeit requiring a longer runtime, as indicated in table 2 .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Heat equation with step IC',\n",
       "        'paragraphs': 'The step IC represents the most challenging case due to the jump in the profile. Figure 7 highlights three regimes of LieSolver optimization: (i) rapid improvement to MSE ‚âà 10 ‚àí 4 \\\\approx 10^{-4} within 15 15 bases, (ii) a stall where additional bases bring limited improvement, and (iii) resumed progress after 40 40 bases culminating in < 10 ‚àí 5 <10^{-5} with 58 bases. Figures 8 and 9 show that the dominant residual is localized near the initial jump. Despite IBC MSE being higher than the target, the domain L2RE reaches 4.6 ‚ãÖ 10 ‚àí 3 4.6\\\\cdot 10^{-3} , which is comparable to other IC cases. The decomposition in figure 9 is less interpretable than in the Gaussian case due to the large number modulated base contributions.\\nA standard PINN plateaus around 10 ‚àí 3 10^{-3} IBC MSE ( figure 10 ) and ‚àº 10 ‚àí 1 \\\\sim 10^{-1} L2RE in the domain. The transition from an extremely sharp IC to a rapidly smoothening solution in the domain is hard for PINNs. Even larger networks fail to capture this dependency and underperform relative to the LieSolver .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Wave equation with step IC',\n",
       "        'paragraphs': 'Figure 11 illustrates effective handling of the challenging wave PDE with a sharp step IC by LieSolver . A Global parameter refinement at 60 and 80 base terms yields pronounced drops in MSE. With a sufficient number of active bases, the optimizer can adjust them effectively and achieve lower error. Figure 13 shows that the solution is primarily composed of Fourier sine modes with localized Gaussian corrections. During optimization, the greedy stage first selects sine bases to fit the dominant high-amplitude oscillatory component. As the remaining residual becomes localized and irregular, the algorithm increasingly adds Gaussian bases to correct for edges and boundary regions.\\nWith Gaussian bases, the residual is fitted in two steps. First, the model picks a positive pulse propagating into the domain. Second, it adds opposite-sign contributions to compensate for positive pulses at the boundaries and to reproduce the reflected wave (see the BC plots, middle and right subplots in figure 13 ). This structure is straightforward for the optimizer, and further MSE reduction is attainable with additional bases and parameters refinement. The domain plot is presented in figure 12 . The model achieves an L2RE of 5.1 ‚ãÖ 10 ‚àí 3 5.1\\\\cdot 10^{-3} on the domain. The PINN handles the step IC better for the wave equation ( Figure 14 ) than for the heat equation ( Figure 10 ). The reason is that the wave PDE lacks parabolic smoothing; there is no abrupt transition from a sharp IC to a rapidly smoothed interior solution. Nevertheless, it does not resolve the sharp features sufficiently well and underperforms relative to LieSolver , achieving only an L2RE of 4.5 ‚ãÖ 10 ‚àí 2 4.5\\\\cdot 10^{-2} in the domain.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '5 Conclusion',\n",
       "    'paragraphs': 'Using Lie symmetries, we generate parametrized functions that exactly solve the corresponding PDE. By incorporating the underlying symmetry structures directly into the model, solving initial-boundary value problems reduces to optimizing these parameters to satisfy the initial-boundary conditions (IBCs). Thus, for any parameter values, the model will solve the PDE by design. Focusing on the common case of linear homogeneous PDEs, we introduced LieSolver , which represents the solution as a linear combination of Lie-symmetry-generated base solutions whose linear amplitudes and symmetry parameters are learned from the IBCs. LieSolver constructs this representation incrementally by greedily adding the base function that yields the largest MSE reduction at each step. We use variable projection to eliminate the linear amplitudes via least squares (LS), yielding a reduced problem in the symmetry parameters, which we solve with a bound-constrained nonlinear least squares (NLLS).\\nThe most notable difference between PINNs [ 32 ] and our proposed method is that we incorporate PDEs into the model‚Äôs structure rather than including the PDE as a loss term in the optimization procedure. This approach offers several advantages: in our model, the loss function effectively serves as a faithful indicator of predictive accuracy, providing a reliable measure of the progress of training. Specifically, a decrease in the test loss ‚Äì unlike in PINNs ‚Äì always indicates an improvement in accuracy. In addition, this property implies the existence of rigorous error bounds for well-posed IBVPs.\\nMoreover, these tailor-made models use comparatively few parameters, which yields significant computational speedups without sacrificing expressivity. This also enables robust optimization with nonlinear least squares (NLLS), improving predictive accuracy. Across our benchmarks on 1D heat and wave equations with Dirichlet boundary conditions, LieSolver achieved IBC MSE below 10 ‚àí 6 10^{-6} with fewer than 40 40 bases (about 100 100 parameters) for smooth ICs and ran one to two orders of magnitude faster than PINNs. For the step IC with a sharp jump it required more bases yet still attained domain L2RE on the order of 10 ‚àí 3 10^{-3} , whereas PINNs plateaued between 10 ‚àí 2 10^{-2} and 10 ‚àí 1 10^{-1} ( tables 1 and 2 ). Furthermore, the compact models provably satisfy the underlying physics (that is, the PDE), which enables the prediction to be expressed as a sparse, analytical expression, thereby enhancing interpretability.\\nDespite the notable advantages of using Lie symmetries, it is important to acknowledge the limitations of our approach. Currently, LieSolver applies only to linear homogeneous PDEs. Although this encompasses a large class of important problems, we aim to generalize our framework in future work. Another drawback is that, for every PDE, the Lie symmetries and at least one seed solution must be derived. However, Lie symmetries can be determined algorithmically and for the most standard problems, they can also be found in the literature. Finding seed solutions is trivial for the class of linear homogeneous PDEs and remains feasible in more general cases.\\nA further weakness is that the performance of LieSolver heavily depends on the chosen set of base solutions, an aspect that requires further investigation to improve automatization.\\nHowever, as is the case in conventional neural networks, performance highly depends on various design choices. Nonetheless, in our approach, the loss constitutes a rigorous means to assess whether a particular choice of base solution is effective or needs to be extended, providing useful criteria for these design choices.\\nIt is also important to note that we have tested LieSolver only on a limited number of problems. Therefore, more extensive studies will be necessary in future work.\\nFrom the aforementioned, several next steps emerge naturally: a more detailed analysis of LieSolver on various problems, an automated selection of base solutions, as well as an extension to more general classes of PDEs. For the latter task, genetic algorithms appear to be a promising direction. By pursuing these steps, we aim to enhance the capabilities and applicability of LieSolver and provide an interpretable, reliable and efficient solver of IBVPs for general PDEs.\\nThe Python implementation is available at https://github.com/oduwancheekee/liesolver .\\nThe authors would like to thank Aleksander Krasowski, Th√©o Tyburn, Gleb Wassiltschuk and Moritz Weckbecker for helpful discussions.',\n",
       "    'subsections': [{'title': 'Code availability',\n",
       "      'paragraphs': 'The Python implementation is available at https://github.com/oduwancheekee/liesolver .\\nThe authors would like to thank Aleksander Krasowski, Th√©o Tyburn, Gleb Wassiltschuk and Moritz Weckbecker for helpful discussions.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix A Appendix',\n",
       "    'paragraphs': 'In addition, to the short summary in section 2 , we will provide a brief overview of how to compute the symmetry group of a system. To keep the description as short as possible, we will omit many details and focus on a practical guide for calculating the symmetry group. For further details, we recommend [ 31 , ch. 2] , which the following is oriented towards. Before stating the so-called infinitesimal criterion, which is the key concept for the determination of symmetry groups, we have to introduce some further definitions.\\nEvery Lie group G G is related to a so-called Lie algebra ùî§ \\\\mathfrak{g} , which is defined to be the tangent space of G G at its identity element (see e.g. [ 10 ] for further details). Due to their vector space structure, it is often much more convenient to work with the Lie algebra ùî§ \\\\mathfrak{g} , instead of the Lie group G G . By Lie‚Äôs third theorem [ 31 , thm. 1.54] there is a one to one correspondence between finite-dimensional Lie algebras ùî§ \\\\mathfrak{g} and connected Lie groups G G . Thus, we will lose no generality, when considering Lie algebras. Elements of the Lie algebras are also called infinitesimal generators of a Lie group.\\nA system of PDEs ùíü \\u200b ( x , u ( d ) ) \\\\mathcal{D}(x,u^{(d)}) is called to be of maximal rank , if its Jacobian J ùíü \\u200b ( x , u ( d ) ) J_{\\\\mathcal{D}}(x,u^{(d)}) with respect to all variables has the maximal rank p p , whenever ùíü \\u200b ( x , u ( d ) ) = 0 \\\\mathcal{D}(x,u^{(d)})=0 . E.g., ùíü \\u200b ( x , u ( 2 ) ) = u t ‚àí u x \\u200b x \\\\mathcal{D}(x,u^{(2)})=u_{t}-u_{xx} has Jacobian J ùíü \\u200b ( x , t ; u , u x , u t , u x \\u200b x , u x \\u200b t , u t \\u200b t ) = ( 0 , 0 ; 0 , 0 , 1 , ‚àí 1 , 0 , 0 ) J_{\\\\mathcal{D}}(x,t;u,u_{x},u_{t},u_{xx},u_{xt},u_{tt})=(0,0;0,0,1,-1,0,0) , which is of maximal rank, whereas ùíü \\u200b ( x , u ( 2 ) ) = ( u t ‚àí u x \\u200b x ) 2 \\\\mathcal{D}(x,u^{(2)})=(u_{t}-u_{xx})^{2} is not of maximal rank.\\nLet ùíü \\\\mathcal{D} be a system of PDEs with maximal rank defined over M ‚äÜ X √ó U M\\\\subseteq X\\\\times U and let G G be a local group of transformations acting on M M . If\\nholds for any infinitesimal generator ùêØ \\\\mathbf{v} of G G , then G G is a symmetry group of ùíü \\\\mathcal{D} .\\nIn fact, this theorem gives us a computable criterion to determine the infinitesimal generators ùêØ \\\\mathbf{v} of the symmetry group. To evaluate equation 41 , we also need to prolong a vector fields, which can be computed for a general vector field\\nby [ 31 , thm. 2.36 and prop. 2.35]\\nwith\\nand the total derivatives\\nLet us illustrate theorem A.1 and these relatively lengthy formulas through a practical example.\\nConsider the heat equation\\nwith n = 2 n=2 , m = 1 m=1 , p = 1 p=1 and d = 2 d=2 . Applying equation 43 and equation 41 to a generic vector field\\nwe will obtain the infinitesimal criterion for symmetry groups as\\nTo resolve the functions œï t \\\\phi^{t} and œï x \\u200b x \\\\phi^{xx} , we get by application of equations 44 and 45\\nand\\nSince œï t = œï x \\u200b x \\\\phi^{t}=\\\\phi^{xx} has to hold independently of the value of derivatives of u u , we can compare them term-wise. When using in addition that u t = u x \\u200b x u_{t}=u_{xx} , we get the following system of equations, which are often called the determining equations :\\nœï t = œï x \\u200b x \\\\phi_{t}=\\\\phi_{xx}\\n‚àí Œæ t = 2 \\u200b œï x \\u200b u ‚àí Œæ x \\u200b x -\\\\xi_{t}=2\\\\phi_{xu}-\\\\xi_{xx}\\n0 = œï u \\u200b u ‚àí 2 \\u200b Œæ x \\u200b u 0=\\\\phi_{uu}-2\\\\xi_{xu}\\n0 = ‚àí Œæ u \\u200b u 0=-\\\\xi_{uu}\\nœï u ‚àí œÑ t = ‚àí œÑ x \\u200b x + œï u ‚àí 2 \\u200b Œæ x \\\\phi_{u}-\\\\tau_{t}=-\\\\tau_{xx}+\\\\phi_{u}-2\\\\xi_{x}\\n‚àí Œæ u = ‚àí 2 \\u200b œÑ x \\u200b u ‚àí 3 \\u200b Œæ u -\\\\xi_{u}=-2\\\\tau_{xu}-3\\\\xi_{u}\\n0 = ‚àí œÑ u \\u200b u 0=-\\\\tau_{uu}\\n‚àí œÑ u = ‚àí œÑ u -\\\\tau_{u}=-\\\\tau_{u}\\n0 = ‚àí 2 \\u200b œÑ x 0=-2\\\\tau_{x}\\n0 = ‚àí 2 \\u200b œÑ u 0=-2\\\\tau_{u}\\nIn order to find the most general infinitesimal generator ùêØ \\\\mathbf{v} from equation 42 , we have to solve this system, to determine the functions Œæ \\u200b ( x , t , u ) , œÑ \\u200b ( x , t , u ) \\\\xi(x,t,u),\\\\tau(x,t,u) and œï \\u200b ( x , t , u ) \\\\phi(x,t,u) . From (ix) and (x), we get that œÑ = œÑ \\u200b ( t ) \\\\tau=\\\\tau(t) depends only on t t . In combination with (vi), Œæ \\\\xi will not depend on u u . Hence, (v) takes the form œÑ t = 2 \\u200b Œæ x \\\\tau_{t}=2\\\\xi_{x} , which implies Œæ \\u200b ( x , t ) = 1 2 \\u200b œÑ t \\u200b x + Œ≥ \\u200b ( t ) \\\\xi(x,t)=\\\\frac{1}{2}\\\\tau_{t}x+\\\\gamma(t) for some function Œ≥ \\\\gamma . Therefore, using (iii), œï \\\\phi will only linearly depend on u u , i.e., œï \\u200b ( x , t , u ) = Œ± \\u200b ( x , t ) + Œ≤ \\u200b ( x , t ) \\u200b u \\\\phi(x,t,u)=\\\\alpha(x,t)+\\\\beta(x,t)u for some functions Œ± , Œ≤ \\\\alpha,\\\\beta . Hence, we have used every equation except (i) and (ii), which takes the form\\nŒ± t = Œ± x \\u200b x \\\\alpha_{t}=\\\\alpha_{xx} , Œ≤ t = Œ≤ x \\u200b x \\\\beta_{t}=\\\\beta_{xx}\\n‚àí 1 2 \\u200b œÑ t \\u200b t \\u200b x ‚àí Œ≥ t = 2 \\u200b Œ≤ x -\\\\frac{1}{2}\\\\tau_{tt}x-\\\\gamma_{t}=2\\\\beta_{x}\\nBy integrating (ii) w.r.t. x x , we get Œ≤ \\u200b ( x , t ) = ‚àí 1 8 \\u200b œÑ t \\u200b t \\u200b x 2 ‚àí 1 2 \\u200b Œ≥ t \\u200b x + œÅ \\u200b ( t ) \\\\beta(x,t)=-\\\\frac{1}{8}\\\\tau_{tt}x^{2}-\\\\frac{1}{2}\\\\gamma_{t}x+\\\\rho(t) . Comparing with (i), we get œÑ t \\u200b t \\u200b t = 0 \\\\tau_{ttt}=0 , Œ≥ t \\u200b t = 0 \\\\gamma_{tt}=0 and œÅ t = ‚àí 1 4 \\u200b œÑ t \\u200b t \\\\rho_{t}=-\\\\frac{1}{4}\\\\tau_{tt} . Hence, œÑ \\u200b ( t ) = 4 \\u200b c 6 \\u200b t 2 + 2 \\u200b c 4 + c 2 \\\\tau(t)=4c_{6}t^{2}+2c_{4}+c_{2} is a quadratic function, Œ≥ \\u200b ( t ) = 2 \\u200b c 5 \\u200b t + c 1 \\\\gamma(t)=2c_{5}t+c_{1} and œÅ \\u200b ( t ) = ‚àí 2 \\u200b c 6 + c 3 \\\\rho(t)=-2c_{6}+c_{3} linear functions, with c 1 , ‚Ä¶ , c 6 ‚àà ‚Ñù c_{1},\\\\ldots,c_{6}\\\\in\\\\mathbb{R} . Thus, the general solution of the determining equations is given by\\nwhere c 1 , ‚Ä¶ , c 6 ‚àà ‚Ñù c_{1},\\\\ldots,c_{6}\\\\in\\\\mathbb{R} and Œ± \\u200b ( x , t ) \\\\alpha(x,t) is any solution of the heat equation Œ± t = Œ± x \\u200b x \\\\alpha_{t}=\\\\alpha_{xx} . Using the usual basis in ‚Ñù 6 \\\\mathbb{R}^{6} , the Lie algebra is spanned by 6 6 vector fields\\nand the subalgebra ùêØ Œ± = Œ± \\u200b ( x , t ) \\u200b ‚àÇ u \\\\mathbf{v}_{\\\\alpha}=\\\\alpha(x,t)\\\\partial_{u} .\\nHence, by theorem A.1 we can determine the infinitesimal generators of the symmetry group. In a next step, we have to relate these generators in the Lie algebra to the group elements. This connection is provided by the exponential map exp : ùî§ ‚Üí G \\\\exp:\\\\mathfrak{g}\\\\to G , whose action on an element ( x , u ) ‚àà M ‚äÜ X √ó ùí∞ (x,u)\\\\in M\\\\subseteq X\\\\times\\\\mathcal{U} is described by\\nLet us illustrate the application of the exponential map by continuing the previous example.\\nTo compute the one-parameter group G 1 G_{1} associated to the generator ùêØ 1 \\\\mathbf{v}_{1} from equation 54 we have to apply ùêØ 1 \\\\mathbf{v}_{1} to a graph of a function, i.e., to an element ( x , t , u ) ‚àà M (x,t,u)\\\\in M . We get ùêØ 1 ‚àò ( x , t , u ) = ‚àÇ x ‚àò ( x , t , u ) = ( 1 , 0 , 0 ) \\\\mathbf{v}_{1}\\\\circ(x,t,u)=\\\\partial_{x}\\\\circ(x,t,u)=(1,0,0) and ùêØ k ‚àò ( x , t , u ) = ( 0 , 0 , 0 ) \\\\mathbf{v}^{k}\\\\circ(x,t,u)=(0,0,0) for any k > 1 k>1 . Thus, the group G 1 G_{1} is acting by\\nwhich is nothing else than a translation in the x x coordinate. Analogue, we get a translation in time\\nTo compute the group G 3 G_{3} note, that ùêØ 3 k ‚àò u = u \\\\mathbf{v}_{3}^{k}\\\\circ u=u for all k ‚àà ‚Ñï k\\\\in\\\\mathbb{N} . Hence, the generator ùêØ 3 \\\\mathbf{v}_{3} leads to a scaling invariance\\nQuite similar is the computation for ùêØ 4 \\\\mathbf{v}_{4} , and with ùêØ 4 k ‚àò x = x \\\\mathbf{v}_{4}^{k}\\\\circ x=x and ùêØ 4 k ‚àò t = 2 k \\u200b t \\\\mathbf{v}_{4}^{k}\\\\circ t=2^{k}t we obtain\\nTo compute the group action of G 5 G_{5} , note that\\nwhich can be shown by induction including a distinction between even and odd k k . Including that relation in the series of equation 55 we obtain\\nTo find the action of the group G 6 G_{6} , we will use another useful property of the exponential map, which is\\nfor any functions A , B A,B with appropriate domains and codomains and where A ‚àò A ‚àí 1 = ùüè A\\\\circ A^{-1}=\\\\mathbf{1} . This relation can directly be shown by the definition equation 55 . In particular, the so-called inversion map\\nhas the property I ‚àò I = ùüè I\\\\circ I=\\\\mathbf{1} . As we can write exp \\u2061 ( œë \\u200b ùêØ 6 ) = I ‚àò exp \\u2061 ( ‚àí œë \\u200b ‚àÇ x ) ‚àò I \\\\exp(\\\\vartheta\\\\mathbf{v}_{6})=I\\\\circ\\\\exp(-\\\\vartheta\\\\partial_{x})\\\\circ I we will find\\nThe last generator ùêØ Œ± \\\\mathbf{v}_{\\\\alpha} will result in the action\\nIn summary, we have now determined the group actions on the graph of a function Œì f ‚äÇ X √ó ùí∞ \\\\Gamma_{f}\\\\subset X\\\\times\\\\mathcal{U} , i.e., the action on every point ( x , u ) (x,u) of that graph. In a final step, we would like to transfer this action onto the function itself. For simplicity, we will focus on the case where the action of the group element g ‚àà G g\\\\in G takes the form ( x ~ , u ~ ) = g ‚àò ( x , u ) = ( Œû g \\u200b ( x ) , Œ¶ g \\u200b ( x , u ) ) (\\\\tilde{x},\\\\tilde{u})=g\\\\circ(x,u)=(\\\\Xi_{g}(x),\\\\Phi_{g}(x,u)) , i.e., that the action of g g on x x will not depend on u u . For the general case, we refer to [ 31 , sec. 2.2] . In this particular case, we have x = Œû g \\u200b ( x ~ ) ‚àí 1 = Œû g ‚àí 1 \\u200b ( x ~ ) x=\\\\Xi_{g}(\\\\tilde{x})^{-1}=\\\\Xi_{g^{-1}}(\\\\tilde{x}) , and therefore\\nBy using equation 66 , we can relate the group actions from example A.3 to transformations of functions. For example, for G 1 G_{1} we have Œû g 1 , œë \\u200b ( x ) = x + œë \\\\Xi_{g_{1,\\\\vartheta}}(x)=x+\\\\vartheta and Œ¶ g 1 , œë \\u200b ( x , u ) = u \\\\Phi_{g_{1,\\\\vartheta}}(x,u)=u . With Œû g 1 , œë ‚àí 1 \\u200b ( x ) = Œû g 1 , ‚àí œë \\u200b ( x ) = x ‚àí œë \\\\Xi_{g_{1,\\\\vartheta}^{-1}}(x)=\\\\Xi_{g_{1,-\\\\vartheta}}(x)=x-\\\\vartheta , we get the transformation\\nas expected. By applying equation 66 to the other groups G 2 , ‚Ä¶ , G 6 , G Œ± G_{2},\\\\ldots,G_{6},G_{\\\\alpha} , we obtain the results presented in example 2.2 .\\nThus, we have a framework that, in principle, allows us to compute a symmetry group for any PDE system. For PDE systems with a certain additional property, we can give even a stronger statement. According to [ 31 , Def. 2.70] , a system of PDEs ùíü \\\\mathcal{D} is called local solvable at a point ( x 0 , u 0 ( d ) ) ‚àà X √ó ùí∞ ( d ) (x_{0},u^{(d)}_{0})\\\\in X\\\\times\\\\mathcal{U}^{(d)} , if there is a smooth solution u = f \\u200b ( x ) u=f(x) for x x in a neighbourhood of x 0 x_{0} , with u 0 ( d ) = pr ( d ) \\u2061 f \\u200b ( x 0 ) u_{0}^{(d)}=\\\\operatorname{pr}^{(d)}f(x_{0}) . The whole PDE system ùíü \\\\mathcal{D} is called locally solvable, if this property holds for any point ( x 0 , u 0 ( d ) ) (x_{0},u^{(d)}_{0}) satisfying ùíü \\u200b ( x 0 , u 0 ( d ) ) = 0 \\\\mathcal{D}(x_{0},u^{(d)}_{0})=0 . For a locally solvable system, we can give a sufficient and necessary description of symmetry groups:\\nLet ùíü \\\\mathcal{D} be a locally solvable system of PDEs with maximal rank defined over M ‚äÜ X √ó U M\\\\subseteq X\\\\times U and let G G be a local group of transformations acting on M M . G G is a symmetry group of ùíü \\\\mathcal{D} , if and only if\\nholds for any infinitesimal generator ùêØ \\\\mathbf{v} of G G .\\nThus, for locally solvable systems, we can derive all symmetry groups of the PDE system with the aforementioned framework. We would like to conclude this section by recapping all the steps to compute the symmetry group:\\nCheck maximal rank and local solvability of ùíü \\\\mathcal{D} .\\nWrite an ansatz for a vector field ùêØ \\\\mathbf{v} on X √ó ùí∞ X\\\\times\\\\mathcal{U} according to equation 42 and compute its prolongation equations 43 , 44 and 45 .\\nDerive the infinitesimal criterion theorems A.1 and A.5 , i.e., the prolongation of ùêØ \\\\mathbf{v} applied on the PDE should vanish whenever the PDE is satisfied.\\nBy comparing the coefficients of the infinitesimal criterion, one obtains an overdetermined system of simple PDEs (‚Äúdetermining equations‚Äù).\\nSolve the determining equations by elementary methods, to get the most general generators ùêØ \\\\mathbf{v} and choose a basis of the subspace generated by ùêØ \\\\mathbf{v} .\\nDerive 1-parameter Lie groups from infinitesimal generators by the exponential map g œë ‚àò ( x , u ) = exp \\u2061 ( œë \\u200b ùêØ ) ‚àò ( x , u ) g_{\\\\vartheta}\\\\circ(x,u)=\\\\exp(\\\\vartheta\\\\mathbf{v})\\\\circ(x,u) .\\nDerive the transformation maps from these group actions by equation 66 .\\nThere are several implementations available, which will solve this task step by step. We refer to [ 36 , 14 , 6 , 7 ] , which giving an overview about different available implementations.',\n",
       "    'subsections': [{'title': 'A.1 Computation of Lie symmetry groups',\n",
       "      'paragraphs': 'In addition, to the short summary in section 2 , we will provide a brief overview of how to compute the symmetry group of a system. To keep the description as short as possible, we will omit many details and focus on a practical guide for calculating the symmetry group. For further details, we recommend [ 31 , ch. 2] , which the following is oriented towards. Before stating the so-called infinitesimal criterion, which is the key concept for the determination of symmetry groups, we have to introduce some further definitions.\\nEvery Lie group G G is related to a so-called Lie algebra ùî§ \\\\mathfrak{g} , which is defined to be the tangent space of G G at its identity element (see e.g. [ 10 ] for further details). Due to their vector space structure, it is often much more convenient to work with the Lie algebra ùî§ \\\\mathfrak{g} , instead of the Lie group G G . By Lie‚Äôs third theorem [ 31 , thm. 1.54] there is a one to one correspondence between finite-dimensional Lie algebras ùî§ \\\\mathfrak{g} and connected Lie groups G G . Thus, we will lose no generality, when considering Lie algebras. Elements of the Lie algebras are also called infinitesimal generators of a Lie group.\\nA system of PDEs ùíü \\u200b ( x , u ( d ) ) \\\\mathcal{D}(x,u^{(d)}) is called to be of maximal rank , if its Jacobian J ùíü \\u200b ( x , u ( d ) ) J_{\\\\mathcal{D}}(x,u^{(d)}) with respect to all variables has the maximal rank p p , whenever ùíü \\u200b ( x , u ( d ) ) = 0 \\\\mathcal{D}(x,u^{(d)})=0 . E.g., ùíü \\u200b ( x , u ( 2 ) ) = u t ‚àí u x \\u200b x \\\\mathcal{D}(x,u^{(2)})=u_{t}-u_{xx} has Jacobian J ùíü \\u200b ( x , t ; u , u x , u t , u x \\u200b x , u x \\u200b t , u t \\u200b t ) = ( 0 , 0 ; 0 , 0 , 1 , ‚àí 1 , 0 , 0 ) J_{\\\\mathcal{D}}(x,t;u,u_{x},u_{t},u_{xx},u_{xt},u_{tt})=(0,0;0,0,1,-1,0,0) , which is of maximal rank, whereas ùíü \\u200b ( x , u ( 2 ) ) = ( u t ‚àí u x \\u200b x ) 2 \\\\mathcal{D}(x,u^{(2)})=(u_{t}-u_{xx})^{2} is not of maximal rank.\\nLet ùíü \\\\mathcal{D} be a system of PDEs with maximal rank defined over M ‚äÜ X √ó U M\\\\subseteq X\\\\times U and let G G be a local group of transformations acting on M M . If\\nholds for any infinitesimal generator ùêØ \\\\mathbf{v} of G G , then G G is a symmetry group of ùíü \\\\mathcal{D} .\\nIn fact, this theorem gives us a computable criterion to determine the infinitesimal generators ùêØ \\\\mathbf{v} of the symmetry group. To evaluate equation 41 , we also need to prolong a vector fields, which can be computed for a general vector field\\nby [ 31 , thm. 2.36 and prop. 2.35]\\nwith\\nand the total derivatives\\nLet us illustrate theorem A.1 and these relatively lengthy formulas through a practical example.\\nConsider the heat equation\\nwith n = 2 n=2 , m = 1 m=1 , p = 1 p=1 and d = 2 d=2 . Applying equation 43 and equation 41 to a generic vector field\\nwe will obtain the infinitesimal criterion for symmetry groups as\\nTo resolve the functions œï t \\\\phi^{t} and œï x \\u200b x \\\\phi^{xx} , we get by application of equations 44 and 45\\nand\\nSince œï t = œï x \\u200b x \\\\phi^{t}=\\\\phi^{xx} has to hold independently of the value of derivatives of u u , we can compare them term-wise. When using in addition that u t = u x \\u200b x u_{t}=u_{xx} , we get the following system of equations, which are often called the determining equations :\\nœï t = œï x \\u200b x \\\\phi_{t}=\\\\phi_{xx}\\n‚àí Œæ t = 2 \\u200b œï x \\u200b u ‚àí Œæ x \\u200b x -\\\\xi_{t}=2\\\\phi_{xu}-\\\\xi_{xx}\\n0 = œï u \\u200b u ‚àí 2 \\u200b Œæ x \\u200b u 0=\\\\phi_{uu}-2\\\\xi_{xu}\\n0 = ‚àí Œæ u \\u200b u 0=-\\\\xi_{uu}\\nœï u ‚àí œÑ t = ‚àí œÑ x \\u200b x + œï u ‚àí 2 \\u200b Œæ x \\\\phi_{u}-\\\\tau_{t}=-\\\\tau_{xx}+\\\\phi_{u}-2\\\\xi_{x}\\n‚àí Œæ u = ‚àí 2 \\u200b œÑ x \\u200b u ‚àí 3 \\u200b Œæ u -\\\\xi_{u}=-2\\\\tau_{xu}-3\\\\xi_{u}\\n0 = ‚àí œÑ u \\u200b u 0=-\\\\tau_{uu}\\n‚àí œÑ u = ‚àí œÑ u -\\\\tau_{u}=-\\\\tau_{u}\\n0 = ‚àí 2 \\u200b œÑ x 0=-2\\\\tau_{x}\\n0 = ‚àí 2 \\u200b œÑ u 0=-2\\\\tau_{u}\\nIn order to find the most general infinitesimal generator ùêØ \\\\mathbf{v} from equation 42 , we have to solve this system, to determine the functions Œæ \\u200b ( x , t , u ) , œÑ \\u200b ( x , t , u ) \\\\xi(x,t,u),\\\\tau(x,t,u) and œï \\u200b ( x , t , u ) \\\\phi(x,t,u) . From (ix) and (x), we get that œÑ = œÑ \\u200b ( t ) \\\\tau=\\\\tau(t) depends only on t t . In combination with (vi), Œæ \\\\xi will not depend on u u . Hence, (v) takes the form œÑ t = 2 \\u200b Œæ x \\\\tau_{t}=2\\\\xi_{x} , which implies Œæ \\u200b ( x , t ) = 1 2 \\u200b œÑ t \\u200b x + Œ≥ \\u200b ( t ) \\\\xi(x,t)=\\\\frac{1}{2}\\\\tau_{t}x+\\\\gamma(t) for some function Œ≥ \\\\gamma . Therefore, using (iii), œï \\\\phi will only linearly depend on u u , i.e., œï \\u200b ( x , t , u ) = Œ± \\u200b ( x , t ) + Œ≤ \\u200b ( x , t ) \\u200b u \\\\phi(x,t,u)=\\\\alpha(x,t)+\\\\beta(x,t)u for some functions Œ± , Œ≤ \\\\alpha,\\\\beta . Hence, we have used every equation except (i) and (ii), which takes the form\\nŒ± t = Œ± x \\u200b x \\\\alpha_{t}=\\\\alpha_{xx} , Œ≤ t = Œ≤ x \\u200b x \\\\beta_{t}=\\\\beta_{xx}\\n‚àí 1 2 \\u200b œÑ t \\u200b t \\u200b x ‚àí Œ≥ t = 2 \\u200b Œ≤ x -\\\\frac{1}{2}\\\\tau_{tt}x-\\\\gamma_{t}=2\\\\beta_{x}\\nBy integrating (ii) w.r.t. x x , we get Œ≤ \\u200b ( x , t ) = ‚àí 1 8 \\u200b œÑ t \\u200b t \\u200b x 2 ‚àí 1 2 \\u200b Œ≥ t \\u200b x + œÅ \\u200b ( t ) \\\\beta(x,t)=-\\\\frac{1}{8}\\\\tau_{tt}x^{2}-\\\\frac{1}{2}\\\\gamma_{t}x+\\\\rho(t) . Comparing with (i), we get œÑ t \\u200b t \\u200b t = 0 \\\\tau_{ttt}=0 , Œ≥ t \\u200b t = 0 \\\\gamma_{tt}=0 and œÅ t = ‚àí 1 4 \\u200b œÑ t \\u200b t \\\\rho_{t}=-\\\\frac{1}{4}\\\\tau_{tt} . Hence, œÑ \\u200b ( t ) = 4 \\u200b c 6 \\u200b t 2 + 2 \\u200b c 4 + c 2 \\\\tau(t)=4c_{6}t^{2}+2c_{4}+c_{2} is a quadratic function, Œ≥ \\u200b ( t ) = 2 \\u200b c 5 \\u200b t + c 1 \\\\gamma(t)=2c_{5}t+c_{1} and œÅ \\u200b ( t ) = ‚àí 2 \\u200b c 6 + c 3 \\\\rho(t)=-2c_{6}+c_{3} linear functions, with c 1 , ‚Ä¶ , c 6 ‚àà ‚Ñù c_{1},\\\\ldots,c_{6}\\\\in\\\\mathbb{R} . Thus, the general solution of the determining equations is given by\\nwhere c 1 , ‚Ä¶ , c 6 ‚àà ‚Ñù c_{1},\\\\ldots,c_{6}\\\\in\\\\mathbb{R} and Œ± \\u200b ( x , t ) \\\\alpha(x,t) is any solution of the heat equation Œ± t = Œ± x \\u200b x \\\\alpha_{t}=\\\\alpha_{xx} . Using the usual basis in ‚Ñù 6 \\\\mathbb{R}^{6} , the Lie algebra is spanned by 6 6 vector fields\\nand the subalgebra ùêØ Œ± = Œ± \\u200b ( x , t ) \\u200b ‚àÇ u \\\\mathbf{v}_{\\\\alpha}=\\\\alpha(x,t)\\\\partial_{u} .\\nHence, by theorem A.1 we can determine the infinitesimal generators of the symmetry group. In a next step, we have to relate these generators in the Lie algebra to the group elements. This connection is provided by the exponential map exp : ùî§ ‚Üí G \\\\exp:\\\\mathfrak{g}\\\\to G , whose action on an element ( x , u ) ‚àà M ‚äÜ X √ó ùí∞ (x,u)\\\\in M\\\\subseteq X\\\\times\\\\mathcal{U} is described by\\nLet us illustrate the application of the exponential map by continuing the previous example.\\nTo compute the one-parameter group G 1 G_{1} associated to the generator ùêØ 1 \\\\mathbf{v}_{1} from equation 54 we have to apply ùêØ 1 \\\\mathbf{v}_{1} to a graph of a function, i.e., to an element ( x , t , u ) ‚àà M (x,t,u)\\\\in M . We get ùêØ 1 ‚àò ( x , t , u ) = ‚àÇ x ‚àò ( x , t , u ) = ( 1 , 0 , 0 ) \\\\mathbf{v}_{1}\\\\circ(x,t,u)=\\\\partial_{x}\\\\circ(x,t,u)=(1,0,0) and ùêØ k ‚àò ( x , t , u ) = ( 0 , 0 , 0 ) \\\\mathbf{v}^{k}\\\\circ(x,t,u)=(0,0,0) for any k > 1 k>1 . Thus, the group G 1 G_{1} is acting by\\nwhich is nothing else than a translation in the x x coordinate. Analogue, we get a translation in time\\nTo compute the group G 3 G_{3} note, that ùêØ 3 k ‚àò u = u \\\\mathbf{v}_{3}^{k}\\\\circ u=u for all k ‚àà ‚Ñï k\\\\in\\\\mathbb{N} . Hence, the generator ùêØ 3 \\\\mathbf{v}_{3} leads to a scaling invariance\\nQuite similar is the computation for ùêØ 4 \\\\mathbf{v}_{4} , and with ùêØ 4 k ‚àò x = x \\\\mathbf{v}_{4}^{k}\\\\circ x=x and ùêØ 4 k ‚àò t = 2 k \\u200b t \\\\mathbf{v}_{4}^{k}\\\\circ t=2^{k}t we obtain\\nTo compute the group action of G 5 G_{5} , note that\\nwhich can be shown by induction including a distinction between even and odd k k . Including that relation in the series of equation 55 we obtain\\nTo find the action of the group G 6 G_{6} , we will use another useful property of the exponential map, which is\\nfor any functions A , B A,B with appropriate domains and codomains and where A ‚àò A ‚àí 1 = ùüè A\\\\circ A^{-1}=\\\\mathbf{1} . This relation can directly be shown by the definition equation 55 . In particular, the so-called inversion map\\nhas the property I ‚àò I = ùüè I\\\\circ I=\\\\mathbf{1} . As we can write exp \\u2061 ( œë \\u200b ùêØ 6 ) = I ‚àò exp \\u2061 ( ‚àí œë \\u200b ‚àÇ x ) ‚àò I \\\\exp(\\\\vartheta\\\\mathbf{v}_{6})=I\\\\circ\\\\exp(-\\\\vartheta\\\\partial_{x})\\\\circ I we will find\\nThe last generator ùêØ Œ± \\\\mathbf{v}_{\\\\alpha} will result in the action\\nIn summary, we have now determined the group actions on the graph of a function Œì f ‚äÇ X √ó ùí∞ \\\\Gamma_{f}\\\\subset X\\\\times\\\\mathcal{U} , i.e., the action on every point ( x , u ) (x,u) of that graph. In a final step, we would like to transfer this action onto the function itself. For simplicity, we will focus on the case where the action of the group element g ‚àà G g\\\\in G takes the form ( x ~ , u ~ ) = g ‚àò ( x , u ) = ( Œû g \\u200b ( x ) , Œ¶ g \\u200b ( x , u ) ) (\\\\tilde{x},\\\\tilde{u})=g\\\\circ(x,u)=(\\\\Xi_{g}(x),\\\\Phi_{g}(x,u)) , i.e., that the action of g g on x x will not depend on u u . For the general case, we refer to [ 31 , sec. 2.2] . In this particular case, we have x = Œû g \\u200b ( x ~ ) ‚àí 1 = Œû g ‚àí 1 \\u200b ( x ~ ) x=\\\\Xi_{g}(\\\\tilde{x})^{-1}=\\\\Xi_{g^{-1}}(\\\\tilde{x}) , and therefore\\nBy using equation 66 , we can relate the group actions from example A.3 to transformations of functions. For example, for G 1 G_{1} we have Œû g 1 , œë \\u200b ( x ) = x + œë \\\\Xi_{g_{1,\\\\vartheta}}(x)=x+\\\\vartheta and Œ¶ g 1 , œë \\u200b ( x , u ) = u \\\\Phi_{g_{1,\\\\vartheta}}(x,u)=u . With Œû g 1 , œë ‚àí 1 \\u200b ( x ) = Œû g 1 , ‚àí œë \\u200b ( x ) = x ‚àí œë \\\\Xi_{g_{1,\\\\vartheta}^{-1}}(x)=\\\\Xi_{g_{1,-\\\\vartheta}}(x)=x-\\\\vartheta , we get the transformation\\nas expected. By applying equation 66 to the other groups G 2 , ‚Ä¶ , G 6 , G Œ± G_{2},\\\\ldots,G_{6},G_{\\\\alpha} , we obtain the results presented in example 2.2 .\\nThus, we have a framework that, in principle, allows us to compute a symmetry group for any PDE system. For PDE systems with a certain additional property, we can give even a stronger statement. According to [ 31 , Def. 2.70] , a system of PDEs ùíü \\\\mathcal{D} is called local solvable at a point ( x 0 , u 0 ( d ) ) ‚àà X √ó ùí∞ ( d ) (x_{0},u^{(d)}_{0})\\\\in X\\\\times\\\\mathcal{U}^{(d)} , if there is a smooth solution u = f \\u200b ( x ) u=f(x) for x x in a neighbourhood of x 0 x_{0} , with u 0 ( d ) = pr ( d ) \\u2061 f \\u200b ( x 0 ) u_{0}^{(d)}=\\\\operatorname{pr}^{(d)}f(x_{0}) . The whole PDE system ùíü \\\\mathcal{D} is called locally solvable, if this property holds for any point ( x 0 , u 0 ( d ) ) (x_{0},u^{(d)}_{0}) satisfying ùíü \\u200b ( x 0 , u 0 ( d ) ) = 0 \\\\mathcal{D}(x_{0},u^{(d)}_{0})=0 . For a locally solvable system, we can give a sufficient and necessary description of symmetry groups:\\nLet ùíü \\\\mathcal{D} be a locally solvable system of PDEs with maximal rank defined over M ‚äÜ X √ó U M\\\\subseteq X\\\\times U and let G G be a local group of transformations acting on M M . G G is a symmetry group of ùíü \\\\mathcal{D} , if and only if\\nholds for any infinitesimal generator ùêØ \\\\mathbf{v} of G G .\\nThus, for locally solvable systems, we can derive all symmetry groups of the PDE system with the aforementioned framework. We would like to conclude this section by recapping all the steps to compute the symmetry group:\\nCheck maximal rank and local solvability of ùíü \\\\mathcal{D} .\\nWrite an ansatz for a vector field ùêØ \\\\mathbf{v} on X √ó ùí∞ X\\\\times\\\\mathcal{U} according to equation 42 and compute its prolongation equations 43 , 44 and 45 .\\nDerive the infinitesimal criterion theorems A.1 and A.5 , i.e., the prolongation of ùêØ \\\\mathbf{v} applied on the PDE should vanish whenever the PDE is satisfied.\\nBy comparing the coefficients of the infinitesimal criterion, one obtains an overdetermined system of simple PDEs (‚Äúdetermining equations‚Äù).\\nSolve the determining equations by elementary methods, to get the most general generators ùêØ \\\\mathbf{v} and choose a basis of the subspace generated by ùêØ \\\\mathbf{v} .\\nDerive 1-parameter Lie groups from infinitesimal generators by the exponential map g œë ‚àò ( x , u ) = exp \\u2061 ( œë \\u200b ùêØ ) ‚àò ( x , u ) g_{\\\\vartheta}\\\\circ(x,u)=\\\\exp(\\\\vartheta\\\\mathbf{v})\\\\circ(x,u) .\\nDerive the transformation maps from these group actions by equation 66 .\\nThere are several implementations available, which will solve this task step by step. We refer to [ 36 , 14 , 6 , 7 ] , which giving an overview about different available implementations.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'We introduce a method for efficiently solving initial-boundary value problems (IBVPs) that uses Lie symmetries to enforce the associated partial differential equation (PDE) exactly by construction. By leveraging symmetry transformations, the model inherently incorporates the physical laws and learns solutions from initial and boundary data. As a result, the loss directly measures the model‚Äôs accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our method enables rigorous error estimation. The approach yields compact models, facilitating an efficient optimization. We implement LieSolver and demonstrate its application to linear homogeneous PDEs with a range of initial conditions, showing that it is faster and more accurate than physics-informed neural networks (PINNs). Overall, our method improves both computational efficiency and the reliability of predictions for PDE-constrained problems.'},\n",
       " '2510.25668v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Visually rich documents (VRDs) serve as primary vehicles for storing and communicating structured knowledge in real-world applications. Unlike plain text, these documents combine different modalities, including text, tables, and figures, embedded in human-friendly layouts that encode semantic relationships. Effectively understanding such documents requires not only extracting textual content but also reasoning over their visual and structural organization. This has given rise to the task of visually rich document understanding (VRDU) (Wang et\\xa0al., 2023 ; Ding et\\xa0al., 2022 ) which aims to develop systems to automatically analyze VRDs and answer user queries, underpining various practical applications (Liang et\\xa0al., 2024 ; Rombach & Fettke, 2024 ) .\\nDespite recent progress of vision-language models (VLMs) on single-page or short documents (Xie et\\xa0al., 2024 ; Lv et\\xa0al., 2023 ; Hu et\\xa0al., 2024 ) , real-world long documents spanning dozens or even hundreds of pages remain highly challenging.\\nFeeding entire documents into a model‚Äôs context is computationally expensive and introduces substantial noise, making it difficult for VLMs to focus on relevant pages (Cho et\\xa0al., 2024 ) .\\nA more scalable alternative is to have the VLM reason only over semantically relevant pages retrieved by a multimodal retriever (Faysse et\\xa0al., 2025 ) , following the retrieval-augmented generation (RAG) paradigm (Cho et\\xa0al., 2024 ; Chen et\\xa0al., 2025a ) .\\nRecent work has extended this idea by building prompting-based pipeline in which VLMs passively perform predefined subtasks such as query reformulation, retrieved content summary or answer synthesis within fixed workflows (Han et\\xa0al., 2025 ; Wang et\\xa0al., 2025b ) .\\nWhile effective, these systems rely on static reasoning patterns and rigid workflows, limiting their ability to generalize or adapt strategies to diverse user queries.\\nThis motivates shifting the research focus to the Agentic VRDU (A-VRDU) task, which requires the model to act as an agent that can actively navigate and reason over long documents to deliver accurate and adaptive question answering beyond fixed RAG pipelines.\\nRecent studies (Chen et\\xa0al., 2025b ; Jin et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) show that modeling search as an action and optimizing the workflow with outcome-based RL yields more generalizable agents that can actively gather information from external databases, offering a promising direction for the open problem of A-VRDU.\\nHowever, extending this framework to fine-tune VLMs for A-VRDU poses unique challenges.\\nUser queries often reference specific documents, page numbers, or require reasoning across consecutive pages, where generic semantic retrieval is inefficient.\\nMoreover, document-level information gathering typically demands multi-turn interaction with retrieval models, where sparse and delayed outcome-based rewards fail to reinforce helpful intermediate steps or discourage redundant actions.\\nA further challenge arises from the high-dimensional visual inputs.\\nWe empirically observe that fully masking the visual tokens when computing the policy gradient, as in existing approaches, leads to unstable training dynamics and can even cause collapse.\\nThese limitations motivate our framework, A ctive L ong- D ocum E nt N avigation (ALDEN), a multi-turn RL framework that trains VLMs as interactive agents for navigation in long, visually-rich documents.\\nThe overall reasoning-action rollout of ALDEN is illustrated in fig. 1 .\\nALDEN expands the action space by introducing the fetch action, which enables direct page-index access to complement search-based retrieval and efficiently handle diverse queries.\\nWe incorporate a cross-level reward function as opposed to the sparse outcome-based reward typically used, which integrates rule-based turn-level supervision with a token-level repetition penalty to provide fine-grained process supervision, encouraging informative evidence collection while discouraging repeated query formulations.\\nFinally, ALDEN incorporates a visual semantic anchoring mechanism, which constrains the hidden states of generated and visual tokens separately during training to preserve the grounding of visual-token representations and improve overall training robustness.\\nWe build a training corpus from DUDE (Van\\xa0Landeghem et\\xa0al., 2023 ) , MPDocVQA (Tito et\\xa0al., 2023b ) , and SlideVQA (Tanaka et\\xa0al., 2023b ) to train an A-VRDU agent with ALDEN and evaluate it on five benchmarks. Experimental results show that ALDEN achieves state-of-the-art performance over strong baselines and demonstrates the effectiveness of its key components. Overall, the A-VRDU task establishes a new paradigm for processing practical, lengthy VRDs, shifting from passive document reading to autonomous navigation and reasoning. ALDEN‚Äôs strong results validate this paradigm and provide guidance for building efficient, robust A-VRDU agents from VLMs.\\nOverall, our main contribution can be summarized as follows:\\nWe propose the agentic visually-rich document understanding (A-VRDU) task that aims to develop agents that can actively navigate and reason over long visually-rich documents.\\nTo perform the A-VRDU task, we introduce ALDEN , a multi-turn RL framework with three key components: an expanded action space featuring a novel fetch action, a cross-level reward function, and a visual semantic anchoring mechanism, which together enable efficient and robust training.\\nWe construct a training corpus for training the A-VRDU agent and conduct extensive experiments on five commonly used VRDU benchmarks, showing that ALDEN significantly outperforms the strongest baseline, improving the answer accuracy by 9.14% on average.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Recent VLMs that process document images directly without OCR (Hu et\\xa0al., 2024 ; Xie et\\xa0al., 2024 ; Feng et\\xa0al., 2024 ; Liu et\\xa0al., 2024b ) have shown strong performance on single-page or short-document benchmarks (Mathew et\\xa0al., 2021 ; Masry et\\xa0al., 2022 ; Mathew et\\xa0al., 2022 ) .\\nIn contrast, real-world documents often span dozens or hundreds of pages, requiring reasoning across dispersed text, tables, and figures (Deng et\\xa0al., 2024 ; Ma et\\xa0al., 2024b ) .\\nExtending context length to encode entire documents (Tito et\\xa0al., 2023b ; Blau et\\xa0al., 2024 ) is computationally prohibitive and introduces noise, while semantic retrieval provides a more scalable way to focus on relevant pages (Chen et\\xa0al., 2025b ; Jin et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) . However, existing retrieval-based methods largely rely on prompting-based workflows (Han et\\xa0al., 2025 ; Wang et\\xa0al., 2025b ) , which are static and brittle.\\nIn contrast, we study A-VRDU task, and propose to fine-tune VLMs with RL, enabling them to serve as VRDU agents capable of active, multi-step retrieval and reasoning.\\nRL was introduced to LLM fine-tuning by Ouyang et\\xa0al. ( 2022 ); Ziegler et\\xa0al. ( 2019 ) through reinforcement learning from human feedback (RLHF), where a learned reward model guides the RL-based tuning of the policy LLM typically via the Proximal Policy Optimization (PPO) algorithm (Schulman et\\xa0al., 2017 ) .\\nRecently, RL with verifiable outcome-based rewards (RLVR) (Shao et\\xa0al., 2024 ) further demonstrates impressive effect in inducing sophisticated reasoning ability in LLMs.\\nBuilding on this progress, several recent studies integrate RL with retrieval-augmented generation (RAG), fine-tuning LLMs as agents that actively gather evidence through retrieval and reason over it (Jin et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) .\\nHowever, extending these methods to the A-VRDU task remains largely unexplored.\\nUnlike open-domain retrieval, VRDU requires exploiting explicit document structure (e.g., page indices), denser supervision to guide multi-turn navigation, and stability against the large number of unconstrained visual tokens introduced by high-resolution document pages, motivating new RL frameworks tailored for this task.',\n",
       "    'subsections': [{'title': '2.1 Visually-rich documents understanding',\n",
       "      'paragraphs': 'Recent VLMs that process document images directly without OCR (Hu et\\xa0al., 2024 ; Xie et\\xa0al., 2024 ; Feng et\\xa0al., 2024 ; Liu et\\xa0al., 2024b ) have shown strong performance on single-page or short-document benchmarks (Mathew et\\xa0al., 2021 ; Masry et\\xa0al., 2022 ; Mathew et\\xa0al., 2022 ) .\\nIn contrast, real-world documents often span dozens or hundreds of pages, requiring reasoning across dispersed text, tables, and figures (Deng et\\xa0al., 2024 ; Ma et\\xa0al., 2024b ) .\\nExtending context length to encode entire documents (Tito et\\xa0al., 2023b ; Blau et\\xa0al., 2024 ) is computationally prohibitive and introduces noise, while semantic retrieval provides a more scalable way to focus on relevant pages (Chen et\\xa0al., 2025b ; Jin et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) . However, existing retrieval-based methods largely rely on prompting-based workflows (Han et\\xa0al., 2025 ; Wang et\\xa0al., 2025b ) , which are static and brittle.\\nIn contrast, we study A-VRDU task, and propose to fine-tune VLMs with RL, enabling them to serve as VRDU agents capable of active, multi-step retrieval and reasoning.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 RL Training for LLMs/VLMs',\n",
       "      'paragraphs': 'RL was introduced to LLM fine-tuning by Ouyang et\\xa0al. ( 2022 ); Ziegler et\\xa0al. ( 2019 ) through reinforcement learning from human feedback (RLHF), where a learned reward model guides the RL-based tuning of the policy LLM typically via the Proximal Policy Optimization (PPO) algorithm (Schulman et\\xa0al., 2017 ) .\\nRecently, RL with verifiable outcome-based rewards (RLVR) (Shao et\\xa0al., 2024 ) further demonstrates impressive effect in inducing sophisticated reasoning ability in LLMs.\\nBuilding on this progress, several recent studies integrate RL with retrieval-augmented generation (RAG), fine-tuning LLMs as agents that actively gather evidence through retrieval and reason over it (Jin et\\xa0al., 2025 ; Song et\\xa0al., 2025 ) .\\nHowever, extending these methods to the A-VRDU task remains largely unexplored.\\nUnlike open-domain retrieval, VRDU requires exploiting explicit document structure (e.g., page indices), denser supervision to guide multi-turn navigation, and stability against the large number of unconstrained visual tokens introduced by high-resolution document pages, motivating new RL frameworks tailored for this task.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Preliminaries',\n",
       "    'paragraphs': 'In the A-VRDU task, a user query q u q_{u} is paired with a document ùíü = ( p 1 , p 2 , ‚ãØ , p | ùíü | ) \\\\mathcal{D}=(p_{1},p_{2},\\\\cdots,p_{|\\\\mathcal{D}|}) that can only be accessed through specific ways, where p i p_{i} denotes the i i -th page and | ùíü | |\\\\mathcal{D}| the total number of pages. The goal is to build an agent that can actively analyzes available information, decides whether and how to collect additional pages from the document, and ultimately generates a final answer y ‚Ä≤ y^{\\\\prime} based on the collected evidence. This sequential decision-making process can be naturally formulated as a Markov Decision Process (MDP) (Bellman, 1957 ) .\\nFormally, at each turn t t , the agent generates an action a t a_{t} from the action space ùíú \\\\mathcal{A} .\\nUpon executing the action, the document returns a visual observation o t ‚àà ùí™ o_{t}\\\\in\\\\mathcal{O} (i.e., a page image) and a scalar reward r t ‚àà ‚Ñù r_{t}\\\\in\\\\mathbb{R} , which reflects the action‚Äôs utility in acquiring useful evidence or answering the query.\\nThe state s t s_{t} is defined as the interaction history up to turn t t , given by s t = [ x , a 1 , o 1 , ‚ãØ , a t ‚àí 1 , o t ‚àí 1 ] s_{t}=[x,a_{1},o_{1},\\\\cdots,a_{t-1},o_{t-1}] , where x x denotes the initial prompt constructed from the query and task instructions.\\nThe agent‚Äôs objective is to maximize the expected cumulative reward ‚àë t = 1 T Œ≥ t \\u200b r \\u200b ( s t , a t ) \\\\sum_{t=1}^{T}\\\\gamma^{t}r(s_{t},a_{t}) , where T T is the maximum number of interaction turns per episode, Œ≥ \\\\gamma denotes the discount factor.\\nPPO algorithm is an actor-critic RL algorithm that has been widely used in RLHF to fine-tune language models toward task-specific preferences.\\nIn the classical RLHF setup, the problem is typically modeled as a contextual bandit, where each episode involves a single interaction step.\\nFormally, given an input prompt x x , the LLM auto-regressively generates a variable-length token sequence ( a 1 1 , ‚ãØ , a 1 L ) ‚àà ùí± L (a_{1}^{1},\\\\cdots,a_{1}^{L})\\\\in\\\\mathcal{V}^{L} as a single action a 1 a_{1} where ùí± \\\\mathcal{V} denotes the vocabulary and L L is the sequence length.\\nA scalar reward r 1 r_{1} is assigned to the action by a learned reward model.\\nSince LLMs operate token-by-token, PPO is actually applied at the token level by treating each token a 1 i ‚àà ùí± a_{1}^{i}\\\\in\\\\mathcal{V} as an action, with state s 1 i = ( x , a 1 1 , ‚Ä¶ , a 1 i ‚àí 1 ) s_{1}^{i}=(x,a_{1}^{1},\\\\dots,a_{1}^{i-1}) defined as the prompt concatenated with the partial response. To propagate the turn-level reward r 1 r_{1} to individual tokens, a token-level reward signal is assigned as\\nwhere œÄ ref \\\\pi_{\\\\text{ref}} is the reference model (e.g., a frozen copy of the pre-trained LLM), the KL \\u200b ( ‚ãÖ ) \\\\text{KL}(\\\\cdot) term acts as a penalty to prevent the policy from drifting too far from the reference model, Œ≤ \\\\beta is the hyperparameter to control the weight of the KL divergence penalty.\\nIn addition to the policy œÄ Œ∏ \\\\pi_{\\\\theta} , a value function V œï \\u200b ( s 1 i ) V_{\\\\phi}(s_{1}^{i}) is trained to predict the expected return at each token position.\\nGeneralized Advantage Estimation (GAE) (Schulman et\\xa0al., 2015 ) is generally used to calculate the advantage of each token-level action:\\nwhere Œª ‚àà [ 0 , 1 ] \\\\lambda\\\\in[0,1] is a hyperparameter to balance the estimation bias and variance.\\nThe value function is then optimized by minimizing the mean squared error between predicted values and GAE-estimated target values V ^ 1 i = A 1 i + V œï \\u200b ( s 1 i ) \\\\hat{V}^{i}_{1}=A_{1}^{i}+V_{\\\\phi}(s_{1}^{i}) .\\nThe LLM is finally optimized by maximizing the following surrogate objective:\\nwhere œÄ Œ∏ \\\\pi_{\\\\theta} and œÄ old \\\\pi_{\\\\text{old}} are the current and old policy models, œµ \\\\epsilon is a clipping-related hyper-parameter introduced in PPO for stabilizing training.\\nThe single-turn PPO framework propagates only immediate rewards to tokens, neglecting each action‚Äôs contribution to final task completion and fine-grained token supervision. We next describe how we adapt it for long-horizon, multi-turn interaction in the A-VRDU task.',\n",
       "    'subsections': [{'title': '3.1 Problem Formulation',\n",
       "      'paragraphs': 'In the A-VRDU task, a user query q u q_{u} is paired with a document ùíü = ( p 1 , p 2 , ‚ãØ , p | ùíü | ) \\\\mathcal{D}=(p_{1},p_{2},\\\\cdots,p_{|\\\\mathcal{D}|}) that can only be accessed through specific ways, where p i p_{i} denotes the i i -th page and | ùíü | |\\\\mathcal{D}| the total number of pages. The goal is to build an agent that can actively analyzes available information, decides whether and how to collect additional pages from the document, and ultimately generates a final answer y ‚Ä≤ y^{\\\\prime} based on the collected evidence. This sequential decision-making process can be naturally formulated as a Markov Decision Process (MDP) (Bellman, 1957 ) .\\nFormally, at each turn t t , the agent generates an action a t a_{t} from the action space ùíú \\\\mathcal{A} .\\nUpon executing the action, the document returns a visual observation o t ‚àà ùí™ o_{t}\\\\in\\\\mathcal{O} (i.e., a page image) and a scalar reward r t ‚àà ‚Ñù r_{t}\\\\in\\\\mathbb{R} , which reflects the action‚Äôs utility in acquiring useful evidence or answering the query.\\nThe state s t s_{t} is defined as the interaction history up to turn t t , given by s t = [ x , a 1 , o 1 , ‚ãØ , a t ‚àí 1 , o t ‚àí 1 ] s_{t}=[x,a_{1},o_{1},\\\\cdots,a_{t-1},o_{t-1}] , where x x denotes the initial prompt constructed from the query and task instructions.\\nThe agent‚Äôs objective is to maximize the expected cumulative reward ‚àë t = 1 T Œ≥ t \\u200b r \\u200b ( s t , a t ) \\\\sum_{t=1}^{T}\\\\gamma^{t}r(s_{t},a_{t}) , where T T is the maximum number of interaction turns per episode, Œ≥ \\\\gamma denotes the discount factor.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Proximal Policy Optimization for fine-tuning LLMs',\n",
       "      'paragraphs': 'PPO algorithm is an actor-critic RL algorithm that has been widely used in RLHF to fine-tune language models toward task-specific preferences.\\nIn the classical RLHF setup, the problem is typically modeled as a contextual bandit, where each episode involves a single interaction step.\\nFormally, given an input prompt x x , the LLM auto-regressively generates a variable-length token sequence ( a 1 1 , ‚ãØ , a 1 L ) ‚àà ùí± L (a_{1}^{1},\\\\cdots,a_{1}^{L})\\\\in\\\\mathcal{V}^{L} as a single action a 1 a_{1} where ùí± \\\\mathcal{V} denotes the vocabulary and L L is the sequence length.\\nA scalar reward r 1 r_{1} is assigned to the action by a learned reward model.\\nSince LLMs operate token-by-token, PPO is actually applied at the token level by treating each token a 1 i ‚àà ùí± a_{1}^{i}\\\\in\\\\mathcal{V} as an action, with state s 1 i = ( x , a 1 1 , ‚Ä¶ , a 1 i ‚àí 1 ) s_{1}^{i}=(x,a_{1}^{1},\\\\dots,a_{1}^{i-1}) defined as the prompt concatenated with the partial response. To propagate the turn-level reward r 1 r_{1} to individual tokens, a token-level reward signal is assigned as\\nwhere œÄ ref \\\\pi_{\\\\text{ref}} is the reference model (e.g., a frozen copy of the pre-trained LLM), the KL \\u200b ( ‚ãÖ ) \\\\text{KL}(\\\\cdot) term acts as a penalty to prevent the policy from drifting too far from the reference model, Œ≤ \\\\beta is the hyperparameter to control the weight of the KL divergence penalty.\\nIn addition to the policy œÄ Œ∏ \\\\pi_{\\\\theta} , a value function V œï \\u200b ( s 1 i ) V_{\\\\phi}(s_{1}^{i}) is trained to predict the expected return at each token position.\\nGeneralized Advantage Estimation (GAE) (Schulman et\\xa0al., 2015 ) is generally used to calculate the advantage of each token-level action:\\nwhere Œª ‚àà [ 0 , 1 ] \\\\lambda\\\\in[0,1] is a hyperparameter to balance the estimation bias and variance.\\nThe value function is then optimized by minimizing the mean squared error between predicted values and GAE-estimated target values V ^ 1 i = A 1 i + V œï \\u200b ( s 1 i ) \\\\hat{V}^{i}_{1}=A_{1}^{i}+V_{\\\\phi}(s_{1}^{i}) .\\nThe LLM is finally optimized by maximizing the following surrogate objective:\\nwhere œÄ Œ∏ \\\\pi_{\\\\theta} and œÄ old \\\\pi_{\\\\text{old}} are the current and old policy models, œµ \\\\epsilon is a clipping-related hyper-parameter introduced in PPO for stabilizing training.\\nThe single-turn PPO framework propagates only immediate rewards to tokens, neglecting each action‚Äôs contribution to final task completion and fine-grained token supervision. We next describe how we adapt it for long-horizon, multi-turn interaction in the A-VRDU task.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Methodology',\n",
       "    'paragraphs': 'We propose A ctive L ong- D ocum E nt N avigation (ALDEN), a reinforcement learning framework for training VLMs as interactive agents that can actively navigate and reason over long, visually rich documents by operating in a multi-turn reasoning‚Äìaction loop, incrementally collecting evidence pages until a question can be confidently answered.\\nTo this end, ALDEN introduces three key components. (i) Expanded action space: the agent is equipped with both a semantic search action for retrieving relevant pages and a novel fetch action for direct page access, enabling flexible exploitation of document structure ( section 4.1 ). (ii) Cross-level reward function: supervision is provided jointly at the turn level and the token level, guiding the agent toward effective evidence collection and accurate answer generation ( section 4.2 ). (iii) Visual semantic anchoring: to stabilize RL training, ALDEN constrains the hidden-state evolution of generated and visual tokens respectively, mitigating drift and preserving semantic grounding during optimization ( section 4.3 ).\\nThe overall RL training pipeline of ALDEN is illustrated in fig. 2 and algorithm 1 .\\nIn Agentic VRDU, agents must flexibly access information that may be referenced either semantically or structurally. Relying solely on semantic retrieval is often insufficient: while it works for open-ended queries, it cannot efficiently resolve explicit page references (e.g.,‚Äúsee page 12‚Äù) or reasoning steps that span consecutive pages.\\nTo address this, ALDEN augments the standard search operation with a complementary fetch action, which enables direct page-index access and better exploits the inherent structure of documents.\\nThe action space thus consists of three options, each expressed in a structured format that combines free-form reasoning with explicit actions:\\nSearch ‚Äî <think>...</think><search>...</search> Generates a reasoning trace within the <think> tags followed by a semantic query enclosed within the <search> tags. An external retrieval module returns a ranked list of pages relevant to the current query using semantic similarity. This action is effective for open-ended queries where relevant content is not explicitly referenced.\\nFetch ‚Äî <think>...</think><fetch>...</fetch> Similar to search, but the agent specifies a page number within the <fetch> tag, enabling direct access to that page without semantic matching.\\nThis action is crucial for handling explicit references to page numbers or structured navigation across consecutive pages.\\nAnswer ‚Äî <think>...</think><answer>...</answer> Outputs the reasoning trace followed by the final answer. This action terminates the rollout.\\nOnce the action is parsed, the document returns the corresponding page images enclosed within the <result> tag. For the search action, the associated page numbers are also returned to provide cues of document structure.\\nTraining agentic VRDU systems requires reward signals that are both structured enough to enforce valid behaviors and fine-grained enough to guide efficient exploration. To this end, ALDEN employs a cross-level reward function that integrates supervision at two complementary levels: turn-level rewards for overall action quality and token-level rewards for local shaping.\\nTurn-level Reward. The immediate turn-level reward r t r_{t} is defined as r t = f t + u t r_{t}=f_{t}+u_{t} ,\\nwhere the format reward f t f_{t} enforces the response format and the result reward u t u_{t} evaluates the quality of the action outcome.\\nThe format reward f t f_{t} is given by:\\nThus, only well-formed responses avoid penalty, ensuring consistent structured outputs across turns.\\nThe result reward is defined based on the action type a t ‚àà { search , fetch , answer } a_{t}\\\\in\\\\{\\\\texttt{search},\\\\texttt{fetch},\\\\texttt{answer}\\\\} ,\\nthe set of page indices collected in the current turn ùíû t = { c 1 , ‚Ä¶ , c | ùíû t | } ‚äÜ { 1 , ‚Ä¶ , | ùíü | } \\\\mathcal{C}_{t}=\\\\{c_{1},\\\\ldots,c_{|\\\\mathcal{C}_{t}|}\\\\}\\\\subseteq\\\\{1,\\\\ldots,|\\\\mathcal{D}|\\\\} ,\\nthe set of ground-truth page indices ùí¢ = { g 1 , ‚Ä¶ , g | ùí¢ | } ‚äÜ { 1 , ‚Ä¶ , | ùíü | } \\\\mathcal{G}=\\\\{g_{1},\\\\ldots,g_{|\\\\mathcal{G}|}\\\\}\\\\subseteq\\\\{1,\\\\ldots,|\\\\mathcal{D}|\\\\} ,\\nand the set of previously accessed pages ‚Ñõ = ‚ãÉ k = 1 t ‚àí 1 ùíû k \\\\mathcal{R}=\\\\bigcup_{k=1}^{t-1}\\\\mathcal{C}_{k} .\\nwhere ùüô \\u200b ( ‚ãÖ ) \\\\mathds{1}(\\\\cdot) denotes the indicator function, Œ± > 1 \\\\alpha>1 scales the\\nreward of answer as the outcome reward, and Œ∑ \\\\eta controls the\\nweight of the repetition penalty.\\nThe term F1 \\u200b ( y , y ‚Ä≤ ) \\\\text{F1}(y,y^{\\\\prime}) is the character-level F1 score between the generated answer y ‚Ä≤ y^{\\\\prime} and the ground-truth answer y y .\\nFor fetch , f i \\u200b d \\u200b x \\u200b ( { c 1 } , ùí¢ ) = e ‚àí d ¬Ø \\u200b ( { c 1 } , ùí¢ ) f_{idx}(\\\\{c_{1}\\\\},\\\\mathcal{G})=e^{-\\\\bar{d}(\\\\{c_{1}\\\\},\\\\mathcal{G})} smoothly rewards fetching pages near the ground-truth pages, where d ¬Ø \\u200b ( i , ùí¢ ) = 1 | ùí¢ | \\u200b ‚àë i = 1 | ùí¢ | | c 1 ‚àí g i | \\\\bar{d}(i,\\\\mathcal{G})=\\\\frac{1}{|\\\\mathcal{G}|}\\\\sum_{i=1}^{|\\\\mathcal{G}|}|c_{1}-g_{i}| . N \\u200b D \\u200b C \\u200b G \\u200b @ \\u200b m NDCG@m evaluates the ranked list of retrieved pages, providing a fine-grained reward for search .\\nFor both fetch and search , f r \\u200b e \\u200b p \\u200b ( ùíû t , ‚Ñõ ) = | ùíû t ‚à© ‚Ñõ | | ùíû t | f_{rep}(\\\\mathcal{C}_{t},\\\\mathcal{R})=\\\\frac{|\\\\mathcal{C}_{t}\\\\cap\\\\mathcal{R}|}{|\\\\mathcal{C}_{t}|} penalizes repeated page collection.\\nTo account for long-horizon credit assignment, following Zhou et\\xa0al. ( 2024 ); Wang et\\xa0al. ( 2025a ) , we extend immediate rewards with turn-level GAE,\\nwhere V œï \\u200b ( s t L ) V_{\\\\phi}(s_{t}^{L}) denotes the value predicted at the last token of the t t -th response, serving as the turn-level value estimate. The resulting V ^ t \\\\hat{V}_{t} replaces the raw r t r_{t} as the per-turn reward signal to provide a richer learning signal that aligns token-level updates with long-horizon objectives.\\nToken-level Reward. Unlike the fetch action, whose argument is a single page number, the search action takes a search query composed of multiple tokens. A turn-level repetition penalty cannot identify which tokens are repeated, and thus fails to effectively curb redundant search actions.\\nTo address this limitation, we further introduce a token-level penalty applied specifically to the query span of search actions.\\nStarting from the second invocation of search within an episode, we compute the maximum Jaccard similarity between the current query‚Äôs n-grams and those of all past queries:\\nwhere Q n \\u200b ( q ) Q_{n}(q) denotes the set of n-grams of the query.\\nTo distribute this penalty at the token level, we assign per-token weights so that tokens inside repeated n-grams receive proportionally higher penalties. For each token u u in the query span a t q \\u200b u \\u200b e \\u200b r \\u200b y a_{t}^{query} , the weight is defined as w u = c u ‚àë v ‚àà a t q \\u200b u \\u200b e \\u200b r \\u200b y c v w_{u}=\\\\frac{c_{u}}{\\\\sum_{v\\\\in a_{t}^{query}}c_{v}} , where c u ‚àà { 0 , 1 , 2 , ‚ãØ } c_{u}\\\\in\\\\{0,1,2,\\\\cdots\\\\} counts how many repeated n-grams include token u u .\\nFinally, the reward assigned to each generated token a t i a_{t}^{i} within turn t t is defined by combining turn-level and token-level signals:\\nThis formulation anchors the turn-level objective to the response boundary, while applying localized penalties to redundant query tokens, yielding a unified cross-level reward signal for token-level PPO training.\\nToken-level GAE is then applied to compute advantages for policy updates as in eq. 2 .\\nA unique challenge in RL training for A-VRDU stems from the large number of visual tokens in the trajectory introduced by high-resolution document pages. Without explicit constraints on these tokens, we empirically observe pronounced training fluctuations and rapid entropy collapse ( fig. 3 ).\\nTo address this issue, we propose a Visual Semantic Anchoring mechanism that constrains hidden states during policy optimization through dual-path KL regularization. The KL term for textual tokens regularizes the policy distribution against a frozen reference model, stabilizing language generation, while the KL term for visual tokens anchors their hidden states to the reference model, preserving semantic grounding and preventing drift.\\nFormally, we define\\nwhere H H denotes the number of visual tokens. Œ≤ gen \\\\beta_{\\\\text{gen}} and Œ≤ obs \\\\beta_{\\\\text{obs}} are independent coefficients. In practice, we set Œ≤ obs > Œ≤ gen \\\\beta_{\\\\text{obs}}>\\\\beta_{\\\\text{gen}} to tightly regularize the much larger observation-token set while allowing more flexibility for generated tokens to adapt to the task.',\n",
       "    'subsections': [{'title': '4.1 Expanded Action Space',\n",
       "      'paragraphs': 'In Agentic VRDU, agents must flexibly access information that may be referenced either semantically or structurally. Relying solely on semantic retrieval is often insufficient: while it works for open-ended queries, it cannot efficiently resolve explicit page references (e.g.,‚Äúsee page 12‚Äù) or reasoning steps that span consecutive pages.\\nTo address this, ALDEN augments the standard search operation with a complementary fetch action, which enables direct page-index access and better exploits the inherent structure of documents.\\nThe action space thus consists of three options, each expressed in a structured format that combines free-form reasoning with explicit actions:\\nSearch ‚Äî <think>...</think><search>...</search> Generates a reasoning trace within the <think> tags followed by a semantic query enclosed within the <search> tags. An external retrieval module returns a ranked list of pages relevant to the current query using semantic similarity. This action is effective for open-ended queries where relevant content is not explicitly referenced.\\nFetch ‚Äî <think>...</think><fetch>...</fetch> Similar to search, but the agent specifies a page number within the <fetch> tag, enabling direct access to that page without semantic matching.\\nThis action is crucial for handling explicit references to page numbers or structured navigation across consecutive pages.\\nAnswer ‚Äî <think>...</think><answer>...</answer> Outputs the reasoning trace followed by the final answer. This action terminates the rollout.\\nOnce the action is parsed, the document returns the corresponding page images enclosed within the <result> tag. For the search action, the associated page numbers are also returned to provide cues of document structure.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Cross-level Reward Modeling',\n",
       "      'paragraphs': 'Training agentic VRDU systems requires reward signals that are both structured enough to enforce valid behaviors and fine-grained enough to guide efficient exploration. To this end, ALDEN employs a cross-level reward function that integrates supervision at two complementary levels: turn-level rewards for overall action quality and token-level rewards for local shaping.\\nTurn-level Reward. The immediate turn-level reward r t r_{t} is defined as r t = f t + u t r_{t}=f_{t}+u_{t} ,\\nwhere the format reward f t f_{t} enforces the response format and the result reward u t u_{t} evaluates the quality of the action outcome.\\nThe format reward f t f_{t} is given by:\\nThus, only well-formed responses avoid penalty, ensuring consistent structured outputs across turns.\\nThe result reward is defined based on the action type a t ‚àà { search , fetch , answer } a_{t}\\\\in\\\\{\\\\texttt{search},\\\\texttt{fetch},\\\\texttt{answer}\\\\} ,\\nthe set of page indices collected in the current turn ùíû t = { c 1 , ‚Ä¶ , c | ùíû t | } ‚äÜ { 1 , ‚Ä¶ , | ùíü | } \\\\mathcal{C}_{t}=\\\\{c_{1},\\\\ldots,c_{|\\\\mathcal{C}_{t}|}\\\\}\\\\subseteq\\\\{1,\\\\ldots,|\\\\mathcal{D}|\\\\} ,\\nthe set of ground-truth page indices ùí¢ = { g 1 , ‚Ä¶ , g | ùí¢ | } ‚äÜ { 1 , ‚Ä¶ , | ùíü | } \\\\mathcal{G}=\\\\{g_{1},\\\\ldots,g_{|\\\\mathcal{G}|}\\\\}\\\\subseteq\\\\{1,\\\\ldots,|\\\\mathcal{D}|\\\\} ,\\nand the set of previously accessed pages ‚Ñõ = ‚ãÉ k = 1 t ‚àí 1 ùíû k \\\\mathcal{R}=\\\\bigcup_{k=1}^{t-1}\\\\mathcal{C}_{k} .\\nwhere ùüô \\u200b ( ‚ãÖ ) \\\\mathds{1}(\\\\cdot) denotes the indicator function, Œ± > 1 \\\\alpha>1 scales the\\nreward of answer as the outcome reward, and Œ∑ \\\\eta controls the\\nweight of the repetition penalty.\\nThe term F1 \\u200b ( y , y ‚Ä≤ ) \\\\text{F1}(y,y^{\\\\prime}) is the character-level F1 score between the generated answer y ‚Ä≤ y^{\\\\prime} and the ground-truth answer y y .\\nFor fetch , f i \\u200b d \\u200b x \\u200b ( { c 1 } , ùí¢ ) = e ‚àí d ¬Ø \\u200b ( { c 1 } , ùí¢ ) f_{idx}(\\\\{c_{1}\\\\},\\\\mathcal{G})=e^{-\\\\bar{d}(\\\\{c_{1}\\\\},\\\\mathcal{G})} smoothly rewards fetching pages near the ground-truth pages, where d ¬Ø \\u200b ( i , ùí¢ ) = 1 | ùí¢ | \\u200b ‚àë i = 1 | ùí¢ | | c 1 ‚àí g i | \\\\bar{d}(i,\\\\mathcal{G})=\\\\frac{1}{|\\\\mathcal{G}|}\\\\sum_{i=1}^{|\\\\mathcal{G}|}|c_{1}-g_{i}| . N \\u200b D \\u200b C \\u200b G \\u200b @ \\u200b m NDCG@m evaluates the ranked list of retrieved pages, providing a fine-grained reward for search .\\nFor both fetch and search , f r \\u200b e \\u200b p \\u200b ( ùíû t , ‚Ñõ ) = | ùíû t ‚à© ‚Ñõ | | ùíû t | f_{rep}(\\\\mathcal{C}_{t},\\\\mathcal{R})=\\\\frac{|\\\\mathcal{C}_{t}\\\\cap\\\\mathcal{R}|}{|\\\\mathcal{C}_{t}|} penalizes repeated page collection.\\nTo account for long-horizon credit assignment, following Zhou et\\xa0al. ( 2024 ); Wang et\\xa0al. ( 2025a ) , we extend immediate rewards with turn-level GAE,\\nwhere V œï \\u200b ( s t L ) V_{\\\\phi}(s_{t}^{L}) denotes the value predicted at the last token of the t t -th response, serving as the turn-level value estimate. The resulting V ^ t \\\\hat{V}_{t} replaces the raw r t r_{t} as the per-turn reward signal to provide a richer learning signal that aligns token-level updates with long-horizon objectives.\\nToken-level Reward. Unlike the fetch action, whose argument is a single page number, the search action takes a search query composed of multiple tokens. A turn-level repetition penalty cannot identify which tokens are repeated, and thus fails to effectively curb redundant search actions.\\nTo address this limitation, we further introduce a token-level penalty applied specifically to the query span of search actions.\\nStarting from the second invocation of search within an episode, we compute the maximum Jaccard similarity between the current query‚Äôs n-grams and those of all past queries:\\nwhere Q n \\u200b ( q ) Q_{n}(q) denotes the set of n-grams of the query.\\nTo distribute this penalty at the token level, we assign per-token weights so that tokens inside repeated n-grams receive proportionally higher penalties. For each token u u in the query span a t q \\u200b u \\u200b e \\u200b r \\u200b y a_{t}^{query} , the weight is defined as w u = c u ‚àë v ‚àà a t q \\u200b u \\u200b e \\u200b r \\u200b y c v w_{u}=\\\\frac{c_{u}}{\\\\sum_{v\\\\in a_{t}^{query}}c_{v}} , where c u ‚àà { 0 , 1 , 2 , ‚ãØ } c_{u}\\\\in\\\\{0,1,2,\\\\cdots\\\\} counts how many repeated n-grams include token u u .\\nFinally, the reward assigned to each generated token a t i a_{t}^{i} within turn t t is defined by combining turn-level and token-level signals:\\nThis formulation anchors the turn-level objective to the response boundary, while applying localized penalties to redundant query tokens, yielding a unified cross-level reward signal for token-level PPO training.\\nToken-level GAE is then applied to compute advantages for policy updates as in eq. 2 .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Visual Semantic Anchoring',\n",
       "      'paragraphs': 'A unique challenge in RL training for A-VRDU stems from the large number of visual tokens in the trajectory introduced by high-resolution document pages. Without explicit constraints on these tokens, we empirically observe pronounced training fluctuations and rapid entropy collapse ( fig. 3 ).\\nTo address this issue, we propose a Visual Semantic Anchoring mechanism that constrains hidden states during policy optimization through dual-path KL regularization. The KL term for textual tokens regularizes the policy distribution against a frozen reference model, stabilizing language generation, while the KL term for visual tokens anchors their hidden states to the reference model, preserving semantic grounding and preventing drift.\\nFormally, we define\\nwhere H H denotes the number of visual tokens. Œ≤ gen \\\\beta_{\\\\text{gen}} and Œ≤ obs \\\\beta_{\\\\text{obs}} are independent coefficients. In practice, we set Œ≤ obs > Œ≤ gen \\\\beta_{\\\\text{obs}}>\\\\beta_{\\\\text{gen}} to tightly regularize the much larger observation-token set while allowing more flexibility for generated tokens to adapt to the task.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Experiments',\n",
       "    'paragraphs': 'We conduct experiments on long VRDU benchmarks to (i) compare ALDEN with strong baselines and (ii) assess the contribution of its key components, including expanded action space, cross-level reward, and visual semantic anchoring, to navigation accuracy, answer quality, and training stability. We first outline datasets, baselines, implementation details, and evaluation metrics ( section 5.1 ), then present main results ( section 5.2 ), followed by ablations ( section 5.3 ) and detailed component analyses ( section 5.4 ).\\nDatasets. We build the training set by merging and processing three multi-page VRDU datasets: DUDE (Van\\xa0Landeghem et\\xa0al., 2023 ) , MPDocVQA (Tito et\\xa0al., 2023a ) , and SlideVQA (Tanaka et\\xa0al., 2023a ) . We filter out documents with fewer than 10 pages. To enrich query diversity, we use GPT-4o (Hurst et\\xa0al., 2024 ) to rewrite part of MPDocVQA, increasing the proportion of page-index‚Äìreferenced queries in the final training corpus. Detailed statistics of the resulting training set are provided in table 1 .\\nThe evaluation is conducted mainly on the following VRDU benchmarks: MMLongBench (Ma et\\xa0al., 2024b ) , LongDocURL (Deng et\\xa0al., 2024 ) , PaperTab (Hui et\\xa0al., 2024 ) , PaperText (Hui et\\xa0al., 2024 ) , and FetaTab (Hui et\\xa0al., 2024 ) .\\nTo evaluate the fetch action, we create DUDE-sub, a DUDE validation subset with 480 general queries and 480 queries containing explicit page references or implicit sequential navigation cues.\\nMore details about the dataset can be seen in appendix A .\\nBaselines. To validate the effectiveness of ALDEN, we compare it with three categories of baselines.\\n(1) Full-Document Input : mainstream state-of-the-art VLMs are prompted with the entire document as context to answer user queries.\\n(2) Visual RAG : methods that retrieve the most relevant document pages using the user query, including M3DocRAG (Cho et\\xa0al., 2024 ) , and ReSearch-VL, a Search-only ALDEN variant trained with GRPO using outcome-based rewards adapted from a fully textual method ReSearch (Chen et\\xa0al., 2025b ) .\\n(3) Hybrid RAG : approaches that augment page images with OCR-extracted text for retrieval and reasoning, including MDocAgent (Han et\\xa0al., 2025 ) , VidoRAG (Wang et\\xa0al., 2025b ) .\\nDetailed baseline configurations can be seen in appendix B\\nImplementation Details. Both the policy and value models are initialized from Qwen2.5-VL-7B-Instruct (Bai et\\xa0al., 2025 ) , and all Visual RAG and Hybrid RAG baselines use the same backbone for fairness. During training, we adopt the single-vector retriever vdr-2b-v1 (Ma et\\xa0al., 2024a ) for images and e5-large-v2 (Wang et\\xa0al., 2022 ) for text. For evaluation, we also report results with the multi-vector retrievers ColQwen2-v1.0 (ColQwen) (Faysse et\\xa0al., 2025 ) for images and ColBERT-v2.0 (ColBERT) (Santhanam et\\xa0al., 2021 ) for text. Unless otherwise noted, each search action retrieves the top- 1 1 candidate page, with a maximum of T = 6 T=6 reasoning‚Äìaction turns. On average, ALDEN collects 1.87 unique pages per query; hence, single-turn RAG baselines are set to retrieve the top-2 pages for a fair comparison. Further implementation details are provided in appendix C .\\nEvaluation Metrics. The primary evaluation metric is GPT-4o‚Äìjudged answer accuracy ( Acc ) on each benchmark.\\nFor finer-grained analysis of ALDEN‚Äôs components, we further assess navigation quality using trajectory-level retrieval recall ( Rec ), precision ( Pre ), F1-score ( F1 ), and the number of unique collected pages ( #UP ).\\nDetailed definitions of these metrics are provided in appendix D .\\nTable 2 reports answer accuracy across all baselines. Directly prompting large VLMs with the entire document performs poorly (Acc < 0.30 <0.30 ), confirming the difficulty of long-document reasoning where irrelevant content overwhelms true evidence. Retrieval-based methods achieve substantially better results. Among Visual RAG approaches, ALDEN with ColQwen attains the highest average accuracy (0.410), surpassing M3DocRAG by 3.2 points.\\nIn Hybrid RAG, baselines such as ViDoRAG and MDocAgent benefit from textual signals but are limited by fixed reasoning pipelines. ALDEN with hybrid retrievers achieves the best overall performance, exceeding the strongest hybrid baseline by +7.47% relative improvement. These results highlight ALDEN ‚Äôs ability to generalize across benchmarks by actively collecting and reasoning over evidence, though modest performance on scientific-paper datasets (PaperText, PaperTab) suggests domain knowledge remains a limiting factor.\\nThe notably larger gain over ReSearch-VL underscores the limitations of GRPO with outcome-based rewards for training multimodal agents in multi-turn, long-horizon settings from base VLMs, which is one of the key motivations for this work.\\nMoreover, ALDEN achieves higher accuracy with a multi-vector retriever at inference despite being trained with a single-vector retriever, indicating that strategies learned with a weaker retriever generalize to stronger ones and suggesting a path to more efficient training.\\nSpecific case study can be seen in appendix E .\\nTo understand the contribution of each component in ALDEN, we further conduct ablation studies on the five benchmarks.\\nTable 3 reports the Acc metric results for the full model and three variants:\\n(i) w/o Fetch , which removes the index-based fetch action and relies solely on semantic retrieval;\\n(ii) w/o Cross-level Reward , which uses only outcome-level supervision without our designed turn- and token-level reward shaping; and\\n(iii) w/o Visual Semantic Anchoring , which omits the constraint on visual hidden states during optimization.\\nRemoving any component consistently lowers accuracy, with the largest drop from omitting fetch , underscoring the value of direct page-index access. Excluding the cross-level reward also substantially hurts performance, confirming the importance of fine-grained reward shaping, while removing visual-semantic anchoring causes milder yet consistent degradation.\\nBuilding on these results, we next provide a detailed component analysis to understand the specific roles of each key design choice in ALDEN.\\nFetch vs. Search To assess the effect of the proposed fetch action, we compare the full ALDEN agent with a search-only variant that disables direct page-index access and relies solely on semantic retrieval.\\nEvaluation on the DUDE-sub dataset, which contains explicit page references and structured navigation queries, shows clear benefits of fetch (Table 4 ). Acc improves from 0.545 to 0.653 and Rec from 0.471 to 0.598, while Pre and F1 also increase, indicating more accurate evidence retrieval.\\nThe number of unique pages rises from 1.03 to 1.19, reflecting broader coverage. These results confirm that combining index-based fetch with semantic search enables more flexible and efficient navigation, especially for queries that reference specific pages or require traversal across consecutive pages.\\nEffect of Reward Design. We evaluate how different reward schemes affect ALDEN‚Äôs retrieval and reasoning (Table 5 ).\\n(i) Outcome-based Only assigns a single scalar reward for final answer correctness.\\n(ii) Turn-level + Outcome adds rule-based turn-level supervision, improving Acc from 0.483 to 0.509 and Rec from 0.483 to 0.497, showing that denser feedback aids evidence localization.\\n(iii) Full ALDEN further introduces token-level shaping, yielding a smaller but consistent gain (Acc 0.513, Rec 0.506) and increasing unique pages from 1.22 to 1.39, indicating reduced query repetition and broader exploration.\\nOverall, the cross-level reward design fosters richer query reformulation and more thorough evidence gathering, enhancing both navigation and answer quality.\\nEffect of Visual Semantic Anchoring. We evaluate the effect of Visual Semantic Anchoring (VSA) on training stability and representation drift, as shown in Figure 3 .\\nWith a larger batch size (512) than in the main experiments (128), the VSA-enabled model achieves steadily increasing answer rewards, while the non-VSA variant fluctuates and collapses (a). VSA also maintains higher policy entropy, supporting healthier exploration (b).\\nFor representation alignment,\\nKL divergence of visual tokens grows unchecked without VSA, indicating hidden-state drift, whereas VSA constrains these values while allowing moderate growth for action tokens (c,d).\\nOverall, VSA achieves stabilizing RL training and preventing drift in visual representations.',\n",
       "    'subsections': [{'title': '5.1 Experimental Setup',\n",
       "      'paragraphs': 'Datasets. We build the training set by merging and processing three multi-page VRDU datasets: DUDE (Van\\xa0Landeghem et\\xa0al., 2023 ) , MPDocVQA (Tito et\\xa0al., 2023a ) , and SlideVQA (Tanaka et\\xa0al., 2023a ) . We filter out documents with fewer than 10 pages. To enrich query diversity, we use GPT-4o (Hurst et\\xa0al., 2024 ) to rewrite part of MPDocVQA, increasing the proportion of page-index‚Äìreferenced queries in the final training corpus. Detailed statistics of the resulting training set are provided in table 1 .\\nThe evaluation is conducted mainly on the following VRDU benchmarks: MMLongBench (Ma et\\xa0al., 2024b ) , LongDocURL (Deng et\\xa0al., 2024 ) , PaperTab (Hui et\\xa0al., 2024 ) , PaperText (Hui et\\xa0al., 2024 ) , and FetaTab (Hui et\\xa0al., 2024 ) .\\nTo evaluate the fetch action, we create DUDE-sub, a DUDE validation subset with 480 general queries and 480 queries containing explicit page references or implicit sequential navigation cues.\\nMore details about the dataset can be seen in appendix A .\\nBaselines. To validate the effectiveness of ALDEN, we compare it with three categories of baselines.\\n(1) Full-Document Input : mainstream state-of-the-art VLMs are prompted with the entire document as context to answer user queries.\\n(2) Visual RAG : methods that retrieve the most relevant document pages using the user query, including M3DocRAG (Cho et\\xa0al., 2024 ) , and ReSearch-VL, a Search-only ALDEN variant trained with GRPO using outcome-based rewards adapted from a fully textual method ReSearch (Chen et\\xa0al., 2025b ) .\\n(3) Hybrid RAG : approaches that augment page images with OCR-extracted text for retrieval and reasoning, including MDocAgent (Han et\\xa0al., 2025 ) , VidoRAG (Wang et\\xa0al., 2025b ) .\\nDetailed baseline configurations can be seen in appendix B\\nImplementation Details. Both the policy and value models are initialized from Qwen2.5-VL-7B-Instruct (Bai et\\xa0al., 2025 ) , and all Visual RAG and Hybrid RAG baselines use the same backbone for fairness. During training, we adopt the single-vector retriever vdr-2b-v1 (Ma et\\xa0al., 2024a ) for images and e5-large-v2 (Wang et\\xa0al., 2022 ) for text. For evaluation, we also report results with the multi-vector retrievers ColQwen2-v1.0 (ColQwen) (Faysse et\\xa0al., 2025 ) for images and ColBERT-v2.0 (ColBERT) (Santhanam et\\xa0al., 2021 ) for text. Unless otherwise noted, each search action retrieves the top- 1 1 candidate page, with a maximum of T = 6 T=6 reasoning‚Äìaction turns. On average, ALDEN collects 1.87 unique pages per query; hence, single-turn RAG baselines are set to retrieve the top-2 pages for a fair comparison. Further implementation details are provided in appendix C .\\nEvaluation Metrics. The primary evaluation metric is GPT-4o‚Äìjudged answer accuracy ( Acc ) on each benchmark.\\nFor finer-grained analysis of ALDEN‚Äôs components, we further assess navigation quality using trajectory-level retrieval recall ( Rec ), precision ( Pre ), F1-score ( F1 ), and the number of unique collected pages ( #UP ).\\nDetailed definitions of these metrics are provided in appendix D .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Main Results',\n",
       "      'paragraphs': 'Table 2 reports answer accuracy across all baselines. Directly prompting large VLMs with the entire document performs poorly (Acc < 0.30 <0.30 ), confirming the difficulty of long-document reasoning where irrelevant content overwhelms true evidence. Retrieval-based methods achieve substantially better results. Among Visual RAG approaches, ALDEN with ColQwen attains the highest average accuracy (0.410), surpassing M3DocRAG by 3.2 points.\\nIn Hybrid RAG, baselines such as ViDoRAG and MDocAgent benefit from textual signals but are limited by fixed reasoning pipelines. ALDEN with hybrid retrievers achieves the best overall performance, exceeding the strongest hybrid baseline by +7.47% relative improvement. These results highlight ALDEN ‚Äôs ability to generalize across benchmarks by actively collecting and reasoning over evidence, though modest performance on scientific-paper datasets (PaperText, PaperTab) suggests domain knowledge remains a limiting factor.\\nThe notably larger gain over ReSearch-VL underscores the limitations of GRPO with outcome-based rewards for training multimodal agents in multi-turn, long-horizon settings from base VLMs, which is one of the key motivations for this work.\\nMoreover, ALDEN achieves higher accuracy with a multi-vector retriever at inference despite being trained with a single-vector retriever, indicating that strategies learned with a weaker retriever generalize to stronger ones and suggesting a path to more efficient training.\\nSpecific case study can be seen in appendix E .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Ablation Study',\n",
       "      'paragraphs': 'To understand the contribution of each component in ALDEN, we further conduct ablation studies on the five benchmarks.\\nTable 3 reports the Acc metric results for the full model and three variants:\\n(i) w/o Fetch , which removes the index-based fetch action and relies solely on semantic retrieval;\\n(ii) w/o Cross-level Reward , which uses only outcome-level supervision without our designed turn- and token-level reward shaping; and\\n(iii) w/o Visual Semantic Anchoring , which omits the constraint on visual hidden states during optimization.\\nRemoving any component consistently lowers accuracy, with the largest drop from omitting fetch , underscoring the value of direct page-index access. Excluding the cross-level reward also substantially hurts performance, confirming the importance of fine-grained reward shaping, while removing visual-semantic anchoring causes milder yet consistent degradation.\\nBuilding on these results, we next provide a detailed component analysis to understand the specific roles of each key design choice in ALDEN.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.4 Component Analysis',\n",
       "      'paragraphs': 'Fetch vs. Search To assess the effect of the proposed fetch action, we compare the full ALDEN agent with a search-only variant that disables direct page-index access and relies solely on semantic retrieval.\\nEvaluation on the DUDE-sub dataset, which contains explicit page references and structured navigation queries, shows clear benefits of fetch (Table 4 ). Acc improves from 0.545 to 0.653 and Rec from 0.471 to 0.598, while Pre and F1 also increase, indicating more accurate evidence retrieval.\\nThe number of unique pages rises from 1.03 to 1.19, reflecting broader coverage. These results confirm that combining index-based fetch with semantic search enables more flexible and efficient navigation, especially for queries that reference specific pages or require traversal across consecutive pages.\\nEffect of Reward Design. We evaluate how different reward schemes affect ALDEN‚Äôs retrieval and reasoning (Table 5 ).\\n(i) Outcome-based Only assigns a single scalar reward for final answer correctness.\\n(ii) Turn-level + Outcome adds rule-based turn-level supervision, improving Acc from 0.483 to 0.509 and Rec from 0.483 to 0.497, showing that denser feedback aids evidence localization.\\n(iii) Full ALDEN further introduces token-level shaping, yielding a smaller but consistent gain (Acc 0.513, Rec 0.506) and increasing unique pages from 1.22 to 1.39, indicating reduced query repetition and broader exploration.\\nOverall, the cross-level reward design fosters richer query reformulation and more thorough evidence gathering, enhancing both navigation and answer quality.\\nEffect of Visual Semantic Anchoring. We evaluate the effect of Visual Semantic Anchoring (VSA) on training stability and representation drift, as shown in Figure 3 .\\nWith a larger batch size (512) than in the main experiments (128), the VSA-enabled model achieves steadily increasing answer rewards, while the non-VSA variant fluctuates and collapses (a). VSA also maintains higher policy entropy, supporting healthier exploration (b).\\nFor representation alignment,\\nKL divergence of visual tokens grows unchecked without VSA, indicating hidden-state drift, whereas VSA constrains these values while allowing moderate growth for action tokens (c,d).\\nOverall, VSA achieves stabilizing RL training and preventing drift in visual representations.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Conclusions',\n",
       "    'paragraphs': 'We introduced the Agentic VRDU task and proposed ALDEN , a reinforcement-learning framework that trains VLMs as autonomous agents capable of multi-turn navigation and evidence gathering.\\nALDEN integrates a fetch action for direct page access, a cross-level reward for fine-grained reward modeling, and a visual semantic anchoring mechanism for stable training.\\nExtensive experiments on multiple long-document benchmarks show that ALDEN achieves state-of-the-art accuracy and improves evidence localization. Ablation studies further confirm the contribution of each component and offer broader insights for multi-turn RL in multimodal agents.\\nThe A-VRDU paradigm marks a shift from passive document reading to autonomous navigation and reasoning across vast information landscapes, and ALDEN‚Äôs strong performance demonstrates the potential of such agents to deliver more accurate, scalable, and adaptive understanding of complex, visually rich documents.\\nWhile promising, the trained agent still faces challenges in balancing exploration and exploitation and in reliably recognizing true evidence pages. Future work could focus on building larger and higher-quality datasets, leveraging trajectories from stronger models with validation and reflection, and adopting curriculum learning to handle tasks of varying difficulty.',\n",
       "    'subsections': []},\n",
       "   {'title': 'LLM Usage Statement',\n",
       "    'paragraphs': 'Large Language Models (LLMs) were used as general-purpose writing and editing aids.\\nSpecifically, OpenAI‚Äôs ChatGPT (GPT-5) assisted in polishing grammar, improving clarity, and suggesting alternative phrasings.\\nAll research ideas, experimental design, data processing, model development, and analysis were conceived and executed solely by the authors.\\nThe LLM provided no novel research insights or substantive scientific contributions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgments',\n",
       "    'paragraphs': 'This work was supported by the Lower Saxony Ministry of Science and Culture and the VW Foundation. The authors would like to thank the German Federal Ministry of Research, Technology and\\nSpace and the German federal states ( http://www.nhr-verein.de/en/our-partners ) for\\nsupporting this work/project as part of the National High-Performance Computing (NHR) joint\\nfunding program.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Reproducibility Statement',\n",
       "    'paragraphs': 'We are committed to ensuring the reproducibility of our results.\\nTo this end, we will release:\\nAll source code for training, evaluation, and data preprocessing, including scripts for dataset construction, reward computation, and reinforcement-learning training with ALDEN.\\nThe processed training corpus derived from DUDE, MPDocVQA, and SlideVQA, along with instructions to regenerate it from the original public datasets.\\nDetailed configuration files specifying model hyperparameters, random seeds, and hardware settings.\\nCheckpoints for both the policy and value models, and prompts used for GPT-4o evaluation.\\nOur experiments were run on NVIDIA A100 GPUs (80GB) with PyTorch 2.4 and HuggingFace Transformers 4.49; exact package versions will be provided in the released code.\\nThese resources will allow other researchers to fully reproduce our training, evaluation, and analysis results.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Datasets',\n",
       "    'paragraphs': 'Training. We construct our training dataset by combining samples from three publicly available multi-page document understanding datasets: DUDE (Van\\xa0Landeghem et\\xa0al., 2023 ) , MPDocVQA (Tito et\\xa0al., 2023a ) , and SlideVQA Tanaka et\\xa0al. ( 2023a ) .\\nThese datasets provide diverse document layouts and question-answering formats, making them well-suited for training models on complex multi-turn document question answering tasks.\\nDUDE is a large-scale benchmark designed for multi-page, visually rich document understanding. It covers diverse domains such as scientific articles, financial and legal reports, technical manuals, and presentations. Each example consists of a full PDF document rendered into page images, paired with a natural-language query and a free-form textual answer, along with page-level ground-truth evidence annotations.\\nSlideVQA contains questions grounded in slide decks, where understanding layout and inter-slide referencing is crucial.\\nIt contains slide decks from diverse topics such as education, business, and research talks, requiring models to reason across sequential pages that mix text, charts, and images.\\nEach example provides a slide deck rendered as ordered page images, a natural-language question, and a free-form textual answer, with annotations of relevant slides for evidence grounding.\\nMPDocVQA extends the traditional single-page VQA setting (originally based on DocVQA) by concatenating additional pages to the original single-page input, while retaining the same set of user questions.\\nHowever, since many of these questions were authored under the assumption that only one page is visible (e.g., ‚ÄúWhat is the date?‚Äù or ‚ÄúWho is the author?‚Äù), they often lack sufficient context to guide document retrieval or navigation.\\nTo address this, we first use GPT-4o (Hurst et\\xa0al., 2024 ) to automatically identify this kind of samples. Then we integrate the index of referred pages into the questions to get page-index-referenced questions, e.g., ‚ÄúIn page 5, what is the date?‚Äù.\\nThe prompt we used is shown below:\\nTo ensure that our model is consistently exposed to multi-page reasoning scenarios, we additionally discard any documents with fewer than 10 pages from all three datasets.\\nThis helps avoid biasing the model toward short-context behavior and ensures a consistent level of document complexity.\\nAfter merging and filtering, we obtain a training set consisting of 30,728 samples, each comprising a user query and its corresponding multi-page document context, answer and the index of evidence pages.\\nFinally, we proportionally sample 1,024 samples from the validation set of these three datasets as our validation set.\\nWe evaluate our method on a diverse set of benchmarks: MMLongBench (Ma et\\xa0al., 2024b ) , LongDocURL (Deng et\\xa0al., 2024 ) , PaperTab (Hui et\\xa0al., 2024 ) , PaperText (Hui et\\xa0al., 2024 ) , and FetaTab (Hui et\\xa0al., 2024 ) .\\nThese datasets span a wide range of scenarios, including both open-domain and closed-domain tasks, and include textual as well as visual content.\\nThe documents also vary in length and structure, ranging from short forms to complex, multi-page documents.\\nThis diversity ensures a comprehensive and fair evaluation of our model‚Äôs performance across real-world document understanding tasks.\\nMMLongBench-Doc is a large-scale benchmark designed to evaluate how multimodal large language models handle long, visually rich documents. It contains over a thousand expert-annotated questions drawn from lengthy PDFs (averaging \\xa050 pages and \\xa020k tokens) that mix text, tables, charts, and images. Tasks require single-page, cross-page, and sometimes unanswerable reasoning, testing a model‚Äôs ability to retrieve and integrate evidence across multiple modalities and extended contexts.\\nLongDocURL is a benchmark for evaluating large vision-language models on long, multimodal documents by combining three core task types: understanding, numerical reasoning, and element locating. It includes 2,325 high-quality question-answer pairs over 396 documents totaling over 33,000 pages, with an average of \\xa085.6 pages per document. Tasks vary in their evidence requirements: some require single-page evidence, others multi-page, and many involve locating evidence across different layout elements (text, tables, figures, and layout).\\nPaperText is a subset in the UDA benchmark made up of academic papers (in PDF form) used for retrieval-augmented generation / document question answering tasks. Each document comes with multiple question-answer pairs drawn from ‚ÄúQasper‚Äù (an academic paper reading comprehension dataset), where questions may be extractive, yes/no, or free-form. The dataset preserves full documents to allow answering from context, rather than just small passages.\\nPaperTab is another subset in UDA also based on academic papers, but the focus is on Q&A pairs where evidence comes from or interacts with tables inside papers. Like PaperText, it retains full PDF documents so that models must locate and reason over tabular content, as well as textual content. The questions are similarly diverse (extractive, yes/no, free-form), and the average size is modest (\\xa010‚Äì11 pages per document).\\nFetaTab is a subset of the UDA (Unstructured Document Analysis) benchmark that focuses on free-form question answering over Wikipedia tables in both HTML and PDF formats. It comprises 878 documents and 1,023 QA pairs, averaging about 14.9 pages per document. The questions are ‚Äúfree-form‚Äù (i.e. natural language answers, not limited to extractive spans or simple yes/no), which requires models to understand table content, context, and sometimes cross-format layout.',\n",
       "    'subsections': [{'title': 'A.1 Training dataset',\n",
       "      'paragraphs': 'Training. We construct our training dataset by combining samples from three publicly available multi-page document understanding datasets: DUDE (Van\\xa0Landeghem et\\xa0al., 2023 ) , MPDocVQA (Tito et\\xa0al., 2023a ) , and SlideVQA Tanaka et\\xa0al. ( 2023a ) .\\nThese datasets provide diverse document layouts and question-answering formats, making them well-suited for training models on complex multi-turn document question answering tasks.\\nDUDE is a large-scale benchmark designed for multi-page, visually rich document understanding. It covers diverse domains such as scientific articles, financial and legal reports, technical manuals, and presentations. Each example consists of a full PDF document rendered into page images, paired with a natural-language query and a free-form textual answer, along with page-level ground-truth evidence annotations.\\nSlideVQA contains questions grounded in slide decks, where understanding layout and inter-slide referencing is crucial.\\nIt contains slide decks from diverse topics such as education, business, and research talks, requiring models to reason across sequential pages that mix text, charts, and images.\\nEach example provides a slide deck rendered as ordered page images, a natural-language question, and a free-form textual answer, with annotations of relevant slides for evidence grounding.\\nMPDocVQA extends the traditional single-page VQA setting (originally based on DocVQA) by concatenating additional pages to the original single-page input, while retaining the same set of user questions.\\nHowever, since many of these questions were authored under the assumption that only one page is visible (e.g., ‚ÄúWhat is the date?‚Äù or ‚ÄúWho is the author?‚Äù), they often lack sufficient context to guide document retrieval or navigation.\\nTo address this, we first use GPT-4o (Hurst et\\xa0al., 2024 ) to automatically identify this kind of samples. Then we integrate the index of referred pages into the questions to get page-index-referenced questions, e.g., ‚ÄúIn page 5, what is the date?‚Äù.\\nThe prompt we used is shown below:\\nTo ensure that our model is consistently exposed to multi-page reasoning scenarios, we additionally discard any documents with fewer than 10 pages from all three datasets.\\nThis helps avoid biasing the model toward short-context behavior and ensures a consistent level of document complexity.\\nAfter merging and filtering, we obtain a training set consisting of 30,728 samples, each comprising a user query and its corresponding multi-page document context, answer and the index of evidence pages.\\nFinally, we proportionally sample 1,024 samples from the validation set of these three datasets as our validation set.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Benchmarks',\n",
       "      'paragraphs': 'We evaluate our method on a diverse set of benchmarks: MMLongBench (Ma et\\xa0al., 2024b ) , LongDocURL (Deng et\\xa0al., 2024 ) , PaperTab (Hui et\\xa0al., 2024 ) , PaperText (Hui et\\xa0al., 2024 ) , and FetaTab (Hui et\\xa0al., 2024 ) .\\nThese datasets span a wide range of scenarios, including both open-domain and closed-domain tasks, and include textual as well as visual content.\\nThe documents also vary in length and structure, ranging from short forms to complex, multi-page documents.\\nThis diversity ensures a comprehensive and fair evaluation of our model‚Äôs performance across real-world document understanding tasks.\\nMMLongBench-Doc is a large-scale benchmark designed to evaluate how multimodal large language models handle long, visually rich documents. It contains over a thousand expert-annotated questions drawn from lengthy PDFs (averaging \\xa050 pages and \\xa020k tokens) that mix text, tables, charts, and images. Tasks require single-page, cross-page, and sometimes unanswerable reasoning, testing a model‚Äôs ability to retrieve and integrate evidence across multiple modalities and extended contexts.\\nLongDocURL is a benchmark for evaluating large vision-language models on long, multimodal documents by combining three core task types: understanding, numerical reasoning, and element locating. It includes 2,325 high-quality question-answer pairs over 396 documents totaling over 33,000 pages, with an average of \\xa085.6 pages per document. Tasks vary in their evidence requirements: some require single-page evidence, others multi-page, and many involve locating evidence across different layout elements (text, tables, figures, and layout).\\nPaperText is a subset in the UDA benchmark made up of academic papers (in PDF form) used for retrieval-augmented generation / document question answering tasks. Each document comes with multiple question-answer pairs drawn from ‚ÄúQasper‚Äù (an academic paper reading comprehension dataset), where questions may be extractive, yes/no, or free-form. The dataset preserves full documents to allow answering from context, rather than just small passages.\\nPaperTab is another subset in UDA also based on academic papers, but the focus is on Q&A pairs where evidence comes from or interacts with tables inside papers. Like PaperText, it retains full PDF documents so that models must locate and reason over tabular content, as well as textual content. The questions are similarly diverse (extractive, yes/no, free-form), and the average size is modest (\\xa010‚Äì11 pages per document).\\nFetaTab is a subset of the UDA (Unstructured Document Analysis) benchmark that focuses on free-form question answering over Wikipedia tables in both HTML and PDF formats. It comprises 878 documents and 1,023 QA pairs, averaging about 14.9 pages per document. The questions are ‚Äúfree-form‚Äù (i.e. natural language answers, not limited to extractive spans or simple yes/no), which requires models to understand table content, context, and sometimes cross-format layout.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Baselines',\n",
       "    'paragraphs': 'To evaluate the effectiveness of ALDEN, we compare it against three categories of methods:\\nBase VLMs supporting multi-image input. These models directly take the entire multi-page document as context without retrieval, leveraging their built-in multi-page visual processing capabilities. For fairness, we select open-source VLMs of similar scale to Qwen2.5-VL-7B, including LLaVA-v1.6-Mistral-7B (Liu et\\xa0al., 2024a ) , Phi-3.5-Vision-Instruct (Abdin et\\xa0al., 2024 ) , LLaVA-One-Vision-7B (Li et\\xa0al., 2024 ) , SmolVLM-Instruct (Marafioti et\\xa0al., 2025 ) , mPLUG-DocOwl2 (Hu et\\xa0al., 2024 ) , LEOPARD (Jia et\\xa0al., 2024 ) , InternVL3.5-8B-Instruct (Wang et\\xa0al., 2025c ) .\\nVisual RAG methods. These methods use the user query to retrieve the most relevant document pages and feed them into the model as context. We include M3DocRAG (Cho et\\xa0al., 2024 ) as a strong baseline, as well as our proposed ALDEN. To isolate the impact of our reward function design, we additionally evaluate a variant that trains the same backbone with GRPO using only outcome-based rewards (no turn-level shaping), mirroring common text-only RLHF setups as in ReSearch (Chen et\\xa0al., 2025b ) . Specifically,\\nM3DocRAG is a multi-modal document understanding framework designed for multi-page and multi-document question answering. It first encodes each page into joint visual-text embeddings using a multi-modal encoder, then retrieves the top-K relevant pages via a MaxSim-based retrieval mechanism, optionally accelerated with FAISS for large-scale documents. Finally, a multi-modal language model processes the retrieved pages to generate precise answers, effectively handling complex queries that require reasoning over both textual and visual content.\\nReSearch introduces a framework that trains large language models to integrate reasoning and search in a unified process. The model learns, via reinforcement learning, when and how to perform search actions during multi-step reasoning, using search results to guide subsequent reasoning steps. By treating search as part of the reasoning chain, ReSearch enables LLMs to solve complex multi-hop tasks, demonstrate self-correction and reflection, and generalize effectively across benchmarks, achieving significant performance gains over baseline models.\\nHybrid RAG methods. These approaches combine visual and textual retrieval by first applying an OCR tool to extract all text from the document. The query is then used to retrieve both the most relevant page image and the most relevant OCR-extracted text, which are jointly fed into the model. We evaluate MDocAgent (Han et\\xa0al., 2025 ) and VidoRAG (Wang et\\xa0al., 2025b ) as a representative method in this category.\\nMDocAgent is a multi-modal, multi-agent framework for document understanding that combines Retrieval-Augmented Generation (RAG) with specialized agents to handle complex documents. The system employs a General Agent for multi-modal context retrieval, a Critical Agent for identifying key information, a Text Agent for analyzing textual content, an Image Agent for interpreting visual elements, and a Summarizing Agent to synthesize results. By coordinating these agents, MDocAgent effectively integrates textual and visual reasoning, achieving significant improvements in accuracy and error reduction compared to existing large vision-language models and RAG-based methods. For all five agents in this framework, we consistently use the original LLaMA3.1-8B as the LLM for the text agent, while employing a consistent VLMs, i.e., Qwen2.5-VL-7B, for remaining agents.\\nViDoRAG is a multi-agent framework designed to enhance the understanding of visually rich documents. It employs a Gaussian Mixture Model (GMM)-based hybrid retrieval strategy to effectively handle multi-modal retrieval, integrating both textual and visual information. The framework incorporates a dynamic iterative reasoning process, utilizing agents such as Seeker, Inspector, and Answer to iteratively refine the understanding and generation of responses. This approach addresses challenges in traditional Retrieval-Augmented Generation (RAG) methods by improving retrieval accuracy and enabling complex reasoning over visual documents. We use Qwen2.5-VL-7B as backbone for all agents in this methods.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Implementation Details',\n",
       "    'paragraphs': 'Our implementation is based on the EasyR1 1 1 1 https://github.com/hiyouga/EasyR1 framework.\\nBoth the policy model and the value function are initialized from Qwen2.5-VL-7B-Instruct (Bai et\\xa0al., 2025 ) .\\nWe use a batch size of 128, with fixed learning rates of 1 √ó 10 ‚àí 6 1\\\\times 10^{-6} for the policy model and 1 √ó 10 ‚àí 5 1\\\\times 10^{-5} for the value function. The maximum number of interaction turns is set to T = 6 T=6 .\\nFor visual inputs, we constrain the number of image pixels to lie between 261 , 070 261{,}070 and 2 , 508 , 800 2{,}508{,}800 .\\nBased on these settings, we set the maximum number of tokens in the trajectory as 19000.\\nThe KL coefficients for generated tokens and observation tokens are set to Œ≤ gen = 0.001 \\\\beta_{\\\\text{gen}}=0.001 and Œ≤ obs = 0.01 \\\\beta_{\\\\text{obs}}=0.01 , respectively.\\nFor the search actions, we used only the top-1 retrieved pages. While calculating the N \\u200b D \\u200b C \\u200b G \\u200b @ \\u200b m NDCG@m metrics, we set m m as 5 to avoid sparse, all zero rewards.\\nBesides, we set the scale coefficient Œ± = 5 \\\\alpha=5 . The weight of repetition penalty is set as Œ∑ = 0.5 \\\\eta=0.5 .\\nFor the calculation of GAE, we set Œ≥ token = 1.0 \\\\gamma_{\\\\text{token}}=1.0 , Œ≥ turn = 0.9 \\\\gamma_{\\\\text{turn}}=0.9 and Œª token = Œª turn = 1.0 \\\\lambda_{\\\\text{token}}=\\\\lambda_{\\\\text{turn}}=1.0 .\\nDuring training, we adopt the single-vector retriever vdr-2b-v1 (Ma et\\xa0al., 2024a ) for images and e5-large-v2 (Wang et\\xa0al., 2022 ) for text for training efficiency. For evaluation, we also report results with the multi-vector retrievers ColQwen2-v1.0 (ColQwen) (Faysse et\\xa0al., 2025 ) for images and ColBERT-v2.0 (ColBERT) (Santhanam et\\xa0al., 2021 ) for text.\\nAll experiments are conducted on 16 NVIDIA A100-80Gb GPUs.\\nThe system prompt that we used during training of Visual RAG variant of ALDEN is shown here:\\nThe system prompt that we used during training of Hybrid RAG variant of ALDEN is shown here',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Evaluation Metrics',\n",
       "    'paragraphs': 'We evaluate models using both answer quality and intermediate navigation metrics.\\nModel-based Accuracy (Acc). Answer quality is assessed with an LLM-as-judge protocol.\\nGiven a predicted answer and the ground-truth reference, GPT-4o is prompted to classify the prediction as Correct , Incorrect , or Tie/Unclear .\\nWe compute accuracy for each benchmark as the percentage of responses judged Correct over all responses:\\nwhere N N is the number of test instances.\\nTrajectory-level Recall (Rec). Let ùí¢ \\\\mathcal{G} denote the set of ground-truth evidence pages for a given query,\\nand let ùíØ \\\\mathcal{T} denote the set of pages collected by the agent along a trajectory.\\nThe trajectory-level recall is defined as:\\nThis metric measures the fraction of ground-truth pages successfully retrieved by the agent\\nover the course of a trajectory, providing an indicator of how effectively the agent gathers relevant information.\\nTrajectory-level Prevision (Pre). Let ùí¢ \\\\mathcal{G} denote the set of ground-truth evidence pages for a given query,\\nand let ùíØ \\\\mathcal{T} denote the set of pages collected by the agent along a trajectory.\\nThe trajectory-level precision is defined as:\\nThis metric measures the fraction of pages collected by the agent that are actually relevant,\\nproviding an indicator of how accurately the agent identifies evidence pages during a trajectory.\\nF1 Score (F1). Based on the trajectory-level precision and recall,\\nthe trajectory-level F1 score is defined as the harmonic mean of the two:\\nThis metric provides a balanced measure of the agent‚Äôs performance,\\naccounting for both its ability to collect relevant pages (recall)\\nand to avoid collecting irrelevant ones (precision) over a trajectory.\\nNumber of uniquely collected pages (#UP).',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix E Case Study',\n",
       "    'paragraphs': 'In this section, we present typical examples from the LongDocURL benchmark to analyze the behavior of the ALDEN model.\\nAs illustrated in the successful cases in table 6 and table 7 , the agent effectively learns to combine the search and fetch actions to gather evidence and correctly answer the question.\\nIn particular, the example in table 7 shows the agent using fetch to complement search when the latter fails to retrieve the true evidence page in a single step, highlighting the benefit of ALDEN ‚Äôs expanded action space.\\nAlongside these successes, we also present failure cases in table 8 and table 9 .\\nThese examples reveal that the model still struggles to verify evidence pages and explore sufficiently when the search action returns results far from the ground-truth pages‚Äîa challenge we leave for future work.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Vision‚Äìlanguage models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization.\\n We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents.\\n ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure.\\n For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals.\\n To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training.\\n Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks.\\n Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.'},\n",
       " '2510.25626v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Large language models (LMs) appear capable of solving a wide range of tasks that rely on deductive reasoning, particularly when post-trained with reinforcement learning (Lightman et\\xa0al., 2024 ; DeepSeek-AI, 2025 ) and scaled to use more compute at test time (Wang et\\xa0al., 2024 ; Muennighoff et\\xa0al., 2025 ; Snell et\\xa0al., 2025 ) .\\nHowever, emerging findings suggest recent reasoning models often generate more tokens than necessary to solve problems, even for simple deductive tasks (Chen et\\xa0al., 2025 ; Pu et\\xa0al., 2025 ) .\\nSuch findings point towards a key dimension of deductive reasoning that standard evaluations of LMs‚Äô reasoning abilities fail to systematically assess‚Äî efficiency .\\nIndeed, more abstractly, in most real-world reasoning tasks, more information is available than is necessary to solve the problem.\\nThis spurious information is not random: it often interacts with relevant information, enabling the derivation of true but irrelevant conclusions.\\nCrucially, it is unknown a priori which pieces of information will be relevant for determining whether a desired conclusion is supported by the evidence. An efficient solution to a problem uses only necessary information and takes as few steps as possible.\\nTo characterize efficiency, we adopt a formalization of deductive reasoning based on logic programming (Kowalski, 1974 ) .\\nOur perspective is that logic programming provides a clean and flexible framework for reasoning within a well-understood proof system.\\nGiven a logic program‚Äîthat is, a set of inference rules and axioms‚Äîa proof of some goal theorem can be viewed as a path in a hypergraph induced by the inference rules, starting from vertices corresponding to axioms and terminating at a vertex corresponding to the goal theorem.\\nThen, the most efficient proof is simply a shortest such path.\\nUsing this machinery, the goal of this paper is to evaluate a language model‚Äôs reasoning efficiency.\\nThus, we require an additional mechanism to bridge the gap between reasoning in logic programming and reasoning in natural language.\\nTo this end, we introduce the notion of a verbalized logic program, in which each theorem in the logic program is associated with a set of natural language strings.\\nVerbalized logic programs allow us to map the deductions performed by an LM‚Äîexpressed as a natural language proof‚Äîonto deductions performed during the execution of a logic program.\\nWhile numerous recent papers use number of generated tokens as a proxy for efficiency (Arora & Zanette, 2025 ; Han et\\xa0al., 2025 ; Ma et\\xa0al., 2025 ) , doing so conflates inefficiency stemming from two separate sources: (1) unnecessary deduction steps and (2) verbosity in the natural language strings expressing those deduction steps.\\nIn contrast, our framework disentangles these two factors, with our paper‚Äôs experiments emphasizing the former.\\nEmpirically, we construct verbalized logic programs for grade school math word problems (GSM problems; Cobbe et\\xa0al., 2021 ; Li et\\xa0al., 2024 ; Zhang et\\xa0al., 2024 ) , adopting methods from Opedal et\\xa0al. ( 2025 ) .\\nOur primary experimental manipulative is the injection of irrelevant axioms into these GSM programs,\\nwhich yields many possible implications that are irrelevant to a goal theorem of interest.\\nWe experiment on problems that vary both in how much the information in the irrelevant axioms overlap with the goal theorem, as well as in how many such axioms are injected,\\ngeneralizing existing datasets that only include a single irrelevant axiom (Shi et\\xa0al., 2023 ; Mirzadeh et\\xa0al., 2025 ) .\\nWe first measure accuracy, showing that current LMs are less accurate on problems containing irrelevant axioms than on equivalent problems without them.\\nThis performance gap often persists even in the simplest cases, where a single irrelevant axiom from the same domain is introduced, and grows larger as more irrelevant axioms are added.\\nWe confirm that this reduction in accuracy is not due to longer inputs alone: LMs usually perform better on control problems of equal length but without irrelevant content.\\nNext, we map the reasoning performed by the LM onto theorems they correspond to when executing the logic program.\\nWe find that while the LMs predict most of the correct intermediate theorems for the problems where they correctly generate the goal, they are often inefficient.\\nIn particular, for GSM problems where about half of the axioms are irrelevant, about half of the model‚Äôs predicted theorems are irrelevant too, i.e., not needed for proving the goal.\\nThe LMs are particularly inefficient when the irrelevant axioms overlap semantically with the query‚Äîfor instance, when the question asks ‚Äúhow many cats does Ryan have?‚Äù and the irrelevant axioms also mention ‚ÄúRyan‚Äù or ‚Äúcats‚Äù .\\nOn the other hand, these results also suggest that the LMs‚Äô search procedure sometimes employ a useful heuristic based on such overlap.\\nOur results shed some light on how LMs, albeit in natural language, perform inference.\\nThe remainder of the paper is structured as follows: ¬ß Àú 2 situates our contributions among related work. ¬ß Àú 3 provides relevant background on logic programming and discusses how reasoning efficiency is measured relative to shortest proofs. ¬ß Àú 4 introduces verbalized logic programs and the specifics of our GSM programs. ¬ß Àú 5 presents experiments and results on how LMs reason on verbalized GSM programs with irrelevant axioms. Apps. A - D give further technical details and empirical results.',\n",
       "    'subsections': [{'title': 'Outline.',\n",
       "      'paragraphs': 'The remainder of the paper is structured as follows: ¬ß Àú 2 situates our contributions among related work. ¬ß Àú 3 provides relevant background on logic programming and discusses how reasoning efficiency is measured relative to shortest proofs. ¬ß Àú 4 introduces verbalized logic programs and the specifics of our GSM programs. ¬ß Àú 5 presents experiments and results on how LMs reason on verbalized GSM programs with irrelevant axioms. Apps. A - D give further technical details and empirical results.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Most studies and benchmarks on LM reasoning evaluate correctness based on the LM‚Äôs final answer (Hendrycks et\\xa0al., 2021 ; Patel et\\xa0al., 2021 ; Rein et\\xa0al., 2024 ; Yu et\\xa0al., 2024 , inter alia ) .\\nHowever, correctness of the final answer does not guarantee correctness of the proof (Lyu et\\xa0al., 2023 ; Turpin et\\xa0al., 2023 ) .\\nSome studies include more fine-grained reasoning evaluations by\\nverifying LM-generated proofs (Gontier et\\xa0al., 2020 ; Frieder et\\xa0al., 2023 ; Nguyen et\\xa0al., 2024 ; Wang et\\xa0al., 2024 ; Petrov et\\xa0al., 2025 ) .\\nWhile useful, many such evaluations rely either on manual scrutiny or heuristic measures of the proof‚Äôs correctness.\\nAn alternative approach is to use proof assistants, e.g., Lean (de\\xa0Moura & Ullrich, 2021 ) , for formal verification (First et\\xa0al., 2023 ; Tsoukalas et\\xa0al., 2024 ) ; however, LMs may have been trained on less amounts of such data as compared to natural language. We perform an automatic evaluation by parsing the LM-generated output into proofs in logic programs.\\nOur work relates to papers that evaluate LMs‚Äô ability to correctly solve problems with irrelevant information (or missing information; Li et\\xa0al., 2025 ). Shi et\\xa0al. ( 2023 ) create such a dataset by appending a single irrelevant statement to problems taken from GSM8k (Cobbe et\\xa0al., 2021 ) . Mirzadeh et\\xa0al. ( 2025 ) seem to take a similar, albeit more manual approach; however, details presented are scarce and their dataset has not yet been made publicly available. Anantheswaran et\\xa0al. ( 2025 ) use a prompting-based method for augmenting problems with several irrelevant statements.\\nWe formalize the notion of irrelevance through logic programming and generalize these previous approaches by generating problems that may have several, arbitrarily placed irrelevant axioms, which can be used together in further inference. Thus, there are many implications that are irrelevant to the goal theorem and the challenge becomes not only to generate correct proofs, but proofs that only contain steps that are necessary . By generating new problems from scratch, our approach avoids bias from memorizing the efficient solution seen during training (see, e.g., Zhang et\\xa0al., 2024 ).\\nOther studies have investigated whether and how transformers can learn search tasks (Gandhi et\\xa0al., 2023 , 2024 ; Kazemi et\\xa0al., 2023 ; Lehnert et\\xa0al., 2024 ; Sanford et\\xa0al., 2024 ; Sel et\\xa0al., 2024 ; Shah et\\xa0al., 2024 ; Saparov et\\xa0al., 2025 ) .\\nWe are interested not only in whether a transformer-based LM can perform accurate search, but in how efficient it is. Efficiency of large (reasoning-based) LMs is a rapidly growing area of research (Sui et\\xa0al., 2025 ) ,\\ndue to their often lavish use of compute (Chen et\\xa0al., 2025 ; Pu et\\xa0al., 2025 ) . Several methods have been proposed to make reasoning more efficient (Han et\\xa0al., 2025 ; Ma et\\xa0al., 2025 ) , e.g., by incorporating length rewards in training (Arora & Zanette, 2025 ; Luo et\\xa0al., 2025 ; Team et\\xa0al., 2025 ) . While these papers focus solely on the number of tokens, we argue that it is more informative to measure efficiency based on the natural language proof, since a long output can be explained either by unnecessary inference steps or by ‚Äúverbose‚Äù verbalizations of the proof. Moreover, an LM should avoid generating redundant tokens (Xia et\\xa0al., 2025 ) , but it should also not skip necessary inference steps in favor of a shorter output.',\n",
       "    'subsections': [{'title': 'Evaluating Reasoning.',\n",
       "      'paragraphs': 'Most studies and benchmarks on LM reasoning evaluate correctness based on the LM‚Äôs final answer (Hendrycks et\\xa0al., 2021 ; Patel et\\xa0al., 2021 ; Rein et\\xa0al., 2024 ; Yu et\\xa0al., 2024 , inter alia ) .\\nHowever, correctness of the final answer does not guarantee correctness of the proof (Lyu et\\xa0al., 2023 ; Turpin et\\xa0al., 2023 ) .\\nSome studies include more fine-grained reasoning evaluations by\\nverifying LM-generated proofs (Gontier et\\xa0al., 2020 ; Frieder et\\xa0al., 2023 ; Nguyen et\\xa0al., 2024 ; Wang et\\xa0al., 2024 ; Petrov et\\xa0al., 2025 ) .\\nWhile useful, many such evaluations rely either on manual scrutiny or heuristic measures of the proof‚Äôs correctness.\\nAn alternative approach is to use proof assistants, e.g., Lean (de\\xa0Moura & Ullrich, 2021 ) , for formal verification (First et\\xa0al., 2023 ; Tsoukalas et\\xa0al., 2024 ) ; however, LMs may have been trained on less amounts of such data as compared to natural language. We perform an automatic evaluation by parsing the LM-generated output into proofs in logic programs.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Irrelevant Information in Reasoning Tasks.',\n",
       "      'paragraphs': 'Our work relates to papers that evaluate LMs‚Äô ability to correctly solve problems with irrelevant information (or missing information; Li et\\xa0al., 2025 ). Shi et\\xa0al. ( 2023 ) create such a dataset by appending a single irrelevant statement to problems taken from GSM8k (Cobbe et\\xa0al., 2021 ) . Mirzadeh et\\xa0al. ( 2025 ) seem to take a similar, albeit more manual approach; however, details presented are scarce and their dataset has not yet been made publicly available. Anantheswaran et\\xa0al. ( 2025 ) use a prompting-based method for augmenting problems with several irrelevant statements.\\nWe formalize the notion of irrelevance through logic programming and generalize these previous approaches by generating problems that may have several, arbitrarily placed irrelevant axioms, which can be used together in further inference. Thus, there are many implications that are irrelevant to the goal theorem and the challenge becomes not only to generate correct proofs, but proofs that only contain steps that are necessary . By generating new problems from scratch, our approach avoids bias from memorizing the efficient solution seen during training (see, e.g., Zhang et\\xa0al., 2024 ).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Search and Efficiency.',\n",
       "      'paragraphs': 'Other studies have investigated whether and how transformers can learn search tasks (Gandhi et\\xa0al., 2023 , 2024 ; Kazemi et\\xa0al., 2023 ; Lehnert et\\xa0al., 2024 ; Sanford et\\xa0al., 2024 ; Sel et\\xa0al., 2024 ; Shah et\\xa0al., 2024 ; Saparov et\\xa0al., 2025 ) .\\nWe are interested not only in whether a transformer-based LM can perform accurate search, but in how efficient it is. Efficiency of large (reasoning-based) LMs is a rapidly growing area of research (Sui et\\xa0al., 2025 ) ,\\ndue to their often lavish use of compute (Chen et\\xa0al., 2025 ; Pu et\\xa0al., 2025 ) . Several methods have been proposed to make reasoning more efficient (Han et\\xa0al., 2025 ; Ma et\\xa0al., 2025 ) , e.g., by incorporating length rewards in training (Arora & Zanette, 2025 ; Luo et\\xa0al., 2025 ; Team et\\xa0al., 2025 ) . While these papers focus solely on the number of tokens, we argue that it is more informative to measure efficiency based on the natural language proof, since a long output can be explained either by unnecessary inference steps or by ‚Äúverbose‚Äù verbalizations of the proof. Moreover, an LM should avoid generating redundant tokens (Xia et\\xa0al., 2025 ) , but it should also not skip necessary inference steps in favor of a shorter output.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Logic Programming and Deductive Reasoning',\n",
       "    'paragraphs': 'This section provides relevant background on logic programming (Kowalski, 1974 ) .\\nIt also introduces the metric we propose for scoring a proof‚Äôs efficiency in ¬ß Àú 3.2 .\\nA signature is a 3 3 -tuple Œ£ = ( Œ£ p , Œ£ x , Œ£ X ) {\\\\Sigma}=({{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) , where Œ£ p {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}} is a set of predicate (or relation ) symbols,\\ndenoted p , q , r , . . . {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{r}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. ; Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} is a set of constants, denoted x , y , z , . . . {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{y}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{z}}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. ; Œ£ X {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} is a set of variables, denoted X , Y , Z , . . . {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. 2 2 2 Many logic programming languages, e.g., Prolog (Colmerauer & Roussel, 1993 ; K√∂rner et\\xa0al., 2022 ) , additionally have the notion of a function.\\nConstants are then just nullary functions.\\nOur notion of logic programming is most similar to Datalog (Vardi, 1982 ; Maier et\\xa0al., 1984 ; Ceri et\\xa0al., 1989 ) , which does not. Every predicate is associated with an arity, which we denote using the function arity ar : Œ£ p ‚Üí ‚Ñï {\\\\small\\\\textsf{ar}}\\\\colon{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}}\\\\rightarrow\\\\mathbb{N} , specifying how many arguments it takes.\\nArguments to predicates are called terms ; they can be either constants ( ground terms) or variables ( non-ground terms).\\nAn atomic formula , called an atom , for short, is an expression of the form p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) , where p ‚àà Œ£ p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}} is a predicate symbol of arity ar \\u200b ( p ) = N {\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N and t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} are all terms.\\nThe Herbrand base H H for signature Œ£ {\\\\Sigma} is the set of all atoms that can be formed by terms in Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , i.e., H = { p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x } H=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\} . 3 3 3 In the case that the signature additionally contains a set of function symbols Œ£ f {{\\\\Sigma}_{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}}} , the Herbrand base is defined as the set of all atoms that can be formed by all terms in the Herbrand universe , which is the smallest set U U that satisfies the equation U = Œ£ x ‚à™ { f ( t 1 , . . . , t N ) ‚à£ f ‚àà Œ£ f , ar ( f ) = N , t 1 , . . . , t N ‚àà U } U={{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup\\\\{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in U\\\\} . Subsets of the Herbrand base I ‚äÜ H I\\\\subseteq H are called interpretations .\\nAn inference rule is an expression of the form b 1 , . . . , b N ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{N}\\\\vdash h , where b 1 , . . . , b K , h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K},h are atoms; b 1 , . . . , b K b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K} is the b ody of the rule and h h is the h ead .\\nFor example,\\nis an inference rule that allows us to conclude that if X is a parent of Y and Y is an ancestor of Z , then X is an ancestor of Z .\\nWe call b 1 , . . . , b K b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K} , i.e., parent \\u200b ( X , Y ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}) and ancestor \\u200b ( Y , Z ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}) in the example above, the premises and h h , i.e., ancestor \\u200b ( X , Z ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}) in the example above, the conclusion .\\nAn inference rule is called range restricted if each variable appearing in the conclusion h h also appears in at least one atom b k b_{k} in the premise. For example, p \\u200b ( X ) ‚ä¢ q \\u200b ( X ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}})\\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}) is range restricted, while p \\u200b ( X ) ‚ä¢ q \\u200b ( X , Y ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}})\\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}) is not.\\nIn this paper, we require all inference rules to be range restricted. 4 4 4 Range restriction ensures that applying the fixpoint operator ( Eq. 1 ) does not create non-ground atoms. Inference rules with a null premise, i.e., where K = 0 K=0 , and a ground conclusion, i.e., where h ‚àà H h\\\\in H , are called axioms . A set of axioms is denoted A A .\\nFor example, ‚ä¢ parent \\u200b ( abraham , isaac ) \\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) or, also written parent \\u200b ( abraham , isaac ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) , omitting the ‚ä¢ \\\\vdash symbol, is an axiom.\\nA logic program ùí´ \\\\mathcal{P} over a signature Œ£ {\\\\Sigma} is a set of inference rules in which all atoms are formed by symbols in Œ£ {\\\\Sigma} .\\nThe following is an example logic program, adapted from Sterling & Shapiro ( 1994 , ¬ß5) :\\nOur notion of logic programming additionally includes types (Abiteboul et\\xa0al., 1995 , ¬ß21.1) and built-ins (Kaminski et\\xa0al., 2017 ) , which we define here.\\nWe partition Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and Œ£ X {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} into T T disjoint subsets, i.e., Œ£ x = Œ£ x 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî Œ£ x T {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}={{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{1}}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{T}} and Œ£ X = Œ£ X 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî Œ£ X T {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}={{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{1}}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{T}} , respectively, and associate each subset with a type . These subsets are paired index-wise, i.e., ( Œ£ x 1 , Œ£ X 1 ) , . . . , ( Œ£ x T , Œ£ X T ) ({{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{1}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{1}}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,({{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{T}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{T}}) , ensuring that the constant and variable types match.\\nIn this paper, we consider three types: (i) natural numbers, denoted ( ‚Ñï x , ‚Ñï X ) (\\\\mathbb{N}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}},\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}) , (ii) strings, ( Œî x ‚àó , Œî X ‚àó ) ({{\\\\Delta}^{*}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{\\\\Delta}^{*}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) , and (iii) sets of strings, ( 2 x Œî ‚àó , 2 X Œî ‚àó ) ({2^{{\\\\Delta}^{\\\\!*}}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{2^{{\\\\Delta}^{\\\\!*}}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) .\\nOur introduction of types is necessitated by our desire to add additional power to our notion of logic programming that is external to the language itself.\\nSpecifically, we will introduce built-in predicates , simply called built-ins through the exposition, that add various arithmetic and set-theoretic operations.\\nWe enumerate these operations: X ‚Ñ§ + Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Addition) , \\\\displaystyle\\\\text{(Integer Addition)}, X ‚Ñ§ ‚àí Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{-}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Subtraction) , \\\\displaystyle\\\\text{(Integer Subtraction)}, X ‚Ñ§ √ó Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\times}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Multiplication) , \\\\displaystyle\\\\text{(Integer Multiplication)}, X ‚Ñ§ = Y ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}} (Integer Equality) , \\\\displaystyle\\\\text{(Integer Equality)}, X ‚Ñ§ ‚â• Y ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\geq}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}} (Integer Comparison) , \\\\displaystyle\\\\text{(Integer Comparison)}, X 2 Œî ‚àó ‚à™ Y 2 Œî ‚àó = Z 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\cup}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Union) , \\\\displaystyle\\\\text{(Set Union)}, X 2 Œî ‚àó ‚à© Y 2 Œî ‚àó = Z 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\cap}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Intersection) , \\\\displaystyle\\\\text{(Set Intersection)}, | X 2 Œî ‚àó | = X ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}} (Set Cardinality) , \\\\displaystyle\\\\text{(Set Cardinality)}, X 2 Œî ‚àó = Y 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Equality) , \\\\displaystyle\\\\text{(Set Equality)}, | X 2 Œî ‚àó | ‚â• X ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\geq}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}} (Set Cardinality Comparison).\\nThe truth value of grounded atoms constructed from built-in predicates is evaluated externally to the logic program.\\nTo do so, we define the built-in evaluator eval : H ‚Üí { T , F } {\\\\small\\\\textsf{eval}}\\\\colon H\\\\rightarrow\\\\{\\\\texttt{T},\\\\texttt{F}\\\\} , that maps all ground built-ins that evaluate to true to T and all ground built-ins that evaluate to false to F .\\nAdditionally, any element of H H that is not a built-in evaluates to F .\\nFor example, eval \\u200b ( 5 + 4 = 9 ) = T {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{9}}})=\\\\texttt{T} , eval \\u200b ( 5 + 4 = 10 ) = F {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{10}}})=\\\\texttt{F} , and eval \\u200b ( parent \\u200b ( abraham , isaac ) ) = F {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}))=\\\\texttt{F} .\\nFor example, to illustrate the use of built-ins in a logic program, we can extend the inference rules from the earlier example to measure the depth of the ancestor relation (e.g., parent, grandparent, great-grandparent, etc):\\nTo assign semantics to a logic program, we require a bit more machinery.\\nA substitution Œ∏ \\\\theta is a finite set of pairs { ( X m , t m ) } m = 1 M \\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m},t_{m})\\\\}_{m=1}^{M} , where X m ‚àà Œ£ X {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} , t m ‚àà Œ£ x ‚à™ Œ£ X t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} , X m ‚â† X m ‚Ä≤ {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\neq{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m^{\\\\prime}} for all m ‚â† m ‚Ä≤ m\\\\neq m^{\\\\prime} , and X m ‚â† t m {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\neq t_{m} for all m m (Sterling & Shapiro, 1994 , p.\\xa014) .\\nAdditionally, a typed substitution is a substitution where, if X m ‚àà Œ£ X k {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{k}} , then t m ‚àà Œ£ x k ‚à™ Œ£ X k t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{k}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{k}} .\\nWe can apply a substitution Œ∏ \\\\theta to an atom b b , denoted b / Œ∏ b/\\\\theta , e.g., parent \\u200b ( X , isaac ) / { ( X , abraham ) } = parent \\u200b ( abraham , isaac ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}})/\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}})\\\\}={\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) .\\nLet Œò \\u200b ( ùí´ ) \\\\Theta(\\\\mathcal{P}) be the set of all typed substitutions under ùí´ \\\\mathcal{P} .\\nWe say that b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ b^{\\\\prime}_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b^{\\\\prime}_{K}\\\\vdash h^{\\\\prime} is an instantiation of b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h if there exists a\\nsubstitution Œ∏ ‚àà Œò \\u200b ( ùí´ ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}) such that b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ = b 1 / Œ∏ , . . . , b K / Œ∏ ‚ä¢ h / Œ∏ b^{\\\\prime}_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b^{\\\\prime}_{K}\\\\vdash h^{\\\\prime}=b_{1}/\\\\theta,.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}/\\\\theta\\\\vdash h/\\\\theta .\\nIf an instantiation has no variables, we call it a ground instantiation . In addition, we say that an atom b b unifies with b ‚Ä≤ b^{\\\\prime} if ‚àÉ Œ∏ ‚àà Œò \\u200b ( ùí´ ) : b / Œ∏ = b ‚Ä≤ / Œ∏ \\\\exists\\\\theta\\\\in\\\\Theta(\\\\mathcal{P}):b/\\\\theta=b^{\\\\prime}/\\\\theta .\\nIf b b unifies with b ‚Ä≤ b^{\\\\prime} we write b ‚â° b ‚Ä≤ b\\\\equiv b^{\\\\prime} .\\nWe define a logic program ùí´ \\\\mathcal{P} ‚Äôs fixpoint operator T ùí´ : 2 H ‚Üí 2 H \\\\mathrm{T}_{\\\\mathcal{P}}\\\\colon 2^{H}\\\\to 2^{H} as\\nwhere I ‚äÜ H I\\\\subseteq H is an interpretation.\\nThe fixpoint operator T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} is inflationary , i.e., for every interpretation I ‚äÜ H I\\\\subseteq H , we have b ‚àà I ‚üπ b ‚àà T ùí´ \\u200b ( I ) b\\\\in I\\\\Longrightarrow b\\\\in\\\\mathrm{T}_{\\\\mathcal{P}}(I) , and monotone , i.e., for every pair of interpretations I 1 , I 2 ‚äÜ H I_{1},I_{2}\\\\subseteq H , we have I 1 ‚äÜ I 2 ‚üπ T ùí´ \\u200b ( I 1 ) ‚äÜ T ùí´ \\u200b ( I 2 ) I_{1}\\\\subseteq I_{2}\\\\Longrightarrow\\\\mathrm{T}_{\\\\mathcal{P}}(I_{1})\\\\subseteq\\\\mathrm{T}_{\\\\mathcal{P}}(I_{2}) .\\nThat T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} is monotone allows us to employ least fixpoint semantics.\\nTo that end, we define the minimal Herbrand model as M = T ùí´ ‚àó \\u200b ( A ) = def ‚ãÉ n = 0 ‚àû T ùí´ n \\u200b ( A ) M=\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{n=0}^{\\\\infty}\\\\mathrm{T}^{n}_{\\\\mathcal{P}}(A) , where T ùí´ n \\\\mathrm{T}^{n}_{\\\\mathcal{P}} denotes the n n -fold application of the fixpoint operator T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} , i.e., M M is T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} ‚Äôs least fixpoint. 5 5 5 Inflationarity is not needed to prove that the minimal Herbrand model M M exists.\\nIndeed, monotonicity and the fact that interpretations of the Herbrand base form a complete lattice suffice to apply Tarski ‚Äôs ( 1955 ) theorem, which guarantees the existence of the least fixpoint.\\nHowever, inflationarity does guarantee that, as we iteratively apply T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} , convergence to the least fixpoint is monotone. Thus, M M is the subset of the Herbrand base that is true given the axioms and inference rules in the program; we call elements of M M theorems .\\nDue to our inclusion of built-ins that encompass basic arithmetic operations, it is undecidable to compute M M (Dantsin et\\xa0al., 2001 ) , i.e.,\\nin general, we cannot decide whether h ‚àà T ùí´ ‚àó \\u200b ( A ) h\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A) for an arbitrary h ‚àà H h\\\\in H .\\nGiven a logic program ùí´ \\\\mathcal{P} , we are often interested in determining whether there exists a theorem in ùí´ \\\\mathcal{P} that is an instantiation of a specific (possibly non-ground) atom.\\nWe refer to such atoms as queries . 6 6 6 In principle, queries may also be ground; however, only the non-ground case is of theoretical interest here, as it extends beyond what can be handled by the machinery introduced so far. For example, building on the running example drawn from Sterling & Shapiro ( 1994 , ¬ß5) , we may wish to ask whether there exists a theorem that instantiates the atom ancestor \\u200b ( X , ishmael ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{ishmael}}}) .\\nAnswering such a query amounts to finding all substitutions for X that make the atom provable from ùí´ \\\\mathcal{P} .\\nWhen X can take on infinitely many instantiations, more sophisticated inference mechanisms‚Äîmost notably, unification (Robinson, 1965 ) ‚Äîare required to perform this kind of non-ground reasoning effectively.\\nIn this paper, however, we restrict attention to queries in which each variable is known a priori to range over a fixed, finite domain, which allows us to avoid additional complexities.\\nA hypergraph ùí¢ \\\\mathcal{G} (B-hypergraph; Gallo et\\xa0al., 1993 ) is a tuple ( V , E ) (V,E) , where V V is a set of vertices, and E ‚äÜ 2 V √ó V E\\\\subseteq 2^{V}\\\\times V is a set of hyperedges , where a hyperedge e = T ‚Ü£ v h e=T\\\\rightarrowtail{v}_{h} consists of a t ail T ‚äÜ V T\\\\subseteq V , with | T | > 0 |T|>0 , and a h ead v h ‚àà V {v}_{h}\\\\in V .\\nWe define the size of a hypergraph as | ùí¢ | = def | V | |\\\\mathcal{G}|\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}|V| where ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) . 7 7 7 We note that this is non-standard; the size of a hypergraph is more often defined as its number of hyperedges or the sum of the cardinalities of its hyperedges (Gallo et\\xa0al., 1993 ) . We use this definition to sync with the experimental setup, which we explain in ¬ß 5 .\\nFuture work could easily adapt our efficiency metric to other definitions. A subhypergraph of a hypergraph ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) is a hypergraph ùí¢ ‚Ä≤ = ( V ‚Ä≤ , E ‚Ä≤ ) \\\\mathcal{G}^{\\\\prime}=(V^{\\\\prime},E^{\\\\prime}) where V ‚Ä≤ ‚äÜ V V^{\\\\prime}\\\\subseteq V and E ‚Ä≤ ‚äÜ E E^{\\\\prime}\\\\subseteq E .\\nGiven S ‚äÜ V S\\\\subseteq V and v ‚àà V v\\\\in V , an ( S , v ) (S,v) - hyperpath in a hypergraph ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) is a finite sequence of distinct hyperedges T 1 ‚Ü£ v h 1 , . . . , T J ‚Ü£ v h J T_{1}\\\\rightarrowtail{v}_{h_{1}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,T_{J}\\\\rightarrowtail{v}_{h_{J}} such that v h J = v {v}_{h_{J}}=v and for every j ‚àà [ J ] : T j ‚äÜ S ‚à™ { v h 1 , . . . , v h j ‚àí 1 } j\\\\in[J]\\\\colon T_{j}\\\\subseteq S\\\\cup\\\\{{v}_{h_{1}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,{v}_{h_{j-1}}\\\\} , i.e., each hyperedge‚Äôs tail consists only of nodes that are either in the source set S S or are heads of previous hyperedges in the sequence.\\nA hyperpath generalizes the notion of a directed path in a graph, but allows each hyperedge to have multiple tail nodes that jointly produce a head node.\\nFinding the shortest ( S , v ) (S,v) -hyperpath in a hypergraph is analogous to context-free parsing (Klein & Manning, 2001 ) and can be executed in polynomial time.\\nLet ùí´ \\\\mathcal{P} be a logic program.\\nA proof forest ( ‚Ñ± , ‚Ñì ) (\\\\mathcal{F},\\\\ell) in ùí´ \\\\mathcal{P} is a pair\\nwhere ‚Ñ± = ( E , V ) \\\\mathcal{F}=(E,V) is a hypergraph and ‚Ñì : V ‚Üí H \\\\ell\\\\colon V\\\\rightarrow H where H H is ùí´ \\\\mathcal{P} ‚Äôs Herbrand base (Heijltjes, 2010 ) .\\nAdditionally, we require that, for every hyperedge e = { t 1 , . . . , t K } ‚Ü£ v h ‚àà E e=\\\\{t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{K}\\\\}\\\\rightarrowtail{v}_{h}\\\\in E , there exists a rule ( b 1 , . . . , b K ‚ä¢ h ) ‚àà ‚Ñõ (b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h)\\\\in\\\\mathcal{R} and a substitution Œ∏ ‚àà Œò \\u200b ( ùí´ ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}) such that b 1 / Œ∏ = ‚Ñì ( t 1 ) , . . . , b K / Œ∏ = ‚Ñì ( t K ) b_{1}/\\\\theta=\\\\ell(t_{1}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}/\\\\theta=\\\\ell(t_{K}) and h / Œ∏ = ‚Ñì \\u200b ( v h ) h/\\\\theta=\\\\ell({v}_{h}) .\\nWe call a proof forest ( ‚Ñ± , ‚Ñì ) (\\\\mathcal{F},\\\\ell) an ( A , h g ) (A,h_{g}) - proof if there exists a ( ‚Ñì ‚àí 1 \\u200b ( A ) , ‚Ñì ‚àí 1 \\u200b ( h g ) ) (\\\\ell^{-1}(A),\\\\ell^{-1}(h_{g})) -hyperpath in ‚Ñã ùí´ \\\\mathcal{H}_{\\\\mathcal{P}} . 8 8 8 We note that the expression ( A , h g ) (A,h_{g}) -hyperpath is a slight abuse of notation in the case that ‚Ñì \\\\ell is not injective adopted for convenience: A A and h g h_{g} are a subset and an element, respectively, of ùí´ \\\\mathcal{P} ‚Äôs Herbrand base‚Äînot of ‚Ñõ ùí´ \\\\mathcal{R}_{\\\\mathcal{P}} ‚Äôs V V . In this context, by A A , we refer to a set X ‚äÜ ‚Ñì ‚àí 1 \\u200b ( A ) X\\\\subseteq\\\\ell^{-1}(A) where ‚Ñì \\u200b ( X ) = A \\\\ell(X)=A , and by h g h_{g} we refer to a set Y ‚äÜ ‚Ñì ‚àí 1 \\u200b ( h g ) Y\\\\subseteq\\\\ell^{-1}(h_{g}) where ‚Ñì \\u200b ( Y ) = h g \\\\ell(Y)=h_{g} . In Fig. Àú 1 , we show an example of a proof in which h g = cont \\u200b ( { a,b,c } , 25 , apple , 2 ) h_{g}={\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{a,b,c\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{25}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{apple}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) in our custom logic program for math word problems ( ¬ß Àú 4 ), together with some axioms in the program that do not contribute to the proof of the goal theorem h g h_{g} .\\nWe call an ( A , h g ) (A,h_{g}) -proof a shortest proof if it has the least number of vertices of all ( A , h g ) (A,h_{g}) -proofs in ùí´ \\\\mathcal{P} .\\nA shortest proof can be found by forward-chaining, discussed in the subsequent paragraph.\\nNow we turn to measuring proof efficiency.\\nConsider an ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} in ùí´ \\\\mathcal{P} .\\nWe define the efficiency of ùí´ \\\\mathcal{P} as efficiency \\u200b ( ùí´ ) = def | ùí´ ‚ãÜ | / | ùí´ | \\\\textsc{efficiency}(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}|\\\\mathcal{P}^{\\\\star}|/|\\\\mathcal{P}| where | ùí´ ‚ãÜ | |\\\\mathcal{P}^{\\\\star}| is the number of vertices in a shortest ( A , h g ) (A,h_{g}) -proof.\\nIn the remainder of the paper, we will refer to an axiom a a as irrelevant if there does not exist a shortest proof ùí´ \\\\mathcal{P} that contains a vertex v v such that ‚Ñì \\u200b ( v ) = a \\\\ell(v)=a .\\nForward chaining (Hayes-Roth et\\xa0al., 1983 ; Poole & Mackworth, 2017 ) is a meta-strategy for theorem proving in logic programming that proceeds from the axioms toward the goal theorem.\\nThe process terminates once the goal theorem is proved.\\nPseudocode for the forward-chaining is given in Alg. Àú 1 in ¬ß Àú A.2 .\\nAs a meta-strategy, each instance of forward chaining defines an ordering over proof steps.\\nDifferent orderings give rise to familiar search algorithms, such as depth-first search (DFS; Tarjan, 1972 ) , breadth-first search (BFS; Moore, 1959 ) , Dijkstra‚Äôs algorithm (Dijkstra, 1959 ) , and heuristic-based search (Pearl, 1984 ) like A* (Hart et\\xa0al., 1968 ) .\\nWhile DFS and BFS ignore information about the goal theorem, such information can guide search more efficiently, as exemplified by goal-aware strategies like Earley‚Äôs algorithm (Earley, 1970 ) for context-free parsing or its more general equivalent in logic programming‚Äîmagic templates (Bancilhon et\\xa0al., 1986 ; Ramakrishnan, 1991 ) .\\nIn the spirit of using top-down information in proof search, in ¬ß Àú 5 , we examine whether LMs make use of lexical overlap with the goal theorem as part of their internal search heuristic.',\n",
       "    'subsections': [{'title': '3.1 Typed Logic Programs with Built-ins',\n",
       "      'paragraphs': 'A signature is a 3 3 -tuple Œ£ = ( Œ£ p , Œ£ x , Œ£ X ) {\\\\Sigma}=({{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) , where Œ£ p {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}} is a set of predicate (or relation ) symbols,\\ndenoted p , q , r , . . . {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{r}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. ; Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} is a set of constants, denoted x , y , z , . . . {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{y}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{z}}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. ; Œ£ X {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} is a set of variables, denoted X , Y , Z , . . . {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. 2 2 2 Many logic programming languages, e.g., Prolog (Colmerauer & Roussel, 1993 ; K√∂rner et\\xa0al., 2022 ) , additionally have the notion of a function.\\nConstants are then just nullary functions.\\nOur notion of logic programming is most similar to Datalog (Vardi, 1982 ; Maier et\\xa0al., 1984 ; Ceri et\\xa0al., 1989 ) , which does not. Every predicate is associated with an arity, which we denote using the function arity ar : Œ£ p ‚Üí ‚Ñï {\\\\small\\\\textsf{ar}}\\\\colon{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}}\\\\rightarrow\\\\mathbb{N} , specifying how many arguments it takes.\\nArguments to predicates are called terms ; they can be either constants ( ground terms) or variables ( non-ground terms).\\nAn atomic formula , called an atom , for short, is an expression of the form p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) , where p ‚àà Œ£ p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}} is a predicate symbol of arity ar \\u200b ( p ) = N {\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N and t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} are all terms.\\nThe Herbrand base H H for signature Œ£ {\\\\Sigma} is the set of all atoms that can be formed by terms in Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , i.e., H = { p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x } H=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\} . 3 3 3 In the case that the signature additionally contains a set of function symbols Œ£ f {{\\\\Sigma}_{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}}} , the Herbrand base is defined as the set of all atoms that can be formed by all terms in the Herbrand universe , which is the smallest set U U that satisfies the equation U = Œ£ x ‚à™ { f ( t 1 , . . . , t N ) ‚à£ f ‚àà Œ£ f , ar ( f ) = N , t 1 , . . . , t N ‚àà U } U={{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup\\\\{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in U\\\\} . Subsets of the Herbrand base I ‚äÜ H I\\\\subseteq H are called interpretations .\\nAn inference rule is an expression of the form b 1 , . . . , b N ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{N}\\\\vdash h , where b 1 , . . . , b K , h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K},h are atoms; b 1 , . . . , b K b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K} is the b ody of the rule and h h is the h ead .\\nFor example,\\nis an inference rule that allows us to conclude that if X is a parent of Y and Y is an ancestor of Z , then X is an ancestor of Z .\\nWe call b 1 , . . . , b K b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K} , i.e., parent \\u200b ( X , Y ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}) and ancestor \\u200b ( Y , Z ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}) in the example above, the premises and h h , i.e., ancestor \\u200b ( X , Z ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}) in the example above, the conclusion .\\nAn inference rule is called range restricted if each variable appearing in the conclusion h h also appears in at least one atom b k b_{k} in the premise. For example, p \\u200b ( X ) ‚ä¢ q \\u200b ( X ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}})\\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}) is range restricted, while p \\u200b ( X ) ‚ä¢ q \\u200b ( X , Y ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}})\\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}) is not.\\nIn this paper, we require all inference rules to be range restricted. 4 4 4 Range restriction ensures that applying the fixpoint operator ( Eq. 1 ) does not create non-ground atoms. Inference rules with a null premise, i.e., where K = 0 K=0 , and a ground conclusion, i.e., where h ‚àà H h\\\\in H , are called axioms . A set of axioms is denoted A A .\\nFor example, ‚ä¢ parent \\u200b ( abraham , isaac ) \\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) or, also written parent \\u200b ( abraham , isaac ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) , omitting the ‚ä¢ \\\\vdash symbol, is an axiom.\\nA logic program ùí´ \\\\mathcal{P} over a signature Œ£ {\\\\Sigma} is a set of inference rules in which all atoms are formed by symbols in Œ£ {\\\\Sigma} .\\nThe following is an example logic program, adapted from Sterling & Shapiro ( 1994 , ¬ß5) :\\nOur notion of logic programming additionally includes types (Abiteboul et\\xa0al., 1995 , ¬ß21.1) and built-ins (Kaminski et\\xa0al., 2017 ) , which we define here.\\nWe partition Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and Œ£ X {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} into T T disjoint subsets, i.e., Œ£ x = Œ£ x 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî Œ£ x T {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}={{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{1}}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{T}} and Œ£ X = Œ£ X 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî Œ£ X T {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}={{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{1}}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{T}} , respectively, and associate each subset with a type . These subsets are paired index-wise, i.e., ( Œ£ x 1 , Œ£ X 1 ) , . . . , ( Œ£ x T , Œ£ X T ) ({{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{1}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{1}}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,({{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{T}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{T}}) , ensuring that the constant and variable types match.\\nIn this paper, we consider three types: (i) natural numbers, denoted ( ‚Ñï x , ‚Ñï X ) (\\\\mathbb{N}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}},\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}) , (ii) strings, ( Œî x ‚àó , Œî X ‚àó ) ({{\\\\Delta}^{*}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{\\\\Delta}^{*}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) , and (iii) sets of strings, ( 2 x Œî ‚àó , 2 X Œî ‚àó ) ({2^{{\\\\Delta}^{\\\\!*}}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{2^{{\\\\Delta}^{\\\\!*}}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) .\\nOur introduction of types is necessitated by our desire to add additional power to our notion of logic programming that is external to the language itself.\\nSpecifically, we will introduce built-in predicates , simply called built-ins through the exposition, that add various arithmetic and set-theoretic operations.\\nWe enumerate these operations: X ‚Ñ§ + Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Addition) , \\\\displaystyle\\\\text{(Integer Addition)}, X ‚Ñ§ ‚àí Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{-}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Subtraction) , \\\\displaystyle\\\\text{(Integer Subtraction)}, X ‚Ñ§ √ó Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\times}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Multiplication) , \\\\displaystyle\\\\text{(Integer Multiplication)}, X ‚Ñ§ = Y ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}} (Integer Equality) , \\\\displaystyle\\\\text{(Integer Equality)}, X ‚Ñ§ ‚â• Y ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\geq}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}} (Integer Comparison) , \\\\displaystyle\\\\text{(Integer Comparison)}, X 2 Œî ‚àó ‚à™ Y 2 Œî ‚àó = Z 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\cup}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Union) , \\\\displaystyle\\\\text{(Set Union)}, X 2 Œî ‚àó ‚à© Y 2 Œî ‚àó = Z 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\cap}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Intersection) , \\\\displaystyle\\\\text{(Set Intersection)}, | X 2 Œî ‚àó | = X ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}} (Set Cardinality) , \\\\displaystyle\\\\text{(Set Cardinality)}, X 2 Œî ‚àó = Y 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Equality) , \\\\displaystyle\\\\text{(Set Equality)}, | X 2 Œî ‚àó | ‚â• X ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\geq}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}} (Set Cardinality Comparison).\\nThe truth value of grounded atoms constructed from built-in predicates is evaluated externally to the logic program.\\nTo do so, we define the built-in evaluator eval : H ‚Üí { T , F } {\\\\small\\\\textsf{eval}}\\\\colon H\\\\rightarrow\\\\{\\\\texttt{T},\\\\texttt{F}\\\\} , that maps all ground built-ins that evaluate to true to T and all ground built-ins that evaluate to false to F .\\nAdditionally, any element of H H that is not a built-in evaluates to F .\\nFor example, eval \\u200b ( 5 + 4 = 9 ) = T {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{9}}})=\\\\texttt{T} , eval \\u200b ( 5 + 4 = 10 ) = F {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{10}}})=\\\\texttt{F} , and eval \\u200b ( parent \\u200b ( abraham , isaac ) ) = F {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}))=\\\\texttt{F} .\\nFor example, to illustrate the use of built-ins in a logic program, we can extend the inference rules from the earlier example to measure the depth of the ancestor relation (e.g., parent, grandparent, great-grandparent, etc):\\nTo assign semantics to a logic program, we require a bit more machinery.\\nA substitution Œ∏ \\\\theta is a finite set of pairs { ( X m , t m ) } m = 1 M \\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m},t_{m})\\\\}_{m=1}^{M} , where X m ‚àà Œ£ X {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} , t m ‚àà Œ£ x ‚à™ Œ£ X t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} , X m ‚â† X m ‚Ä≤ {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\neq{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m^{\\\\prime}} for all m ‚â† m ‚Ä≤ m\\\\neq m^{\\\\prime} , and X m ‚â† t m {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\neq t_{m} for all m m (Sterling & Shapiro, 1994 , p.\\xa014) .\\nAdditionally, a typed substitution is a substitution where, if X m ‚àà Œ£ X k {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{k}} , then t m ‚àà Œ£ x k ‚à™ Œ£ X k t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{k}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{k}} .\\nWe can apply a substitution Œ∏ \\\\theta to an atom b b , denoted b / Œ∏ b/\\\\theta , e.g., parent \\u200b ( X , isaac ) / { ( X , abraham ) } = parent \\u200b ( abraham , isaac ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}})/\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}})\\\\}={\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) .\\nLet Œò \\u200b ( ùí´ ) \\\\Theta(\\\\mathcal{P}) be the set of all typed substitutions under ùí´ \\\\mathcal{P} .\\nWe say that b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ b^{\\\\prime}_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b^{\\\\prime}_{K}\\\\vdash h^{\\\\prime} is an instantiation of b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h if there exists a\\nsubstitution Œ∏ ‚àà Œò \\u200b ( ùí´ ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}) such that b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ = b 1 / Œ∏ , . . . , b K / Œ∏ ‚ä¢ h / Œ∏ b^{\\\\prime}_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b^{\\\\prime}_{K}\\\\vdash h^{\\\\prime}=b_{1}/\\\\theta,.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}/\\\\theta\\\\vdash h/\\\\theta .\\nIf an instantiation has no variables, we call it a ground instantiation . In addition, we say that an atom b b unifies with b ‚Ä≤ b^{\\\\prime} if ‚àÉ Œ∏ ‚àà Œò \\u200b ( ùí´ ) : b / Œ∏ = b ‚Ä≤ / Œ∏ \\\\exists\\\\theta\\\\in\\\\Theta(\\\\mathcal{P}):b/\\\\theta=b^{\\\\prime}/\\\\theta .\\nIf b b unifies with b ‚Ä≤ b^{\\\\prime} we write b ‚â° b ‚Ä≤ b\\\\equiv b^{\\\\prime} .\\nWe define a logic program ùí´ \\\\mathcal{P} ‚Äôs fixpoint operator T ùí´ : 2 H ‚Üí 2 H \\\\mathrm{T}_{\\\\mathcal{P}}\\\\colon 2^{H}\\\\to 2^{H} as\\nwhere I ‚äÜ H I\\\\subseteq H is an interpretation.\\nThe fixpoint operator T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} is inflationary , i.e., for every interpretation I ‚äÜ H I\\\\subseteq H , we have b ‚àà I ‚üπ b ‚àà T ùí´ \\u200b ( I ) b\\\\in I\\\\Longrightarrow b\\\\in\\\\mathrm{T}_{\\\\mathcal{P}}(I) , and monotone , i.e., for every pair of interpretations I 1 , I 2 ‚äÜ H I_{1},I_{2}\\\\subseteq H , we have I 1 ‚äÜ I 2 ‚üπ T ùí´ \\u200b ( I 1 ) ‚äÜ T ùí´ \\u200b ( I 2 ) I_{1}\\\\subseteq I_{2}\\\\Longrightarrow\\\\mathrm{T}_{\\\\mathcal{P}}(I_{1})\\\\subseteq\\\\mathrm{T}_{\\\\mathcal{P}}(I_{2}) .\\nThat T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} is monotone allows us to employ least fixpoint semantics.\\nTo that end, we define the minimal Herbrand model as M = T ùí´ ‚àó \\u200b ( A ) = def ‚ãÉ n = 0 ‚àû T ùí´ n \\u200b ( A ) M=\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{n=0}^{\\\\infty}\\\\mathrm{T}^{n}_{\\\\mathcal{P}}(A) , where T ùí´ n \\\\mathrm{T}^{n}_{\\\\mathcal{P}} denotes the n n -fold application of the fixpoint operator T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} , i.e., M M is T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} ‚Äôs least fixpoint. 5 5 5 Inflationarity is not needed to prove that the minimal Herbrand model M M exists.\\nIndeed, monotonicity and the fact that interpretations of the Herbrand base form a complete lattice suffice to apply Tarski ‚Äôs ( 1955 ) theorem, which guarantees the existence of the least fixpoint.\\nHowever, inflationarity does guarantee that, as we iteratively apply T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} , convergence to the least fixpoint is monotone. Thus, M M is the subset of the Herbrand base that is true given the axioms and inference rules in the program; we call elements of M M theorems .\\nDue to our inclusion of built-ins that encompass basic arithmetic operations, it is undecidable to compute M M (Dantsin et\\xa0al., 2001 ) , i.e.,\\nin general, we cannot decide whether h ‚àà T ùí´ ‚àó \\u200b ( A ) h\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A) for an arbitrary h ‚àà H h\\\\in H .\\nGiven a logic program ùí´ \\\\mathcal{P} , we are often interested in determining whether there exists a theorem in ùí´ \\\\mathcal{P} that is an instantiation of a specific (possibly non-ground) atom.\\nWe refer to such atoms as queries . 6 6 6 In principle, queries may also be ground; however, only the non-ground case is of theoretical interest here, as it extends beyond what can be handled by the machinery introduced so far. For example, building on the running example drawn from Sterling & Shapiro ( 1994 , ¬ß5) , we may wish to ask whether there exists a theorem that instantiates the atom ancestor \\u200b ( X , ishmael ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{ishmael}}}) .\\nAnswering such a query amounts to finding all substitutions for X that make the atom provable from ùí´ \\\\mathcal{P} .\\nWhen X can take on infinitely many instantiations, more sophisticated inference mechanisms‚Äîmost notably, unification (Robinson, 1965 ) ‚Äîare required to perform this kind of non-ground reasoning effectively.\\nIn this paper, however, we restrict attention to queries in which each variable is known a priori to range over a fixed, finite domain, which allows us to avoid additional complexities.',\n",
       "      'subsections': [{'title': 'Basic Notions.',\n",
       "        'paragraphs': 'A signature is a 3 3 -tuple Œ£ = ( Œ£ p , Œ£ x , Œ£ X ) {\\\\Sigma}=({{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) , where Œ£ p {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}} is a set of predicate (or relation ) symbols,\\ndenoted p , q , r , . . . {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{r}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. ; Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} is a set of constants, denoted x , y , z , . . . {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{y}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{z}}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. ; Œ£ X {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} is a set of variables, denoted X , Y , Z , . . . {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu. 2 2 2 Many logic programming languages, e.g., Prolog (Colmerauer & Roussel, 1993 ; K√∂rner et\\xa0al., 2022 ) , additionally have the notion of a function.\\nConstants are then just nullary functions.\\nOur notion of logic programming is most similar to Datalog (Vardi, 1982 ; Maier et\\xa0al., 1984 ; Ceri et\\xa0al., 1989 ) , which does not. Every predicate is associated with an arity, which we denote using the function arity ar : Œ£ p ‚Üí ‚Ñï {\\\\small\\\\textsf{ar}}\\\\colon{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}}\\\\rightarrow\\\\mathbb{N} , specifying how many arguments it takes.\\nArguments to predicates are called terms ; they can be either constants ( ground terms) or variables ( non-ground terms).\\nAn atomic formula , called an atom , for short, is an expression of the form p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) , where p ‚àà Œ£ p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}} is a predicate symbol of arity ar \\u200b ( p ) = N {\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N and t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} are all terms.\\nThe Herbrand base H H for signature Œ£ {\\\\Sigma} is the set of all atoms that can be formed by terms in Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , i.e., H = { p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x } H=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\} . 3 3 3 In the case that the signature additionally contains a set of function symbols Œ£ f {{\\\\Sigma}_{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}}} , the Herbrand base is defined as the set of all atoms that can be formed by all terms in the Herbrand universe , which is the smallest set U U that satisfies the equation U = Œ£ x ‚à™ { f ( t 1 , . . . , t N ) ‚à£ f ‚àà Œ£ f , ar ( f ) = N , t 1 , . . . , t N ‚àà U } U={{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup\\\\{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.78515625,0.33203125,0.17578125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.78515625,0.33203125,0.17578125}f})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in U\\\\} . Subsets of the Herbrand base I ‚äÜ H I\\\\subseteq H are called interpretations .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Logic Programming.',\n",
       "        'paragraphs': 'An inference rule is an expression of the form b 1 , . . . , b N ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{N}\\\\vdash h , where b 1 , . . . , b K , h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K},h are atoms; b 1 , . . . , b K b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K} is the b ody of the rule and h h is the h ead .\\nFor example,\\nis an inference rule that allows us to conclude that if X is a parent of Y and Y is an ancestor of Z , then X is an ancestor of Z .\\nWe call b 1 , . . . , b K b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K} , i.e., parent \\u200b ( X , Y ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}) and ancestor \\u200b ( Y , Z ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}) in the example above, the premises and h h , i.e., ancestor \\u200b ( X , Z ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}) in the example above, the conclusion .\\nAn inference rule is called range restricted if each variable appearing in the conclusion h h also appears in at least one atom b k b_{k} in the premise. For example, p \\u200b ( X ) ‚ä¢ q \\u200b ( X ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}})\\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}) is range restricted, while p \\u200b ( X ) ‚ä¢ q \\u200b ( X , Y ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}})\\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}) is not.\\nIn this paper, we require all inference rules to be range restricted. 4 4 4 Range restriction ensures that applying the fixpoint operator ( Eq. 1 ) does not create non-ground atoms. Inference rules with a null premise, i.e., where K = 0 K=0 , and a ground conclusion, i.e., where h ‚àà H h\\\\in H , are called axioms . A set of axioms is denoted A A .\\nFor example, ‚ä¢ parent \\u200b ( abraham , isaac ) \\\\vdash{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) or, also written parent \\u200b ( abraham , isaac ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) , omitting the ‚ä¢ \\\\vdash symbol, is an axiom.\\nA logic program ùí´ \\\\mathcal{P} over a signature Œ£ {\\\\Sigma} is a set of inference rules in which all atoms are formed by symbols in Œ£ {\\\\Sigma} .\\nThe following is an example logic program, adapted from Sterling & Shapiro ( 1994 , ¬ß5) :',\n",
       "        'subsections': []},\n",
       "       {'title': 'Types and Built-ins.',\n",
       "        'paragraphs': 'Our notion of logic programming additionally includes types (Abiteboul et\\xa0al., 1995 , ¬ß21.1) and built-ins (Kaminski et\\xa0al., 2017 ) , which we define here.\\nWe partition Œ£ x {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and Œ£ X {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} into T T disjoint subsets, i.e., Œ£ x = Œ£ x 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî Œ£ x T {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}={{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{1}}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{T}} and Œ£ X = Œ£ X 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî Œ£ X T {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}={{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{1}}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{T}} , respectively, and associate each subset with a type . These subsets are paired index-wise, i.e., ( Œ£ x 1 , Œ£ X 1 ) , . . . , ( Œ£ x T , Œ£ X T ) ({{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{1}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{1}}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,({{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{T}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{T}}) , ensuring that the constant and variable types match.\\nIn this paper, we consider three types: (i) natural numbers, denoted ( ‚Ñï x , ‚Ñï X ) (\\\\mathbb{N}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}},\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}) , (ii) strings, ( Œî x ‚àó , Œî X ‚àó ) ({{\\\\Delta}^{*}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{\\\\Delta}^{*}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) , and (iii) sets of strings, ( 2 x Œî ‚àó , 2 X Œî ‚àó ) ({2^{{\\\\Delta}^{\\\\!*}}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{2^{{\\\\Delta}^{\\\\!*}}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}) .\\nOur introduction of types is necessitated by our desire to add additional power to our notion of logic programming that is external to the language itself.\\nSpecifically, we will introduce built-in predicates , simply called built-ins through the exposition, that add various arithmetic and set-theoretic operations.\\nWe enumerate these operations: X ‚Ñ§ + Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Addition) , \\\\displaystyle\\\\text{(Integer Addition)}, X ‚Ñ§ ‚àí Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{-}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Subtraction) , \\\\displaystyle\\\\text{(Integer Subtraction)}, X ‚Ñ§ √ó Y ‚Ñ§ = Z ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\times}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{\\\\mathbb{Z}} (Integer Multiplication) , \\\\displaystyle\\\\text{(Integer Multiplication)}, X ‚Ñ§ = Y ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}} (Integer Equality) , \\\\displaystyle\\\\text{(Integer Equality)}, X ‚Ñ§ ‚â• Y ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\geq}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{\\\\mathbb{Z}} (Integer Comparison) , \\\\displaystyle\\\\text{(Integer Comparison)}, X 2 Œî ‚àó ‚à™ Y 2 Œî ‚àó = Z 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\cup}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Union) , \\\\displaystyle\\\\text{(Set Union)}, X 2 Œî ‚àó ‚à© Y 2 Œî ‚àó = Z 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\cap}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Z}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Intersection) , \\\\displaystyle\\\\text{(Set Intersection)}, | X 2 Œî ‚àó | = X ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}} (Set Cardinality) , \\\\displaystyle\\\\text{(Set Cardinality)}, X 2 Œî ‚àó = Y 2 Œî ‚àó \\\\displaystyle{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Y}}}_{2^{{\\\\Delta}^{\\\\!*}}} (Set Equality) , \\\\displaystyle\\\\text{(Set Equality)}, | X 2 Œî ‚àó | ‚â• X ‚Ñ§ \\\\displaystyle{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{2^{{\\\\Delta}^{\\\\!*}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{|}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{\\\\geq}}{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}} (Set Cardinality Comparison).\\nThe truth value of grounded atoms constructed from built-in predicates is evaluated externally to the logic program.\\nTo do so, we define the built-in evaluator eval : H ‚Üí { T , F } {\\\\small\\\\textsf{eval}}\\\\colon H\\\\rightarrow\\\\{\\\\texttt{T},\\\\texttt{F}\\\\} , that maps all ground built-ins that evaluate to true to T and all ground built-ins that evaluate to false to F .\\nAdditionally, any element of H H that is not a built-in evaluates to F .\\nFor example, eval \\u200b ( 5 + 4 = 9 ) = T {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{9}}})=\\\\texttt{T} , eval \\u200b ( 5 + 4 = 10 ) = F {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{=}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{10}}})=\\\\texttt{F} , and eval \\u200b ( parent \\u200b ( abraham , isaac ) ) = F {\\\\small\\\\textsf{eval}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}))=\\\\texttt{F} .\\nFor example, to illustrate the use of built-ins in a logic program, we can extend the inference rules from the earlier example to measure the depth of the ancestor relation (e.g., parent, grandparent, great-grandparent, etc):',\n",
       "        'subsections': []},\n",
       "       {'title': 'Substitutions and Semantics.',\n",
       "        'paragraphs': 'To assign semantics to a logic program, we require a bit more machinery.\\nA substitution Œ∏ \\\\theta is a finite set of pairs { ( X m , t m ) } m = 1 M \\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m},t_{m})\\\\}_{m=1}^{M} , where X m ‚àà Œ£ X {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} , t m ‚àà Œ£ x ‚à™ Œ£ X t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} , X m ‚â† X m ‚Ä≤ {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\neq{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m^{\\\\prime}} for all m ‚â† m ‚Ä≤ m\\\\neq m^{\\\\prime} , and X m ‚â† t m {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\neq t_{m} for all m m (Sterling & Shapiro, 1994 , p.\\xa014) .\\nAdditionally, a typed substitution is a substitution where, if X m ‚àà Œ£ X k {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{k}} , then t m ‚àà Œ£ x k ‚à™ Œ£ X k t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{k}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{k}} .\\nWe can apply a substitution Œ∏ \\\\theta to an atom b b , denoted b / Œ∏ b/\\\\theta , e.g., parent \\u200b ( X , isaac ) / { ( X , abraham ) } = parent \\u200b ( abraham , isaac ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}})/\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}})\\\\}={\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{parent}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{abraham}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{isaac}}}) .\\nLet Œò \\u200b ( ùí´ ) \\\\Theta(\\\\mathcal{P}) be the set of all typed substitutions under ùí´ \\\\mathcal{P} .\\nWe say that b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ b^{\\\\prime}_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b^{\\\\prime}_{K}\\\\vdash h^{\\\\prime} is an instantiation of b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h if there exists a\\nsubstitution Œ∏ ‚àà Œò \\u200b ( ùí´ ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}) such that b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ = b 1 / Œ∏ , . . . , b K / Œ∏ ‚ä¢ h / Œ∏ b^{\\\\prime}_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b^{\\\\prime}_{K}\\\\vdash h^{\\\\prime}=b_{1}/\\\\theta,.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}/\\\\theta\\\\vdash h/\\\\theta .\\nIf an instantiation has no variables, we call it a ground instantiation . In addition, we say that an atom b b unifies with b ‚Ä≤ b^{\\\\prime} if ‚àÉ Œ∏ ‚àà Œò \\u200b ( ùí´ ) : b / Œ∏ = b ‚Ä≤ / Œ∏ \\\\exists\\\\theta\\\\in\\\\Theta(\\\\mathcal{P}):b/\\\\theta=b^{\\\\prime}/\\\\theta .\\nIf b b unifies with b ‚Ä≤ b^{\\\\prime} we write b ‚â° b ‚Ä≤ b\\\\equiv b^{\\\\prime} .\\nWe define a logic program ùí´ \\\\mathcal{P} ‚Äôs fixpoint operator T ùí´ : 2 H ‚Üí 2 H \\\\mathrm{T}_{\\\\mathcal{P}}\\\\colon 2^{H}\\\\to 2^{H} as\\nwhere I ‚äÜ H I\\\\subseteq H is an interpretation.\\nThe fixpoint operator T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} is inflationary , i.e., for every interpretation I ‚äÜ H I\\\\subseteq H , we have b ‚àà I ‚üπ b ‚àà T ùí´ \\u200b ( I ) b\\\\in I\\\\Longrightarrow b\\\\in\\\\mathrm{T}_{\\\\mathcal{P}}(I) , and monotone , i.e., for every pair of interpretations I 1 , I 2 ‚äÜ H I_{1},I_{2}\\\\subseteq H , we have I 1 ‚äÜ I 2 ‚üπ T ùí´ \\u200b ( I 1 ) ‚äÜ T ùí´ \\u200b ( I 2 ) I_{1}\\\\subseteq I_{2}\\\\Longrightarrow\\\\mathrm{T}_{\\\\mathcal{P}}(I_{1})\\\\subseteq\\\\mathrm{T}_{\\\\mathcal{P}}(I_{2}) .\\nThat T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} is monotone allows us to employ least fixpoint semantics.\\nTo that end, we define the minimal Herbrand model as M = T ùí´ ‚àó \\u200b ( A ) = def ‚ãÉ n = 0 ‚àû T ùí´ n \\u200b ( A ) M=\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{n=0}^{\\\\infty}\\\\mathrm{T}^{n}_{\\\\mathcal{P}}(A) , where T ùí´ n \\\\mathrm{T}^{n}_{\\\\mathcal{P}} denotes the n n -fold application of the fixpoint operator T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} , i.e., M M is T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} ‚Äôs least fixpoint. 5 5 5 Inflationarity is not needed to prove that the minimal Herbrand model M M exists.\\nIndeed, monotonicity and the fact that interpretations of the Herbrand base form a complete lattice suffice to apply Tarski ‚Äôs ( 1955 ) theorem, which guarantees the existence of the least fixpoint.\\nHowever, inflationarity does guarantee that, as we iteratively apply T ùí´ \\\\mathrm{T}_{\\\\mathcal{P}} , convergence to the least fixpoint is monotone. Thus, M M is the subset of the Herbrand base that is true given the axioms and inference rules in the program; we call elements of M M theorems .\\nDue to our inclusion of built-ins that encompass basic arithmetic operations, it is undecidable to compute M M (Dantsin et\\xa0al., 2001 ) , i.e.,\\nin general, we cannot decide whether h ‚àà T ùí´ ‚àó \\u200b ( A ) h\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A) for an arbitrary h ‚àà H h\\\\in H .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Queries.',\n",
       "        'paragraphs': 'Given a logic program ùí´ \\\\mathcal{P} , we are often interested in determining whether there exists a theorem in ùí´ \\\\mathcal{P} that is an instantiation of a specific (possibly non-ground) atom.\\nWe refer to such atoms as queries . 6 6 6 In principle, queries may also be ground; however, only the non-ground case is of theoretical interest here, as it extends beyond what can be handled by the machinery introduced so far. For example, building on the running example drawn from Sterling & Shapiro ( 1994 , ¬ß5) , we may wish to ask whether there exists a theorem that instantiates the atom ancestor \\u200b ( X , ishmael ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{ancestor}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{ishmael}}}) .\\nAnswering such a query amounts to finding all substitutions for X that make the atom provable from ùí´ \\\\mathcal{P} .\\nWhen X can take on infinitely many instantiations, more sophisticated inference mechanisms‚Äîmost notably, unification (Robinson, 1965 ) ‚Äîare required to perform this kind of non-ground reasoning effectively.\\nIn this paper, however, we restrict attention to queries in which each variable is known a priori to range over a fixed, finite domain, which allows us to avoid additional complexities.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.2 Deductive Reasoning',\n",
       "      'paragraphs': 'A hypergraph ùí¢ \\\\mathcal{G} (B-hypergraph; Gallo et\\xa0al., 1993 ) is a tuple ( V , E ) (V,E) , where V V is a set of vertices, and E ‚äÜ 2 V √ó V E\\\\subseteq 2^{V}\\\\times V is a set of hyperedges , where a hyperedge e = T ‚Ü£ v h e=T\\\\rightarrowtail{v}_{h} consists of a t ail T ‚äÜ V T\\\\subseteq V , with | T | > 0 |T|>0 , and a h ead v h ‚àà V {v}_{h}\\\\in V .\\nWe define the size of a hypergraph as | ùí¢ | = def | V | |\\\\mathcal{G}|\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}|V| where ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) . 7 7 7 We note that this is non-standard; the size of a hypergraph is more often defined as its number of hyperedges or the sum of the cardinalities of its hyperedges (Gallo et\\xa0al., 1993 ) . We use this definition to sync with the experimental setup, which we explain in ¬ß 5 .\\nFuture work could easily adapt our efficiency metric to other definitions. A subhypergraph of a hypergraph ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) is a hypergraph ùí¢ ‚Ä≤ = ( V ‚Ä≤ , E ‚Ä≤ ) \\\\mathcal{G}^{\\\\prime}=(V^{\\\\prime},E^{\\\\prime}) where V ‚Ä≤ ‚äÜ V V^{\\\\prime}\\\\subseteq V and E ‚Ä≤ ‚äÜ E E^{\\\\prime}\\\\subseteq E .\\nGiven S ‚äÜ V S\\\\subseteq V and v ‚àà V v\\\\in V , an ( S , v ) (S,v) - hyperpath in a hypergraph ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) is a finite sequence of distinct hyperedges T 1 ‚Ü£ v h 1 , . . . , T J ‚Ü£ v h J T_{1}\\\\rightarrowtail{v}_{h_{1}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,T_{J}\\\\rightarrowtail{v}_{h_{J}} such that v h J = v {v}_{h_{J}}=v and for every j ‚àà [ J ] : T j ‚äÜ S ‚à™ { v h 1 , . . . , v h j ‚àí 1 } j\\\\in[J]\\\\colon T_{j}\\\\subseteq S\\\\cup\\\\{{v}_{h_{1}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,{v}_{h_{j-1}}\\\\} , i.e., each hyperedge‚Äôs tail consists only of nodes that are either in the source set S S or are heads of previous hyperedges in the sequence.\\nA hyperpath generalizes the notion of a directed path in a graph, but allows each hyperedge to have multiple tail nodes that jointly produce a head node.\\nFinding the shortest ( S , v ) (S,v) -hyperpath in a hypergraph is analogous to context-free parsing (Klein & Manning, 2001 ) and can be executed in polynomial time.\\nLet ùí´ \\\\mathcal{P} be a logic program.\\nA proof forest ( ‚Ñ± , ‚Ñì ) (\\\\mathcal{F},\\\\ell) in ùí´ \\\\mathcal{P} is a pair\\nwhere ‚Ñ± = ( E , V ) \\\\mathcal{F}=(E,V) is a hypergraph and ‚Ñì : V ‚Üí H \\\\ell\\\\colon V\\\\rightarrow H where H H is ùí´ \\\\mathcal{P} ‚Äôs Herbrand base (Heijltjes, 2010 ) .\\nAdditionally, we require that, for every hyperedge e = { t 1 , . . . , t K } ‚Ü£ v h ‚àà E e=\\\\{t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{K}\\\\}\\\\rightarrowtail{v}_{h}\\\\in E , there exists a rule ( b 1 , . . . , b K ‚ä¢ h ) ‚àà ‚Ñõ (b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h)\\\\in\\\\mathcal{R} and a substitution Œ∏ ‚àà Œò \\u200b ( ùí´ ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}) such that b 1 / Œ∏ = ‚Ñì ( t 1 ) , . . . , b K / Œ∏ = ‚Ñì ( t K ) b_{1}/\\\\theta=\\\\ell(t_{1}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}/\\\\theta=\\\\ell(t_{K}) and h / Œ∏ = ‚Ñì \\u200b ( v h ) h/\\\\theta=\\\\ell({v}_{h}) .\\nWe call a proof forest ( ‚Ñ± , ‚Ñì ) (\\\\mathcal{F},\\\\ell) an ( A , h g ) (A,h_{g}) - proof if there exists a ( ‚Ñì ‚àí 1 \\u200b ( A ) , ‚Ñì ‚àí 1 \\u200b ( h g ) ) (\\\\ell^{-1}(A),\\\\ell^{-1}(h_{g})) -hyperpath in ‚Ñã ùí´ \\\\mathcal{H}_{\\\\mathcal{P}} . 8 8 8 We note that the expression ( A , h g ) (A,h_{g}) -hyperpath is a slight abuse of notation in the case that ‚Ñì \\\\ell is not injective adopted for convenience: A A and h g h_{g} are a subset and an element, respectively, of ùí´ \\\\mathcal{P} ‚Äôs Herbrand base‚Äînot of ‚Ñõ ùí´ \\\\mathcal{R}_{\\\\mathcal{P}} ‚Äôs V V . In this context, by A A , we refer to a set X ‚äÜ ‚Ñì ‚àí 1 \\u200b ( A ) X\\\\subseteq\\\\ell^{-1}(A) where ‚Ñì \\u200b ( X ) = A \\\\ell(X)=A , and by h g h_{g} we refer to a set Y ‚äÜ ‚Ñì ‚àí 1 \\u200b ( h g ) Y\\\\subseteq\\\\ell^{-1}(h_{g}) where ‚Ñì \\u200b ( Y ) = h g \\\\ell(Y)=h_{g} . In Fig. Àú 1 , we show an example of a proof in which h g = cont \\u200b ( { a,b,c } , 25 , apple , 2 ) h_{g}={\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{a,b,c\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{25}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{apple}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) in our custom logic program for math word problems ( ¬ß Àú 4 ), together with some axioms in the program that do not contribute to the proof of the goal theorem h g h_{g} .\\nWe call an ( A , h g ) (A,h_{g}) -proof a shortest proof if it has the least number of vertices of all ( A , h g ) (A,h_{g}) -proofs in ùí´ \\\\mathcal{P} .\\nA shortest proof can be found by forward-chaining, discussed in the subsequent paragraph.\\nNow we turn to measuring proof efficiency.\\nConsider an ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} in ùí´ \\\\mathcal{P} .\\nWe define the efficiency of ùí´ \\\\mathcal{P} as efficiency \\u200b ( ùí´ ) = def | ùí´ ‚ãÜ | / | ùí´ | \\\\textsc{efficiency}(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}|\\\\mathcal{P}^{\\\\star}|/|\\\\mathcal{P}| where | ùí´ ‚ãÜ | |\\\\mathcal{P}^{\\\\star}| is the number of vertices in a shortest ( A , h g ) (A,h_{g}) -proof.\\nIn the remainder of the paper, we will refer to an axiom a a as irrelevant if there does not exist a shortest proof ùí´ \\\\mathcal{P} that contains a vertex v v such that ‚Ñì \\u200b ( v ) = a \\\\ell(v)=a .\\nForward chaining (Hayes-Roth et\\xa0al., 1983 ; Poole & Mackworth, 2017 ) is a meta-strategy for theorem proving in logic programming that proceeds from the axioms toward the goal theorem.\\nThe process terminates once the goal theorem is proved.\\nPseudocode for the forward-chaining is given in Alg. Àú 1 in ¬ß Àú A.2 .\\nAs a meta-strategy, each instance of forward chaining defines an ordering over proof steps.\\nDifferent orderings give rise to familiar search algorithms, such as depth-first search (DFS; Tarjan, 1972 ) , breadth-first search (BFS; Moore, 1959 ) , Dijkstra‚Äôs algorithm (Dijkstra, 1959 ) , and heuristic-based search (Pearl, 1984 ) like A* (Hart et\\xa0al., 1968 ) .\\nWhile DFS and BFS ignore information about the goal theorem, such information can guide search more efficiently, as exemplified by goal-aware strategies like Earley‚Äôs algorithm (Earley, 1970 ) for context-free parsing or its more general equivalent in logic programming‚Äîmagic templates (Bancilhon et\\xa0al., 1986 ; Ramakrishnan, 1991 ) .\\nIn the spirit of using top-down information in proof search, in ¬ß Àú 5 , we examine whether LMs make use of lexical overlap with the goal theorem as part of their internal search heuristic.',\n",
       "      'subsections': [{'title': 'Hypergraphs.',\n",
       "        'paragraphs': 'A hypergraph ùí¢ \\\\mathcal{G} (B-hypergraph; Gallo et\\xa0al., 1993 ) is a tuple ( V , E ) (V,E) , where V V is a set of vertices, and E ‚äÜ 2 V √ó V E\\\\subseteq 2^{V}\\\\times V is a set of hyperedges , where a hyperedge e = T ‚Ü£ v h e=T\\\\rightarrowtail{v}_{h} consists of a t ail T ‚äÜ V T\\\\subseteq V , with | T | > 0 |T|>0 , and a h ead v h ‚àà V {v}_{h}\\\\in V .\\nWe define the size of a hypergraph as | ùí¢ | = def | V | |\\\\mathcal{G}|\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}|V| where ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) . 7 7 7 We note that this is non-standard; the size of a hypergraph is more often defined as its number of hyperedges or the sum of the cardinalities of its hyperedges (Gallo et\\xa0al., 1993 ) . We use this definition to sync with the experimental setup, which we explain in ¬ß 5 .\\nFuture work could easily adapt our efficiency metric to other definitions. A subhypergraph of a hypergraph ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) is a hypergraph ùí¢ ‚Ä≤ = ( V ‚Ä≤ , E ‚Ä≤ ) \\\\mathcal{G}^{\\\\prime}=(V^{\\\\prime},E^{\\\\prime}) where V ‚Ä≤ ‚äÜ V V^{\\\\prime}\\\\subseteq V and E ‚Ä≤ ‚äÜ E E^{\\\\prime}\\\\subseteq E .\\nGiven S ‚äÜ V S\\\\subseteq V and v ‚àà V v\\\\in V , an ( S , v ) (S,v) - hyperpath in a hypergraph ùí¢ = ( V , E ) \\\\mathcal{G}=(V,E) is a finite sequence of distinct hyperedges T 1 ‚Ü£ v h 1 , . . . , T J ‚Ü£ v h J T_{1}\\\\rightarrowtail{v}_{h_{1}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,T_{J}\\\\rightarrowtail{v}_{h_{J}} such that v h J = v {v}_{h_{J}}=v and for every j ‚àà [ J ] : T j ‚äÜ S ‚à™ { v h 1 , . . . , v h j ‚àí 1 } j\\\\in[J]\\\\colon T_{j}\\\\subseteq S\\\\cup\\\\{{v}_{h_{1}},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,{v}_{h_{j-1}}\\\\} , i.e., each hyperedge‚Äôs tail consists only of nodes that are either in the source set S S or are heads of previous hyperedges in the sequence.\\nA hyperpath generalizes the notion of a directed path in a graph, but allows each hyperedge to have multiple tail nodes that jointly produce a head node.\\nFinding the shortest ( S , v ) (S,v) -hyperpath in a hypergraph is analogous to context-free parsing (Klein & Manning, 2001 ) and can be executed in polynomial time.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Proof Forests, Proofs and Proof Efficiency.',\n",
       "        'paragraphs': 'Let ùí´ \\\\mathcal{P} be a logic program.\\nA proof forest ( ‚Ñ± , ‚Ñì ) (\\\\mathcal{F},\\\\ell) in ùí´ \\\\mathcal{P} is a pair\\nwhere ‚Ñ± = ( E , V ) \\\\mathcal{F}=(E,V) is a hypergraph and ‚Ñì : V ‚Üí H \\\\ell\\\\colon V\\\\rightarrow H where H H is ùí´ \\\\mathcal{P} ‚Äôs Herbrand base (Heijltjes, 2010 ) .\\nAdditionally, we require that, for every hyperedge e = { t 1 , . . . , t K } ‚Ü£ v h ‚àà E e=\\\\{t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{K}\\\\}\\\\rightarrowtail{v}_{h}\\\\in E , there exists a rule ( b 1 , . . . , b K ‚ä¢ h ) ‚àà ‚Ñõ (b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h)\\\\in\\\\mathcal{R} and a substitution Œ∏ ‚àà Œò \\u200b ( ùí´ ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}) such that b 1 / Œ∏ = ‚Ñì ( t 1 ) , . . . , b K / Œ∏ = ‚Ñì ( t K ) b_{1}/\\\\theta=\\\\ell(t_{1}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}/\\\\theta=\\\\ell(t_{K}) and h / Œ∏ = ‚Ñì \\u200b ( v h ) h/\\\\theta=\\\\ell({v}_{h}) .\\nWe call a proof forest ( ‚Ñ± , ‚Ñì ) (\\\\mathcal{F},\\\\ell) an ( A , h g ) (A,h_{g}) - proof if there exists a ( ‚Ñì ‚àí 1 \\u200b ( A ) , ‚Ñì ‚àí 1 \\u200b ( h g ) ) (\\\\ell^{-1}(A),\\\\ell^{-1}(h_{g})) -hyperpath in ‚Ñã ùí´ \\\\mathcal{H}_{\\\\mathcal{P}} . 8 8 8 We note that the expression ( A , h g ) (A,h_{g}) -hyperpath is a slight abuse of notation in the case that ‚Ñì \\\\ell is not injective adopted for convenience: A A and h g h_{g} are a subset and an element, respectively, of ùí´ \\\\mathcal{P} ‚Äôs Herbrand base‚Äînot of ‚Ñõ ùí´ \\\\mathcal{R}_{\\\\mathcal{P}} ‚Äôs V V . In this context, by A A , we refer to a set X ‚äÜ ‚Ñì ‚àí 1 \\u200b ( A ) X\\\\subseteq\\\\ell^{-1}(A) where ‚Ñì \\u200b ( X ) = A \\\\ell(X)=A , and by h g h_{g} we refer to a set Y ‚äÜ ‚Ñì ‚àí 1 \\u200b ( h g ) Y\\\\subseteq\\\\ell^{-1}(h_{g}) where ‚Ñì \\u200b ( Y ) = h g \\\\ell(Y)=h_{g} . In Fig. Àú 1 , we show an example of a proof in which h g = cont \\u200b ( { a,b,c } , 25 , apple , 2 ) h_{g}={\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{a,b,c\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{25}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{apple}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) in our custom logic program for math word problems ( ¬ß Àú 4 ), together with some axioms in the program that do not contribute to the proof of the goal theorem h g h_{g} .\\nWe call an ( A , h g ) (A,h_{g}) -proof a shortest proof if it has the least number of vertices of all ( A , h g ) (A,h_{g}) -proofs in ùí´ \\\\mathcal{P} .\\nA shortest proof can be found by forward-chaining, discussed in the subsequent paragraph.\\nNow we turn to measuring proof efficiency.\\nConsider an ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} in ùí´ \\\\mathcal{P} .\\nWe define the efficiency of ùí´ \\\\mathcal{P} as efficiency \\u200b ( ùí´ ) = def | ùí´ ‚ãÜ | / | ùí´ | \\\\textsc{efficiency}(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}|\\\\mathcal{P}^{\\\\star}|/|\\\\mathcal{P}| where | ùí´ ‚ãÜ | |\\\\mathcal{P}^{\\\\star}| is the number of vertices in a shortest ( A , h g ) (A,h_{g}) -proof.\\nIn the remainder of the paper, we will refer to an axiom a a as irrelevant if there does not exist a shortest proof ùí´ \\\\mathcal{P} that contains a vertex v v such that ‚Ñì \\u200b ( v ) = a \\\\ell(v)=a .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Forward Chaining.',\n",
       "        'paragraphs': 'Forward chaining (Hayes-Roth et\\xa0al., 1983 ; Poole & Mackworth, 2017 ) is a meta-strategy for theorem proving in logic programming that proceeds from the axioms toward the goal theorem.\\nThe process terminates once the goal theorem is proved.\\nPseudocode for the forward-chaining is given in Alg. Àú 1 in ¬ß Àú A.2 .\\nAs a meta-strategy, each instance of forward chaining defines an ordering over proof steps.\\nDifferent orderings give rise to familiar search algorithms, such as depth-first search (DFS; Tarjan, 1972 ) , breadth-first search (BFS; Moore, 1959 ) , Dijkstra‚Äôs algorithm (Dijkstra, 1959 ) , and heuristic-based search (Pearl, 1984 ) like A* (Hart et\\xa0al., 1968 ) .\\nWhile DFS and BFS ignore information about the goal theorem, such information can guide search more efficiently, as exemplified by goal-aware strategies like Earley‚Äôs algorithm (Earley, 1970 ) for context-free parsing or its more general equivalent in logic programming‚Äîmagic templates (Bancilhon et\\xa0al., 1986 ; Ramakrishnan, 1991 ) .\\nIn the spirit of using top-down information in proof search, in ¬ß Àú 5 , we examine whether LMs make use of lexical overlap with the goal theorem as part of their internal search heuristic.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '4 Evaluating Language Models on Grade School Math Word Problems',\n",
       "    'paragraphs': 'We are interested in reasoning that takes places in natural language , particularly as performed by LMs.\\nTo this end, we consider grade school math (GSM) word problems as empirical test domain.\\nSuch problems are commonly used for training and evaluating LMs on reasoning tasks (Cobbe et\\xa0al., 2021 ; Patel et\\xa0al., 2021 ) . In ¬ß Àú 4.1 , we introduce a family of logic programs, using the technical notions introduced in ¬ß Àú 3 , that correspond to a natural class of such math word problems.\\nWe also describe a simple manner to convert text generated by an LM to a proof in such logic programs in ¬ß Àú 4.2 .\\nFinally, in ¬ß Àú 4.3 we explain how we generate problem descriptions in natural language that contain irrelevant axioms.\\nWe consider a particular family of logic programs to represent GSM problems, adapted from Opedal et\\xa0al. ( 2023 , 2025 ) .\\nAll of the logic programs have the signature Œ£ W = ( Œ£ p W , Œ£ x W , Œ£ X W ) {{\\\\Sigma}^{\\\\text{W}}}=({{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}},{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{\\\\text{W}}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{\\\\text{W}}}) .\\nThe set of predicate symbols Œ£ p W = { cont , comp , transfer , rate , compeq } {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}}=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{compeq}}\\\\} corresponds to arithmetic concepts that occur in GSM word problems (Riley et\\xa0al., 1983 ) , e.g., cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} for denoting how many entities an agent cont ains, comp {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}} for comp aring the number of entities across multiple agents, or transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} for expressing one agent transfer ing entities to another.\\nWe partition Œ£ x W = 2 x A ‚äî E x ‚äî ‚Ñï x q ‚äî ‚Ñï x t {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{\\\\text{W}}}={2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}}\\\\sqcup{{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\sqcup{\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\sqcup{\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and Œ£ X W = 2 X A ‚äî E X ‚äî ‚Ñï X q ‚äî ‚Ñï X t {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{\\\\text{W}}}={2_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{{{A}}}}\\\\sqcup{{E}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\sqcup{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{q}}\\\\sqcup{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{t}} into the following pairs:\\nsets of strings ( 2 x A , 2 X A ) ({2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}},{2_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{{{A}}}}) called sets of agents ( who possesses), strings ( E x , E X {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{E}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} ) called entities ( what is possessed),\\nnatural numbers ( ‚Ñï x q , ‚Ñï X q ) ({\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{q}}) called quantities ( how much is possessed),\\nand natural numbers ( ‚Ñï x t , ‚Ñï X t ) ({\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{t}}) called timestamps ( time of possession).\\nWe note that 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} is a power set of the set of agent strings A x {{{A}}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , which enables us to code a state where multiple agents possess the same entity jointly.\\nThe family of logic programs all share the same inference rules, which are given in Table Àú 1 . Fig. Àú 1 shows an example proof using these inference rules, omitting built-ins.\\nWe note that possession may change over time and is governed by the transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} predicate in Rule (2). 9 9 9 The time component requires the addition of Rule (6), which expresses that all theorems with time T ‚Ñï {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}} that are not affected by a proof step using Rule (2) maintain their truth value at T ‚Ñï + 1 {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}} . This is an instance of the frame problem (McCarthy & Hayes, 1969 ; Sandewall, 1972 ; Hanks & McDermott, 1987 ) .\\nTo express this rule, we introduce negation into the logic program; see ¬ß A.1 for background on negation in logic programming.\\nThe only negated predicate in Table 1 is ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} ; because ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} only appears in the body of an inference rule, the program is trivially stratified ( ¬ß A.1 ). That is ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atoms can not be proved true, but we assume ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atoms if the corresponding transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atom is not known to be true from the axioms.\\nThe rules given in Table Àú 1 are held constant across all logic programs in our family.\\nIndeed, what distinguishes one program from another, then, is the choice of axioms.\\nFor example, consider the axiom cont \\u200b ( { ryan } , 5 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) which expresses that ‚ÄúRyan has 5 cats at time step 2‚Äù ; the predicate cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} represents the semantics of possession, and { ryan } ‚àà 2 x A {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}}\\\\in{2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , 5 ‚àà ‚Ñï x q {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}\\\\in{\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , cat ‚àà E x {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}}\\\\in{{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , and 2 ‚àà ‚Ñï x t {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}\\\\in{\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} are all ground atoms of different types.\\nAdditionally, we only consider axiom sets A W ‚äÇ H ¬Ø A_{\\\\text{W}}\\\\subset\\\\overline{H} that have the following property:\\nWe call an interpretation I ‚äÜ H ¬Ø I\\\\subseteq\\\\overline{H} of a logic program in our family ( Table Àú 1 ) numerically consistent if there do not exist two substitutions Œ∏ = { ( A ùíú , t a ) , ( X ‚Ñ§ , t ) , ( E Œî ‚àó , t e ) , ( T ‚Ñï , t t ) } \\\\theta=\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},t_{a}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}},t),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},t_{e}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}},t_{t})\\\\} and Œ∏ ‚Ä≤ = { ( A ùíú , t a ) , ( X ‚Ñ§ , t ‚Ä≤ ) , ( E Œî ‚àó , t e ) , ( T ‚Ñï , t t ) } \\\\theta^{\\\\prime}=\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},t_{a}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}},t^{\\\\prime}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},t_{e}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}},t_{t})\\\\} such that t ‚â† t ‚Ä≤ t\\\\neq t^{\\\\prime} and both cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) / Œ∏ , cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) / Œ∏ ‚Ä≤ ‚àà T ùí´ W ‚àó \\u200b ( I ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}})/\\\\theta,{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}})/\\\\theta^{\\\\prime}\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}_{\\\\text{W}}}(I) .\\nThis ensures that the minimal Herbrand model does not contain contradictory pairs such as cont \\u200b ( { ryan } , 5 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) and cont \\u200b ( { ryan } , 4 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) .\\nFor example, the following set of axioms is numerically consistent: A W = { cont \\u200b ( { ryan } , 5 , cat , 2 ) , comp \\u200b ( { eleanor } , { ryan } , 3 , cat , 2 ) } A_{\\\\text{W}}=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}),{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{eleanor\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}})\\\\} , expressing that ‚ÄúRyan has 5 cats‚Äù and that ‚ÄúEleanor has 3 more cats than Ryan‚Äù . If we were to, e.g., include the two axioms { cont \\u200b ( { andreas } , 7 , cat , 2 ) , comp \\u200b ( { eleanor } , { andreas } , 3 , cat , 2 ) } \\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{andreas\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{7}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}),{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{eleanor\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{andreas\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}})\\\\} , we would obtain a numerically inconsistent axiom set.\\nInference in our family of logic programs is decidable under numerically consistent axiom sets. See App. Àú B for details.\\nWe give a brief formal introduction to language modeling.\\nLet Œì \\\\Gamma be an alphabet of tokens and Œì ‚àó \\\\Gamma^{\\\\!*} be the set of all strings over Œì \\\\Gamma , its Kleene closure.\\nWe write ùíò ‚àà Œì ‚àó {\\\\bm{w}}\\\\in\\\\Gamma^{\\\\!*} for a string, w t {{w}_{t}} for the token at the t th t^{\\\\text{th}} position in ùíò = w 1 ‚ãÖ ‚ãÖ ‚ãÖ w T {\\\\bm{w}}=w_{1}\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot w_{T} , and | ùíò | = T |{\\\\bm{w}}|=T for the number of tokens in ùíò {\\\\bm{w}} , i.e., its length.\\nA language model ( LM ) p p is a probability distribution on Œì ‚àó \\\\Gamma^{\\\\!*} .\\nLet eos ‚àâ Œì {\\\\textsc{eos}}\\\\notin\\\\Gamma be a distinguished symbol denoting the end of a token string.\\nThe probability of a string ùíò {\\\\bm{w}} can be written autoregressively as p \\u200b ( ùíò ) = p ‚Üí \\u200b ( eos ‚à£ ùíò ) \\u200b ‚àè t = 1 | ùíò | p ‚Üí \\u200b ( w t ‚à£ ùíò < t ) p({\\\\bm{w}})=\\\\overrightarrow{p}({\\\\textsc{eos}}\\\\mid{\\\\bm{w}})\\\\prod_{t=1}^{|{\\\\bm{w}}|}\\\\overrightarrow{p}({{w}_{t}}\\\\mid{{\\\\bm{w}}_{<t}}) ,\\nwhere p ‚Üí ( ‚ãÖ ‚à£ ùíÑ ) \\\\overrightarrow{p}(\\\\cdot\\\\mid{\\\\bm{c}}) is a probability distribution over Œì ¬Ø = def Œì ‚à™ { eos } \\\\overline{\\\\Gamma}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\Gamma\\\\cup\\\\{{\\\\textsc{eos}}\\\\} conditioned on the context ùíÑ ‚àà Œì ‚àó {\\\\bm{c}}\\\\in\\\\Gamma^{\\\\!*} .\\nTo bridge reasoning in formal proof systems to reasoning in natural language, we introduce the idea of a verbalized logic program .\\nGiven a logic program ùí´ \\\\mathcal{P} with negation ( ¬ß Àú A.1 ),\\nlet H ¬Ø \\\\overline{H} be its extended Herband base.\\nWe associate each atom b ‚àà H ¬Ø b\\\\in\\\\overline{H} with a set of natural language strings.\\nTo that end, we define a verbalizer ŒΩ ùí´ : H ¬Ø ‚Üí 2 Œì ‚àó \\\\nu_{\\\\mathcal{P}}\\\\colon\\\\overline{H}\\\\rightarrow 2^{\\\\Gamma^{\\\\!*}} , where each ŒΩ ùí´ \\u200b ( b ) \\\\nu_{\\\\mathcal{P}}(b) is a disjoint set.\\nFor every b ‚àà H ¬Ø b\\\\in\\\\overline{H} ,\\nthe set ŒΩ ùí´ \\u200b ( b ) \\\\nu_{\\\\mathcal{P}}(b) represents the various ways in which the meaning of b b can be expressed in natural language.\\nIn this paper, we take a straightforward approach.\\nFor each b ‚àà H ¬Ø b\\\\in\\\\overline{H} , we construct a finite set G b ‚äÇ Œì ‚àó G_{b}\\\\subset\\\\Gamma^{\\\\!*} .\\nMoreover, we enforce disjointness, i.e., G b 2 ‚à© G b 1 = ‚àÖ G_{b_{2}}\\\\cap G_{b_{1}}=\\\\varnothing for b 1 , b 2 ‚àà H ¬Ø b_{1},b_{2}\\\\in\\\\overline{H} , and that, for all b ‚àà H b\\\\in H , each string in G b G_{b} ends in a distinguished separator symbol‚Äîin our case a period ‚Äú.‚Äù ‚Äîthat appears nowhere else in the string.\\nThese assumptions allow for trivial linear-time parsing of natural language text into a sequence of atoms in the verbalized logic programs.\\nIn practice, we generate the strings in each G b G_{b} with a series of hand-written templates.\\nWe give examples in Fig. Àú 2 .\\nWe introduce a simple sampling algorithm, presented and analyzed in App. Àú C , that samples a ground goal theorem h g ‚â° cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) h_{g}\\\\equiv{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}) and a shortest proof of h g h_{g} under the rules ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} from Table Àú 1 . By construction, the axioms A W A_{\\\\text{W}} in the shortest proof are numerically consistent and all axioms in A W A_{\\\\text{W}} will be used in the proof of h g h_{g} in the program ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} ; we denote this dependency by A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) . 10 10 10 A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) is the unique smallest set of axioms ( Lemma 1 ) needed to prove h g h_{g} , justifying the function notation. That is, A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) contains no irrelevant axioms.\\nHowever, to produce logic programs that do contain irrelevant axioms, we do the following.\\nWe sample an additional M M distinct distractor theorems { h ~ m } m = 1 M \\\\{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m}\\\\}_{m=1}^{M} .\\nFor each distractor theorem h ~ m \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m} , we again apply our axiom sampling procedure to generate distractor axioms A ~ m \\u200b ( h ~ m ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m}) .\\nImportantly, we are able to show that A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) remain on a shortest proof of h g h_{g} even in the case that we consider the augmented logic program ùí´ ~ W = ‚Ñõ W ‚äî A W ( h g ) ‚äî A ~ 1 ( h ~ 1 ) ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M ( h ~ M ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{\\\\mathcal{P}}$}}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}}(h_{g})\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M}) ; see ¬ß Àú C.4 .\\nIn our experiments, we vary the size of the irrelevant axiom sets A ~ = A ~ 1 ( h ~ 1 ) ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M ( h ~ M ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}=\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M}) as follows: (i) a single irrelevant axiom ( w/ axiom ), where M = 1 M=1 , | A ~ 1 \\u200b ( h ~ 1 ) | = 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})|=1 , and A ~ 1 \\u200b ( h ~ 1 ) = { h ~ 1 } \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})=\\\\{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1}\\\\} (i.e., the distractor theorem is trivial); (ii) a single irrelevant tree ( w/ tree ), where M = 1 M=1 , | A ~ 1 \\u200b ( h ~ 1 ) | > 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})|>1 , and h ~ 1 \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1} has a non-trivial proof; (iii) multiple irrelevant trees ( w/ multiple trees ), where M = 3 M=3 , | A ~ m \\u200b ( h ~ m ) | > 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m})|>1 for all m m , and h ~ 1 , h ~ 2 , h ~ 3 \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1},\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{2},\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{3} all have non-trivial proofs.\\nWe additionally control for overlap between the set of agents, i.e., elements of 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , and the entities, i.e., elements of E x {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , of the goal theorem h g ‚â° cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) h_{g}\\\\equiv{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}) ,\\nand those present in A ~ \\\\textstyle\\\\widetilde{A} .\\nIntuitively, for a query asking how many telescopes ‚ÄúBernhard‚Äù has, we should make use of the information in ‚Äútelescope‚Äù and ‚ÄúBernhard‚Äù when deciding whether to take a certain deduction step. By constructing irrelevant axioms that also mention ‚Äútelescope‚Äù and/or ‚ÄúBernhard‚Äù , it becomes harder to distinguish what is relevant and what is not.\\nWe distinguish four cases: (i) neither the set of agents nor the entity occur in A ~ \\\\textstyle\\\\widetilde{A} ( no overlap ), (ii) the agent does not occur in A ~ \\\\textstyle\\\\widetilde{A} but the entity does ( entity overlap ), (iii) the entity does not occur in A ~ \\\\textstyle\\\\widetilde{A} but the set of agents does ( agent overlap ),\\nand (iv) entity overlap in which the agents occurring in A ~ \\\\textstyle\\\\widetilde{A} have lexical overlap with the set of agents ( agent and entity overlap ), e.g., if bernhard is in h g h_{g} then A ~ \\\\textstyle\\\\widetilde{A} contains agents like bernhard‚Äôs_student or bernhard‚Äôs_son .\\nEntities that do not overlap are always made topically related, e.g., if A W A_{\\\\text{W}} contains axioms with telescope , then A ~ \\\\textstyle\\\\widetilde{A} may contain binocular .',\n",
       "    'subsections': [{'title': '4.1 Modeling Math Word Problems with Verbalized Logic Programs',\n",
       "      'paragraphs': 'We consider a particular family of logic programs to represent GSM problems, adapted from Opedal et\\xa0al. ( 2023 , 2025 ) .\\nAll of the logic programs have the signature Œ£ W = ( Œ£ p W , Œ£ x W , Œ£ X W ) {{\\\\Sigma}^{\\\\text{W}}}=({{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}},{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{\\\\text{W}}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{\\\\text{W}}}) .\\nThe set of predicate symbols Œ£ p W = { cont , comp , transfer , rate , compeq } {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}}=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{compeq}}\\\\} corresponds to arithmetic concepts that occur in GSM word problems (Riley et\\xa0al., 1983 ) , e.g., cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} for denoting how many entities an agent cont ains, comp {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}} for comp aring the number of entities across multiple agents, or transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} for expressing one agent transfer ing entities to another.\\nWe partition Œ£ x W = 2 x A ‚äî E x ‚äî ‚Ñï x q ‚äî ‚Ñï x t {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{\\\\text{W}}}={2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}}\\\\sqcup{{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\sqcup{\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\sqcup{\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and Œ£ X W = 2 X A ‚äî E X ‚äî ‚Ñï X q ‚äî ‚Ñï X t {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{\\\\text{W}}}={2_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{{{A}}}}\\\\sqcup{{E}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\sqcup{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{q}}\\\\sqcup{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{t}} into the following pairs:\\nsets of strings ( 2 x A , 2 X A ) ({2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}},{2_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{{{A}}}}) called sets of agents ( who possesses), strings ( E x , E X {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{E}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} ) called entities ( what is possessed),\\nnatural numbers ( ‚Ñï x q , ‚Ñï X q ) ({\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{q}}) called quantities ( how much is possessed),\\nand natural numbers ( ‚Ñï x t , ‚Ñï X t ) ({\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{t}}) called timestamps ( time of possession).\\nWe note that 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} is a power set of the set of agent strings A x {{{A}}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , which enables us to code a state where multiple agents possess the same entity jointly.\\nThe family of logic programs all share the same inference rules, which are given in Table Àú 1 . Fig. Àú 1 shows an example proof using these inference rules, omitting built-ins.\\nWe note that possession may change over time and is governed by the transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} predicate in Rule (2). 9 9 9 The time component requires the addition of Rule (6), which expresses that all theorems with time T ‚Ñï {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}} that are not affected by a proof step using Rule (2) maintain their truth value at T ‚Ñï + 1 {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}} . This is an instance of the frame problem (McCarthy & Hayes, 1969 ; Sandewall, 1972 ; Hanks & McDermott, 1987 ) .\\nTo express this rule, we introduce negation into the logic program; see ¬ß A.1 for background on negation in logic programming.\\nThe only negated predicate in Table 1 is ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} ; because ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} only appears in the body of an inference rule, the program is trivially stratified ( ¬ß A.1 ). That is ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atoms can not be proved true, but we assume ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atoms if the corresponding transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atom is not known to be true from the axioms.\\nThe rules given in Table Àú 1 are held constant across all logic programs in our family.\\nIndeed, what distinguishes one program from another, then, is the choice of axioms.\\nFor example, consider the axiom cont \\u200b ( { ryan } , 5 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) which expresses that ‚ÄúRyan has 5 cats at time step 2‚Äù ; the predicate cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} represents the semantics of possession, and { ryan } ‚àà 2 x A {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}}\\\\in{2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , 5 ‚àà ‚Ñï x q {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}\\\\in{\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , cat ‚àà E x {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}}\\\\in{{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , and 2 ‚àà ‚Ñï x t {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}\\\\in{\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} are all ground atoms of different types.\\nAdditionally, we only consider axiom sets A W ‚äÇ H ¬Ø A_{\\\\text{W}}\\\\subset\\\\overline{H} that have the following property:\\nWe call an interpretation I ‚äÜ H ¬Ø I\\\\subseteq\\\\overline{H} of a logic program in our family ( Table Àú 1 ) numerically consistent if there do not exist two substitutions Œ∏ = { ( A ùíú , t a ) , ( X ‚Ñ§ , t ) , ( E Œî ‚àó , t e ) , ( T ‚Ñï , t t ) } \\\\theta=\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},t_{a}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}},t),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},t_{e}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}},t_{t})\\\\} and Œ∏ ‚Ä≤ = { ( A ùíú , t a ) , ( X ‚Ñ§ , t ‚Ä≤ ) , ( E Œî ‚àó , t e ) , ( T ‚Ñï , t t ) } \\\\theta^{\\\\prime}=\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},t_{a}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}},t^{\\\\prime}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},t_{e}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}},t_{t})\\\\} such that t ‚â† t ‚Ä≤ t\\\\neq t^{\\\\prime} and both cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) / Œ∏ , cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) / Œ∏ ‚Ä≤ ‚àà T ùí´ W ‚àó \\u200b ( I ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}})/\\\\theta,{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}})/\\\\theta^{\\\\prime}\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}_{\\\\text{W}}}(I) .\\nThis ensures that the minimal Herbrand model does not contain contradictory pairs such as cont \\u200b ( { ryan } , 5 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) and cont \\u200b ( { ryan } , 4 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) .\\nFor example, the following set of axioms is numerically consistent: A W = { cont \\u200b ( { ryan } , 5 , cat , 2 ) , comp \\u200b ( { eleanor } , { ryan } , 3 , cat , 2 ) } A_{\\\\text{W}}=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}),{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{eleanor\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}})\\\\} , expressing that ‚ÄúRyan has 5 cats‚Äù and that ‚ÄúEleanor has 3 more cats than Ryan‚Äù . If we were to, e.g., include the two axioms { cont \\u200b ( { andreas } , 7 , cat , 2 ) , comp \\u200b ( { eleanor } , { andreas } , 3 , cat , 2 ) } \\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{andreas\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{7}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}),{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{eleanor\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{andreas\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}})\\\\} , we would obtain a numerically inconsistent axiom set.\\nInference in our family of logic programs is decidable under numerically consistent axiom sets. See App. Àú B for details.',\n",
       "      'subsections': [{'title': 'Logic Programming for Grade School Math Word Problems.',\n",
       "        'paragraphs': 'We consider a particular family of logic programs to represent GSM problems, adapted from Opedal et\\xa0al. ( 2023 , 2025 ) .\\nAll of the logic programs have the signature Œ£ W = ( Œ£ p W , Œ£ x W , Œ£ X W ) {{\\\\Sigma}^{\\\\text{W}}}=({{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}},{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{\\\\text{W}}},{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{\\\\text{W}}}) .\\nThe set of predicate symbols Œ£ p W = { cont , comp , transfer , rate , compeq } {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}}=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{compeq}}\\\\} corresponds to arithmetic concepts that occur in GSM word problems (Riley et\\xa0al., 1983 ) , e.g., cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} for denoting how many entities an agent cont ains, comp {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}} for comp aring the number of entities across multiple agents, or transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} for expressing one agent transfer ing entities to another.\\nWe partition Œ£ x W = 2 x A ‚äî E x ‚äî ‚Ñï x q ‚äî ‚Ñï x t {{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{\\\\text{W}}}={2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}}\\\\sqcup{{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\sqcup{\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\sqcup{\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and Œ£ X W = 2 X A ‚äî E X ‚äî ‚Ñï X q ‚äî ‚Ñï X t {{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{\\\\text{W}}}={2_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{{{A}}}}\\\\sqcup{{E}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\sqcup{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{q}}\\\\sqcup{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{t}} into the following pairs:\\nsets of strings ( 2 x A , 2 X A ) ({2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}},{2_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{{{A}}}}) called sets of agents ( who possesses), strings ( E x , E X {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{{E}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}} ) called entities ( what is possessed),\\nnatural numbers ( ‚Ñï x q , ‚Ñï X q ) ({\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{q}}) called quantities ( how much is possessed),\\nand natural numbers ( ‚Ñï x t , ‚Ñï X t ) ({\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}},{\\\\mathbb{N}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}^{t}}) called timestamps ( time of possession).\\nWe note that 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} is a power set of the set of agent strings A x {{{A}}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , which enables us to code a state where multiple agents possess the same entity jointly.\\nThe family of logic programs all share the same inference rules, which are given in Table Àú 1 . Fig. Àú 1 shows an example proof using these inference rules, omitting built-ins.\\nWe note that possession may change over time and is governed by the transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} predicate in Rule (2). 9 9 9 The time component requires the addition of Rule (6), which expresses that all theorems with time T ‚Ñï {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}} that are not affected by a proof step using Rule (2) maintain their truth value at T ‚Ñï + 1 {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}} . This is an instance of the frame problem (McCarthy & Hayes, 1969 ; Sandewall, 1972 ; Hanks & McDermott, 1987 ) .\\nTo express this rule, we introduce negation into the logic program; see ¬ß A.1 for background on negation in logic programming.\\nThe only negated predicate in Table 1 is ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} ; because ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} only appears in the body of an inference rule, the program is trivially stratified ( ¬ß A.1 ). That is ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atoms can not be proved true, but we assume ¬¨ transfer \\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atoms if the corresponding transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} atom is not known to be true from the axioms.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Axioms.',\n",
       "        'paragraphs': 'The rules given in Table Àú 1 are held constant across all logic programs in our family.\\nIndeed, what distinguishes one program from another, then, is the choice of axioms.\\nFor example, consider the axiom cont \\u200b ( { ryan } , 5 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) which expresses that ‚ÄúRyan has 5 cats at time step 2‚Äù ; the predicate cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} represents the semantics of possession, and { ryan } ‚àà 2 x A {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}}\\\\in{2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , 5 ‚àà ‚Ñï x q {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}}\\\\in{\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , cat ‚àà E x {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}}\\\\in{{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , and 2 ‚àà ‚Ñï x t {\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}\\\\in{\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} are all ground atoms of different types.\\nAdditionally, we only consider axiom sets A W ‚äÇ H ¬Ø A_{\\\\text{W}}\\\\subset\\\\overline{H} that have the following property:\\nWe call an interpretation I ‚äÜ H ¬Ø I\\\\subseteq\\\\overline{H} of a logic program in our family ( Table Àú 1 ) numerically consistent if there do not exist two substitutions Œ∏ = { ( A ùíú , t a ) , ( X ‚Ñ§ , t ) , ( E Œî ‚àó , t e ) , ( T ‚Ñï , t t ) } \\\\theta=\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},t_{a}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}},t),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},t_{e}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}},t_{t})\\\\} and Œ∏ ‚Ä≤ = { ( A ùíú , t a ) , ( X ‚Ñ§ , t ‚Ä≤ ) , ( E Œî ‚àó , t e ) , ( T ‚Ñï , t t ) } \\\\theta^{\\\\prime}=\\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},t_{a}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{Z}},t^{\\\\prime}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},t_{e}),({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}},t_{t})\\\\} such that t ‚â† t ‚Ä≤ t\\\\neq t^{\\\\prime} and both cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) / Œ∏ , cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) / Œ∏ ‚Ä≤ ‚àà T ùí´ W ‚àó \\u200b ( I ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}})/\\\\theta,{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}})/\\\\theta^{\\\\prime}\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}_{\\\\text{W}}}(I) .\\nThis ensures that the minimal Herbrand model does not contain contradictory pairs such as cont \\u200b ( { ryan } , 5 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) and cont \\u200b ( { ryan } , 4 , cat , 2 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}) .\\nFor example, the following set of axioms is numerically consistent: A W = { cont \\u200b ( { ryan } , 5 , cat , 2 ) , comp \\u200b ( { eleanor } , { ryan } , 3 , cat , 2 ) } A_{\\\\text{W}}=\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}),{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{eleanor\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{ryan\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}})\\\\} , expressing that ‚ÄúRyan has 5 cats‚Äù and that ‚ÄúEleanor has 3 more cats than Ryan‚Äù . If we were to, e.g., include the two axioms { cont \\u200b ( { andreas } , 7 , cat , 2 ) , comp \\u200b ( { eleanor } , { andreas } , 3 , cat , 2 ) } \\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{andreas\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{7}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}),{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{eleanor\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{andreas\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{cat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}})\\\\} , we would obtain a numerically inconsistent axiom set.\\nInference in our family of logic programs is decidable under numerically consistent axiom sets. See App. Àú B for details.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.2 Language Modeling and Proofs in Natural Language',\n",
       "      'paragraphs': 'We give a brief formal introduction to language modeling.\\nLet Œì \\\\Gamma be an alphabet of tokens and Œì ‚àó \\\\Gamma^{\\\\!*} be the set of all strings over Œì \\\\Gamma , its Kleene closure.\\nWe write ùíò ‚àà Œì ‚àó {\\\\bm{w}}\\\\in\\\\Gamma^{\\\\!*} for a string, w t {{w}_{t}} for the token at the t th t^{\\\\text{th}} position in ùíò = w 1 ‚ãÖ ‚ãÖ ‚ãÖ w T {\\\\bm{w}}=w_{1}\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot w_{T} , and | ùíò | = T |{\\\\bm{w}}|=T for the number of tokens in ùíò {\\\\bm{w}} , i.e., its length.\\nA language model ( LM ) p p is a probability distribution on Œì ‚àó \\\\Gamma^{\\\\!*} .\\nLet eos ‚àâ Œì {\\\\textsc{eos}}\\\\notin\\\\Gamma be a distinguished symbol denoting the end of a token string.\\nThe probability of a string ùíò {\\\\bm{w}} can be written autoregressively as p \\u200b ( ùíò ) = p ‚Üí \\u200b ( eos ‚à£ ùíò ) \\u200b ‚àè t = 1 | ùíò | p ‚Üí \\u200b ( w t ‚à£ ùíò < t ) p({\\\\bm{w}})=\\\\overrightarrow{p}({\\\\textsc{eos}}\\\\mid{\\\\bm{w}})\\\\prod_{t=1}^{|{\\\\bm{w}}|}\\\\overrightarrow{p}({{w}_{t}}\\\\mid{{\\\\bm{w}}_{<t}}) ,\\nwhere p ‚Üí ( ‚ãÖ ‚à£ ùíÑ ) \\\\overrightarrow{p}(\\\\cdot\\\\mid{\\\\bm{c}}) is a probability distribution over Œì ¬Ø = def Œì ‚à™ { eos } \\\\overline{\\\\Gamma}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\Gamma\\\\cup\\\\{{\\\\textsc{eos}}\\\\} conditioned on the context ùíÑ ‚àà Œì ‚àó {\\\\bm{c}}\\\\in\\\\Gamma^{\\\\!*} .\\nTo bridge reasoning in formal proof systems to reasoning in natural language, we introduce the idea of a verbalized logic program .\\nGiven a logic program ùí´ \\\\mathcal{P} with negation ( ¬ß Àú A.1 ),\\nlet H ¬Ø \\\\overline{H} be its extended Herband base.\\nWe associate each atom b ‚àà H ¬Ø b\\\\in\\\\overline{H} with a set of natural language strings.\\nTo that end, we define a verbalizer ŒΩ ùí´ : H ¬Ø ‚Üí 2 Œì ‚àó \\\\nu_{\\\\mathcal{P}}\\\\colon\\\\overline{H}\\\\rightarrow 2^{\\\\Gamma^{\\\\!*}} , where each ŒΩ ùí´ \\u200b ( b ) \\\\nu_{\\\\mathcal{P}}(b) is a disjoint set.\\nFor every b ‚àà H ¬Ø b\\\\in\\\\overline{H} ,\\nthe set ŒΩ ùí´ \\u200b ( b ) \\\\nu_{\\\\mathcal{P}}(b) represents the various ways in which the meaning of b b can be expressed in natural language.\\nIn this paper, we take a straightforward approach.\\nFor each b ‚àà H ¬Ø b\\\\in\\\\overline{H} , we construct a finite set G b ‚äÇ Œì ‚àó G_{b}\\\\subset\\\\Gamma^{\\\\!*} .\\nMoreover, we enforce disjointness, i.e., G b 2 ‚à© G b 1 = ‚àÖ G_{b_{2}}\\\\cap G_{b_{1}}=\\\\varnothing for b 1 , b 2 ‚àà H ¬Ø b_{1},b_{2}\\\\in\\\\overline{H} , and that, for all b ‚àà H b\\\\in H , each string in G b G_{b} ends in a distinguished separator symbol‚Äîin our case a period ‚Äú.‚Äù ‚Äîthat appears nowhere else in the string.\\nThese assumptions allow for trivial linear-time parsing of natural language text into a sequence of atoms in the verbalized logic programs.\\nIn practice, we generate the strings in each G b G_{b} with a series of hand-written templates.\\nWe give examples in Fig. Àú 2 .',\n",
       "      'subsections': [{'title': 'Language Modeling.',\n",
       "        'paragraphs': 'We give a brief formal introduction to language modeling.\\nLet Œì \\\\Gamma be an alphabet of tokens and Œì ‚àó \\\\Gamma^{\\\\!*} be the set of all strings over Œì \\\\Gamma , its Kleene closure.\\nWe write ùíò ‚àà Œì ‚àó {\\\\bm{w}}\\\\in\\\\Gamma^{\\\\!*} for a string, w t {{w}_{t}} for the token at the t th t^{\\\\text{th}} position in ùíò = w 1 ‚ãÖ ‚ãÖ ‚ãÖ w T {\\\\bm{w}}=w_{1}\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot w_{T} , and | ùíò | = T |{\\\\bm{w}}|=T for the number of tokens in ùíò {\\\\bm{w}} , i.e., its length.\\nA language model ( LM ) p p is a probability distribution on Œì ‚àó \\\\Gamma^{\\\\!*} .\\nLet eos ‚àâ Œì {\\\\textsc{eos}}\\\\notin\\\\Gamma be a distinguished symbol denoting the end of a token string.\\nThe probability of a string ùíò {\\\\bm{w}} can be written autoregressively as p \\u200b ( ùíò ) = p ‚Üí \\u200b ( eos ‚à£ ùíò ) \\u200b ‚àè t = 1 | ùíò | p ‚Üí \\u200b ( w t ‚à£ ùíò < t ) p({\\\\bm{w}})=\\\\overrightarrow{p}({\\\\textsc{eos}}\\\\mid{\\\\bm{w}})\\\\prod_{t=1}^{|{\\\\bm{w}}|}\\\\overrightarrow{p}({{w}_{t}}\\\\mid{{\\\\bm{w}}_{<t}}) ,\\nwhere p ‚Üí ( ‚ãÖ ‚à£ ùíÑ ) \\\\overrightarrow{p}(\\\\cdot\\\\mid{\\\\bm{c}}) is a probability distribution over Œì ¬Ø = def Œì ‚à™ { eos } \\\\overline{\\\\Gamma}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\Gamma\\\\cup\\\\{{\\\\textsc{eos}}\\\\} conditioned on the context ùíÑ ‚àà Œì ‚àó {\\\\bm{c}}\\\\in\\\\Gamma^{\\\\!*} .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Verbalized Logic Programs.',\n",
       "        'paragraphs': 'To bridge reasoning in formal proof systems to reasoning in natural language, we introduce the idea of a verbalized logic program .\\nGiven a logic program ùí´ \\\\mathcal{P} with negation ( ¬ß Àú A.1 ),\\nlet H ¬Ø \\\\overline{H} be its extended Herband base.\\nWe associate each atom b ‚àà H ¬Ø b\\\\in\\\\overline{H} with a set of natural language strings.\\nTo that end, we define a verbalizer ŒΩ ùí´ : H ¬Ø ‚Üí 2 Œì ‚àó \\\\nu_{\\\\mathcal{P}}\\\\colon\\\\overline{H}\\\\rightarrow 2^{\\\\Gamma^{\\\\!*}} , where each ŒΩ ùí´ \\u200b ( b ) \\\\nu_{\\\\mathcal{P}}(b) is a disjoint set.\\nFor every b ‚àà H ¬Ø b\\\\in\\\\overline{H} ,\\nthe set ŒΩ ùí´ \\u200b ( b ) \\\\nu_{\\\\mathcal{P}}(b) represents the various ways in which the meaning of b b can be expressed in natural language.\\nIn this paper, we take a straightforward approach.\\nFor each b ‚àà H ¬Ø b\\\\in\\\\overline{H} , we construct a finite set G b ‚äÇ Œì ‚àó G_{b}\\\\subset\\\\Gamma^{\\\\!*} .\\nMoreover, we enforce disjointness, i.e., G b 2 ‚à© G b 1 = ‚àÖ G_{b_{2}}\\\\cap G_{b_{1}}=\\\\varnothing for b 1 , b 2 ‚àà H ¬Ø b_{1},b_{2}\\\\in\\\\overline{H} , and that, for all b ‚àà H b\\\\in H , each string in G b G_{b} ends in a distinguished separator symbol‚Äîin our case a period ‚Äú.‚Äù ‚Äîthat appears nowhere else in the string.\\nThese assumptions allow for trivial linear-time parsing of natural language text into a sequence of atoms in the verbalized logic programs.\\nIn practice, we generate the strings in each G b G_{b} with a series of hand-written templates.\\nWe give examples in Fig. Àú 2 .',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.3 Generating Problems with Irrelevant Axioms',\n",
       "      'paragraphs': 'We introduce a simple sampling algorithm, presented and analyzed in App. Àú C , that samples a ground goal theorem h g ‚â° cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) h_{g}\\\\equiv{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}) and a shortest proof of h g h_{g} under the rules ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} from Table Àú 1 . By construction, the axioms A W A_{\\\\text{W}} in the shortest proof are numerically consistent and all axioms in A W A_{\\\\text{W}} will be used in the proof of h g h_{g} in the program ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} ; we denote this dependency by A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) . 10 10 10 A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) is the unique smallest set of axioms ( Lemma 1 ) needed to prove h g h_{g} , justifying the function notation. That is, A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) contains no irrelevant axioms.\\nHowever, to produce logic programs that do contain irrelevant axioms, we do the following.\\nWe sample an additional M M distinct distractor theorems { h ~ m } m = 1 M \\\\{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m}\\\\}_{m=1}^{M} .\\nFor each distractor theorem h ~ m \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m} , we again apply our axiom sampling procedure to generate distractor axioms A ~ m \\u200b ( h ~ m ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m}) .\\nImportantly, we are able to show that A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) remain on a shortest proof of h g h_{g} even in the case that we consider the augmented logic program ùí´ ~ W = ‚Ñõ W ‚äî A W ( h g ) ‚äî A ~ 1 ( h ~ 1 ) ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M ( h ~ M ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{\\\\mathcal{P}}$}}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}}(h_{g})\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M}) ; see ¬ß Àú C.4 .\\nIn our experiments, we vary the size of the irrelevant axiom sets A ~ = A ~ 1 ( h ~ 1 ) ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M ( h ~ M ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}=\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M}) as follows: (i) a single irrelevant axiom ( w/ axiom ), where M = 1 M=1 , | A ~ 1 \\u200b ( h ~ 1 ) | = 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})|=1 , and A ~ 1 \\u200b ( h ~ 1 ) = { h ~ 1 } \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})=\\\\{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1}\\\\} (i.e., the distractor theorem is trivial); (ii) a single irrelevant tree ( w/ tree ), where M = 1 M=1 , | A ~ 1 \\u200b ( h ~ 1 ) | > 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})|>1 , and h ~ 1 \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1} has a non-trivial proof; (iii) multiple irrelevant trees ( w/ multiple trees ), where M = 3 M=3 , | A ~ m \\u200b ( h ~ m ) | > 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m})|>1 for all m m , and h ~ 1 , h ~ 2 , h ~ 3 \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1},\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{2},\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{3} all have non-trivial proofs.\\nWe additionally control for overlap between the set of agents, i.e., elements of 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , and the entities, i.e., elements of E x {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , of the goal theorem h g ‚â° cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) h_{g}\\\\equiv{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}) ,\\nand those present in A ~ \\\\textstyle\\\\widetilde{A} .\\nIntuitively, for a query asking how many telescopes ‚ÄúBernhard‚Äù has, we should make use of the information in ‚Äútelescope‚Äù and ‚ÄúBernhard‚Äù when deciding whether to take a certain deduction step. By constructing irrelevant axioms that also mention ‚Äútelescope‚Äù and/or ‚ÄúBernhard‚Äù , it becomes harder to distinguish what is relevant and what is not.\\nWe distinguish four cases: (i) neither the set of agents nor the entity occur in A ~ \\\\textstyle\\\\widetilde{A} ( no overlap ), (ii) the agent does not occur in A ~ \\\\textstyle\\\\widetilde{A} but the entity does ( entity overlap ), (iii) the entity does not occur in A ~ \\\\textstyle\\\\widetilde{A} but the set of agents does ( agent overlap ),\\nand (iv) entity overlap in which the agents occurring in A ~ \\\\textstyle\\\\widetilde{A} have lexical overlap with the set of agents ( agent and entity overlap ), e.g., if bernhard is in h g h_{g} then A ~ \\\\textstyle\\\\widetilde{A} contains agents like bernhard‚Äôs_student or bernhard‚Äôs_son .\\nEntities that do not overlap are always made topically related, e.g., if A W A_{\\\\text{W}} contains axioms with telescope , then A ~ \\\\textstyle\\\\widetilde{A} may contain binocular .',\n",
       "      'subsections': [{'title': 'Sampling Axiom Sets.',\n",
       "        'paragraphs': 'We introduce a simple sampling algorithm, presented and analyzed in App. Àú C , that samples a ground goal theorem h g ‚â° cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) h_{g}\\\\equiv{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}) and a shortest proof of h g h_{g} under the rules ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} from Table Àú 1 . By construction, the axioms A W A_{\\\\text{W}} in the shortest proof are numerically consistent and all axioms in A W A_{\\\\text{W}} will be used in the proof of h g h_{g} in the program ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} ; we denote this dependency by A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) . 10 10 10 A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) is the unique smallest set of axioms ( Lemma 1 ) needed to prove h g h_{g} , justifying the function notation. That is, A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) contains no irrelevant axioms.\\nHowever, to produce logic programs that do contain irrelevant axioms, we do the following.\\nWe sample an additional M M distinct distractor theorems { h ~ m } m = 1 M \\\\{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m}\\\\}_{m=1}^{M} .\\nFor each distractor theorem h ~ m \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m} , we again apply our axiom sampling procedure to generate distractor axioms A ~ m \\u200b ( h ~ m ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m}) .\\nImportantly, we are able to show that A W \\u200b ( h g ) A_{\\\\text{W}}(h_{g}) remain on a shortest proof of h g h_{g} even in the case that we consider the augmented logic program ùí´ ~ W = ‚Ñõ W ‚äî A W ( h g ) ‚äî A ~ 1 ( h ~ 1 ) ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M ( h ~ M ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{\\\\mathcal{P}}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{\\\\mathcal{P}}$}}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}}(h_{g})\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M}) ; see ¬ß Àú C.4 .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Structural Overlap.',\n",
       "        'paragraphs': 'In our experiments, we vary the size of the irrelevant axiom sets A ~ = A ~ 1 ( h ~ 1 ) ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M ( h ~ M ) \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}=\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M}) as follows: (i) a single irrelevant axiom ( w/ axiom ), where M = 1 M=1 , | A ~ 1 \\u200b ( h ~ 1 ) | = 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})|=1 , and A ~ 1 \\u200b ( h ~ 1 ) = { h ~ 1 } \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})=\\\\{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1}\\\\} (i.e., the distractor theorem is trivial); (ii) a single irrelevant tree ( w/ tree ), where M = 1 M=1 , | A ~ 1 \\u200b ( h ~ 1 ) | > 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1})|>1 , and h ~ 1 \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1} has a non-trivial proof; (iii) multiple irrelevant trees ( w/ multiple trees ), where M = 3 M=3 , | A ~ m \\u200b ( h ~ m ) | > 1 |\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m}(\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{m})|>1 for all m m , and h ~ 1 , h ~ 2 , h ~ 3 \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1},\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{2},\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{3} all have non-trivial proofs.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Agent and Entity Overlap.',\n",
       "        'paragraphs': 'We additionally control for overlap between the set of agents, i.e., elements of 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , and the entities, i.e., elements of E x {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} , of the goal theorem h g ‚â° cont \\u200b ( A ùíú , X ‚Ñï , E Œî ‚àó , T ‚Ñï ) h_{g}\\\\equiv{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}_{\\\\mathcal{A}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{\\\\mathbb{N}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}}_{{\\\\Delta}^{\\\\!*}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}) ,\\nand those present in A ~ \\\\textstyle\\\\widetilde{A} .\\nIntuitively, for a query asking how many telescopes ‚ÄúBernhard‚Äù has, we should make use of the information in ‚Äútelescope‚Äù and ‚ÄúBernhard‚Äù when deciding whether to take a certain deduction step. By constructing irrelevant axioms that also mention ‚Äútelescope‚Äù and/or ‚ÄúBernhard‚Äù , it becomes harder to distinguish what is relevant and what is not.\\nWe distinguish four cases: (i) neither the set of agents nor the entity occur in A ~ \\\\textstyle\\\\widetilde{A} ( no overlap ), (ii) the agent does not occur in A ~ \\\\textstyle\\\\widetilde{A} but the entity does ( entity overlap ), (iii) the entity does not occur in A ~ \\\\textstyle\\\\widetilde{A} but the set of agents does ( agent overlap ),\\nand (iv) entity overlap in which the agents occurring in A ~ \\\\textstyle\\\\widetilde{A} have lexical overlap with the set of agents ( agent and entity overlap ), e.g., if bernhard is in h g h_{g} then A ~ \\\\textstyle\\\\widetilde{A} contains agents like bernhard‚Äôs_student or bernhard‚Äôs_son .\\nEntities that do not overlap are always made topically related, e.g., if A W A_{\\\\text{W}} contains axioms with telescope , then A ~ \\\\textstyle\\\\widetilde{A} may contain binocular .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '5 Experiments',\n",
       "    'paragraphs': 'We use proofs generated from the family of verbalized logic programs introduced in ¬ß Àú 4 to help understand how LMs reason about GSM problems.\\nOur primary experimental manipulative is irrelevant axioms with respect to a goal theorem introduced into a logic program, which allows us to analyze how LMs fare in the face of irrelevant axioms.\\nWe generate 500 problems with varying structural, agent and entity overlap, as discussed above.\\nSee ¬ß Àú D.1 for more details on the make-up of the dataset.\\nWe refer to the problems without any irrelevant axioms as base problems .\\nWe additionally generate control problems that have the same number of axioms as the problems with irrelevant axioms, except that all axioms are relevant.\\nTheir shortest proofs contain the base problem‚Äôs shortest proof as a subproof.\\nThis controls for the possible confounder of problem length (see, e.g., Leeb et\\xa0al., 2025 ).\\nWe consider both non-ground queries, corresponding to questions like ‚ÄúHow many drones does Yanick have?‚Äù , 11 11 11 Assuming numerical consistency ensures there exists at most one ground atom in the minimal Herbrand model that unifies with the non-ground query.\\nThis ensures that forward chaining would halt in finite time. and ground queries, corresponding to questions like ‚ÄúShow that Yanick has 5 drones.‚Äù .\\nWe use one ‚Äúvanilla‚Äù LM and three reasoning LMs in our experiments: Llama-3.1-8B-Instruct (Llama Team, 2024 ) , Qwen2.5-Math-7B-Instruct (Yang et\\xa0al., 2024 ) , Qwen‚Äôs reasoning model QwQ-32B, and DeepSeek-R1 (DeepSeek-AI, 2025 ) . We generate strings using ancestral sampling, restricting the context length to 4000 4000 tokens.\\nOur experimental design is based on in-context learning (Brown et\\xa0al., 2020 ) .\\nSpecifically, we use five fixed in-context examples of shortest proofs to expose the LM to proofs in our verbalized logic programs.\\nThe verbalized proofs are ordered under a DFS traversal of the theorems in the proof, such that the axioms are popped in the same order they occur in the verbalized text.\\nSee ¬ß Àú D.2 for the prompt.\\nWe begin by analyzing how irrelevant axioms influence an LM‚Äôs ability to generate the correct goal theorem for non-ground queries.\\nWe employ the two-step prompting strategy given by Kojima et\\xa0al. ( 2022 ) .\\nIn the first step, the model is prompted to produce a natural-language proof outlining its reasoning process.\\nThis natural-language process is then mapped to a proof in the verbalized logic program.\\nIn the second step, we prompted the LM a second time‚Äîconditioned on the proof it generated‚Äîto produce the goal theorem.\\nIn Table Àú 2 , we report model accuracy in generating the correct goal theorem.\\nWe observe that even a single irrelevant axiom reduces model performance, particularly for Llama-3.1 and Qwen2.5.\\nPerformance degrades further as additional irrelevant axioms are introduced.\\nThe performance of the most capable model, DeepSeek-R1, is nearly saturated at perfect accuracy, though slight decreases are still observed when irrelevant axioms are included.\\nAcross models, accuracy on problems containing irrelevant axioms is almost always lower than on the corresponding control examples, suggesting that irrelevance has a substantial effect on accuracy beyond what can be explained by longer problem statements.\\nThe only exception is Llama-3.1, whose performance on problems with one irrelevant axiom exceeds that of the corresponding control.\\nQwQ-32B stands out as an outlier: for this model, performance on the control problems is significantly higher than on the original base problems.\\nIn Fig. Àú 7 ( ¬ß Àú D.4 ), we present results stratified by agent and entity overlap ( ¬ß Àú 4.3 ).\\nA consistent pattern emerges: such overlap between the goal and irrelevant axioms makes solving the problem more difficult.\\nCompared to no overlap, performance drops with both kinds of overlap, suggesting both serve as heuristics during search. While drops are typically larger for agent overlap than for entity overlap, we note that this could be partially due to the entities being topically related ( ¬ß Àú 4.3 ).\\nIn the following subsection, we analyze the proofs in greater detail to further illuminate these effects.\\nIn Fig. Àú 3 we plot the empirical distribution over the number of tokens in the model‚Äôs output and compare it to the number of tokens in the natural language annotation of the shortest proof, taking only the proofs that concluded at the correct goal theorem. This analysis, as well as those in the remainder of this section, are done on the problems with multiple irrelevant trees (i.e., the kind of structural overlap with the most axioms; ¬ß Àú 4.3 ).\\nWe observe that all models often use more tokens than are in the annotations, suggesting that they use more compute than necessary to prove goal theorems. This is consistent with findings on reasoning models ( ¬ß Àú 2 ), but holds also for the Llama model. However, these results do not confirm that the models generate irrelevant theorems‚Äîthey might just be more verbose than our annotations.\\nThis leads us into our efficiency analysis in line with the technical exposition in ¬ß Àú 3.2 .\\nThis analysis is performed for Llama-3.1-8B-Instruct and Qwen2.5-Math-7B-Instruct since only those models followed the required formatting ( ¬ß Àú 4.2 ); 12 12 12 We manually verified parsing accuracy on the theorems generated by the models for a subset of 20 20 randomly sampled examples. An additional class representing that there is no match with any annotated theorem in the proof is included as well. The parser predicted the correct (or correctly predicted no) match in 394 / 397 = 99.2 % 394/397=99.2\\\\% of the theorems for Llama-3.1-8B-Instruct, and 381 / 384 = 99.2 % 381/384=99.2\\\\% of the theorems for Qwen2.5-Math-7B-Instruct. however, we perform a more crude analysis based on only the arithmetic expressions for the other two models in ¬ß Àú D.4 .\\nWe report the efficiency metric presented in ¬ß Àú 3.2 for the problems where the LM generated the correct goal theorem, comparing against the shortest proof generated with our method ( App. Àú C ). 13 13 13 LMs often generated the same theorems multiple times. We chose to ignore such duplicates in the evaluation. We omit built-ins from the efficiency analysis since those are not verbalized.\\nAdditionally, we note that it might be the case that all irrelevant steps the models take are axioms; the LMs may simply be stating that some axioms are irrelevant to the query.\\nWe therefore also consider a metric in which only non-axiom theorems are counted.\\nIn ¬ß Àú D.4 we provide an additional analysis on search order.\\nThe main results are shown in Table Àú 3 (non-ground queries), with scores stratified by agent and entity overlap. The efficiency scores are far from 100 % 100\\\\% , meaning that the models predict several theorems beyond the required ones present in the shortest proof.\\nAdditionally, we observe that the efficiency scores vary significantly across the type of overlap ( p < 0.001 ) (p<0.001) , so we conclude that lexical information in the query has a substantial effect on proof planning.\\nWe finally comment on the results that only consider non-axioms, presented at the last row in Table Àú 3 .\\nWe again observe efficiency scores that are considerably below 100 % 100\\\\% , showing that the LMs prove theorems that are irrelevant to the query. ¬ß Àú D.3 gives an example where Llama-3.1-8B proves irrelevant theorems.\\nWe compare the performance to the same problems when presented with ground queries.\\nQueries tend to be non-ground in the GSM domain (Riley et\\xa0al., 1983 ; Cobbe et\\xa0al., 2021 ) .\\nSince LMs are heavily influenced by training data, we therefore expect them to perform better on non-ground queries.\\nOur results on the same verbalized logic programs as before, but with ground queries, are presented on the right-hand side of Table Àú 3 . We observe lower efficiency scores, suggesting that LMs are indeed worse at proving ground theorems in this domain.',\n",
       "    'subsections': [{'title': 'Language Models.',\n",
       "      'paragraphs': 'We use one ‚Äúvanilla‚Äù LM and three reasoning LMs in our experiments: Llama-3.1-8B-Instruct (Llama Team, 2024 ) , Qwen2.5-Math-7B-Instruct (Yang et\\xa0al., 2024 ) , Qwen‚Äôs reasoning model QwQ-32B, and DeepSeek-R1 (DeepSeek-AI, 2025 ) . We generate strings using ancestral sampling, restricting the context length to 4000 4000 tokens.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Prompting.',\n",
       "      'paragraphs': 'Our experimental design is based on in-context learning (Brown et\\xa0al., 2020 ) .\\nSpecifically, we use five fixed in-context examples of shortest proofs to expose the LM to proofs in our verbalized logic programs.\\nThe verbalized proofs are ordered under a DFS traversal of the theorems in the proof, such that the axioms are popped in the same order they occur in the verbalized text.\\nSee ¬ß Àú D.2 for the prompt.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.1 Addition of Irrelevant Axioms and Answer Accuracy',\n",
       "      'paragraphs': 'We begin by analyzing how irrelevant axioms influence an LM‚Äôs ability to generate the correct goal theorem for non-ground queries.\\nWe employ the two-step prompting strategy given by Kojima et\\xa0al. ( 2022 ) .\\nIn the first step, the model is prompted to produce a natural-language proof outlining its reasoning process.\\nThis natural-language process is then mapped to a proof in the verbalized logic program.\\nIn the second step, we prompted the LM a second time‚Äîconditioned on the proof it generated‚Äîto produce the goal theorem.\\nIn Table Àú 2 , we report model accuracy in generating the correct goal theorem.\\nWe observe that even a single irrelevant axiom reduces model performance, particularly for Llama-3.1 and Qwen2.5.\\nPerformance degrades further as additional irrelevant axioms are introduced.\\nThe performance of the most capable model, DeepSeek-R1, is nearly saturated at perfect accuracy, though slight decreases are still observed when irrelevant axioms are included.\\nAcross models, accuracy on problems containing irrelevant axioms is almost always lower than on the corresponding control examples, suggesting that irrelevance has a substantial effect on accuracy beyond what can be explained by longer problem statements.\\nThe only exception is Llama-3.1, whose performance on problems with one irrelevant axiom exceeds that of the corresponding control.\\nQwQ-32B stands out as an outlier: for this model, performance on the control problems is significantly higher than on the original base problems.\\nIn Fig. Àú 7 ( ¬ß Àú D.4 ), we present results stratified by agent and entity overlap ( ¬ß Àú 4.3 ).\\nA consistent pattern emerges: such overlap between the goal and irrelevant axioms makes solving the problem more difficult.\\nCompared to no overlap, performance drops with both kinds of overlap, suggesting both serve as heuristics during search. While drops are typically larger for agent overlap than for entity overlap, we note that this could be partially due to the entities being topically related ( ¬ß Àú 4.3 ).\\nIn the following subsection, we analyze the proofs in greater detail to further illuminate these effects.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Addition of Irrelevant Axioms and Efficiency',\n",
       "      'paragraphs': 'In Fig. Àú 3 we plot the empirical distribution over the number of tokens in the model‚Äôs output and compare it to the number of tokens in the natural language annotation of the shortest proof, taking only the proofs that concluded at the correct goal theorem. This analysis, as well as those in the remainder of this section, are done on the problems with multiple irrelevant trees (i.e., the kind of structural overlap with the most axioms; ¬ß Àú 4.3 ).\\nWe observe that all models often use more tokens than are in the annotations, suggesting that they use more compute than necessary to prove goal theorems. This is consistent with findings on reasoning models ( ¬ß Àú 2 ), but holds also for the Llama model. However, these results do not confirm that the models generate irrelevant theorems‚Äîthey might just be more verbose than our annotations.\\nThis leads us into our efficiency analysis in line with the technical exposition in ¬ß Àú 3.2 .\\nThis analysis is performed for Llama-3.1-8B-Instruct and Qwen2.5-Math-7B-Instruct since only those models followed the required formatting ( ¬ß Àú 4.2 ); 12 12 12 We manually verified parsing accuracy on the theorems generated by the models for a subset of 20 20 randomly sampled examples. An additional class representing that there is no match with any annotated theorem in the proof is included as well. The parser predicted the correct (or correctly predicted no) match in 394 / 397 = 99.2 % 394/397=99.2\\\\% of the theorems for Llama-3.1-8B-Instruct, and 381 / 384 = 99.2 % 381/384=99.2\\\\% of the theorems for Qwen2.5-Math-7B-Instruct. however, we perform a more crude analysis based on only the arithmetic expressions for the other two models in ¬ß Àú D.4 .\\nWe report the efficiency metric presented in ¬ß Àú 3.2 for the problems where the LM generated the correct goal theorem, comparing against the shortest proof generated with our method ( App. Àú C ). 13 13 13 LMs often generated the same theorems multiple times. We chose to ignore such duplicates in the evaluation. We omit built-ins from the efficiency analysis since those are not verbalized.\\nAdditionally, we note that it might be the case that all irrelevant steps the models take are axioms; the LMs may simply be stating that some axioms are irrelevant to the query.\\nWe therefore also consider a metric in which only non-axiom theorems are counted.\\nIn ¬ß Àú D.4 we provide an additional analysis on search order.\\nThe main results are shown in Table Àú 3 (non-ground queries), with scores stratified by agent and entity overlap. The efficiency scores are far from 100 % 100\\\\% , meaning that the models predict several theorems beyond the required ones present in the shortest proof.\\nAdditionally, we observe that the efficiency scores vary significantly across the type of overlap ( p < 0.001 ) (p<0.001) , so we conclude that lexical information in the query has a substantial effect on proof planning.\\nWe finally comment on the results that only consider non-axioms, presented at the last row in Table Àú 3 .\\nWe again observe efficiency scores that are considerably below 100 % 100\\\\% , showing that the LMs prove theorems that are irrelevant to the query. ¬ß Àú D.3 gives an example where Llama-3.1-8B proves irrelevant theorems.\\nWe compare the performance to the same problems when presented with ground queries.\\nQueries tend to be non-ground in the GSM domain (Riley et\\xa0al., 1983 ; Cobbe et\\xa0al., 2021 ) .\\nSince LMs are heavily influenced by training data, we therefore expect them to perform better on non-ground queries.\\nOur results on the same verbalized logic programs as before, but with ground queries, are presented on the right-hand side of Table Àú 3 . We observe lower efficiency scores, suggesting that LMs are indeed worse at proving ground theorems in this domain.',\n",
       "      'subsections': [{'title': 'Efficiency Evaluation.',\n",
       "        'paragraphs': 'This analysis is performed for Llama-3.1-8B-Instruct and Qwen2.5-Math-7B-Instruct since only those models followed the required formatting ( ¬ß Àú 4.2 ); 12 12 12 We manually verified parsing accuracy on the theorems generated by the models for a subset of 20 20 randomly sampled examples. An additional class representing that there is no match with any annotated theorem in the proof is included as well. The parser predicted the correct (or correctly predicted no) match in 394 / 397 = 99.2 % 394/397=99.2\\\\% of the theorems for Llama-3.1-8B-Instruct, and 381 / 384 = 99.2 % 381/384=99.2\\\\% of the theorems for Qwen2.5-Math-7B-Instruct. however, we perform a more crude analysis based on only the arithmetic expressions for the other two models in ¬ß Àú D.4 .\\nWe report the efficiency metric presented in ¬ß Àú 3.2 for the problems where the LM generated the correct goal theorem, comparing against the shortest proof generated with our method ( App. Àú C ). 13 13 13 LMs often generated the same theorems multiple times. We chose to ignore such duplicates in the evaluation. We omit built-ins from the efficiency analysis since those are not verbalized.\\nAdditionally, we note that it might be the case that all irrelevant steps the models take are axioms; the LMs may simply be stating that some axioms are irrelevant to the query.\\nWe therefore also consider a metric in which only non-axiom theorems are counted.\\nIn ¬ß Àú D.4 we provide an additional analysis on search order.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Efficiency for Non-ground Queries.',\n",
       "        'paragraphs': 'The main results are shown in Table Àú 3 (non-ground queries), with scores stratified by agent and entity overlap. The efficiency scores are far from 100 % 100\\\\% , meaning that the models predict several theorems beyond the required ones present in the shortest proof.\\nAdditionally, we observe that the efficiency scores vary significantly across the type of overlap ( p < 0.001 ) (p<0.001) , so we conclude that lexical information in the query has a substantial effect on proof planning.\\nWe finally comment on the results that only consider non-axioms, presented at the last row in Table Àú 3 .\\nWe again observe efficiency scores that are considerably below 100 % 100\\\\% , showing that the LMs prove theorems that are irrelevant to the query. ¬ß Àú D.3 gives an example where Llama-3.1-8B proves irrelevant theorems.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Comparison to Ground Queries.',\n",
       "        'paragraphs': 'We compare the performance to the same problems when presented with ground queries.\\nQueries tend to be non-ground in the GSM domain (Riley et\\xa0al., 1983 ; Cobbe et\\xa0al., 2021 ) .\\nSince LMs are heavily influenced by training data, we therefore expect them to perform better on non-ground queries.\\nOur results on the same verbalized logic programs as before, but with ground queries, are presented on the right-hand side of Table Àú 3 . We observe lower efficiency scores, suggesting that LMs are indeed worse at proving ground theorems in this domain.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '6 Conclusion',\n",
       "    'paragraphs': 'This paper provided a framework based on logic programming for studying deductive reasoning in language models, with a particular emphasis on efficiency. This framework enables us to disentangle efficiency due to generating irrelevant theorems from efficiency due to verbose natural language verbalizations of those theorems.\\nWe applied this framework to empirically investigate how language models perform reasoning on math word problems that have many irrelevant axioms. We found that introducing irrelevant axioms into reasoning problems leads to significantly lower answer accuracies for most models‚Äîeven when controlling for problem length‚Äîand proofs that exhibit frequent detours through irrelevant theorems.\\nOur work highlights the need to improve models in terms of reasoning efficiency, as well as the advantages to viewing deductive reasoning through the lens of logic programming.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgments and Disclosure of Funding',\n",
       "    'paragraphs': 'We thank Juan Luis Gastaldi for useful discussion and criticisms.\\nAndreas Opedal acknowledges funding from the Max Planck ETH Center for Learning Systems.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Further Background',\n",
       "    'paragraphs': 'It is also useful to introduce a notion of negation into logic programming.\\nWe denote negation by ¬¨ \\\\neg .\\nTo accommodate negation, we introduce the extended Herbrand base H ¬Ø = def H ‚à™ { ¬¨ h ‚à£ h ‚àà H } \\\\overline{H}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}H\\\\cup\\\\{\\\\neg h\\\\mid h\\\\in H\\\\} .\\nWe call an interpretation I ‚äÜ H ¬Ø I\\\\subseteq\\\\overline{H} consistent iff b ‚àà I ‚üπ ¬¨ b ‚àâ I b\\\\in I\\\\Longrightarrow\\\\neg b\\\\not\\\\in I .\\nFurthermore, a logic program ùí´ \\\\mathcal{P} is called consistency-preserving if I I is consistent implies T ùí´ \\u200b ( I ) \\\\mathrm{T}_{\\\\mathcal{P}}(I) is consistent.\\nOne simple way to check whether a logic program is consistency-preserving is to inspect the program‚Äôs dependency structure.\\nConstruct the predicate dependency graph G G as follows.\\nCreate a vertex p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} for every distinct predicate symbol p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} appearing in ùí´ \\\\mathcal{P} .\\nFor each rule r = ( b 1 , . . . , b K ‚ä¢ h ) ‚àà ùí´ r=(b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h)\\\\in\\\\mathcal{P} and each predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} appearing among r r ‚Äôs premises,\\nadd a directed edge p ‚Üí q \\\\smash{\\\\hbox to11.94pt{\\\\vbox to11.94pt{\\\\pgfpicture\\\\makeatletter\\\\hbox{\\\\quad\\\\lower-5.97012pt\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }\\\\nullfont\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }{{}}\\\\hbox{\\\\hbox{{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{}\\\\pgfsys@moveto{5.92012pt}{0.0pt}\\\\pgfsys@curveto{5.92012pt}{3.26962pt}{3.26962pt}{5.92012pt}{0.0pt}{5.92012pt}\\\\pgfsys@curveto{-3.26962pt}{5.92012pt}{-5.92012pt}{3.26962pt}{-5.92012pt}{0.0pt}\\\\pgfsys@curveto{-5.92012pt}{-3.26962pt}{-3.26962pt}{-5.92012pt}{0.0pt}{-5.92012pt}\\\\pgfsys@curveto{3.26962pt}{-5.92012pt}{5.92012pt}{-3.26962pt}{5.92012pt}{0.0pt}\\\\pgfsys@closepath\\\\pgfsys@moveto{0.0pt}{0.0pt}\\\\pgfsys@stroke\\\\pgfsys@invoke{ }\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}{{{{}}\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.20834pt}{-1.25697pt}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\raisebox{0.43057pt}{\\\\scriptsize${\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}$}}}\\n}}\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope{{{}}}{}{}\\\\hss}\\\\pgfsys@discardpath\\\\pgfsys@invoke{ }\\\\pgfsys@endscope\\\\hss}}\\\\endpgfpicture}}}\\\\rightarrow\\\\smash{\\\\hbox to11.94pt{\\\\vbox to11.94pt{\\\\pgfpicture\\\\makeatletter\\\\hbox{\\\\quad\\\\lower-5.97012pt\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }\\\\nullfont\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }{{}}\\\\hbox{\\\\hbox{{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{}\\\\pgfsys@moveto{5.92012pt}{0.0pt}\\\\pgfsys@curveto{5.92012pt}{3.26962pt}{3.26962pt}{5.92012pt}{0.0pt}{5.92012pt}\\\\pgfsys@curveto{-3.26962pt}{5.92012pt}{-5.92012pt}{3.26962pt}{-5.92012pt}{0.0pt}\\\\pgfsys@curveto{-5.92012pt}{-3.26962pt}{-3.26962pt}{-5.92012pt}{0.0pt}{-5.92012pt}\\\\pgfsys@curveto{3.26962pt}{-5.92012pt}{5.92012pt}{-3.26962pt}{5.92012pt}{0.0pt}\\\\pgfsys@closepath\\\\pgfsys@moveto{0.0pt}{0.0pt}\\\\pgfsys@stroke\\\\pgfsys@invoke{ }\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}{{{{}}\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.1007pt}{-1.25697pt}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\raisebox{0.43057pt}{\\\\scriptsize${\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}$}}}\\n}}\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope{{{}}}{}{}\\\\hss}\\\\pgfsys@discardpath\\\\pgfsys@invoke{ }\\\\pgfsys@endscope\\\\hss}}\\\\endpgfpicture}}} where q {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}} is h h ‚Äôs predicate symbol.\\nThen, label the edge ‚àí - if p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} is negated and + + otherwise.\\nWe say that G G has a negative cycle if there exists a directed cycle that contains at least one negatively labeled edge.\\nLogic programs that do not have negative cycles are called stratified (Abiteboul et\\xa0al., 1995 , ¬ß15.2) .\\nIt is easy to see that any stratified logic program has a consistency-preserving fixpoint operator.\\nWe give background details and pseudocode for the forward chaining algorithm; see Alg. Àú 1 .\\nThe algorithm takes a logic program ùí´ = ‚Ñõ ‚äî A \\\\mathcal{P}=\\\\mathcal{R}\\\\sqcup A with a set of ground goal theorems ‚Ñã g \\\\mathcal{H}_{g} and returns T (true) if all goal theorems in ‚Ñã g \\\\mathcal{H}_{g} can be proved.\\nIt uses a priority queue Q Q and a set ùíû \\\\mathcal{C} .\\nThe priority queue Q Q is called the agenda . It keeps track of theorems that are to be used to prove new theorems. The order in which the axioms are pushed to Q Q will determine the order in which they are popped and Q Q may implement any arbitrary priority policy. For example, a last-in first-out (LIFO) policy yields a depth-first search (DFS) order, while a first-in first-out (FIFO) policy yields a breadth-first search (BFS) order.\\nThe set ùíû \\\\mathcal{C} is called a chart . The chart keeps track of theorems that have been proved. Theorems are added to ùíû \\\\mathcal{C} after they have been popped from Q Q .\\nWhen a theorem is popped, it is used to prove new theorems if applicable. The algorithm iterates over all rules in the program and proves all conclusions that can be proved from the popped premise together with theorems that have previously been added to ùíû \\\\mathcal{C} . For a conclusion to be proved, there must exist an instantiation of the rule for which the premises in the instantiated ground rule are in ùíû \\\\mathcal{C} .\\nThe algorithm iteratively removes theorems from ‚Ñã g \\\\mathcal{H}_{g} as they are proved, and terminates with the result T when ‚Ñã g \\\\mathcal{H}_{g} has been emptied.\\nIf all theorems that could be proved have been popped and ‚Ñã g \\\\mathcal{H}_{g} remains non-empty, the algorithm returns F (false).\\nThe algorithm may not terminate, however, if the minimal Herbrand model is infinitely large.',\n",
       "    'subsections': [{'title': 'A.1 Negation in Logic Programming',\n",
       "      'paragraphs': 'It is also useful to introduce a notion of negation into logic programming.\\nWe denote negation by ¬¨ \\\\neg .\\nTo accommodate negation, we introduce the extended Herbrand base H ¬Ø = def H ‚à™ { ¬¨ h ‚à£ h ‚àà H } \\\\overline{H}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}H\\\\cup\\\\{\\\\neg h\\\\mid h\\\\in H\\\\} .\\nWe call an interpretation I ‚äÜ H ¬Ø I\\\\subseteq\\\\overline{H} consistent iff b ‚àà I ‚üπ ¬¨ b ‚àâ I b\\\\in I\\\\Longrightarrow\\\\neg b\\\\not\\\\in I .\\nFurthermore, a logic program ùí´ \\\\mathcal{P} is called consistency-preserving if I I is consistent implies T ùí´ \\u200b ( I ) \\\\mathrm{T}_{\\\\mathcal{P}}(I) is consistent.\\nOne simple way to check whether a logic program is consistency-preserving is to inspect the program‚Äôs dependency structure.\\nConstruct the predicate dependency graph G G as follows.\\nCreate a vertex p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} for every distinct predicate symbol p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} appearing in ùí´ \\\\mathcal{P} .\\nFor each rule r = ( b 1 , . . . , b K ‚ä¢ h ) ‚àà ùí´ r=(b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h)\\\\in\\\\mathcal{P} and each predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} appearing among r r ‚Äôs premises,\\nadd a directed edge p ‚Üí q \\\\smash{\\\\hbox to11.94pt{\\\\vbox to11.94pt{\\\\pgfpicture\\\\makeatletter\\\\hbox{\\\\quad\\\\lower-5.97012pt\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }\\\\nullfont\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }{{}}\\\\hbox{\\\\hbox{{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{}\\\\pgfsys@moveto{5.92012pt}{0.0pt}\\\\pgfsys@curveto{5.92012pt}{3.26962pt}{3.26962pt}{5.92012pt}{0.0pt}{5.92012pt}\\\\pgfsys@curveto{-3.26962pt}{5.92012pt}{-5.92012pt}{3.26962pt}{-5.92012pt}{0.0pt}\\\\pgfsys@curveto{-5.92012pt}{-3.26962pt}{-3.26962pt}{-5.92012pt}{0.0pt}{-5.92012pt}\\\\pgfsys@curveto{3.26962pt}{-5.92012pt}{5.92012pt}{-3.26962pt}{5.92012pt}{0.0pt}\\\\pgfsys@closepath\\\\pgfsys@moveto{0.0pt}{0.0pt}\\\\pgfsys@stroke\\\\pgfsys@invoke{ }\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}{{{{}}\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.20834pt}{-1.25697pt}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\raisebox{0.43057pt}{\\\\scriptsize${\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}$}}}\\n}}\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope{{{}}}{}{}\\\\hss}\\\\pgfsys@discardpath\\\\pgfsys@invoke{ }\\\\pgfsys@endscope\\\\hss}}\\\\endpgfpicture}}}\\\\rightarrow\\\\smash{\\\\hbox to11.94pt{\\\\vbox to11.94pt{\\\\pgfpicture\\\\makeatletter\\\\hbox{\\\\quad\\\\lower-5.97012pt\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }\\\\nullfont\\\\hbox to0.0pt{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }{{}}\\\\hbox{\\\\hbox{{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@setlinewidth{\\\\the\\\\pgflinewidth}\\\\pgfsys@invoke{ }{}\\\\pgfsys@moveto{5.92012pt}{0.0pt}\\\\pgfsys@curveto{5.92012pt}{3.26962pt}{3.26962pt}{5.92012pt}{0.0pt}{5.92012pt}\\\\pgfsys@curveto{-3.26962pt}{5.92012pt}{-5.92012pt}{3.26962pt}{-5.92012pt}{0.0pt}\\\\pgfsys@curveto{-5.92012pt}{-3.26962pt}{-3.26962pt}{-5.92012pt}{0.0pt}{-5.92012pt}\\\\pgfsys@curveto{3.26962pt}{-5.92012pt}{5.92012pt}{-3.26962pt}{5.92012pt}{0.0pt}\\\\pgfsys@closepath\\\\pgfsys@moveto{0.0pt}{0.0pt}\\\\pgfsys@stroke\\\\pgfsys@invoke{ }\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}{{{{}}\\\\pgfsys@beginscope\\\\pgfsys@invoke{ }\\\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.1007pt}{-1.25697pt}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\\\pgfsys@color@rgb@stroke{0}{0}{0}\\\\pgfsys@invoke{ }\\\\pgfsys@color@rgb@fill{0}{0}{0}\\\\pgfsys@invoke{ }\\\\hbox{{\\\\raisebox{0.43057pt}{\\\\scriptsize${\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}}$}}}\\n}}\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope}}}\\n\\\\pgfsys@invoke{ }\\\\pgfsys@endscope{{{}}}{}{}\\\\hss}\\\\pgfsys@discardpath\\\\pgfsys@invoke{ }\\\\pgfsys@endscope\\\\hss}}\\\\endpgfpicture}}} where q {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{q}} is h h ‚Äôs predicate symbol.\\nThen, label the edge ‚àí - if p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} is negated and + + otherwise.\\nWe say that G G has a negative cycle if there exists a directed cycle that contains at least one negatively labeled edge.\\nLogic programs that do not have negative cycles are called stratified (Abiteboul et\\xa0al., 1995 , ¬ß15.2) .\\nIt is easy to see that any stratified logic program has a consistency-preserving fixpoint operator.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Forward Chaining',\n",
       "      'paragraphs': 'We give background details and pseudocode for the forward chaining algorithm; see Alg. Àú 1 .\\nThe algorithm takes a logic program ùí´ = ‚Ñõ ‚äî A \\\\mathcal{P}=\\\\mathcal{R}\\\\sqcup A with a set of ground goal theorems ‚Ñã g \\\\mathcal{H}_{g} and returns T (true) if all goal theorems in ‚Ñã g \\\\mathcal{H}_{g} can be proved.\\nIt uses a priority queue Q Q and a set ùíû \\\\mathcal{C} .\\nThe priority queue Q Q is called the agenda . It keeps track of theorems that are to be used to prove new theorems. The order in which the axioms are pushed to Q Q will determine the order in which they are popped and Q Q may implement any arbitrary priority policy. For example, a last-in first-out (LIFO) policy yields a depth-first search (DFS) order, while a first-in first-out (FIFO) policy yields a breadth-first search (BFS) order.\\nThe set ùíû \\\\mathcal{C} is called a chart . The chart keeps track of theorems that have been proved. Theorems are added to ùíû \\\\mathcal{C} after they have been popped from Q Q .\\nWhen a theorem is popped, it is used to prove new theorems if applicable. The algorithm iterates over all rules in the program and proves all conclusions that can be proved from the popped premise together with theorems that have previously been added to ùíû \\\\mathcal{C} . For a conclusion to be proved, there must exist an instantiation of the rule for which the premises in the instantiated ground rule are in ùíû \\\\mathcal{C} .\\nThe algorithm iteratively removes theorems from ‚Ñã g \\\\mathcal{H}_{g} as they are proved, and terminates with the result T when ‚Ñã g \\\\mathcal{H}_{g} has been emptied.\\nIf all theorems that could be proved have been popped and ‚Ñã g \\\\mathcal{H}_{g} remains non-empty, the algorithm returns F (false).\\nThe algorithm may not terminate, however, if the minimal Herbrand model is infinitely large.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Decidability of GSM Programs',\n",
       "    'paragraphs': 'Let ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} be a logic program where ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is specified in Table Àú 1 and A W A_{\\\\text{W}} is a numerically consistent subset of ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} ‚Äôs extended Herbrand base H ¬Ø \\\\overline{H} .\\nThen, inference in ùí´ W \\\\mathcal{P}_{\\\\text{W}} is decidable, i.e., it is decidable to determine whether h ‚àà T ùí´ ‚àó \\u200b ( A W ) h\\\\in\\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A_{\\\\text{W}}) for an arbitrary h ‚àà H ¬Ø h\\\\in\\\\overline{H} .\\nFirst, observe that the extended Herbrand base H ¬Ø \\\\overline{H} of ùí´ W \\\\mathcal{P}_{\\\\text{W}} is countably infinite due to the unbounded sets of quantity constants ‚Ñï x q {\\\\mathbb{N}^{q}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} and timestamp constants ‚Ñï x t {\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} .\\nHowever, since the axioms A W A_{\\\\text{W}} were chosen to be numerically consistent and the sets Œ£ p W {{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}^{\\\\text{W}}} , 2 x A {2_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{{{A}}}} , and E x {{E}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} are all finite, it follows that T ùí´ ‚àó \\u200b ( A W ) \\\\mathrm{T}^{*}_{\\\\mathcal{P}}(A_{\\\\text{W}}) ‚Äîor equivalently M M ‚Äîcontains only finitely many theorems for each element of ‚Ñï x t {\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} .\\nConsequently, reasoning in ùí´ W \\\\mathcal{P}_{\\\\text{W}} reduces to Presburger arithmetic, as the only built-in predicate that applies to timestamps is T ‚Ñï + 1 {\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}}_{\\\\mathbb{N}}{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{+}}{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}} , present in Rule\\xa0(2) in Table Àú 1 .\\nSince Presburger arithmetic is famously decidable (Presburger, 1929 ; Haase, 2018 ) , it follows that inference in ùí´ W \\\\mathcal{P}_{\\\\text{W}} is decidable.\\n‚àé',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Data Generation',\n",
       "    'paragraphs': 'We adapt and apply Opedal et\\xa0al. ‚Äôs ( 2025 ) method for generating shortest proofs with axioms that are numerically consistent. ¬ß Àú C.2 introduces and discusses pseudocode ( Alg. Àú 2 ), ¬ß Àú C.3 explains how we use the algorithm to generate a proof along with irrelevant axioms, and ¬ß Àú C.4 shows that the algorithm indeed returns a shortest proof. To do so, we define a class of logic programs of which ùí´ W \\\\mathcal{P}_{\\\\text{W}} is a member, and we will present a generalization of Opedal et\\xa0al. ‚Äôs ( 2025 ) method that generates shortest proofs under any program in this class. We start by defining this class of logic programs in ¬ß Àú C.1 below.\\nWe define the expanded Herbrand base containing all ground and non-ground atoms‚Äîincluding negation ( ¬ß Àú A.1 )‚Äîas H ~ = def { p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X } ‚à™ { ¬¨ p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X } \\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\}\\\\cup\\\\{\\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\} .\\nWe let C : H ~ ‚Ü¶ 2 Œ£ x ‚à™ Œ£ X C:\\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mapsto 2^{{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}} be a function that selects a set of terms from an input atom.\\nWe refer to C \\u200b ( b ) C(b) as the C C -terms of b b .\\nOne example of such a function in ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is to return the set of agent terms from a given atom, e.g., C \\u200b ( cont \\u200b ( A , 5 , boat , 1 ) ) = { A } C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}))=\\\\{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}\\\\} and C \\u200b ( comp \\u200b ( A , { haruki } , 2 , boat , 1 ) ) = { A , { haruki } } C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}))=\\\\{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}}\\\\} . In the following, we will also define C C of an inference rule.\\nIndeed, r = b 1 , . . . , b K ‚ä¢ h r=b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , we define C \\u200b ( r ) = def ‚ãÉ k = 1 K C \\u200b ( b k ) C(r)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{k=1}^{K}C(b_{k}) as the union of the C C -terms in the premises of the inference rule r r . We similarly define C C of a hyperedge: Given a hyperedge e = { b 1 , . . . , b K } ‚Ü£ h e=\\\\{b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\}\\\\rightarrowtail h , C \\u200b ( e ) = def ‚ãÉ k = 1 K C \\u200b ( b k ) C(e)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{k=1}^{K}C(b_{k}) .\\nWe say a set of inference rules ‚Ñõ \\\\mathcal{R} is C C - conserving if, for every inference rule b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , we have C \\u200b ( b k ) ‚â† { } C(b_{k})\\\\neq\\\\{\\\\} for all k k , C \\u200b ( h ) ‚â† { } C(h)\\\\neq\\\\{\\\\} , and one of the following conditions holds:\\nThe rule is an introduction rule for a predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , and can be written as\\nHere, we require the premises to be non- p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} atoms. We require the C C -terms in the conclusion appear in the premises: C ( p ( t 1 , . . . , t N ) ) = ‚ãÉ k = 1 K C ( b k ) C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}))=\\\\bigcup_{k=1}^{K}C(b_{k}) . We also require that the C C -terms of the premises are disjoint: C \\u200b ( b k ) ‚à© C \\u200b ( b j ) = ‚àÖ C(b_{k})\\\\cap C(b_{j})=\\\\varnothing for all k ‚â† j k\\\\neq j . We require that ‚Ñõ \\\\mathcal{R} have at most one introduction rule for each predicate.\\nThe rule is an elimination rule for a predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , and can be written as\\nHere, we require that the C C -term of the conclusion is distinct from that of the premises: b 1 , . . . , b K ‚àí 1 b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1} , C \\u200b ( h ) ‚à© C \\u200b ( b k ) = ‚àÖ C(h)\\\\cap C(b_{k})=\\\\varnothing for all k = 1 , . . . , K ‚àí 1 k=1,.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,K-1 . We also require that the C C -term of the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) contains the C C -terms of all other premises and the conclusion: C ( h ) ‚à™ ‚ãÉ k = 1 K ‚àí 1 C ( b k ) ‚äÜ C ( p ( t 1 , . . . , t N ) ) C(h)\\\\cup\\\\bigcup_{k=1}^{K-1}C(b_{k})\\\\subseteq C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Finally, we require the C C -terms of the other premises are disjoint: C \\u200b ( b k ) ‚à© C \\u200b ( b j ) = ‚àÖ C(b_{k})\\\\cap C(b_{j})=\\\\varnothing for all 0 < k , j < K 0<k,j<K and k ‚â† j k\\\\neq j .\\nThe rule is a union rule , where there is a built-in premise requiring the C C -terms of the conclusion be equal to the union of the C C -terms of the premises: C \\u200b ( h ) = ‚ãÉ k = 1 K C \\u200b ( b k ) C(h)=\\\\bigcup_{k=1}^{K}C(b_{k}) . This rule must be unique in ‚Ñõ \\\\mathcal{R} .\\nThe rule is an unused rule : it contains premises that do not unify with the conclusion of any rule.\\nFinally, if ‚Ñõ \\\\mathcal{R} has multiple elimination rules for a predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , we require that those rules be distinct in the following way: For any two such rules, written b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h and b 1 ‚Ä≤ , . . . , b K ‚àí 1 ‚Ä≤ , p ( t 1 ‚Ä≤ , . . . , t N ‚Ä≤ ) ‚ä¢ h ‚Ä≤ b_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1}^{\\\\prime},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}^{\\\\prime})\\\\vdash h^{\\\\prime} , for any Œ∏ \\\\theta such that h / Œ∏ = h ‚Ä≤ h/\\\\theta=h^{\\\\prime} , p ( t 1 , . . . , t N ) / Œ∏ ‚â† p ( t 1 ‚Ä≤ , . . . , t N ‚Ä≤ ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})/\\\\theta\\\\neq{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}^{\\\\prime}) .\\nGiven a C C -conserving set of inference rules ‚Ñõ \\\\mathcal{R} , we will use the following short-hand notation to refer to specific subsets of rules:\\n‚Ñõ i \\\\mathcal{R}_{\\\\textsc{i}} is the set of introduction rules in ‚Ñõ \\\\mathcal{R} .\\n‚Ñõ e \\\\mathcal{R}_{\\\\textsc{e}} is the set of elimination rules in ‚Ñõ \\\\mathcal{R} .\\n‚Ñõ u \\\\mathcal{R}_{\\\\textsc{u}} is the set of union rules in ‚Ñõ \\\\mathcal{R} .\\nLet E : H ~ ‚Ü¶ 2 Œ£ x ‚à™ Œ£ X E\\\\colon\\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mapsto 2^{{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}} be a function. We say a set of inference rules ‚Ñõ \\\\mathcal{R} is E E - monotonic if, for every inference rule b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , we have E \\u200b ( b k ) ‚â† { } E(b_{k})\\\\neq\\\\{\\\\} for all k k , E \\u200b ( h ) ‚â† { } E(h)\\\\neq\\\\{\\\\} , and one of the following conditions holds:\\nThe rule is an introduction rule; see above.\\nThe rule is an elimination rule; see above.\\nThe rule preserves E E -terms: E ( b 1 ) = ‚ãÖ ‚ãÖ ‚ãÖ = E ( b K ) = E ( h ) E(b_{1})=\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot=E(b_{K})=E(h) .\\nTo prove the optimality of our generated proofs, we need additional constraints on the relationship between introduction and elimination rules. Let C : H ~ ‚Ü¶ 2 Œ£ x ‚à™ Œ£ X C\\\\colon\\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mapsto 2^{{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}} be a function. Consider each elimination rule b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h , and let C b = def ‚ãÉ k = 1 K ‚àí 1 C \\u200b ( b k ) C_{b}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{k=1}^{K-1}C(b_{k}) and C h = def C \\u200b ( h ) C_{h}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}C(h) . Construct a hypergraph using the following procedure: Start by mapping the atom p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) to a vertex v 0 v_{0} , i.e., we set ‚Ñì ( v 0 ) = ( p ( t 1 , . . . , t N ) ) \\\\ell(v_{0})=({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Repeat the following: For any vertex v v that contains C C -terms in both C b C_{b} and C h C_{h} (i.e., C \\u200b ( ‚Ñì ‚àí 1 \\u200b ( v ) ) ‚à© C b ‚â† ‚àÖ C(\\\\ell^{-1}(v))\\\\cap C_{b}\\\\neq\\\\varnothing and C \\u200b ( ‚Ñì ‚àí 1 \\u200b ( v ) ) ‚à© C h ‚â† ‚àÖ C(\\\\ell^{-1}(v))\\\\cap C_{h}\\\\neq\\\\varnothing ), and for any introduction rule b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ ‚àà ‚Ñõ b_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}^{\\\\prime}\\\\vdash h^{\\\\prime}\\\\in\\\\mathcal{R} , add a vertex for each unified premise b k / Œ∏ b_{k}/\\\\theta where ‚Ñì \\u200b ( h ‚Ä≤ / Œ∏ ) = v \\\\ell(h^{\\\\prime}/\\\\theta)=v and a hyperedge from { ‚Ñì ‚àí 1 \\u200b ( b k / Œ∏ ) } \\\\{\\\\ell^{-1}(b_{k}/\\\\theta)\\\\} to ‚Ñì ‚àí 1 \\u200b ( v ) \\\\ell^{-1}(v) . We say the elimination rule is C C - locally reducible if either: (1) the hypergraph contains no edges, or (2) there exists a vertex in this hypergraph that identifies with the conclusion of the elimination rule h h . If all elimination rules in ‚Ñõ \\\\mathcal{R} are locally reducible, we say ‚Ñõ \\\\mathcal{R} is in harmony . One consequence of harmony is that, in any proof ùí´ \\\\mathcal{P} containing a proof step with an elimination rule, if the subproof of the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) does not contain any axioms with C C -terms from both C b C_{b} and C h C_{h} , which is true of any proof generated by the procedure in App. Àú C , then the conclusion of the elimination rule must be an axiom of the subproof. Therefore, ùí´ \\\\mathcal{P} is not optimal.\\nWe note that ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is agent-conserving , i.e., ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is C C -conserving where C C is the function that returns the agents of any atom.\\nIn particular, if we inspect the rules in Table Àú 1 , we observe that (1a) and (1b) are elimination rules for comp {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}} , and (1c) is the corresponding introduction rule. Rules (2a) and (2b) are elimination rules for transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} . Rule (4) is an elimination rule for rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} . There are no corresponding introduction rules for transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} and rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} . (5a) is an elimination rule for compeq {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{compeq}} and (5b) is the corresponding introduction rule. Rule (3) is a union rule. Rule (6) is an unused rule. ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is also entity-monotonic with (4) being the elimination rule for rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} . ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is also in harmony, with respect to both agents and entities: (1a) and (1b) are each locally reducible with (1c). (5a) is locally reducible with a combination of (5b) and (1c).\\nA ground substitution { ( X m , t m ) } m = 1 M \\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m},t_{m})\\\\}_{m=1}^{M} is a typed substitution where t m ‚àà Œ£ x k t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{k}} , for all m ‚àà [ M ] m\\\\in[M] .\\nWe define Œò g \\u200b ( ùí´ ) \\\\Theta_{\\\\textsc{g}}(\\\\mathcal{P}) to be the set of all ground substitutions under the logic program ùí´ \\\\mathcal{P} .\\nOpedal et\\xa0al. ( 2025 ) provide a method to generates the shortest proof for a goal h g h_{g} , i.e., a ( ‚Ñì ‚àí 1 \\u200b ( A W ) , ‚Ñì ‚àí 1 \\u200b ( h g ) ) (\\\\ell^{-1}(A_{\\\\text{W}}),\\\\ell^{-1}(h_{g})) -hyperpath in ‚Ñã ùí´ W \\\\mathcal{H}_{\\\\mathcal{P}_{\\\\text{W}}} ; see ¬ß Àú 3.2 .\\nWe present the pseudocode in Alg. Àú 2 . The method takes any logic program ùí´ \\\\mathcal{P} with C C -conserving rules, e.g., ùí´ W \\\\mathcal{P}_{\\\\text{W}} as described in Table Àú 1 , a goal theorem h g h_{g} ,\\na set of forbidden ground theorems A √ó A_{\\\\times} , and a maximum depth D D . The set A √ó A_{\\\\times} becomes relevant when generating irrelevant axioms, which will be discussed in ¬ß Àú C.3 . For now, we assume that A √ó = ‚àÖ A_{\\\\times}=\\\\varnothing . The algorithm will terminate and return a proof of depth D D . In our experiments with ùí´ W \\\\mathcal{P}_{\\\\text{W}} , we use the following arguments: We sample the goal h g h_{g} to be a ground atom of the form cont \\u200b ( A , Q , E , T ) / Œ∏ {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Q}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}})/\\\\theta for some Œ∏ ‚àà Œò \\u200b ( ùí´ W ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}_{\\\\text{W}}) .\\nFor the agent set we make an arbitrary choice, sampling an agent set with cardinality { 1 , 2 , 3 , 4 } \\\\{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}\\\\} with probabilities { 1 ‚Ü¶ 1 / 2 , 2 ‚Ü¶ 1 / 6 , 3 ‚Ü¶ 1 / 6 , 4 ‚Ü¶ 1 / 6 } \\\\{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}\\\\mapsto\\\\nicefrac{{1}}{{2}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}\\\\mapsto\\\\nicefrac{{1}}{{6}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}}\\\\mapsto\\\\nicefrac{{1}}{{6}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}\\\\mapsto\\\\nicefrac{{1}}{{6}}\\\\} .\\nWe restrict the cardinality to 4 in order avoid generating GSM problems that are too large; see ¬ß Àú D.1 for dataset statistics.\\nThe timestamp is set to an arbitrary value larger than the value of D, which ensures that the timestamps remain in ‚Ñï x t {\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} throughout the generation procedure. We sample D D uniformly at random from { 1 , 2 , 3 } \\\\{1,2,3\\\\} for every generated proof.\\nSampling proceeds recursively in a top-down manner.\\nWe then sample an inference rule b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , i.e., where h h is the conclusion. This yields a tree.\\nWe then repeat the procedure recursively for each leaf node until the stopping criterion, i.e., required depth, has been reached.\\nImportantly, all premises are sampled without replacement; this ensures that any individual theorem will not be used more than once in the generated proof, i.e., the proof is acyclic.\\nFor example, suppose we have cont \\u200b ( { haruki } , 5 , boat , 1 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}) and sample an instantiation of Rule\\xa0(1a) in Table Àú 1 . We generate two new premises, cont \\u200b ( { abu } , 3 , boat , 1 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{abu\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}) and comp \\u200b ( { haruki } , { abu } , 2 , boat , 1 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{abu\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}) ,\\nwhere the agent { abu } is a new agent that does not appear elsewhere in the proof.\\nIn Alg. Àú 2 , this is handled by maintaining a set set of generated non-ground atoms, and by restricting the substitutions such that new C C -terms are mapped to new objects. We only sample atoms that are not in set .\\nWe comment on two more details of Alg. Àú 2 . First, whenever we sample an elimination rule b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h , we require the last premise, p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) , to be an axiom, i.e., we do not recursively apply the algorithm on this premise. This is due to the fact that the rules in ùí´ \\\\mathcal{P} are in harmony, and so the addition of introduction rules preceding elimination rules could result in locally-reducible regions in the output proof, and so the proof would not be shortest.\\nSecond, if we sample a union rule, we always require the C C -terms of the premises to be singleton sets.\\nOtherwise, a shorter proof could have been obtained by several instantiations of the union rule. 14 14 14 This is analogous to folding and unfolding (Tamaki & Sato, 1984 ) .\\nThroughout the algorithm, we sample substitutions from Œò g \\u200b ( ùí´ ) \\\\Theta_{\\\\textsc{g}}(\\\\mathcal{P}) .\\nBy substituting under the constraints laid out by the arithmetic built-ins we ensure numerical consistency.\\nHowever, numerical substitutions may fail due to unsatisfiable constraints, i.e., Œò ‚Ä≤ \\\\Theta^{\\\\prime} in Alg. Àú 2 may become empty before we sample from it. We therefore perform rejection sampling until a successful substitution has been found. In a few cases, this may run prohibitively long. We therefore retry a maximum of 1000 1000 times before rejecting the candidate proof and sampling a new one.\\nWe apply Alg. Àú 2 described in the previous subsection to generate irrelevant axioms. As mentioned in ¬ß Àú 4.3 , we augment a logic program ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} ‚Äîas sampled by the method described above‚Äîwith M M additional sets of irrelevant axioms A ~ 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M} . In simple terms, we do so by applying the sampling procedure again, once for every A ~ m \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m} , with additional restrictions to not generate duplicate axioms. As we will show later, this procedure for generating irrelevant axioms requires the additional constraint that the inference rules ‚Ñõ \\\\mathcal{R} in the logic program ùí´ \\\\mathcal{P} be E E -monotonic. With each application of the sampling procedure, we provide a perturbed goal argument h g h_{g} . At least one C C -term or E E -term of the original goal will be replaced with a new value (e.g., in ùí´ W \\\\mathcal{P}_{\\\\text{W}} , either the agent, the entity, or both will be perturbed). Precisely which term is perturbed depends on the experimental setting (see ¬ß Àú 4.3 ). The set of forbidden theorems A √ó A_{\\\\times} is initialized to be the theorems in the shortest proof (including the axioms) from ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} , thus, avoiding generating those theorems. The same is done incrementally for the irrelevant axioms as they are generated, guaranteeing disjoint sets.\\nMoreover, for practical reasons, the distribution from which we sample is slightly modified. In particular, we restrict cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} predicates to have singleton agent sets; otherwise, the number of irrelevant axioms could get very large.\\nFurthermore, we exclude the rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} predicate, as we found that overlapping agents and entities could sometimes be hard to instantiate with a rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} atom, since it requires a particular semantic relationship between the two entity terms in the atom, e.g., ‚Äúapples‚Äù per ‚Äúbasket‚Äù .\\nEnforcing these restrictions led to substantially more efficient rejection sampling during the substitution part of Alg. Àú 2 .\\nWhen ordering the axioms in natural language, it is undesirable to have a predictable ordering of relevant and irrelevant axioms, e.g., appending all irrelevant axioms to follow the relevant ones.\\nWe therefore randomly reorder them in a manner that respects the ordering of the values of the timestamp.\\nHere, we will show that the above procedure for generating proofs produces a shortest proof‚Äîa proof from axioms A A to a goal h g h_{g} for which there does not exist a shorter proof of h g h_{g} from A A . Furthermore, we will show that each generated proof is unique : There are no other shortest proofs with the same axioms and goal theorem.\\nC C -conservation, monotonicity, and harmony of the inference rules alone is insufficient to guarantee that the proofs generated by the procedure described in App. Àú C are shortest. We need additional properties of the generated proofs, as well as few additional definitions.\\nA linear chain is a sequence of hyperedges e 1 , . . . , e M e_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,e_{M} where the head of every edge is in the tail of the next edge (except for the last edge). More precisely, for all m m , we have e m = { b m , 1 , . . . , b m , K } ‚Ü£ h m e_{m}=\\\\{b_{m,1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{m,K}\\\\}\\\\rightarrowtail h_{m} , and for all 0 < m < M 0<m<M , h m ‚àà { b m + 1 , 1 , b m + 1 , 2 , . . . } h_{m}\\\\in\\\\{b_{m+1,1},b_{m+1,2},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.\\\\} .\\nGiven an ( A , h g ) (A,h_{g}) -proof ùí´ = ( ( V , E ) , ‚Ñì ) \\\\mathcal{P}=((V,E),\\\\ell) , and an atom x ‚àà H ¬Ø x\\\\in\\\\overline{H} in the extended Herbrand base H ¬Ø \\\\overline{H} ( ¬ß Àú A.1 ) where ‚Ñì ‚àí 1 \\u200b ( x ) ‚àà V \\\\ell^{-1}(x)\\\\in V , a subproof of x x is an ( A , x ) (A,x) -proof ùí´ x = ( ( V x , E x ) , ‚Ñì ) \\\\mathcal{P}_{x}=((V_{x},E_{x}),\\\\ell) where V x ‚äÜ V V_{x}\\\\subseteq V and E x ‚äÜ E E_{x}\\\\subseteq E . Using this, we can define the subproof relation: If ùí´ x \\\\mathcal{P}_{x} is a subproof of some atom in ùí´ \\\\mathcal{P} , we write ùí´ x ‚™Ø ùí´ \\\\mathcal{P}_{x}\\\\preceq\\\\mathcal{P} .\\nWe say a subproof ùí´ x \\\\mathcal{P}_{x} of x x contains an atom y y , or similarly, y y is in ùí´ x \\\\mathcal{P}_{x} , if there is an ( A , x ) (A,x) -hyperpath where ‚Ñì ‚àí 1 \\u200b ( y ) \\\\ell^{-1}(y) is in the head or tail of any edge in the hyperpath. Similarly, we say a subproof ùí´ x \\\\mathcal{P}_{x} of x x contains a C C -term t t , or t t is in ùí´ x \\\\mathcal{P}_{x} , if there is some y y such that t ‚àà C \\u200b ( y ) t\\\\in C(y) and ùí´ x \\\\mathcal{P}_{x} contains y y .\\nLet ‚Ñõ \\\\mathcal{R} be a set of inference rules that are C C -conserving, E E -monotonic, and in harmony, 15 15 15 With respect to both C C and E E . let ùí´ \\\\mathcal{P} be an ( A , h g ) (A,h_{g}) -proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A generated by the procedure in ¬ß Àú C.3 , and let A ~ \\\\textstyle\\\\widetilde{A} be the set of irrelevant axioms generated by ¬ß Àú C.3 .\\nThere does not exist an ( A ‚Ä≤ , h g ) (A^{\\\\prime},h_{g}) -proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} in ‚Ñõ ‚äî A ‚äî A ~ \\\\mathcal{R}\\\\sqcup A\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}} such that | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| .\\nTo prove Thm. Àú 1 we introduce two lemmas.\\nFirst, in Lemma Àú 1 , we show that the shortest proofs (without irrelevant axioms) are unique. That is, for a given goal theorem and set of axioms, there is no shortest proof that is different than the one we generate.\\nThen, we show that the irrelevant axioms generated in ¬ß Àú C.3 generated through Alg. Àú 2 can not be used to yield another shortest proof of the goal theorem; this is done in Lemma Àú 2 .\\nTaken together, these lemmas imply Thm. Àú 1 .\\nLet ‚Ñõ \\\\mathcal{R} be a set of inference rules that is C C -conserving, A A be a set of axioms.\\nThen, for all theorems h g h_{g} in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A such that ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} could be generated by Alg. Àú 2 , we have that ùí´ \\\\mathcal{P} is h g h_{g} ‚Äôs unique shortest proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A .\\nWe perform structural induction on the subproof relation ‚™Ø \\\\preceq .\\nWe first observe that ùí´ \\\\mathcal{P} can not have cycles: By definition, a cycle contains several vertices labeled with the same theorem. This is not possible since Alg. Àú 2 maintains a set of already sampled theorems set that can never be sampled again: Specifically, line 16 excludes theorems in set from being sampled and line 24 adds generated theorems to set .\\nBase Case. The base case is the smallest possible proof containing a single axiom that is also the goal theorem, i.e., an ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} of h g h_{g} with | ùí´ | = 1 |\\\\mathcal{P}|=1 . This is clearly the unique shortest proof in this case.\\nInductive Case. Consider the last hyperedge { ‚Ñì ‚àí 1 ( b 1 ) , . . . , ‚Ñì ‚àí 1 ( b K ) } ‚Ü£ ‚Ñì ‚àí 1 ( h ) \\\\{\\\\ell^{-1}(b_{1}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,\\\\ell^{-1}(b_{K})\\\\}\\\\rightarrowtail\\\\ell^{-1}(h) , i.e., the hyperedge for which the head is labeled with the goal theorem h h of the subproof.\\nThe inductive hypothesis states that for any atom x ‚â† h x\\\\neq h in ùí´ \\\\mathcal{P} , the subproof of x x in ùí´ \\\\mathcal{P} is the unique shortest ( A , x ) (A,x) -proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A .\\nWe consider the various cases of inference rules in the logic program for the last hyperedge and show that the unique shortest proof of the conclusion is obtained by combining the unique shortest proofs of the premises under the inference rule corresponding to that hyperedge.\\nWe now consider four cases.\\nCase 1: (Introduction) . The inference rule corresponding to the last hyperedge is an instantiation of an introduction rule:\\nBecause there are no other introduction rules for p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , C ( p ( t 1 , . . . , t N ) ) = ‚ãÉ k = 1 K C ( b k ) C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}))=\\\\bigcup_{k=1}^{K}C(b_{k}) , and the atoms in the subproofs of the premises are disjoint, the above introduction rule is the only way to prove p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) . Thus, we conclude that ùí´ \\\\mathcal{P} is the unique shortest proof of p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) .\\nCase 2: (Elimination) . The inference rule corresponding to the last hyperedge is an instantiation of an elimination rule: b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h .\\nDue to the restriction in the generative process‚Äîsee ¬ß Àú C.2 ‚Äîthe premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) is not expanded when generating ùí´ \\\\mathcal{P} because it is an axiom.\\nFurthermore, since C \\u200b ( h ) ‚à© C \\u200b ( b k ) = ‚àÖ C(h)\\\\cap C(b_{k})=\\\\varnothing for all premises b k b_{k} , it is impossible to prove h h using only the atoms in the subproofs of the premises b 1 , . . . , b K ‚àí 1 b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1} . In fact, since C ( h ) ‚äÜ C ( p ( t 1 , . . . , t N ) ) C(h)\\\\subseteq C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) , the axiom p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must appear in any proof of h h . The logic program may have other elimination rules for the same predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , but since such rules are distinct due to C C -conservation, those other rules cannot be used to derive h h from b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) . Therefore, we conclude that ùí´ \\\\mathcal{P} is the unique shortest proof of h h .\\nCase 3: (Union) . The inference rule corresponding to the last hyperedge is an instantiation of a union rule: b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h . Since the generation procedure in ¬ß Àú C.2 only generates hyperedges corresponding to union rules where the premises have disjoint C C -terms, i.e., C \\u200b ( b k ) ‚à© C \\u200b ( b j ) = ‚àÖ C(b_{k})\\\\cap C(b_{j})=\\\\varnothing for all k ‚â† j k\\\\neq j , and there is no way to prove h h other than the union rule, this case is identical to Case 1 above.\\nCase 4: (Unused) . The inference rule corresponding to the last hyperedge is an instantiation of an unused rule. But the procedure in ¬ß Àú C.2 never includes such inference rules, so this is impossible.\\n‚àé\\nLet ‚Ñõ \\\\mathcal{R} be a set of inference rules that is C C -conserving, E E -monotonic, and in harmony, let ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} be the proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A generated by ¬ß Àú C.3 , and let A ~ \\\\textstyle\\\\widetilde{A} be the set of irrelevant axioms generated by ¬ß Àú C.3 .\\nThen, there does not exist an ( A ‚Ä≤ , h g ) (A^{\\\\prime},h_{g}) -proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} in ‚Ñõ ‚äî A ‚äî A ~ \\\\mathcal{R}\\\\sqcup A\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}} such that A ‚Ä≤ ‚â† A A^{\\\\prime}\\\\neq A and | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| .\\nWe offer a proof by contradiction.\\nBy way of contradiction, assume there exists an ( A ‚Ä≤ , h g ) (A^{\\\\prime},h_{g}) -proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} in ‚Ñõ ‚äî A ‚äî A ~ \\\\mathcal{R}\\\\sqcup A\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}} such that A ‚Ä≤ ‚â† A A^{\\\\prime}\\\\neq A and | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| . Take ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} to be the shortest such proof.\\nBecause, by construction of the algorithm, the proof ùí´ \\\\mathcal{P} consists of a single hyperpath starting from A A .\\nThus, A ‚Ä≤ ‚â† A A^{\\\\prime}\\\\neq A implies there exists an a ‚àà A ‚Ä≤ a\\\\in A^{\\\\prime} where a ‚àâ A a\\\\not\\\\in A .\\nRecall that ùí´ = ( ( V , E ) , ‚Ñì ) \\\\mathcal{P}=((V,E),\\\\ell) is itself a proof forest.\\nLet C \\u200b ( ùí´ ) = def ‚ãÉ e ‚àà E C \\u200b ( e ) C(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{e\\\\in E}C(e) be the set of C C -terms in ùí´ \\\\mathcal{P} , i.e., the relevant C C -terms . Similarly, let E \\u200b ( ùí´ ) = def ‚ãÉ e ‚àà E E \\u200b ( e ) E(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{e\\\\in E}E(e) be set of E E -terms in ùí´ \\\\mathcal{P} , i.e., the relevant E E -terms.\\nWe now consider two cases.\\nCase 1: C \\u200b ( a ) ‚äÜ C \\u200b ( ùí´ ) C(a)\\\\subseteq C(\\\\mathcal{P}) (The irrelevant axiom only has relevant C C -terms). The generation procedure described in ¬ß Àú C.3 can only generate an irrelevant atom with relevant C C -terms when generating proof trees that have E E -terms that are distinct from those in the relevant proof. Therefore, the E E -terms in the proof containing a a , i.e., the irrelevant E E -terms, are disjoint from those in ùí´ \\\\mathcal{P} , i.e., the relevant E E -terms.\\nTake the linear chain e 1 , . . . , e M ‚àà ùí´ ‚Ä≤ e_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,e_{M}\\\\in\\\\mathcal{P}^{\\\\prime} such that { a } \\\\{a\\\\} is the tail of e 1 e_{1} and h g h_{g} is the head of e M e_{M} .\\nLet h m h_{m} be the head of edge e m e_{m} .\\nSelect the first e m e_{m} such that E \\u200b ( h m ) E(h_{m}) contains an E E -term that is not relevant and E \\u200b ( h m + 1 ) E(h_{m+1}) has only relevant E E -terms. There must be at least one such edge since h g h_{g} has only relevant E E -terms but a a does not. Because ‚Ñõ \\\\mathcal{R} is E E -monotonic, only elimination rules allow an E E -term from a premise to be absent from the conclusion, and so e m + 1 e_{m+1} must be an instantiation of an elimination rule\\nand E ( h ) ‚à™ E ( b k ) = E ( p ( t 1 , . . . , t N ) ) E(h)\\\\cup E(b_{k})=E({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) , for any k k . The E E -terms of h m h_{m} and h m + 1 h_{m+1} are included in E ( p ( t 1 , . . . , t N ) ) E({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Thus, the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must contain both relevant and irrelevant E E -terms. Note that in the generation procedure described in ¬ß Àú C.3 , no axiom is generated that contains both relevant and irrelevant E E -terms. Therefore, p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must be derived from axioms other than a a .\\nHowever, because ‚Ñõ \\\\mathcal{R} is in harmony, the subproof of this premise must contain h h , and so the proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} is not shortest, which is a contradiction.\\nCase 2 C \\u200b ( a ) ‚äà C \\u200b ( ùí´ ) C(a)\\\\nsubseteq C(\\\\mathcal{P}) (The irrelevant axiom has an irrelevant C C -term). Take the linear chain e 1 , . . . , e M ‚àà ùí´ ‚Ä≤ e_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,e_{M}\\\\in\\\\mathcal{P}^{\\\\prime} such that { a } \\\\{a\\\\} is the tail of e 1 e_{1} and h g h_{g} is the head of e M e_{M} .\\nLet h m h_{m} be the head of edge e m e_{m} .\\nSelect the first e m e_{m} such that C \\u200b ( h m ) ‚äà C \\u200b ( ùí´ ) C(h_{m})\\\\nsubseteq C(\\\\mathcal{P}) and C \\u200b ( h m + 1 ) ‚äÜ C \\u200b ( ùí´ ) C(h_{m+1})\\\\subseteq C(\\\\mathcal{P}) . There must be at least one such edge since h g h_{g} has only relevant C C -terms but a a does not. Since the ‚Ñõ \\\\mathcal{R} is C C -conserving, only elimination rules allow a C C -term from a premise to be absent from the conclusion, and so e m + 1 e_{m+1} must be an instantiation of an elimination rule\\nand C ( h ) ‚à™ ‚ãÉ j = 1 K ‚àí 1 C ( b j ) ‚äÜ C ( p ( t 1 , . . . , t N ) ) C(h)\\\\cup\\\\bigcup_{j=1}^{K-1}C(b_{j})\\\\subseteq C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . That is, the C C -terms of h m h_{m} and h m + 1 h_{m+1} are included in C ( p ( t 1 , . . . , t N ) ) C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Thus, the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must contain both relevant and irrelevant C C -terms. Note that in the generation procedure described in ¬ß Àú C.3 , no axiom is generated that contains both relevant and irrelevant C C -terms. Therefore, p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must be derived from other axioms. However, since ‚Ñõ \\\\mathcal{R} is in harmony, the subproof of this premise must contain h h , and so the proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} is not shortest, which is a contradiction.\\n‚àé\\nWe can now prove Thm. Àú 1 using the above lemmas.\\nSee 1\\nBy Lemma Àú 1 , there exists one unique shortest proof ùí´ \\\\mathcal{P} of a goal theorem from a set of relevant axioms and this is the proof we generate as the ground-truth proof. By Lemma Àú 2 , there will be no additional proofs ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} where | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| when generating irrelevant axioms through our procedure. Therefore, all our generated proof systems have a unique shortest proof, which is identical to the ground-truth proof that is generated through our procedure.\\n‚àé',\n",
       "    'subsections': [{'title': 'C.1 Additional Definitions',\n",
       "      'paragraphs': 'We define the expanded Herbrand base containing all ground and non-ground atoms‚Äîincluding negation ( ¬ß Àú A.1 )‚Äîas H ~ = def { p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X } ‚à™ { ¬¨ p ( t 1 , . . . , t N ) ‚à£ p ‚àà Œ£ p , ar ( p ) = N , t 1 , . . . , t N ‚àà Œ£ x ‚à™ Œ£ X } \\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\}\\\\cup\\\\{\\\\neg{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\mid{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}}},{\\\\small\\\\textsf{ar}}({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}})=N,t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}\\\\} .\\nWe let C : H ~ ‚Ü¶ 2 Œ£ x ‚à™ Œ£ X C:\\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mapsto 2^{{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}} be a function that selects a set of terms from an input atom.\\nWe refer to C \\u200b ( b ) C(b) as the C C -terms of b b .\\nOne example of such a function in ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is to return the set of agent terms from a given atom, e.g., C \\u200b ( cont \\u200b ( A , 5 , boat , 1 ) ) = { A } C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}))=\\\\{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}}\\\\} and C \\u200b ( comp \\u200b ( A , { haruki } , 2 , boat , 1 ) ) = { A , { haruki } } C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}))=\\\\{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}}\\\\} . In the following, we will also define C C of an inference rule.\\nIndeed, r = b 1 , . . . , b K ‚ä¢ h r=b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , we define C \\u200b ( r ) = def ‚ãÉ k = 1 K C \\u200b ( b k ) C(r)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{k=1}^{K}C(b_{k}) as the union of the C C -terms in the premises of the inference rule r r . We similarly define C C of a hyperedge: Given a hyperedge e = { b 1 , . . . , b K } ‚Ü£ h e=\\\\{b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\}\\\\rightarrowtail h , C \\u200b ( e ) = def ‚ãÉ k = 1 K C \\u200b ( b k ) C(e)\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{k=1}^{K}C(b_{k}) .\\nWe say a set of inference rules ‚Ñõ \\\\mathcal{R} is C C - conserving if, for every inference rule b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , we have C \\u200b ( b k ) ‚â† { } C(b_{k})\\\\neq\\\\{\\\\} for all k k , C \\u200b ( h ) ‚â† { } C(h)\\\\neq\\\\{\\\\} , and one of the following conditions holds:\\nThe rule is an introduction rule for a predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , and can be written as\\nHere, we require the premises to be non- p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} atoms. We require the C C -terms in the conclusion appear in the premises: C ( p ( t 1 , . . . , t N ) ) = ‚ãÉ k = 1 K C ( b k ) C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}))=\\\\bigcup_{k=1}^{K}C(b_{k}) . We also require that the C C -terms of the premises are disjoint: C \\u200b ( b k ) ‚à© C \\u200b ( b j ) = ‚àÖ C(b_{k})\\\\cap C(b_{j})=\\\\varnothing for all k ‚â† j k\\\\neq j . We require that ‚Ñõ \\\\mathcal{R} have at most one introduction rule for each predicate.\\nThe rule is an elimination rule for a predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , and can be written as\\nHere, we require that the C C -term of the conclusion is distinct from that of the premises: b 1 , . . . , b K ‚àí 1 b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1} , C \\u200b ( h ) ‚à© C \\u200b ( b k ) = ‚àÖ C(h)\\\\cap C(b_{k})=\\\\varnothing for all k = 1 , . . . , K ‚àí 1 k=1,.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,K-1 . We also require that the C C -term of the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) contains the C C -terms of all other premises and the conclusion: C ( h ) ‚à™ ‚ãÉ k = 1 K ‚àí 1 C ( b k ) ‚äÜ C ( p ( t 1 , . . . , t N ) ) C(h)\\\\cup\\\\bigcup_{k=1}^{K-1}C(b_{k})\\\\subseteq C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Finally, we require the C C -terms of the other premises are disjoint: C \\u200b ( b k ) ‚à© C \\u200b ( b j ) = ‚àÖ C(b_{k})\\\\cap C(b_{j})=\\\\varnothing for all 0 < k , j < K 0<k,j<K and k ‚â† j k\\\\neq j .\\nThe rule is a union rule , where there is a built-in premise requiring the C C -terms of the conclusion be equal to the union of the C C -terms of the premises: C \\u200b ( h ) = ‚ãÉ k = 1 K C \\u200b ( b k ) C(h)=\\\\bigcup_{k=1}^{K}C(b_{k}) . This rule must be unique in ‚Ñõ \\\\mathcal{R} .\\nThe rule is an unused rule : it contains premises that do not unify with the conclusion of any rule.\\nFinally, if ‚Ñõ \\\\mathcal{R} has multiple elimination rules for a predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , we require that those rules be distinct in the following way: For any two such rules, written b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h and b 1 ‚Ä≤ , . . . , b K ‚àí 1 ‚Ä≤ , p ( t 1 ‚Ä≤ , . . . , t N ‚Ä≤ ) ‚ä¢ h ‚Ä≤ b_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1}^{\\\\prime},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}^{\\\\prime})\\\\vdash h^{\\\\prime} , for any Œ∏ \\\\theta such that h / Œ∏ = h ‚Ä≤ h/\\\\theta=h^{\\\\prime} , p ( t 1 , . . . , t N ) / Œ∏ ‚â† p ( t 1 ‚Ä≤ , . . . , t N ‚Ä≤ ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})/\\\\theta\\\\neq{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}^{\\\\prime}) .\\nGiven a C C -conserving set of inference rules ‚Ñõ \\\\mathcal{R} , we will use the following short-hand notation to refer to specific subsets of rules:\\n‚Ñõ i \\\\mathcal{R}_{\\\\textsc{i}} is the set of introduction rules in ‚Ñõ \\\\mathcal{R} .\\n‚Ñõ e \\\\mathcal{R}_{\\\\textsc{e}} is the set of elimination rules in ‚Ñõ \\\\mathcal{R} .\\n‚Ñõ u \\\\mathcal{R}_{\\\\textsc{u}} is the set of union rules in ‚Ñõ \\\\mathcal{R} .\\nLet E : H ~ ‚Ü¶ 2 Œ£ x ‚à™ Œ£ X E\\\\colon\\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mapsto 2^{{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}} be a function. We say a set of inference rules ‚Ñõ \\\\mathcal{R} is E E - monotonic if, for every inference rule b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , we have E \\u200b ( b k ) ‚â† { } E(b_{k})\\\\neq\\\\{\\\\} for all k k , E \\u200b ( h ) ‚â† { } E(h)\\\\neq\\\\{\\\\} , and one of the following conditions holds:\\nThe rule is an introduction rule; see above.\\nThe rule is an elimination rule; see above.\\nThe rule preserves E E -terms: E ( b 1 ) = ‚ãÖ ‚ãÖ ‚ãÖ = E ( b K ) = E ( h ) E(b_{1})=\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot=E(b_{K})=E(h) .\\nTo prove the optimality of our generated proofs, we need additional constraints on the relationship between introduction and elimination rules. Let C : H ~ ‚Ü¶ 2 Œ£ x ‚à™ Œ£ X C\\\\colon\\\\smash{\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{H}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{H}$}}}\\\\mapsto 2^{{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}}\\\\cup{{\\\\Sigma}_{{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}}}} be a function. Consider each elimination rule b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h , and let C b = def ‚ãÉ k = 1 K ‚àí 1 C \\u200b ( b k ) C_{b}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{k=1}^{K-1}C(b_{k}) and C h = def C \\u200b ( h ) C_{h}\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}C(h) . Construct a hypergraph using the following procedure: Start by mapping the atom p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) to a vertex v 0 v_{0} , i.e., we set ‚Ñì ( v 0 ) = ( p ( t 1 , . . . , t N ) ) \\\\ell(v_{0})=({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Repeat the following: For any vertex v v that contains C C -terms in both C b C_{b} and C h C_{h} (i.e., C \\u200b ( ‚Ñì ‚àí 1 \\u200b ( v ) ) ‚à© C b ‚â† ‚àÖ C(\\\\ell^{-1}(v))\\\\cap C_{b}\\\\neq\\\\varnothing and C \\u200b ( ‚Ñì ‚àí 1 \\u200b ( v ) ) ‚à© C h ‚â† ‚àÖ C(\\\\ell^{-1}(v))\\\\cap C_{h}\\\\neq\\\\varnothing ), and for any introduction rule b 1 ‚Ä≤ , . . . , b K ‚Ä≤ ‚ä¢ h ‚Ä≤ ‚àà ‚Ñõ b_{1}^{\\\\prime},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}^{\\\\prime}\\\\vdash h^{\\\\prime}\\\\in\\\\mathcal{R} , add a vertex for each unified premise b k / Œ∏ b_{k}/\\\\theta where ‚Ñì \\u200b ( h ‚Ä≤ / Œ∏ ) = v \\\\ell(h^{\\\\prime}/\\\\theta)=v and a hyperedge from { ‚Ñì ‚àí 1 \\u200b ( b k / Œ∏ ) } \\\\{\\\\ell^{-1}(b_{k}/\\\\theta)\\\\} to ‚Ñì ‚àí 1 \\u200b ( v ) \\\\ell^{-1}(v) . We say the elimination rule is C C - locally reducible if either: (1) the hypergraph contains no edges, or (2) there exists a vertex in this hypergraph that identifies with the conclusion of the elimination rule h h . If all elimination rules in ‚Ñõ \\\\mathcal{R} are locally reducible, we say ‚Ñõ \\\\mathcal{R} is in harmony . One consequence of harmony is that, in any proof ùí´ \\\\mathcal{P} containing a proof step with an elimination rule, if the subproof of the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) does not contain any axioms with C C -terms from both C b C_{b} and C h C_{h} , which is true of any proof generated by the procedure in App. Àú C , then the conclusion of the elimination rule must be an axiom of the subproof. Therefore, ùí´ \\\\mathcal{P} is not optimal.\\nWe note that ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is agent-conserving , i.e., ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is C C -conserving where C C is the function that returns the agents of any atom.\\nIn particular, if we inspect the rules in Table Àú 1 , we observe that (1a) and (1b) are elimination rules for comp {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}} , and (1c) is the corresponding introduction rule. Rules (2a) and (2b) are elimination rules for transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} . Rule (4) is an elimination rule for rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} . There are no corresponding introduction rules for transfer {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{transfer}} and rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} . (5a) is an elimination rule for compeq {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{compeq}} and (5b) is the corresponding introduction rule. Rule (3) is a union rule. Rule (6) is an unused rule. ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is also entity-monotonic with (4) being the elimination rule for rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} . ‚Ñõ W \\\\mathcal{R}_{\\\\text{W}} is also in harmony, with respect to both agents and entities: (1a) and (1b) are each locally reducible with (1c). (5a) is locally reducible with a combination of (5b) and (1c).\\nA ground substitution { ( X m , t m ) } m = 1 M \\\\{({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{X}}}_{m},t_{m})\\\\}_{m=1}^{M} is a typed substitution where t m ‚àà Œ£ x k t_{m}\\\\in{{\\\\Sigma}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}^{k}} , for all m ‚àà [ M ] m\\\\in[M] .\\nWe define Œò g \\u200b ( ùí´ ) \\\\Theta_{\\\\textsc{g}}(\\\\mathcal{P}) to be the set of all ground substitutions under the logic program ùí´ \\\\mathcal{P} .',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.2 Applying Opedal et\\xa0al.‚Äôs (2025) Method',\n",
       "      'paragraphs': 'Opedal et\\xa0al. ( 2025 ) provide a method to generates the shortest proof for a goal h g h_{g} , i.e., a ( ‚Ñì ‚àí 1 \\u200b ( A W ) , ‚Ñì ‚àí 1 \\u200b ( h g ) ) (\\\\ell^{-1}(A_{\\\\text{W}}),\\\\ell^{-1}(h_{g})) -hyperpath in ‚Ñã ùí´ W \\\\mathcal{H}_{\\\\mathcal{P}_{\\\\text{W}}} ; see ¬ß Àú 3.2 .\\nWe present the pseudocode in Alg. Àú 2 . The method takes any logic program ùí´ \\\\mathcal{P} with C C -conserving rules, e.g., ùí´ W \\\\mathcal{P}_{\\\\text{W}} as described in Table Àú 1 , a goal theorem h g h_{g} ,\\na set of forbidden ground theorems A √ó A_{\\\\times} , and a maximum depth D D . The set A √ó A_{\\\\times} becomes relevant when generating irrelevant axioms, which will be discussed in ¬ß Àú C.3 . For now, we assume that A √ó = ‚àÖ A_{\\\\times}=\\\\varnothing . The algorithm will terminate and return a proof of depth D D . In our experiments with ùí´ W \\\\mathcal{P}_{\\\\text{W}} , we use the following arguments: We sample the goal h g h_{g} to be a ground atom of the form cont \\u200b ( A , Q , E , T ) / Œ∏ {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{A}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{Q}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{E}}},{\\\\color[rgb]{0,.50,.18}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0,.50,.18}\\\\textit{{T}}})/\\\\theta for some Œ∏ ‚àà Œò \\u200b ( ùí´ W ) \\\\theta\\\\in\\\\Theta(\\\\mathcal{P}_{\\\\text{W}}) .\\nFor the agent set we make an arbitrary choice, sampling an agent set with cardinality { 1 , 2 , 3 , 4 } \\\\{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}\\\\} with probabilities { 1 ‚Ü¶ 1 / 2 , 2 ‚Ü¶ 1 / 6 , 3 ‚Ü¶ 1 / 6 , 4 ‚Ü¶ 1 / 6 } \\\\{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}\\\\mapsto\\\\nicefrac{{1}}{{2}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}}\\\\mapsto\\\\nicefrac{{1}}{{6}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}}\\\\mapsto\\\\nicefrac{{1}}{{6}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{4}}}\\\\mapsto\\\\nicefrac{{1}}{{6}}\\\\} .\\nWe restrict the cardinality to 4 in order avoid generating GSM problems that are too large; see ¬ß Àú D.1 for dataset statistics.\\nThe timestamp is set to an arbitrary value larger than the value of D, which ensures that the timestamps remain in ‚Ñï x t {\\\\mathbb{N}^{t}_{{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{x}}}}} throughout the generation procedure. We sample D D uniformly at random from { 1 , 2 , 3 } \\\\{1,2,3\\\\} for every generated proof.\\nSampling proceeds recursively in a top-down manner.\\nWe then sample an inference rule b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h , i.e., where h h is the conclusion. This yields a tree.\\nWe then repeat the procedure recursively for each leaf node until the stopping criterion, i.e., required depth, has been reached.\\nImportantly, all premises are sampled without replacement; this ensures that any individual theorem will not be used more than once in the generated proof, i.e., the proof is acyclic.\\nFor example, suppose we have cont \\u200b ( { haruki } , 5 , boat , 1 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{5}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}) and sample an instantiation of Rule\\xa0(1a) in Table Àú 1 . We generate two new premises, cont \\u200b ( { abu } , 3 , boat , 1 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{abu\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{3}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}) and comp \\u200b ( { haruki } , { abu } , 2 , boat , 1 ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{comp}}({\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{haruki\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{\\\\{abu\\\\}}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{2}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{boat}}},{\\\\color[rgb]{0.0,0.0,0.5}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\\\textit{{1}}}) ,\\nwhere the agent { abu } is a new agent that does not appear elsewhere in the proof.\\nIn Alg. Àú 2 , this is handled by maintaining a set set of generated non-ground atoms, and by restricting the substitutions such that new C C -terms are mapped to new objects. We only sample atoms that are not in set .\\nWe comment on two more details of Alg. Àú 2 . First, whenever we sample an elimination rule b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h , we require the last premise, p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) , to be an axiom, i.e., we do not recursively apply the algorithm on this premise. This is due to the fact that the rules in ùí´ \\\\mathcal{P} are in harmony, and so the addition of introduction rules preceding elimination rules could result in locally-reducible regions in the output proof, and so the proof would not be shortest.\\nSecond, if we sample a union rule, we always require the C C -terms of the premises to be singleton sets.\\nOtherwise, a shorter proof could have been obtained by several instantiations of the union rule. 14 14 14 This is analogous to folding and unfolding (Tamaki & Sato, 1984 ) .\\nThroughout the algorithm, we sample substitutions from Œò g \\u200b ( ùí´ ) \\\\Theta_{\\\\textsc{g}}(\\\\mathcal{P}) .\\nBy substituting under the constraints laid out by the arithmetic built-ins we ensure numerical consistency.\\nHowever, numerical substitutions may fail due to unsatisfiable constraints, i.e., Œò ‚Ä≤ \\\\Theta^{\\\\prime} in Alg. Àú 2 may become empty before we sample from it. We therefore perform rejection sampling until a successful substitution has been found. In a few cases, this may run prohibitively long. We therefore retry a maximum of 1000 1000 times before rejecting the candidate proof and sampling a new one.',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.3 Generating Math Word Programs With Irrelevant Axioms',\n",
       "      'paragraphs': 'We apply Alg. Àú 2 described in the previous subsection to generate irrelevant axioms. As mentioned in ¬ß Àú 4.3 , we augment a logic program ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} ‚Äîas sampled by the method described above‚Äîwith M M additional sets of irrelevant axioms A ~ 1 ‚äî ‚ãÖ ‚ãÖ ‚ãÖ ‚äî A ~ M \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{1}\\\\sqcup\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\mkern-1.0mu\\\\cdot\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{M} . In simple terms, we do so by applying the sampling procedure again, once for every A ~ m \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}}_{m} , with additional restrictions to not generate duplicate axioms. As we will show later, this procedure for generating irrelevant axioms requires the additional constraint that the inference rules ‚Ñõ \\\\mathcal{R} in the logic program ùí´ \\\\mathcal{P} be E E -monotonic. With each application of the sampling procedure, we provide a perturbed goal argument h g h_{g} . At least one C C -term or E E -term of the original goal will be replaced with a new value (e.g., in ùí´ W \\\\mathcal{P}_{\\\\text{W}} , either the agent, the entity, or both will be perturbed). Precisely which term is perturbed depends on the experimental setting (see ¬ß Àú 4.3 ). The set of forbidden theorems A √ó A_{\\\\times} is initialized to be the theorems in the shortest proof (including the axioms) from ùí´ W = ‚Ñõ W ‚äî A W \\\\mathcal{P}_{\\\\text{W}}=\\\\mathcal{R}_{\\\\text{W}}\\\\sqcup A_{\\\\text{W}} , thus, avoiding generating those theorems. The same is done incrementally for the irrelevant axioms as they are generated, guaranteeing disjoint sets.\\nMoreover, for practical reasons, the distribution from which we sample is slightly modified. In particular, we restrict cont {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{cont}} predicates to have singleton agent sets; otherwise, the number of irrelevant axioms could get very large.\\nFurthermore, we exclude the rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} predicate, as we found that overlapping agents and entities could sometimes be hard to instantiate with a rate {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{rate}} atom, since it requires a particular semantic relationship between the two entity terms in the atom, e.g., ‚Äúapples‚Äù per ‚Äúbasket‚Äù .\\nEnforcing these restrictions led to substantially more efficient rejection sampling during the substitution part of Alg. Àú 2 .\\nWhen ordering the axioms in natural language, it is undesirable to have a predictable ordering of relevant and irrelevant axioms, e.g., appending all irrelevant axioms to follow the relevant ones.\\nWe therefore randomly reorder them in a manner that respects the ordering of the values of the timestamp.',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.4 Uniqueness and Optimality of Generated Proofs',\n",
       "      'paragraphs': 'Here, we will show that the above procedure for generating proofs produces a shortest proof‚Äîa proof from axioms A A to a goal h g h_{g} for which there does not exist a shorter proof of h g h_{g} from A A . Furthermore, we will show that each generated proof is unique : There are no other shortest proofs with the same axioms and goal theorem.\\nC C -conservation, monotonicity, and harmony of the inference rules alone is insufficient to guarantee that the proofs generated by the procedure described in App. Àú C are shortest. We need additional properties of the generated proofs, as well as few additional definitions.\\nA linear chain is a sequence of hyperedges e 1 , . . . , e M e_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,e_{M} where the head of every edge is in the tail of the next edge (except for the last edge). More precisely, for all m m , we have e m = { b m , 1 , . . . , b m , K } ‚Ü£ h m e_{m}=\\\\{b_{m,1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{m,K}\\\\}\\\\rightarrowtail h_{m} , and for all 0 < m < M 0<m<M , h m ‚àà { b m + 1 , 1 , b m + 1 , 2 , . . . } h_{m}\\\\in\\\\{b_{m+1,1},b_{m+1,2},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.\\\\} .\\nGiven an ( A , h g ) (A,h_{g}) -proof ùí´ = ( ( V , E ) , ‚Ñì ) \\\\mathcal{P}=((V,E),\\\\ell) , and an atom x ‚àà H ¬Ø x\\\\in\\\\overline{H} in the extended Herbrand base H ¬Ø \\\\overline{H} ( ¬ß Àú A.1 ) where ‚Ñì ‚àí 1 \\u200b ( x ) ‚àà V \\\\ell^{-1}(x)\\\\in V , a subproof of x x is an ( A , x ) (A,x) -proof ùí´ x = ( ( V x , E x ) , ‚Ñì ) \\\\mathcal{P}_{x}=((V_{x},E_{x}),\\\\ell) where V x ‚äÜ V V_{x}\\\\subseteq V and E x ‚äÜ E E_{x}\\\\subseteq E . Using this, we can define the subproof relation: If ùí´ x \\\\mathcal{P}_{x} is a subproof of some atom in ùí´ \\\\mathcal{P} , we write ùí´ x ‚™Ø ùí´ \\\\mathcal{P}_{x}\\\\preceq\\\\mathcal{P} .\\nWe say a subproof ùí´ x \\\\mathcal{P}_{x} of x x contains an atom y y , or similarly, y y is in ùí´ x \\\\mathcal{P}_{x} , if there is an ( A , x ) (A,x) -hyperpath where ‚Ñì ‚àí 1 \\u200b ( y ) \\\\ell^{-1}(y) is in the head or tail of any edge in the hyperpath. Similarly, we say a subproof ùí´ x \\\\mathcal{P}_{x} of x x contains a C C -term t t , or t t is in ùí´ x \\\\mathcal{P}_{x} , if there is some y y such that t ‚àà C \\u200b ( y ) t\\\\in C(y) and ùí´ x \\\\mathcal{P}_{x} contains y y .\\nLet ‚Ñõ \\\\mathcal{R} be a set of inference rules that are C C -conserving, E E -monotonic, and in harmony, 15 15 15 With respect to both C C and E E . let ùí´ \\\\mathcal{P} be an ( A , h g ) (A,h_{g}) -proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A generated by the procedure in ¬ß Àú C.3 , and let A ~ \\\\textstyle\\\\widetilde{A} be the set of irrelevant axioms generated by ¬ß Àú C.3 .\\nThere does not exist an ( A ‚Ä≤ , h g ) (A^{\\\\prime},h_{g}) -proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} in ‚Ñõ ‚äî A ‚äî A ~ \\\\mathcal{R}\\\\sqcup A\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}} such that | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| .\\nTo prove Thm. Àú 1 we introduce two lemmas.\\nFirst, in Lemma Àú 1 , we show that the shortest proofs (without irrelevant axioms) are unique. That is, for a given goal theorem and set of axioms, there is no shortest proof that is different than the one we generate.\\nThen, we show that the irrelevant axioms generated in ¬ß Àú C.3 generated through Alg. Àú 2 can not be used to yield another shortest proof of the goal theorem; this is done in Lemma Àú 2 .\\nTaken together, these lemmas imply Thm. Àú 1 .\\nLet ‚Ñõ \\\\mathcal{R} be a set of inference rules that is C C -conserving, A A be a set of axioms.\\nThen, for all theorems h g h_{g} in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A such that ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} could be generated by Alg. Àú 2 , we have that ùí´ \\\\mathcal{P} is h g h_{g} ‚Äôs unique shortest proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A .\\nWe perform structural induction on the subproof relation ‚™Ø \\\\preceq .\\nWe first observe that ùí´ \\\\mathcal{P} can not have cycles: By definition, a cycle contains several vertices labeled with the same theorem. This is not possible since Alg. Àú 2 maintains a set of already sampled theorems set that can never be sampled again: Specifically, line 16 excludes theorems in set from being sampled and line 24 adds generated theorems to set .\\nBase Case. The base case is the smallest possible proof containing a single axiom that is also the goal theorem, i.e., an ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} of h g h_{g} with | ùí´ | = 1 |\\\\mathcal{P}|=1 . This is clearly the unique shortest proof in this case.\\nInductive Case. Consider the last hyperedge { ‚Ñì ‚àí 1 ( b 1 ) , . . . , ‚Ñì ‚àí 1 ( b K ) } ‚Ü£ ‚Ñì ‚àí 1 ( h ) \\\\{\\\\ell^{-1}(b_{1}),.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,\\\\ell^{-1}(b_{K})\\\\}\\\\rightarrowtail\\\\ell^{-1}(h) , i.e., the hyperedge for which the head is labeled with the goal theorem h h of the subproof.\\nThe inductive hypothesis states that for any atom x ‚â† h x\\\\neq h in ùí´ \\\\mathcal{P} , the subproof of x x in ùí´ \\\\mathcal{P} is the unique shortest ( A , x ) (A,x) -proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A .\\nWe consider the various cases of inference rules in the logic program for the last hyperedge and show that the unique shortest proof of the conclusion is obtained by combining the unique shortest proofs of the premises under the inference rule corresponding to that hyperedge.\\nWe now consider four cases.\\nCase 1: (Introduction) . The inference rule corresponding to the last hyperedge is an instantiation of an introduction rule:\\nBecause there are no other introduction rules for p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , C ( p ( t 1 , . . . , t N ) ) = ‚ãÉ k = 1 K C ( b k ) C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}))=\\\\bigcup_{k=1}^{K}C(b_{k}) , and the atoms in the subproofs of the premises are disjoint, the above introduction rule is the only way to prove p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) . Thus, we conclude that ùí´ \\\\mathcal{P} is the unique shortest proof of p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) .\\nCase 2: (Elimination) . The inference rule corresponding to the last hyperedge is an instantiation of an elimination rule: b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})\\\\vdash h .\\nDue to the restriction in the generative process‚Äîsee ¬ß Àú C.2 ‚Äîthe premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) is not expanded when generating ùí´ \\\\mathcal{P} because it is an axiom.\\nFurthermore, since C \\u200b ( h ) ‚à© C \\u200b ( b k ) = ‚àÖ C(h)\\\\cap C(b_{k})=\\\\varnothing for all premises b k b_{k} , it is impossible to prove h h using only the atoms in the subproofs of the premises b 1 , . . . , b K ‚àí 1 b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1} . In fact, since C ( h ) ‚äÜ C ( p ( t 1 , . . . , t N ) ) C(h)\\\\subseteq C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) , the axiom p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must appear in any proof of h h . The logic program may have other elimination rules for the same predicate p {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}} , but since such rules are distinct due to C C -conservation, those other rules cannot be used to derive h h from b 1 , . . . , b K ‚àí 1 , p ( t 1 , . . . , t N ) b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K-1},{\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) . Therefore, we conclude that ùí´ \\\\mathcal{P} is the unique shortest proof of h h .\\nCase 3: (Union) . The inference rule corresponding to the last hyperedge is an instantiation of a union rule: b 1 , . . . , b K ‚ä¢ h b_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,b_{K}\\\\vdash h . Since the generation procedure in ¬ß Àú C.2 only generates hyperedges corresponding to union rules where the premises have disjoint C C -terms, i.e., C \\u200b ( b k ) ‚à© C \\u200b ( b j ) = ‚àÖ C(b_{k})\\\\cap C(b_{j})=\\\\varnothing for all k ‚â† j k\\\\neq j , and there is no way to prove h h other than the union rule, this case is identical to Case 1 above.\\nCase 4: (Unused) . The inference rule corresponding to the last hyperedge is an instantiation of an unused rule. But the procedure in ¬ß Àú C.2 never includes such inference rules, so this is impossible.\\n‚àé\\nLet ‚Ñõ \\\\mathcal{R} be a set of inference rules that is C C -conserving, E E -monotonic, and in harmony, let ( A , h g ) (A,h_{g}) -proof ùí´ \\\\mathcal{P} be the proof in ‚Ñõ ‚äî A \\\\mathcal{R}\\\\sqcup A generated by ¬ß Àú C.3 , and let A ~ \\\\textstyle\\\\widetilde{A} be the set of irrelevant axioms generated by ¬ß Àú C.3 .\\nThen, there does not exist an ( A ‚Ä≤ , h g ) (A^{\\\\prime},h_{g}) -proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} in ‚Ñõ ‚äî A ‚äî A ~ \\\\mathcal{R}\\\\sqcup A\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}} such that A ‚Ä≤ ‚â† A A^{\\\\prime}\\\\neq A and | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| .\\nWe offer a proof by contradiction.\\nBy way of contradiction, assume there exists an ( A ‚Ä≤ , h g ) (A^{\\\\prime},h_{g}) -proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} in ‚Ñõ ‚äî A ‚äî A ~ \\\\mathcal{R}\\\\sqcup A\\\\sqcup\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{A}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{A}$}} such that A ‚Ä≤ ‚â† A A^{\\\\prime}\\\\neq A and | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| . Take ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} to be the shortest such proof.\\nBecause, by construction of the algorithm, the proof ùí´ \\\\mathcal{P} consists of a single hyperpath starting from A A .\\nThus, A ‚Ä≤ ‚â† A A^{\\\\prime}\\\\neq A implies there exists an a ‚àà A ‚Ä≤ a\\\\in A^{\\\\prime} where a ‚àâ A a\\\\not\\\\in A .\\nRecall that ùí´ = ( ( V , E ) , ‚Ñì ) \\\\mathcal{P}=((V,E),\\\\ell) is itself a proof forest.\\nLet C \\u200b ( ùí´ ) = def ‚ãÉ e ‚àà E C \\u200b ( e ) C(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{e\\\\in E}C(e) be the set of C C -terms in ùí´ \\\\mathcal{P} , i.e., the relevant C C -terms . Similarly, let E \\u200b ( ùí´ ) = def ‚ãÉ e ‚àà E E \\u200b ( e ) E(\\\\mathcal{P})\\\\mathrel{\\\\mathop{=}\\\\limits^{\\\\vbox to1.20552pt{\\\\kern-2.0pt\\\\hbox{$\\\\scriptstyle\\\\text{\\\\tiny def}$}\\\\vss}}}\\\\bigcup_{e\\\\in E}E(e) be set of E E -terms in ùí´ \\\\mathcal{P} , i.e., the relevant E E -terms.\\nWe now consider two cases.\\nCase 1: C \\u200b ( a ) ‚äÜ C \\u200b ( ùí´ ) C(a)\\\\subseteq C(\\\\mathcal{P}) (The irrelevant axiom only has relevant C C -terms). The generation procedure described in ¬ß Àú C.3 can only generate an irrelevant atom with relevant C C -terms when generating proof trees that have E E -terms that are distinct from those in the relevant proof. Therefore, the E E -terms in the proof containing a a , i.e., the irrelevant E E -terms, are disjoint from those in ùí´ \\\\mathcal{P} , i.e., the relevant E E -terms.\\nTake the linear chain e 1 , . . . , e M ‚àà ùí´ ‚Ä≤ e_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,e_{M}\\\\in\\\\mathcal{P}^{\\\\prime} such that { a } \\\\{a\\\\} is the tail of e 1 e_{1} and h g h_{g} is the head of e M e_{M} .\\nLet h m h_{m} be the head of edge e m e_{m} .\\nSelect the first e m e_{m} such that E \\u200b ( h m ) E(h_{m}) contains an E E -term that is not relevant and E \\u200b ( h m + 1 ) E(h_{m+1}) has only relevant E E -terms. There must be at least one such edge since h g h_{g} has only relevant E E -terms but a a does not. Because ‚Ñõ \\\\mathcal{R} is E E -monotonic, only elimination rules allow an E E -term from a premise to be absent from the conclusion, and so e m + 1 e_{m+1} must be an instantiation of an elimination rule\\nand E ( h ) ‚à™ E ( b k ) = E ( p ( t 1 , . . . , t N ) ) E(h)\\\\cup E(b_{k})=E({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) , for any k k . The E E -terms of h m h_{m} and h m + 1 h_{m+1} are included in E ( p ( t 1 , . . . , t N ) ) E({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Thus, the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must contain both relevant and irrelevant E E -terms. Note that in the generation procedure described in ¬ß Àú C.3 , no axiom is generated that contains both relevant and irrelevant E E -terms. Therefore, p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must be derived from axioms other than a a .\\nHowever, because ‚Ñõ \\\\mathcal{R} is in harmony, the subproof of this premise must contain h h , and so the proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} is not shortest, which is a contradiction.\\nCase 2 C \\u200b ( a ) ‚äà C \\u200b ( ùí´ ) C(a)\\\\nsubseteq C(\\\\mathcal{P}) (The irrelevant axiom has an irrelevant C C -term). Take the linear chain e 1 , . . . , e M ‚àà ùí´ ‚Ä≤ e_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,e_{M}\\\\in\\\\mathcal{P}^{\\\\prime} such that { a } \\\\{a\\\\} is the tail of e 1 e_{1} and h g h_{g} is the head of e M e_{M} .\\nLet h m h_{m} be the head of edge e m e_{m} .\\nSelect the first e m e_{m} such that C \\u200b ( h m ) ‚äà C \\u200b ( ùí´ ) C(h_{m})\\\\nsubseteq C(\\\\mathcal{P}) and C \\u200b ( h m + 1 ) ‚äÜ C \\u200b ( ùí´ ) C(h_{m+1})\\\\subseteq C(\\\\mathcal{P}) . There must be at least one such edge since h g h_{g} has only relevant C C -terms but a a does not. Since the ‚Ñõ \\\\mathcal{R} is C C -conserving, only elimination rules allow a C C -term from a premise to be absent from the conclusion, and so e m + 1 e_{m+1} must be an instantiation of an elimination rule\\nand C ( h ) ‚à™ ‚ãÉ j = 1 K ‚àí 1 C ( b j ) ‚äÜ C ( p ( t 1 , . . . , t N ) ) C(h)\\\\cup\\\\bigcup_{j=1}^{K-1}C(b_{j})\\\\subseteq C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . That is, the C C -terms of h m h_{m} and h m + 1 h_{m+1} are included in C ( p ( t 1 , . . . , t N ) ) C({\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N})) . Thus, the premise p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must contain both relevant and irrelevant C C -terms. Note that in the generation procedure described in ¬ß Àú C.3 , no axiom is generated that contains both relevant and irrelevant C C -terms. Therefore, p ( t 1 , . . . , t N ) {\\\\color[rgb]{0.640625,0.02734375,0.453125}\\\\definecolor[named]{pgfstrokecolor}{rgb}{0.640625,0.02734375,0.453125}\\\\mathrm{p}}(t_{1},.\\\\mkern-1.0mu.\\\\mkern-1.0mu.,t_{N}) must be derived from other axioms. However, since ‚Ñõ \\\\mathcal{R} is in harmony, the subproof of this premise must contain h h , and so the proof ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} is not shortest, which is a contradiction.\\n‚àé\\nWe can now prove Thm. Àú 1 using the above lemmas.\\nSee 1\\nBy Lemma Àú 1 , there exists one unique shortest proof ùí´ \\\\mathcal{P} of a goal theorem from a set of relevant axioms and this is the proof we generate as the ground-truth proof. By Lemma Àú 2 , there will be no additional proofs ùí´ ‚Ä≤ \\\\mathcal{P}^{\\\\prime} where | ùí´ ‚Ä≤ | ‚â§ | ùí´ | |\\\\mathcal{P}^{\\\\prime}|\\\\leq|\\\\mathcal{P}| when generating irrelevant axioms through our procedure. Therefore, all our generated proof systems have a unique shortest proof, which is identical to the ground-truth proof that is generated through our procedure.\\n‚àé',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix D More Details on Experiments',\n",
       "    'paragraphs': 'Here, we present more statistics on the datasets used in our experiments ( ¬ß Àú 5 ). Fig. Àú 4 shows the distribution of the number of edges in the proofs in the base problems, which is equivalent to the number of non-axiom theorems. The numbers range from 2 to 13. 16 16 16 For reference, GSM8K (Cobbe et\\xa0al., 2021 ) includes problems with 2 to 8 steps. The distribution is skewed towards shorter problems. This was done by design in order to prevent saturation when evaluating models with different levels of capabilities.\\nFig. Àú 5 shows the number of axioms the GSM problems contain, including irrelevant ones.\\nAs expected, adding one irrelevant axiom (w/ axiom) yields a distribution that is shifted by exactly one additional axiom as compared to the base distribution. When adding a single irrelevant tree (w/ tree), the distribution shifts further and spreads out. This trend is further amplified when adding multiple irrelevant trees (w/ multiple trees).\\nSee Fig. Àú 6 for the prompt used in our experiments.\\n[PROMPT 1] You are a helpful assistant tasked with solving math word problems. You follow the formatting of the problems and the solutions as given in the examples below. Q: Lib possesses 20 puzzles. Shanna has 4 puzzles more than Lib. Shanna then donates 5 puzzles to Costa. Shanna has 6 puzzles more than Valeria. Valeria has 11 puzzles more than Clementius. How many puzzles does Clementius have totaled? A: Let‚Äôs think step by step. 1. Lib possesses 20 puzzles. 2. Shanna has 4 puzzles more than Lib. 3. So Shanna has 20 + 4 = 24 puzzles. 4. Shanna then donates 5 puzzles to Costa. 5. So Shanna has 24 - 5 = 19 puzzles. 6. Shanna has 6 puzzles more than Valeria. 7. So Valeria has 19 - 6 = 13 puzzles. 8. Valeria has 11 puzzles more than Clementius. 9. So Clementius has 13 - 11 = 2 puzzles. Therefore, the answer (arabic numerals) is 2. [‚Ä¶repeat‚Ä¶] Q: Stacee owns 18 necklaces. Stacee then gets 14 more necklaces from Felic. Valencia has 11 necklaces less than Stacee. How many necklaces does Valencia have? A: Let‚Äôs think step by step. [GENERATION 1] 1. Stacee owns 18 necklaces. 2. Stacee then gets 14 more necklaces from Felic. 3. So Stacee has 18 + 14 = 32 necklaces. 4. Valencia has 11 necklaces less than Stacee. 5. So Valencia has 32 - 11 = 21 necklaces. [PROMPT 2] Therefore, the answer (arabic numerals) is [GENERATION 2] 21\\nIn this section we give an example proof in natural language. Consider the following problem, with the irrelevant axioms marked in bold :\\nAnatola has 67 keys. Ichabod has 23 packets. Clayborne has 55 crayons. Pieter has 20 parrots. Ichabod has 30 packets less than Winnah. Pieter then gives Yorgos 6 parrots. Clayborne has 96 crayons less than Imogene. Anatola has 55 keys less than Rahel. Dennie has 19 parrots. Dianna owns 4 parrots. The number of parrots that Edgar has more than Pieter is the same as the difference between the number of parrots that Dennie has compared to Dianna. Edgar has 3 parrots more than Eddie. How many parrots does Eddie have in all?\\nNote that the only axioms that are relevant are the ones containing information about parrots.\\nThe shortest proof for this problem requires the application of four inference rules and can be written in natural language as follows:\\nPieter has 20 parrots.\\nPieter then gives Yorgos 6 parrots.\\nSo Pieter has 20 - 6 = 14 parrots.\\nDennie has 19 parrots.\\nDianna owns 4 parrots.\\nSo the difference between the number of parrots Dennie and Dianna have is 19 - 4 = 15.\\nThe number of parrots that Edgar has more than Pieter is the same as the difference between the number of parrots that Dennie has compared to Dianna.\\nSo Edgar has 14 + 15 = 29 parrots.\\nEdgar has 3 parrots more than Eddie.\\nSo Eddie has 29 - 3 = 26 parrots.\\nIn this case, employing a best-first search heuristic based on lexical overlap with the query (i.e., prioritizing theorems that contain information about parrots) will yield the above shortest proof. Llama-3.1-8B-Instruct did not opt for that strategy. Instead, it generated the following, inefficient solution to this problem:\\nAnatola has 67 keys.\\nAnatola has 55 keys less than Rahel.\\nSo Rahel has 67 + 55 = 122 keys.\\nIchabod has 30 packets less than Winnah.\\nSo Winnah has 23 + 30 = 53 packets.\\nClayborne has 96 crayons less than Imogene.\\nSo Imogene has 55 + 96 = 151 crayons.\\nPieter has 20 parrots.\\nPieter then gives Yorgos 6 parrots.\\nSo Pieter has 20 - 6 = 14 parrots.\\nThe number of parrots that Edgar has more than Pieter is the same as the difference between the number of parrots that Dennie has compared to Dianna.\\nDennie has 19 parrots and Dianna has 4 parrots.\\nThe difference between Dennie and Dianna is 19 - 4 = 15.\\nSo Edgar has 15 more parrots than Pieter.\\nEdgar has 15 + 14 = 29 parrots.\\nEdgar has 3 parrots more than Eddie.\\nSo Eddie has 29 - 3 = 26 parrots.\\nTherefore, the answer (arabic numerals) is 26.\\nFig. Àú 7 shows further stratified results that were mentioned in ¬ß Àú 5.1 . We briefly note that these results suggest that the models might make use of a heuristic based on such overlap. (Recall that the problem sets are otherwise identical.) One could reasonably expect that a model could learn such a heuristic, based on previous work suggesting that models may derive the conclusion to a proof step greedily within a forward pass before initiating the step in the output tokens (Kudo et\\xa0al., 2024 ; Wu et\\xa0al., 2024 ) .\\nSince QwQ-32B and DeepSeek-R1 did not generate output corresponding to strings in our verbalized program, we performed a separate, more crude analysis for those two models.\\nRather than matching the full generated token sequence, we match only the arithmetic expressions. They are matched with the ground-truth expressions from the built-ins of the corresponding inference rules. This can only be done for theorems that are not axioms, since only those require an arithmetic built-in (i.e, arithmetic expression) to prove.\\nArithmetic expressions are extracted flexibly, accounting for formatting irregularities and natural language that may be interleaved. The parser may also predict no match for a particular model output if no matches are found.\\nWe manually verified parsing accuracy (macro-average) on model outputs over 20 randomly selected example problems. The parser predicted the correct match (or correctly predicted no match) in 94.4% of cases for QwQ-32B and 94.8% of cases for DeepSeek-R1.\\nTable Àú 4 displays the results. We observe results that are overall consistent with the main conclusions of the paper; even state-of-the-art models like DeepSeek-R1‚Äîdespite performing well in terms of accuracy ( Table Àú 2 )‚Äîappear to generate theorems that are irrelevant to the goal theorem. As with the two models discussed in the main text, the efficiency scores decrease when there is agent and/or entity overlap between the irrelevant axioms and the goal theorem.\\nWhile our results in ¬ß Àú 5.2 suggest that LMs make use of information in the query to prove goals, they also reveal that LMs generate irrelevant theorems. Thus, LMs appear to use some heuristic, albeit an imperfect one, in their reasoning.\\nHere, we present a limited analysis on whether this heuristic could be in combination with either DFS or BFS. We do so by examining the order in which intermediate theorems, both relevant and irrelevant, are generated by the LM, and comparing this order to the respective reference orders.\\nFurthermore, we compare the LM‚Äôs ordering to the theorems in the shortest proof as visited in DFS order as a control.\\nBefore presenting the results, we make note of a few limitations of this analysis.\\nFirst, the only irrelevant theorems we consider in the DFS and BFS orderings are those that are generated by Alg. Àú 2 when sampling the axioms for the irrelevant goal theorems h ~ 1 . . . h ~ M \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1}.\\\\mkern-1.0mu.\\\\mkern-1.0mu.\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M} ( ¬ß Àú 4.3 ).\\nIn addition, we note that the in-context examples were ordered according to DFS; see ¬ß Àú 5 . However, the examples were chosen so that the ordering of theorems that are not axioms would be the same under both DFS and BFS. We therefore ignore the axioms as we compare the orderings.\\nConcretely, the DFS and BFS orderings correspond to the order in which atoms are popped from the chart ùíû \\\\mathcal{C} in Alg. Àú 1 , where: (i) ùíû \\\\mathcal{C} is implemented as a stack for DFS and a queue for BFS, (ii) the axioms are popped from the agenda Q Q in the order in which they are presented (i.e., pushed) to the model in natural language (as described in ¬ß Àú 5 ), and (iii) we do not include pops of axioms.\\nAs our metric, we compute the Levenshtein distance between the sequence of indices produced by the LM and the ground truth reference ordering under the different search orders, normalized by the longest of the two sequences.\\nThe results are shown in Fig. Àú 8 . We observe that the models‚Äô search orders are closer to DFS than to BFS across all types of query overlap. Furthermore, for Llama-3.1, they are closer to the DFS-based ordering required for the shortest proof than DFS when there is no agent or entity overlap between the goal and the irrelevant axioms. Qwen2.5-Math appears to be less efficient however, consistent with the results in Table Àú 3 . These findings suggest that LMs tend to follow a depth-first exploration of the proof space in combination with the superficial heuristic. However, future work should perform a more rigorous analysis to confirm these preliminary findings.',\n",
       "    'subsections': [{'title': 'D.1 Dataset Statistics',\n",
       "      'paragraphs': 'Here, we present more statistics on the datasets used in our experiments ( ¬ß Àú 5 ). Fig. Àú 4 shows the distribution of the number of edges in the proofs in the base problems, which is equivalent to the number of non-axiom theorems. The numbers range from 2 to 13. 16 16 16 For reference, GSM8K (Cobbe et\\xa0al., 2021 ) includes problems with 2 to 8 steps. The distribution is skewed towards shorter problems. This was done by design in order to prevent saturation when evaluating models with different levels of capabilities.\\nFig. Àú 5 shows the number of axioms the GSM problems contain, including irrelevant ones.\\nAs expected, adding one irrelevant axiom (w/ axiom) yields a distribution that is shifted by exactly one additional axiom as compared to the base distribution. When adding a single irrelevant tree (w/ tree), the distribution shifts further and spreads out. This trend is further amplified when adding multiple irrelevant trees (w/ multiple trees).',\n",
       "      'subsections': []},\n",
       "     {'title': 'D.2 Prompt',\n",
       "      'paragraphs': 'See Fig. Àú 6 for the prompt used in our experiments.\\n[PROMPT 1] You are a helpful assistant tasked with solving math word problems. You follow the formatting of the problems and the solutions as given in the examples below. Q: Lib possesses 20 puzzles. Shanna has 4 puzzles more than Lib. Shanna then donates 5 puzzles to Costa. Shanna has 6 puzzles more than Valeria. Valeria has 11 puzzles more than Clementius. How many puzzles does Clementius have totaled? A: Let‚Äôs think step by step. 1. Lib possesses 20 puzzles. 2. Shanna has 4 puzzles more than Lib. 3. So Shanna has 20 + 4 = 24 puzzles. 4. Shanna then donates 5 puzzles to Costa. 5. So Shanna has 24 - 5 = 19 puzzles. 6. Shanna has 6 puzzles more than Valeria. 7. So Valeria has 19 - 6 = 13 puzzles. 8. Valeria has 11 puzzles more than Clementius. 9. So Clementius has 13 - 11 = 2 puzzles. Therefore, the answer (arabic numerals) is 2. [‚Ä¶repeat‚Ä¶] Q: Stacee owns 18 necklaces. Stacee then gets 14 more necklaces from Felic. Valencia has 11 necklaces less than Stacee. How many necklaces does Valencia have? A: Let‚Äôs think step by step. [GENERATION 1] 1. Stacee owns 18 necklaces. 2. Stacee then gets 14 more necklaces from Felic. 3. So Stacee has 18 + 14 = 32 necklaces. 4. Valencia has 11 necklaces less than Stacee. 5. So Valencia has 32 - 11 = 21 necklaces. [PROMPT 2] Therefore, the answer (arabic numerals) is [GENERATION 2] 21',\n",
       "      'subsections': []},\n",
       "     {'title': 'D.3 Example Proof',\n",
       "      'paragraphs': 'In this section we give an example proof in natural language. Consider the following problem, with the irrelevant axioms marked in bold :\\nAnatola has 67 keys. Ichabod has 23 packets. Clayborne has 55 crayons. Pieter has 20 parrots. Ichabod has 30 packets less than Winnah. Pieter then gives Yorgos 6 parrots. Clayborne has 96 crayons less than Imogene. Anatola has 55 keys less than Rahel. Dennie has 19 parrots. Dianna owns 4 parrots. The number of parrots that Edgar has more than Pieter is the same as the difference between the number of parrots that Dennie has compared to Dianna. Edgar has 3 parrots more than Eddie. How many parrots does Eddie have in all?\\nNote that the only axioms that are relevant are the ones containing information about parrots.\\nThe shortest proof for this problem requires the application of four inference rules and can be written in natural language as follows:\\nPieter has 20 parrots.\\nPieter then gives Yorgos 6 parrots.\\nSo Pieter has 20 - 6 = 14 parrots.\\nDennie has 19 parrots.\\nDianna owns 4 parrots.\\nSo the difference between the number of parrots Dennie and Dianna have is 19 - 4 = 15.\\nThe number of parrots that Edgar has more than Pieter is the same as the difference between the number of parrots that Dennie has compared to Dianna.\\nSo Edgar has 14 + 15 = 29 parrots.\\nEdgar has 3 parrots more than Eddie.\\nSo Eddie has 29 - 3 = 26 parrots.\\nIn this case, employing a best-first search heuristic based on lexical overlap with the query (i.e., prioritizing theorems that contain information about parrots) will yield the above shortest proof. Llama-3.1-8B-Instruct did not opt for that strategy. Instead, it generated the following, inefficient solution to this problem:\\nAnatola has 67 keys.\\nAnatola has 55 keys less than Rahel.\\nSo Rahel has 67 + 55 = 122 keys.\\nIchabod has 30 packets less than Winnah.\\nSo Winnah has 23 + 30 = 53 packets.\\nClayborne has 96 crayons less than Imogene.\\nSo Imogene has 55 + 96 = 151 crayons.\\nPieter has 20 parrots.\\nPieter then gives Yorgos 6 parrots.\\nSo Pieter has 20 - 6 = 14 parrots.\\nThe number of parrots that Edgar has more than Pieter is the same as the difference between the number of parrots that Dennie has compared to Dianna.\\nDennie has 19 parrots and Dianna has 4 parrots.\\nThe difference between Dennie and Dianna is 19 - 4 = 15.\\nSo Edgar has 15 more parrots than Pieter.\\nEdgar has 15 + 14 = 29 parrots.\\nEdgar has 3 parrots more than Eddie.\\nSo Eddie has 29 - 3 = 26 parrots.\\nTherefore, the answer (arabic numerals) is 26.',\n",
       "      'subsections': []},\n",
       "     {'title': 'D.4 Further Empirical Results',\n",
       "      'paragraphs': 'Fig. Àú 7 shows further stratified results that were mentioned in ¬ß Àú 5.1 . We briefly note that these results suggest that the models might make use of a heuristic based on such overlap. (Recall that the problem sets are otherwise identical.) One could reasonably expect that a model could learn such a heuristic, based on previous work suggesting that models may derive the conclusion to a proof step greedily within a forward pass before initiating the step in the output tokens (Kudo et\\xa0al., 2024 ; Wu et\\xa0al., 2024 ) .\\nSince QwQ-32B and DeepSeek-R1 did not generate output corresponding to strings in our verbalized program, we performed a separate, more crude analysis for those two models.\\nRather than matching the full generated token sequence, we match only the arithmetic expressions. They are matched with the ground-truth expressions from the built-ins of the corresponding inference rules. This can only be done for theorems that are not axioms, since only those require an arithmetic built-in (i.e, arithmetic expression) to prove.\\nArithmetic expressions are extracted flexibly, accounting for formatting irregularities and natural language that may be interleaved. The parser may also predict no match for a particular model output if no matches are found.\\nWe manually verified parsing accuracy (macro-average) on model outputs over 20 randomly selected example problems. The parser predicted the correct match (or correctly predicted no match) in 94.4% of cases for QwQ-32B and 94.8% of cases for DeepSeek-R1.\\nTable Àú 4 displays the results. We observe results that are overall consistent with the main conclusions of the paper; even state-of-the-art models like DeepSeek-R1‚Äîdespite performing well in terms of accuracy ( Table Àú 2 )‚Äîappear to generate theorems that are irrelevant to the goal theorem. As with the two models discussed in the main text, the efficiency scores decrease when there is agent and/or entity overlap between the irrelevant axioms and the goal theorem.\\nWhile our results in ¬ß Àú 5.2 suggest that LMs make use of information in the query to prove goals, they also reveal that LMs generate irrelevant theorems. Thus, LMs appear to use some heuristic, albeit an imperfect one, in their reasoning.\\nHere, we present a limited analysis on whether this heuristic could be in combination with either DFS or BFS. We do so by examining the order in which intermediate theorems, both relevant and irrelevant, are generated by the LM, and comparing this order to the respective reference orders.\\nFurthermore, we compare the LM‚Äôs ordering to the theorems in the shortest proof as visited in DFS order as a control.\\nBefore presenting the results, we make note of a few limitations of this analysis.\\nFirst, the only irrelevant theorems we consider in the DFS and BFS orderings are those that are generated by Alg. Àú 2 when sampling the axioms for the irrelevant goal theorems h ~ 1 . . . h ~ M \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1}.\\\\mkern-1.0mu.\\\\mkern-1.0mu.\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M} ( ¬ß Àú 4.3 ).\\nIn addition, we note that the in-context examples were ordered according to DFS; see ¬ß Àú 5 . However, the examples were chosen so that the ordering of theorems that are not axioms would be the same under both DFS and BFS. We therefore ignore the axioms as we compare the orderings.\\nConcretely, the DFS and BFS orderings correspond to the order in which atoms are popped from the chart ùíû \\\\mathcal{C} in Alg. Àú 1 , where: (i) ùíû \\\\mathcal{C} is implemented as a stack for DFS and a queue for BFS, (ii) the axioms are popped from the agenda Q Q in the order in which they are presented (i.e., pushed) to the model in natural language (as described in ¬ß Àú 5 ), and (iii) we do not include pops of axioms.\\nAs our metric, we compute the Levenshtein distance between the sequence of indices produced by the LM and the ground truth reference ordering under the different search orders, normalized by the longest of the two sequences.\\nThe results are shown in Fig. Àú 8 . We observe that the models‚Äô search orders are closer to DFS than to BFS across all types of query overlap. Furthermore, for Llama-3.1, they are closer to the DFS-based ordering required for the shortest proof than DFS when there is no agent or entity overlap between the goal and the irrelevant axioms. Qwen2.5-Math appears to be less efficient however, consistent with the results in Table Àú 3 . These findings suggest that LMs tend to follow a depth-first exploration of the proof space in combination with the superficial heuristic. However, future work should perform a more rigorous analysis to confirm these preliminary findings.',\n",
       "      'subsections': [{'title': 'Accuracy Stratified by Agent and Entity Overlap.',\n",
       "        'paragraphs': 'Fig. Àú 7 shows further stratified results that were mentioned in ¬ß Àú 5.1 . We briefly note that these results suggest that the models might make use of a heuristic based on such overlap. (Recall that the problem sets are otherwise identical.) One could reasonably expect that a model could learn such a heuristic, based on previous work suggesting that models may derive the conclusion to a proof step greedily within a forward pass before initiating the step in the output tokens (Kudo et\\xa0al., 2024 ; Wu et\\xa0al., 2024 ) .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Parsing Reasoning Model Outputs.',\n",
       "        'paragraphs': 'Since QwQ-32B and DeepSeek-R1 did not generate output corresponding to strings in our verbalized program, we performed a separate, more crude analysis for those two models.\\nRather than matching the full generated token sequence, we match only the arithmetic expressions. They are matched with the ground-truth expressions from the built-ins of the corresponding inference rules. This can only be done for theorems that are not axioms, since only those require an arithmetic built-in (i.e, arithmetic expression) to prove.\\nArithmetic expressions are extracted flexibly, accounting for formatting irregularities and natural language that may be interleaved. The parser may also predict no match for a particular model output if no matches are found.\\nWe manually verified parsing accuracy (macro-average) on model outputs over 20 randomly selected example problems. The parser predicted the correct match (or correctly predicted no match) in 94.4% of cases for QwQ-32B and 94.8% of cases for DeepSeek-R1.\\nTable Àú 4 displays the results. We observe results that are overall consistent with the main conclusions of the paper; even state-of-the-art models like DeepSeek-R1‚Äîdespite performing well in terms of accuracy ( Table Àú 2 )‚Äîappear to generate theorems that are irrelevant to the goal theorem. As with the two models discussed in the main text, the efficiency scores decrease when there is agent and/or entity overlap between the irrelevant axioms and the goal theorem.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Search Order.',\n",
       "        'paragraphs': 'While our results in ¬ß Àú 5.2 suggest that LMs make use of information in the query to prove goals, they also reveal that LMs generate irrelevant theorems. Thus, LMs appear to use some heuristic, albeit an imperfect one, in their reasoning.\\nHere, we present a limited analysis on whether this heuristic could be in combination with either DFS or BFS. We do so by examining the order in which intermediate theorems, both relevant and irrelevant, are generated by the LM, and comparing this order to the respective reference orders.\\nFurthermore, we compare the LM‚Äôs ordering to the theorems in the shortest proof as visited in DFS order as a control.\\nBefore presenting the results, we make note of a few limitations of this analysis.\\nFirst, the only irrelevant theorems we consider in the DFS and BFS orderings are those that are generated by Alg. Àú 2 when sampling the axioms for the irrelevant goal theorems h ~ 1 . . . h ~ M \\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{1}.\\\\mkern-1.0mu.\\\\mkern-1.0mu.\\\\mathchoice{\\\\hbox{$\\\\displaystyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\textstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptstyle\\\\widetilde{h}$}}{\\\\hbox{$\\\\scriptscriptstyle\\\\widetilde{h}$}}_{M} ( ¬ß Àú 4.3 ).\\nIn addition, we note that the in-context examples were ordered according to DFS; see ¬ß Àú 5 . However, the examples were chosen so that the ordering of theorems that are not axioms would be the same under both DFS and BFS. We therefore ignore the axioms as we compare the orderings.\\nConcretely, the DFS and BFS orderings correspond to the order in which atoms are popped from the chart ùíû \\\\mathcal{C} in Alg. Àú 1 , where: (i) ùíû \\\\mathcal{C} is implemented as a stack for DFS and a queue for BFS, (ii) the axioms are popped from the agenda Q Q in the order in which they are presented (i.e., pushed) to the model in natural language (as described in ¬ß Àú 5 ), and (iii) we do not include pops of axioms.\\nAs our metric, we compute the Levenshtein distance between the sequence of indices produced by the LM and the ground truth reference ordering under the different search orders, normalized by the longest of the two sequences.\\nThe results are shown in Fig. Àú 8 . We observe that the models‚Äô search orders are closer to DFS than to BFS across all types of query overlap. Furthermore, for Llama-3.1, they are closer to the DFS-based ordering required for the shortest proof than DFS when there is no agent or entity overlap between the goal and the irrelevant axioms. Qwen2.5-Math appears to be less efficient however, consistent with the results in Table Àú 3 . These findings suggest that LMs tend to follow a depth-first exploration of the proof space in combination with the superficial heuristic. However, future work should perform a more rigorous analysis to confirm these preliminary findings.',\n",
       "        'subsections': []}]}]}],\n",
       "  'abstract': 'Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of human-like reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language‚Äîas generated by an LM‚Äîwith shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions‚Äîeven with minimal, domain-consistent distractions‚Äîand the proofs they generate frequently exhibit detours through irrelevant inferences.111Code for this project is available at https://github.com/rycolab/reasoning-efficiency.'},\n",
       " '2510.25014v1': {'content': [{'title': 'Introduction',\n",
       "    'paragraphs': 'Large Language Models (LLMs) are transforming non-player character (NPC) interactions in games, enabling natural language-driven experiences that transcend traditional script constraints (Wang et\\xa0al. 2024 ; Maleki and Zhao 2024 ; Relu Games 2025 ; Krafton 2025 ) . A critical yet underexplored application for these dynamic NPCs is in-game trading, a core mechanic connecting economic activities and character development (Kim et\\xa0al. 2024 ) . While LLMs promise to replace static menus with emergent gameplay, enabling context-aware recommendations and relationship-based negotiations (Figure 1 ), their deployment faces a fundamental challenge.\\nThe core tension lies in reconciling LLMs‚Äô creative flexibility with the semi-structured procedures of commercial transactions. In-game trading must follow a flow of browse, offer, review, and confirm, serving as an essential safeguard to protect player assets. Yet, the unconstrained, intent-driven nature of LLMs makes them prone to circumventing this structured flow (Agrawal et\\xa0al. 2024 ; Shukuri et\\xa0al. 2023 ; Wu et\\xa0al. 2024 ) . For instance, their tendency to directly satisfy a perceived user goal can lead a simple price inquiry to trigger an unwanted purchase, or a negotiation skipping the critical review step. Such procedural violations erode player trust and undermine system integrity.\\nThis challenge extends beyond gaming to any rule-governed human-AI interaction requiring both conversational naturalness and procedural compliance, such as in customer service protocols or medical consultations. Despite its importance, existing approaches either sacrifice flexibility for structure (Shukuri et\\xa0al. 2023 ) or fail to enforce critical procedural constraints, leaving a significant gap in deploying LLMs for real-world applications where both qualities are essential.\\nTo address this, we introduce Autoregressive State-Tracking Prompting (ASTP), a prompting methodology that resolves the tension between flexibility and control by making state-tracking an explicit, autoregressive process. Instead of relying on general reasoning heuristics (e.g., ‚Äúthink step-by-step‚Äù), ASTP requires the LLM to infer and output the previous dialogue state‚Äôs label before predicting the next. This verifiable process is embedded in a structured Prime‚ÄìGuide‚ÄìEnforce workflow that maintains procedural compliance without compromising conversational naturalness. Additionally, we demonstrate that this state-aware architecture enables a practical post-processing mechanism that boosts numerical reliability, allowing smaller models to match the accuracy of larger ones with substantial speed gains.\\nOur main contributions are as follows:\\nWe propose a prompting strategy, Autoregressive State-Tracking Prompting (ASTP) , that makes dialogue state transitions explicit and verifiable.\\nWe design a structured prompt workflow, Prime‚ÄìGuide‚ÄìEnforce , that significantly improves procedural compliance, increasing adherence to key safeguards from 78.1% to 99.6%.\\nWe develop a placeholder-based post-processing method, PPP , that improves price calculation accuracy in trading tasks from 84.3% to 99.3%, enabling smaller models to match the accuracy of larger ones.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Related Work',\n",
       "    'paragraphs': 'One of the well-established fields for managing goal-oriented dialogue interactions is Task-Oriented Dialogue (TOD). TOD aims to help a user complete a specific task, e.g., booking a flight. A traditional TOD system relies on core components like a dialogue state tracker, which fills predefined ‚Äòslots‚Äô (e.g., destination, time) from user utterances (Feng et\\xa0al. 2023 ; Hudeƒçek and Du≈°ek 2023 ; Li et\\xa0al. 2024 ) , and a dialogue policy, which decides the next system action (e.g., calling API or requesting additional information) based on the current state (Schick et\\xa0al. 2023 ; Cao et\\xa0al. 2024 ) .\\nThe paradigm has shifted towards end-to-end systems that replace this modular pipeline (Hosseini-Asl et\\xa0al. 2020 ; Saley et\\xa0al. 2024 ) . AutoTOD (Xu et\\xa0al. 2024 ) , specifically, re-architects the workflow using a single unified prompt that decomposes a complex goal into a series of ‚Äòsub-tasks‚Äô. The system then selects the most appropriate sub-task for a user‚Äôs intent in each turn and executes the prompt for each sub-task, guiding the LLM to implicitly handle state-tracking and policy decisions. This highlights a fundamental characteristic of the TOD paradigm: its dialogue policy is optimized to achieve a goal, not to enforce a specific sequence of interactions. This goal-driven optimization, while effective for flexible task handling, is fundamentally misaligned with the demand for strict procedural adherence, as it makes the system prone to bypassing critical safeguards like a final review.\\nAnother approach to governing LLM interactions involves explicitly defining the conversational path. Recognizing that open-ended generation is often insufficient for goal-oriented tasks, this line of research focuses on providing a blueprint to guide the LLM (Agrawal et\\xa0al. 2024 ; Du et\\xa0al. 2025 ) . However, while these methods excel at the design of the flow, the challenge of developing a robust runtime mechanism to compel the LLM to adhere to it remains underexplored.\\nDistinct from defining a conversational path, other methods model the workflow based on the task-solving state,\\nenhancing control and efficiency (Wu et\\xa0al. 2024 ; Rozanov and Rei 2024 ) .\\nPrompting techniques that enhance an LLM‚Äôs reasoning, such as Chain-of-Thought (CoT), have proven remarkably effective at unlocking complex reasoning capabilities in large models (Wei et\\xa0al. 2022 ; Kojima et\\xa0al. 2022 ; Wang et\\xa0al. 2022 ; Bai et\\xa0al. 2022 ; Yao et\\xa0al. 2023 ; Madaan et\\xa0al. 2023 ) . These methods typically prompt the LLM to ‚Äúthink step-by-step‚Äù before responding, which can improve performance on a wide range of tasks. However, their reasoning process is often unconstrained and generic. In contrast, for applications requiring procedural adherence, explicitly specifying what the LLM should think about offers a more robust approach. Our approach transforms vague ‚Äôthinking‚Äô into a ‚Äôverifiable procedure‚Äô by instructing the model to first identify the previous state and then compelling it to report that state in its response. This targeted approach provides a more robust anchor for the model‚Äôs reasoning, making it more suitable for procedural tasks than generic instructions.\\nMeanwhile, research in our direct application domain, game NPCs, has focused on a different trajectory. Generative Agents (Park et\\xa0al. 2023 ) aim to create believable\\nNPCs through sophisticated memory mechanisms, while recent work on persistence (Song 2025 ) and narrative generation (Peng et\\xa0al. 2024 ) pushes creative boundaries.\\nYet these systems deliberately avoid structured constraints, prioritizing emergent behavior (Lan et\\xa0al. 2024 ) over reliability. Even MART (Kim et\\xa0al. 2024 ) , which specifically targets merchant NPCs, reveals this fundamental gap. Specifically, it handles price negotiation but cannot manage multi-item transactions or enforce the purchase confirmations essential for protecting player assets.\\nIn summary, the landscape of related work reveals a clear and critical gap. Task-oriented systems prioritize goal completion over procedural flow, design-focused frameworks lack enforcement mechanisms, general reasoning techniques are not tailored for strict procedural control, and research in game NPCs has favored creativity over compliance. This highlights the need for a new approach specifically designed to ensure robust, verifiable procedural adherence within conversational agents. Our work aims to fill this gap.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Proposed Method',\n",
       "    'paragraphs': 'This section details a method for building a reliable merchant NPC by addressing both conversational coherence and transactional integrity. The method first introduces Autoregressive State-Tracking Prompting (ASTP) to enforce a logical dialogue flow, and then integrates a state-specific post-processing rule as a vital component to guarantee computational accuracy within that flow.\\nIn the game, players can naturally converse and trade with merchant NPCs. The prompt for the merchant NPC is structured to provide rich contextual information for the LLM. While the prompt contains several components, the core of the proposed method lies in the design of the Dialogue Guidelines within a unified prompt. The full prompt consists of the following.\\nSystem Instructions : Defines the NPC‚Äôs persona, role, and behavioral directives. It also includes situational context like the in-game location and time to ground its response in the game world.\\nGame World Data : Contains two distinct sets of item data: a list of all items existing in the game world, and a separate list of the NPC‚Äôs actual sellable inventory. This dual structure allows the NPC to act as both an immersive world informant and a reliable merchant.\\nDialogue History : Presents preceding conversation turns, including the player‚Äôs latest utterance and the context inferred by the NPC from previous interactions.\\nDialogue Guidelines : Provides the core logic for dialogue state management. These rules instruct the LLM on how to infer the current dialogue state and when to transition between states, which will be detailed in the following sections.\\nExpected Response Format : Instructs the LLM to generate its output as a single, well-formed JSON object. This approach relies on the LLM‚Äôs capability to follow formatting instructions within the prompt, rather than utilizing specialized structured output functionalities. Key fields in the JSON object include NPC‚Äôs spoken dialogue, the inferred dialogue state, and data pertinent to the current transaction, which contains information such as an array of items with their quantities and prices.\\nWe define the system‚Äôs dialogue state space ùíÆ \\\\mathcal{S} as follows:\\nBase state set ( S base S_{\\\\text{base}} ): Main dialogue states unrelated to trading.\\nTrade state set ( S trade S_{\\\\text{trade}} ): Specific states possible within a trading context.\\nComplete state space ( ùíÆ \\\\mathcal{S} ): The union of both sets.\\nThe dialogue history h t h_{t} up to turn t t is defined as a sequence of user utterances ( u u ) and system responses ( r r ):\\nWe define a state extraction function to retrieve the previous state from the last response:\\nThe core functionality of Autoregressive State-Tracking Prompting (ASTP) is modeled as:\\nGiven ( h t , u t + 1 , ùí¢ ) (h_{t},u_{t+1},\\\\mathcal{G}) ,\\nthe Autoregressive State-Tracking Prompting function\\ncomputes ( s t , s t + 1 , r t + 1 ) (s_{t},s_{t+1},r_{t+1}) through the following sequential process:\\nState Inference: Infers the previous dialogue state from the conversation history\\nState Transition: Determines the current state based on the inferred previous state and user input\\nResponse Generation: Generates an appropriate NPC response for the determined state\\nwhere, ‚Ñã , ùí∞ , ùí¢ , ‚Ñõ \\\\mathcal{H},\\\\mathcal{U},\\\\mathcal{G},\\\\mathcal{R} denote the spaces of dialogue histories, user utterances, game contexts included in the prompt, and system responses, respectively.\\nThe key insight is that s t s_{t} is not directly extracted but inferred by the LLM from the dialogue history, enabling state-aware decision making.\\nWhile the ASTP function autonomously infers states, it must satisfy certain transition constraints for system stability and rule compliance. Let ‚Ñ∞ ‚äÜ ùíÆ √ó ùíÆ \\\\mathcal{E}\\\\subseteq\\\\mathcal{S}\\\\times\\\\mathcal{S} denote the set of valid state transitions permitted by the system.\\nAny valid execution of f ASTP f_{\\\\text{ASTP}} must satisfy:\\nTransition Validity: ( s t , s t + 1 ) ‚àà ‚Ñ∞ (s_{t},s_{t+1})\\\\in\\\\mathcal{E}\\nCritical Transaction Constraint:\\nInference Consistency:\\nRelying on implicit context can lead to critical procedural ambiguities. For instance, after an NPC offers a price (the OFFER_SELL state), a user‚Äôs agreement like ‚ÄúGreat, let‚Äôs do it!‚Äù is ambiguous: should the system proceed to the mandatory review step ( FINAL_CHECK ) or directly execute the sale ( COMMIT_SALE )? ASTP resolves this by treating the previous state, s t s_{t} , as an explicit intermediate variable that must be identified before determining the next state, s t + 1 s_{t+1} . This factorization of the state determination process makes it a more transparent and structured task.\\nTranslating the formal ASTP model into a reliable prompt-based implementation is a non-trivial challenge. Experiments reveal that performance is highly sensitive not only to inclusion of specific rules but critically to their structural arrangement within the prompt.\\nTherefore, the design strategically creates a distinct cognitive workflow for the LLM: it first primes the model by demanding upfront identification of the previous state (Element 3), then guides its reasoning with conditional rules (Elements 1 and 2), and finally enforces compliance by requiring the previous state identification to be reported in the response (Element 4). This architectural choice transforms state-tracking from a mere suggestion into an explicit, verifiable task, proving essential for achieving robust procedural adherence.\\nThe framework implements this workflow through four key design elements incorporated into the Dialogue Guidelines (Figure 2 ):\\nElement 1: Basic State Definitions define what each dialogue state represents and the NPC‚Äôs appropriate behaviors within it. This element provides the foundational knowledge for the ‚ÄúGuiding‚Äù phase of the workflow.\\nElement 2: State Transition Conditions specifies the prerequisite conditions for entering each state. These conditions are intentionally coupled with their corresponding state definitions in the prompt, a structural arrangement that reduces cognitive load and makes the ‚ÄúGuiding‚Äù phase more effective than referencing a separate set of rules.\\nElement 3: Directive to Identify Previous State instructs the model to explicitly identify the previous dialogue state before any other reasoning. This directive serves as the critical ‚ÄúPriming‚Äù stage of the workflow.\\nElement 4: Directive to Respond including Previous State requires the model to report its identified previous state within its required JSON response. This final step acts as the ‚ÄúEnforcing‚Äù stage, making the entire state-tracking process transparent and verifiable.\\nIt is a well-documented challenge that LLMs, being optimized for language, can struggle with precise mathematical computations.\\nEven when correctly listing item quantities and individual prices, errors may occur in final price calculations, compromising transaction integrity.\\nThis issue is critical not just for evaluating the LLM‚Äôs arithmetic skills, but for ensuring a trustworthy player experience, as the total price spoken by the NPC must be perfectly consistent with the underlying transaction data.\\nSolution : The solution implements accurate price calculations through a placeholder-based, post-processing system. The key to this approach lies in its state-specific application, as it is activated only during OFFER_SELL states. This state is defined as the critical point where the NPC proposes the official, system-calculated price for a transaction before any creative negotiation begins. This targeted enforcement guarantees calculation accuracy when it is most critical, while preserving the LLM‚Äôs conversational flexibility in other states. The system works as follows:\\nPlaceholder usage : The prompt includes the rule ‚Äúuse __PRICE__ for the final price amount‚Äù (Figure 2 )\\nAccurate calculation : The system calculates the correct price based on responded item information and replaces the placeholder\\nError prevention : The accurately calculated price is included in dialogue history, ensuring correct price references in subsequent conversations\\nResponse example :\\nOriginal LLM response :\\nNPC dialogue after post-processing : ‚ÄúTwo iron swords and a mana potion are 150 gold.‚Äù\\nAdvantages : This approach achieves 99.3% accurate price calculation without additional computational overhead, eliminating the need for complex schemas or\\nTool use mechanisms. The single response processing maintains system efficiency while ensuring mathematical precision. This state-based prompting approach demonstrates how external system functionalities such as accurate calculation and post-processing can be integrated into LLM-driven interactions, providing a foundation for broader applications in rule-governed interactive systems.',\n",
       "    'subsections': [{'title': 'Prompt Architecture',\n",
       "      'paragraphs': 'In the game, players can naturally converse and trade with merchant NPCs. The prompt for the merchant NPC is structured to provide rich contextual information for the LLM. While the prompt contains several components, the core of the proposed method lies in the design of the Dialogue Guidelines within a unified prompt. The full prompt consists of the following.\\nSystem Instructions : Defines the NPC‚Äôs persona, role, and behavioral directives. It also includes situational context like the in-game location and time to ground its response in the game world.\\nGame World Data : Contains two distinct sets of item data: a list of all items existing in the game world, and a separate list of the NPC‚Äôs actual sellable inventory. This dual structure allows the NPC to act as both an immersive world informant and a reliable merchant.\\nDialogue History : Presents preceding conversation turns, including the player‚Äôs latest utterance and the context inferred by the NPC from previous interactions.\\nDialogue Guidelines : Provides the core logic for dialogue state management. These rules instruct the LLM on how to infer the current dialogue state and when to transition between states, which will be detailed in the following sections.\\nExpected Response Format : Instructs the LLM to generate its output as a single, well-formed JSON object. This approach relies on the LLM‚Äôs capability to follow formatting instructions within the prompt, rather than utilizing specialized structured output functionalities. Key fields in the JSON object include NPC‚Äôs spoken dialogue, the inferred dialogue state, and data pertinent to the current transaction, which contains information such as an array of items with their quantities and prices.',\n",
       "      'subsections': []},\n",
       "     {'title': 'ASTP: Formalization and Principles',\n",
       "      'paragraphs': 'We define the system‚Äôs dialogue state space ùíÆ \\\\mathcal{S} as follows:\\nBase state set ( S base S_{\\\\text{base}} ): Main dialogue states unrelated to trading.\\nTrade state set ( S trade S_{\\\\text{trade}} ): Specific states possible within a trading context.\\nComplete state space ( ùíÆ \\\\mathcal{S} ): The union of both sets.\\nThe dialogue history h t h_{t} up to turn t t is defined as a sequence of user utterances ( u u ) and system responses ( r r ):\\nWe define a state extraction function to retrieve the previous state from the last response:\\nThe core functionality of Autoregressive State-Tracking Prompting (ASTP) is modeled as:\\nGiven ( h t , u t + 1 , ùí¢ ) (h_{t},u_{t+1},\\\\mathcal{G}) ,\\nthe Autoregressive State-Tracking Prompting function\\ncomputes ( s t , s t + 1 , r t + 1 ) (s_{t},s_{t+1},r_{t+1}) through the following sequential process:\\nState Inference: Infers the previous dialogue state from the conversation history\\nState Transition: Determines the current state based on the inferred previous state and user input\\nResponse Generation: Generates an appropriate NPC response for the determined state\\nwhere, ‚Ñã , ùí∞ , ùí¢ , ‚Ñõ \\\\mathcal{H},\\\\mathcal{U},\\\\mathcal{G},\\\\mathcal{R} denote the spaces of dialogue histories, user utterances, game contexts included in the prompt, and system responses, respectively.\\nThe key insight is that s t s_{t} is not directly extracted but inferred by the LLM from the dialogue history, enabling state-aware decision making.\\nWhile the ASTP function autonomously infers states, it must satisfy certain transition constraints for system stability and rule compliance. Let ‚Ñ∞ ‚äÜ ùíÆ √ó ùíÆ \\\\mathcal{E}\\\\subseteq\\\\mathcal{S}\\\\times\\\\mathcal{S} denote the set of valid state transitions permitted by the system.\\nAny valid execution of f ASTP f_{\\\\text{ASTP}} must satisfy:\\nTransition Validity: ( s t , s t + 1 ) ‚àà ‚Ñ∞ (s_{t},s_{t+1})\\\\in\\\\mathcal{E}\\nCritical Transaction Constraint:\\nInference Consistency:\\nRelying on implicit context can lead to critical procedural ambiguities. For instance, after an NPC offers a price (the OFFER_SELL state), a user‚Äôs agreement like ‚ÄúGreat, let‚Äôs do it!‚Äù is ambiguous: should the system proceed to the mandatory review step ( FINAL_CHECK ) or directly execute the sale ( COMMIT_SALE )? ASTP resolves this by treating the previous state, s t s_{t} , as an explicit intermediate variable that must be identified before determining the next state, s t + 1 s_{t+1} . This factorization of the state determination process makes it a more transparent and structured task.',\n",
       "      'subsections': [{'title': 'State Space',\n",
       "        'paragraphs': 'We define the system‚Äôs dialogue state space ùíÆ \\\\mathcal{S} as follows:\\nBase state set ( S base S_{\\\\text{base}} ): Main dialogue states unrelated to trading.\\nTrade state set ( S trade S_{\\\\text{trade}} ): Specific states possible within a trading context.\\nComplete state space ( ùíÆ \\\\mathcal{S} ): The union of both sets.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Dialogue History',\n",
       "        'paragraphs': 'The dialogue history h t h_{t} up to turn t t is defined as a sequence of user utterances ( u u ) and system responses ( r r ):\\nWe define a state extraction function to retrieve the previous state from the last response:',\n",
       "        'subsections': []},\n",
       "       {'title': 'ASTP Function',\n",
       "        'paragraphs': 'The core functionality of Autoregressive State-Tracking Prompting (ASTP) is modeled as:\\nGiven ( h t , u t + 1 , ùí¢ ) (h_{t},u_{t+1},\\\\mathcal{G}) ,\\nthe Autoregressive State-Tracking Prompting function\\ncomputes ( s t , s t + 1 , r t + 1 ) (s_{t},s_{t+1},r_{t+1}) through the following sequential process:\\nState Inference: Infers the previous dialogue state from the conversation history\\nState Transition: Determines the current state based on the inferred previous state and user input\\nResponse Generation: Generates an appropriate NPC response for the determined state\\nwhere, ‚Ñã , ùí∞ , ùí¢ , ‚Ñõ \\\\mathcal{H},\\\\mathcal{U},\\\\mathcal{G},\\\\mathcal{R} denote the spaces of dialogue histories, user utterances, game contexts included in the prompt, and system responses, respectively.\\nThe key insight is that s t s_{t} is not directly extracted but inferred by the LLM from the dialogue history, enabling state-aware decision making.',\n",
       "        'subsections': []},\n",
       "       {'title': 'State Transition Constraints',\n",
       "        'paragraphs': 'While the ASTP function autonomously infers states, it must satisfy certain transition constraints for system stability and rule compliance. Let ‚Ñ∞ ‚äÜ ùíÆ √ó ùíÆ \\\\mathcal{E}\\\\subseteq\\\\mathcal{S}\\\\times\\\\mathcal{S} denote the set of valid state transitions permitted by the system.\\nAny valid execution of f ASTP f_{\\\\text{ASTP}} must satisfy:\\nTransition Validity: ( s t , s t + 1 ) ‚àà ‚Ñ∞ (s_{t},s_{t+1})\\\\in\\\\mathcal{E}\\nCritical Transaction Constraint:\\nInference Consistency:\\nRelying on implicit context can lead to critical procedural ambiguities. For instance, after an NPC offers a price (the OFFER_SELL state), a user‚Äôs agreement like ‚ÄúGreat, let‚Äôs do it!‚Äù is ambiguous: should the system proceed to the mandatory review step ( FINAL_CHECK ) or directly execute the sale ( COMMIT_SALE )? ASTP resolves this by treating the previous state, s t s_{t} , as an explicit intermediate variable that must be identified before determining the next state, s t + 1 s_{t+1} . This factorization of the state determination process makes it a more transparent and structured task.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'ASTP: Implementation with Key Design Elements',\n",
       "      'paragraphs': 'Translating the formal ASTP model into a reliable prompt-based implementation is a non-trivial challenge. Experiments reveal that performance is highly sensitive not only to inclusion of specific rules but critically to their structural arrangement within the prompt.\\nTherefore, the design strategically creates a distinct cognitive workflow for the LLM: it first primes the model by demanding upfront identification of the previous state (Element 3), then guides its reasoning with conditional rules (Elements 1 and 2), and finally enforces compliance by requiring the previous state identification to be reported in the response (Element 4). This architectural choice transforms state-tracking from a mere suggestion into an explicit, verifiable task, proving essential for achieving robust procedural adherence.\\nThe framework implements this workflow through four key design elements incorporated into the Dialogue Guidelines (Figure 2 ):\\nElement 1: Basic State Definitions define what each dialogue state represents and the NPC‚Äôs appropriate behaviors within it. This element provides the foundational knowledge for the ‚ÄúGuiding‚Äù phase of the workflow.\\nElement 2: State Transition Conditions specifies the prerequisite conditions for entering each state. These conditions are intentionally coupled with their corresponding state definitions in the prompt, a structural arrangement that reduces cognitive load and makes the ‚ÄúGuiding‚Äù phase more effective than referencing a separate set of rules.\\nElement 3: Directive to Identify Previous State instructs the model to explicitly identify the previous dialogue state before any other reasoning. This directive serves as the critical ‚ÄúPriming‚Äù stage of the workflow.\\nElement 4: Directive to Respond including Previous State requires the model to report its identified previous state within its required JSON response. This final step acts as the ‚ÄúEnforcing‚Äù stage, making the entire state-tracking process transparent and verifiable.',\n",
       "      'subsections': []},\n",
       "     {'title': 'State-specific Rules for Transaction Integrity',\n",
       "      'paragraphs': 'It is a well-documented challenge that LLMs, being optimized for language, can struggle with precise mathematical computations.\\nEven when correctly listing item quantities and individual prices, errors may occur in final price calculations, compromising transaction integrity.\\nThis issue is critical not just for evaluating the LLM‚Äôs arithmetic skills, but for ensuring a trustworthy player experience, as the total price spoken by the NPC must be perfectly consistent with the underlying transaction data.\\nSolution : The solution implements accurate price calculations through a placeholder-based, post-processing system. The key to this approach lies in its state-specific application, as it is activated only during OFFER_SELL states. This state is defined as the critical point where the NPC proposes the official, system-calculated price for a transaction before any creative negotiation begins. This targeted enforcement guarantees calculation accuracy when it is most critical, while preserving the LLM‚Äôs conversational flexibility in other states. The system works as follows:\\nPlaceholder usage : The prompt includes the rule ‚Äúuse __PRICE__ for the final price amount‚Äù (Figure 2 )\\nAccurate calculation : The system calculates the correct price based on responded item information and replaces the placeholder\\nError prevention : The accurately calculated price is included in dialogue history, ensuring correct price references in subsequent conversations\\nResponse example :\\nOriginal LLM response :\\nNPC dialogue after post-processing : ‚ÄúTwo iron swords and a mana potion are 150 gold.‚Äù\\nAdvantages : This approach achieves 99.3% accurate price calculation without additional computational overhead, eliminating the need for complex schemas or\\nTool use mechanisms. The single response processing maintains system efficiency while ensuring mathematical precision. This state-based prompting approach demonstrates how external system functionalities such as accurate calculation and post-processing can be integrated into LLM-driven interactions, providing a foundation for broader applications in rule-governed interactive systems.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Experiments and Results',\n",
       "    'paragraphs': 'This section empirically evaluates the Autoregressive State-Tracking Prompting (ASTP) methodology, assessing its effectiveness in dialogue flow adherence and state-specific\\npost-processing within dynamic trading interactions.\\nTo ensure reproducibility, all experiments utilized a virtual player LLM interacting with an LLM-driven NPC over 300 dialogues. For each dialogue, the run was initiated with a fixed seed (0-299) that fixed the player‚Äôs initial utterance.\\nThe virtual player‚Äôs objective was to naturally converse and purchase items, though it was not required to complete a purchase in every interaction. This objective was guided by behavioral guidelines that included asking for item information, negotiating prices, and changing the selected items mid-trade.\\nThe conversations followed two scenarios: (1) Specific Item Purchase , where the player requested up to six items randomly drawn from 20 sellable and 32 unsellable items\\n, with purchase quantities for each item randomly selected from 1 to 5; and (2) Item Recommendation , where the player requested items for a specific purpose (e.g., ‚Äúfor a goblin battle‚Äù).\\nOn average, conversation in Scenario 2 were approximately 9% longer than in Scenario 1 across all experimental settings, reflecting the more exploratory nature.\\nAll experiments were run on a machine with Intel Core Ultra 7 155H CPU, 16GB of RAM, and integrated Intel Arc Graphics. LLM inferences were performed via the Google Gemini API. 1 1 1 All API calls were made in July 2025. Unless otherwise noted, the default configuration used gemini-2.5-flash model with a temperature of 0.7 and a thinking budget of 0. The full prompts for all experimental setups are provided in the Appendix.\\nExperimental Design :\\nASTP‚Äôs compliance with dialogue flow was evaluated through an ablation study of its four key elements and a comparison against methods adapted from other studies. The four elements are: (1) state definitions, (2) state transition conditions, (3) directive to identify the previous state, and (4) directive to output the previous state. As detailed in Table 1 , the ASTP baselines were constructed by cumulatively adding these elements, which allowed isolation of their impact.\\nFor AutoTOD (Xu et\\xa0al. 2024 ) , one of the comparison methods, the exact content of Baseline1 was converted into the prompt structure from the official AutoTOD implementation 2 2 2 https://github.com/DaDaMrX/AutoTOD . This approach was chosen because both prompts fundamentally rely on state definitions, making for a direct comparison of prompt structures. In the adapted version, each trade state was framed as a ‚Äòsub-task‚Äô, and all external API-calling functions were removed as the experiment relies on in-context data.\\nFor the DFI-Inspired method, an explicit state-transition graph was added to the Baseline1 prompt. This technique was inspired by a concept for applying an dialogue flow to constrain an LLM, which was illustrated as a potential application in Dialog Flow Induction (DFI) (Agrawal et\\xa0al. 2024 ) . For the Zero-shot-Chain-of-Thought (ZS-COT) (Kojima et\\xa0al. 2022 ) comparison, the Baseline2 prompt was augmented by adding the one-line instruction: ‚ÄúBefore generating a response, first think step by step to determine the correct context and action. Your thought process should be recorded in the npc_thoughts field.‚Äù This method was tested as an alternative to using Element 3 and 4, representing a different approach to enhancing the model‚Äôs reasoning. Figure 3 provides a visual comparison of these distinct prompt architectures. Finally, AutoTOD+ and DFI-Inspired+ were created by augmenting each respective base prompt with all four ASTP elements.\\nEvaluation Metric :\\nThe primary evaluation metric is the State Transition Compliance Rate (STCR), defined as the proportion of transactions that correctly pass through FINAL_CHECK before finalizing in COMMIT_SALE . 3 3 3 To ensure a fair comparison, STCR was calculated on a normalized sample of dialogues that reached the COMMIT_SALE state (N=218 for Scenario1, N=274 for Scenario2), the minimum number observed across all methods. As an irreversible step, this transition is fundamentally different from the flow‚Äôs other, more flexible transitions. These flexible transitions, such as re-entering negotiation from the final review stage, are designed for conversational fluidity, where the LLM‚Äôs inference is primarily driven by user intent.\\nThe transition into COMMIT_SALE , however, is intentionally rigid. Here, the LLM‚Äôs task transcends intent recognition: it must prioritize a procedural precondition (the previous state) over the user‚Äôs immediate utterance. This deliberate core tension makes it the ultimate test of a model‚Äôs ability to adhere to critical safeguards designed to prevent unintended transactions.\\nKey Results :\\nAs shown in Table 1 , the full ASTP method achieved near-perfect compliance. The ablation study reveals that while adding explicit transition rules (Element 2) significantly improved performance, the key to robust control lies in the interaction between Elements 3 and 4. Instructing the model only to identify the previous state (Element 3) yielded no benefit, likely by introducing ambiguity without an enforcement mechanism. However, compelling the model to output its inference (Element 4) proved highly effective. The highest compliance rate was achieved when both elements were combined, suggesting that the ‚Äòidentify‚Äô directive provides a clear signal for the ‚Äòoutput‚Äô directive‚Äôs powerful enforcement mechanism.\\nA comparison with other methods offers further insights into the importance of prompt structure.\\nThe ZS-CoT method, while improving upon Baseline2, achieved a lower STCR than the full ASTP. This is conjectured to be because a generic ‚Äúthink-by-step‚Äù instruction, while encouraging an internal reasoning process, lacks the explicit enforcement mechanism of ASTP: the mandatory output of the inferred state. Lacking this constraint, ZS-CoT is more susceptible to deviating from procedural rules when faced with strong user intent. In particular, the performance gap between ZS-CoT and ASTP widens in Scenario 2, which involves more exploratory, recommendation-based conversations. The increased ambiguity and conversational complexity in this scenario place a higher cognitive load on the model. ASTP‚Äôs rigid mechanism forcing the LLM to re-anchor its context by identifying and outputting the previous state proves to be more robust in these complex situations.\\nBaseline1 yielded a higher STCR than AutoTOD despite both using only state definitions. This is conjectured to stem from their differing prompting paradigm. AutoTOD‚Äôs fragmented sub-task structure frames the task as a classification problem each turn, making it prone to skipping steps when faced with a strong user intent (e.g., a ‚ÄúYes‚Äù to purchase). In contrast, Baseline1‚Äôs prompt presents state definitions as a unified list. This contextual proximity appears to implicitly suggest a sequential flow, guiding the model more effectively.\\nThe just-in-time instruction in Baseline2-which couples a transition rule directly with its state definition-also proved more effective than DFI-Inspired‚Äôs approach of referencing a separate state-transition graph. The latter likely increases the model‚Äôs congnitive load, leading to a slightly lower compliance rate.\\nMost notably, augmenting these other methods with all four ASTP elements (AutoTOD+, DFI-Inspired+) improved their performance. This demonstrates that ASTP‚Äôs core principles are not tied to a specific prompt format but are portable principles that can enhance an LLM‚Äôs ability to adhere to strict procedural rules. The state transition patterns in Figure 3 confirm that ASTP also induces natural, context-appropriate dialogue flows.\\nLLMs struggle with precise arithmetic operations, making simple calculation errors like ‚Äú2√ó50 + 30 = 120 gold.‚Äù This poses significant problems for price calculations in trading, potentially undermining player trust and transaction integrity.\\nExperimental Design :\\nUtilizing dialogues from Scenario 1,\\nthis experiment compares performance across two key dimensions:\\n(Table 2 ):\\nASTP variants : ASTP, ASTP -PPP (an ablated version without Placeholder Post-Processing), ASTP -PPP +SO (the ablated version combined with Structured Output)\\nModel variants : gemini-2.5-flash, gemini-2.0-flash (lighter version), gemini-2.5-pro (high-performance)\\nEvaluation Metric : The evaluation focuses on price accuracy, along with token usage and response time.\\nPrice accuracy is the percentage of NPC responses where the total price stated by the LLM matches the sum calculated from the item details (i.e., prices and quantities) also provided within the same LLM response.\\nKey Results :\\nASTP showed consistent improvements in accuracy while maintaining computational efficiency across multiple evaluation dimensions.\\nFirst, in terms of accuracy, ASTP reached 100.0% in the OFFERÀôSELL state, compared to 81.4% with ASTP -PPP . This improvement also extended to subsequent states, where ASTP achieved 99.3% accuracy versus 84.3% for ASTP -PPP . These results suggest that accurate initial pricing may help reduce downstream errors in multi-turn dialogues.\\nSecond, these accuracy gains were achieved without notable increases in computational cost. Token usage (441.6 vs 437.7) and response time (2.4 seconds) remained comparable to ASTP -PPP . By contrast, the ASTP -PPP +SO variant uses fewer completion tokens, but its price accuracy is much lower at 87.8% and 88.0%. For reliable transactions, ASTP‚Äôs accuracy is more crucial than this gain in token efficiency.\\nFurthermore, a high variability in thought token usage was observed in some configurations (e.g., 22.9 for ASTP -PPP +SO and 325.0 for the gemini-2.5-pro model). This likely reflects that the model‚Äôs internal reasoning demands fluctuate significantly depending on the dialogue context of each turn.\\nThird, ASTP also showed promising results with smaller models. When tested with gemini-2.0-flash, it achieved 100.0% accuracy in OFFER_SELL and 90.4% in other states, outperforming the ASTP -PPP , which achieved 48.7% and 59.0%, respectively.\\nThis demonstrates the robustness of the method on smaller models.\\nLastly, ASTP using gemini-2.5-flash reached comparable accuracy to ASTP -PPP with gemini-2.5-pro while requiring significantly fewer resources: 0 vs 1575.2 thought tokens (as the thinking budget for gemini-2.5-pro cannot be disabled via its API) and 2.4 vs 21.2 seconds in response time. These findings suggest that incorporating state-specific post-processing may allow smaller models to approximate the accuracy of larger ones, with considerably lower computational demands.\\nBeyond model performance, the choice of the placeholder keyword was observed to affect the robustness of the post-processing step.\\nThe concise keyword __PRICE__ consistently produced accurate and parsable outputs. In contrast, more complex variants such as __PRICE_PLACEHOLDER__ occasionally resulted in malformed completions like __PRICE_PLACEHOLDER_ or __PRICE_PLACEHOLDE__ . These inconsistencies highlight the importance of using short, unambiguous keywords to minimize generation errors.',\n",
       "    'subsections': [{'title': 'Experimental Setting',\n",
       "      'paragraphs': 'To ensure reproducibility, all experiments utilized a virtual player LLM interacting with an LLM-driven NPC over 300 dialogues. For each dialogue, the run was initiated with a fixed seed (0-299) that fixed the player‚Äôs initial utterance.\\nThe virtual player‚Äôs objective was to naturally converse and purchase items, though it was not required to complete a purchase in every interaction. This objective was guided by behavioral guidelines that included asking for item information, negotiating prices, and changing the selected items mid-trade.\\nThe conversations followed two scenarios: (1) Specific Item Purchase , where the player requested up to six items randomly drawn from 20 sellable and 32 unsellable items\\n, with purchase quantities for each item randomly selected from 1 to 5; and (2) Item Recommendation , where the player requested items for a specific purpose (e.g., ‚Äúfor a goblin battle‚Äù).\\nOn average, conversation in Scenario 2 were approximately 9% longer than in Scenario 1 across all experimental settings, reflecting the more exploratory nature.\\nAll experiments were run on a machine with Intel Core Ultra 7 155H CPU, 16GB of RAM, and integrated Intel Arc Graphics. LLM inferences were performed via the Google Gemini API. 1 1 1 All API calls were made in July 2025. Unless otherwise noted, the default configuration used gemini-2.5-flash model with a temperature of 0.7 and a thinking budget of 0. The full prompts for all experimental setups are provided in the Appendix.',\n",
       "      'subsections': []},\n",
       "     {'title': 'State Transition Compliance',\n",
       "      'paragraphs': 'Experimental Design :\\nASTP‚Äôs compliance with dialogue flow was evaluated through an ablation study of its four key elements and a comparison against methods adapted from other studies. The four elements are: (1) state definitions, (2) state transition conditions, (3) directive to identify the previous state, and (4) directive to output the previous state. As detailed in Table 1 , the ASTP baselines were constructed by cumulatively adding these elements, which allowed isolation of their impact.\\nFor AutoTOD (Xu et\\xa0al. 2024 ) , one of the comparison methods, the exact content of Baseline1 was converted into the prompt structure from the official AutoTOD implementation 2 2 2 https://github.com/DaDaMrX/AutoTOD . This approach was chosen because both prompts fundamentally rely on state definitions, making for a direct comparison of prompt structures. In the adapted version, each trade state was framed as a ‚Äòsub-task‚Äô, and all external API-calling functions were removed as the experiment relies on in-context data.\\nFor the DFI-Inspired method, an explicit state-transition graph was added to the Baseline1 prompt. This technique was inspired by a concept for applying an dialogue flow to constrain an LLM, which was illustrated as a potential application in Dialog Flow Induction (DFI) (Agrawal et\\xa0al. 2024 ) . For the Zero-shot-Chain-of-Thought (ZS-COT) (Kojima et\\xa0al. 2022 ) comparison, the Baseline2 prompt was augmented by adding the one-line instruction: ‚ÄúBefore generating a response, first think step by step to determine the correct context and action. Your thought process should be recorded in the npc_thoughts field.‚Äù This method was tested as an alternative to using Element 3 and 4, representing a different approach to enhancing the model‚Äôs reasoning. Figure 3 provides a visual comparison of these distinct prompt architectures. Finally, AutoTOD+ and DFI-Inspired+ were created by augmenting each respective base prompt with all four ASTP elements.\\nEvaluation Metric :\\nThe primary evaluation metric is the State Transition Compliance Rate (STCR), defined as the proportion of transactions that correctly pass through FINAL_CHECK before finalizing in COMMIT_SALE . 3 3 3 To ensure a fair comparison, STCR was calculated on a normalized sample of dialogues that reached the COMMIT_SALE state (N=218 for Scenario1, N=274 for Scenario2), the minimum number observed across all methods. As an irreversible step, this transition is fundamentally different from the flow‚Äôs other, more flexible transitions. These flexible transitions, such as re-entering negotiation from the final review stage, are designed for conversational fluidity, where the LLM‚Äôs inference is primarily driven by user intent.\\nThe transition into COMMIT_SALE , however, is intentionally rigid. Here, the LLM‚Äôs task transcends intent recognition: it must prioritize a procedural precondition (the previous state) over the user‚Äôs immediate utterance. This deliberate core tension makes it the ultimate test of a model‚Äôs ability to adhere to critical safeguards designed to prevent unintended transactions.\\nKey Results :\\nAs shown in Table 1 , the full ASTP method achieved near-perfect compliance. The ablation study reveals that while adding explicit transition rules (Element 2) significantly improved performance, the key to robust control lies in the interaction between Elements 3 and 4. Instructing the model only to identify the previous state (Element 3) yielded no benefit, likely by introducing ambiguity without an enforcement mechanism. However, compelling the model to output its inference (Element 4) proved highly effective. The highest compliance rate was achieved when both elements were combined, suggesting that the ‚Äòidentify‚Äô directive provides a clear signal for the ‚Äòoutput‚Äô directive‚Äôs powerful enforcement mechanism.\\nA comparison with other methods offers further insights into the importance of prompt structure.\\nThe ZS-CoT method, while improving upon Baseline2, achieved a lower STCR than the full ASTP. This is conjectured to be because a generic ‚Äúthink-by-step‚Äù instruction, while encouraging an internal reasoning process, lacks the explicit enforcement mechanism of ASTP: the mandatory output of the inferred state. Lacking this constraint, ZS-CoT is more susceptible to deviating from procedural rules when faced with strong user intent. In particular, the performance gap between ZS-CoT and ASTP widens in Scenario 2, which involves more exploratory, recommendation-based conversations. The increased ambiguity and conversational complexity in this scenario place a higher cognitive load on the model. ASTP‚Äôs rigid mechanism forcing the LLM to re-anchor its context by identifying and outputting the previous state proves to be more robust in these complex situations.\\nBaseline1 yielded a higher STCR than AutoTOD despite both using only state definitions. This is conjectured to stem from their differing prompting paradigm. AutoTOD‚Äôs fragmented sub-task structure frames the task as a classification problem each turn, making it prone to skipping steps when faced with a strong user intent (e.g., a ‚ÄúYes‚Äù to purchase). In contrast, Baseline1‚Äôs prompt presents state definitions as a unified list. This contextual proximity appears to implicitly suggest a sequential flow, guiding the model more effectively.\\nThe just-in-time instruction in Baseline2-which couples a transition rule directly with its state definition-also proved more effective than DFI-Inspired‚Äôs approach of referencing a separate state-transition graph. The latter likely increases the model‚Äôs congnitive load, leading to a slightly lower compliance rate.\\nMost notably, augmenting these other methods with all four ASTP elements (AutoTOD+, DFI-Inspired+) improved their performance. This demonstrates that ASTP‚Äôs core principles are not tied to a specific prompt format but are portable principles that can enhance an LLM‚Äôs ability to adhere to strict procedural rules. The state transition patterns in Figure 3 confirm that ASTP also induces natural, context-appropriate dialogue flows.',\n",
       "      'subsections': []},\n",
       "     {'title': 'State-specific Post-processing',\n",
       "      'paragraphs': 'LLMs struggle with precise arithmetic operations, making simple calculation errors like ‚Äú2√ó50 + 30 = 120 gold.‚Äù This poses significant problems for price calculations in trading, potentially undermining player trust and transaction integrity.\\nExperimental Design :\\nUtilizing dialogues from Scenario 1,\\nthis experiment compares performance across two key dimensions:\\n(Table 2 ):\\nASTP variants : ASTP, ASTP -PPP (an ablated version without Placeholder Post-Processing), ASTP -PPP +SO (the ablated version combined with Structured Output)\\nModel variants : gemini-2.5-flash, gemini-2.0-flash (lighter version), gemini-2.5-pro (high-performance)\\nEvaluation Metric : The evaluation focuses on price accuracy, along with token usage and response time.\\nPrice accuracy is the percentage of NPC responses where the total price stated by the LLM matches the sum calculated from the item details (i.e., prices and quantities) also provided within the same LLM response.\\nKey Results :\\nASTP showed consistent improvements in accuracy while maintaining computational efficiency across multiple evaluation dimensions.\\nFirst, in terms of accuracy, ASTP reached 100.0% in the OFFERÀôSELL state, compared to 81.4% with ASTP -PPP . This improvement also extended to subsequent states, where ASTP achieved 99.3% accuracy versus 84.3% for ASTP -PPP . These results suggest that accurate initial pricing may help reduce downstream errors in multi-turn dialogues.\\nSecond, these accuracy gains were achieved without notable increases in computational cost. Token usage (441.6 vs 437.7) and response time (2.4 seconds) remained comparable to ASTP -PPP . By contrast, the ASTP -PPP +SO variant uses fewer completion tokens, but its price accuracy is much lower at 87.8% and 88.0%. For reliable transactions, ASTP‚Äôs accuracy is more crucial than this gain in token efficiency.\\nFurthermore, a high variability in thought token usage was observed in some configurations (e.g., 22.9 for ASTP -PPP +SO and 325.0 for the gemini-2.5-pro model). This likely reflects that the model‚Äôs internal reasoning demands fluctuate significantly depending on the dialogue context of each turn.\\nThird, ASTP also showed promising results with smaller models. When tested with gemini-2.0-flash, it achieved 100.0% accuracy in OFFER_SELL and 90.4% in other states, outperforming the ASTP -PPP , which achieved 48.7% and 59.0%, respectively.\\nThis demonstrates the robustness of the method on smaller models.\\nLastly, ASTP using gemini-2.5-flash reached comparable accuracy to ASTP -PPP with gemini-2.5-pro while requiring significantly fewer resources: 0 vs 1575.2 thought tokens (as the thinking budget for gemini-2.5-pro cannot be disabled via its API) and 2.4 vs 21.2 seconds in response time. These findings suggest that incorporating state-specific post-processing may allow smaller models to approximate the accuracy of larger ones, with considerably lower computational demands.\\nBeyond model performance, the choice of the placeholder keyword was observed to affect the robustness of the post-processing step.\\nThe concise keyword __PRICE__ consistently produced accurate and parsable outputs. In contrast, more complex variants such as __PRICE_PLACEHOLDER__ occasionally resulted in malformed completions like __PRICE_PLACEHOLDER_ or __PRICE_PLACEHOLDE__ . These inconsistencies highlight the importance of using short, unambiguous keywords to minimize generation errors.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Conclusion',\n",
       "    'paragraphs': 'We present Autoregressive State-Tracking Prompting (ASTP), a prompting methodology that enables LLMs to balance conversational flexibility with strict procedural adherence. By explicitly inferring and outputting the previous dialogue state label, ASTP transforms implicit reasoning into a verifiable process.\\nOur structured Prime‚ÄìGuide‚ÄìEnforce workflow improves procedural compliance from 78.1% to 99.6%. Leveraging this state-aware architecture, our placeholder-based post-processing (PPP) method improves price calculation accuracy from 84.3% to 99.3% without added latency. PPP also allows smaller models to match the accuracy of larger ones with up to 9√ó faster responses.\\nThese results demonstrate ASTP‚Äôs practicality for rule-governed applications like in-game trading, providing transactional integrity and fast response times .\\nBeyond games, we believe ASTP principles offer a foundational step toward enhancing reliability in other domains where language must remain both expressive and rule-abiding.\\nTo solidify this foundation, future work should investigate the scalability of ASTP across a larger number of states and more complex transition rules.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Detailed Prompts for All Experimental Setups',\n",
       "    'paragraphs': 'The prompts detailed below differ slightly from the conceptual model in the main paper in their structure and terminology. These were pragmatic choices made for implementation and to ensure the core concepts were presented as clearly as possible.\\nFirst, regarding the structure, the main paper presents a flattened state space ( ùíÆ = S base ‚à™ S trade \\\\mathcal{S}=S_{\\\\text{base}}\\\\cup S_{\\\\text{trade}} ) to best illustrate the direct, autoregressive transition dynamics central to ASTP. The practical implementation in the prompts, however, utilizes a two-level hierarchy ( context_type and context_subtype ). This allows the model to first broadly classify the user‚Äôs intent as TRADE or non-trading ( CASUAL , END ). The central mechanism of ASTP (the autoregressive tracking of the previous state label) operates on the substates within the TRADE context.\\nDespite this structural difference, the simplified model in the main paper accurately captures the fundamental principles of the verifiable, state-aware process our work introduces.\\nSecond, regarding terminology, the main paper uses the terms state and substate, while the actual prompts use the functionally equivalent terms context and subcontext. This choice was made during development for intuitive implementation, and the underlying logic remains identical.\\nThis prompt serves as the base for the ablation study.\\nUtilizes: Element 1 (State Definitions).\\nThis prompt builds on Baseline 1.\\nAdds: Element 2 (State Transition Conditions).\\nThis prompt builds on Baseline 2.\\nAdds: Element 3 (Directive to Identify Previous State).\\nThis prompt builds on Baseline 2 in a different branch of the ablation study.\\nAdds: Element 4 (Directive to Respond including Previous State).\\nThis is the complete prompt for our proposed method.\\nUtilizes: All Elements (1 + 2 + 3 + 4).\\nThis prompt is an ablated version of the ASTP prompt.\\nRemoves: Placeholder Post-Processing (PPP) mechanism.\\nConsequently, the rule in the <RESPONSE_FORMAT> section that forbids the use of the __PRICE__ placeholder in later states is no longer necessary and was removed. All other fields in this section remain identical to the ASTP prompt.\\nThis prompt augments Baseline 2.\\nAdds: A generic ‚Äùthink step-by-step‚Äù instruction.\\nThis prompt reframes the content of Baseline 1.\\nRestructures: Rules are converted into a ‚Äòsub-task‚Äô based format. 4 4 4 https://github.com/DaDaMrX/AutoTOD\\nThis prompt augments Baseline 1.\\nAdds: An explicit state-transition graph as a separate section.',\n",
       "    'subsections': [{'title': 'Ablation: Baseline 1',\n",
       "      'paragraphs': 'This prompt serves as the base for the ablation study.\\nUtilizes: Element 1 (State Definitions).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Ablation: Baseline 2',\n",
       "      'paragraphs': 'This prompt builds on Baseline 1.\\nAdds: Element 2 (State Transition Conditions).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Ablation: Baseline 3',\n",
       "      'paragraphs': 'This prompt builds on Baseline 2.\\nAdds: Element 3 (Directive to Identify Previous State).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Ablation: Baseline 4',\n",
       "      'paragraphs': 'This prompt builds on Baseline 2 in a different branch of the ablation study.\\nAdds: Element 4 (Directive to Respond including Previous State).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Proposed: ASTP',\n",
       "      'paragraphs': 'This is the complete prompt for our proposed method.\\nUtilizes: All Elements (1 + 2 + 3 + 4).',\n",
       "      'subsections': []},\n",
       "     {'title': 'Ablation: ASTP-PPP',\n",
       "      'paragraphs': 'This prompt is an ablated version of the ASTP prompt.\\nRemoves: Placeholder Post-Processing (PPP) mechanism.\\nConsequently, the rule in the <RESPONSE_FORMAT> section that forbids the use of the __PRICE__ placeholder in later states is no longer necessary and was removed. All other fields in this section remain identical to the ASTP prompt.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Comparative: ZS-CoT',\n",
       "      'paragraphs': 'This prompt augments Baseline 2.\\nAdds: A generic ‚Äùthink step-by-step‚Äù instruction.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Comparative: AutoTOD',\n",
       "      'paragraphs': 'This prompt reframes the content of Baseline 1.\\nRestructures: Rules are converted into a ‚Äòsub-task‚Äô based format. 4 4 4 https://github.com/DaDaMrX/AutoTOD',\n",
       "      'subsections': []},\n",
       "     {'title': 'Comparative: DFI-Inspired',\n",
       "      'paragraphs': 'This prompt augments Baseline 1.\\nAdds: An explicit state-transition graph as a separate section.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Item Data Used in Prompts',\n",
       "    'paragraphs': 'The following table details the full set of item data available for the experiments. This data is used to populate the {game_items} and {merchant_inventory} placeholders within the <GAME_ITEM_LIST> and <CHARACTER_INVENTORY> prompt sections, respectively. For each dialogue turn, the relevant data is formatted as a JSON string before being inserted into the final prompt.\\nSpecifically, the {game_items} placeholder is populated with a JSON string containing all 52 items, using only their item_id and item_name from the table. The {merchant_inventory} placeholder is populated with a JSON string containing only the 20 items marked as ‚ÄòFor Sale‚Äô, utilizing their item_id , item_name , price , and quantity . This ensures the model has access to both broad world knowledge and the specific, sellable inventory for the transaction.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Dialogue Examples',\n",
       "    'paragraphs': 'This section provides full dialogue transcripts to qualitatively illustrate the model‚Äôs performance and justify our key claims. Tables 4 and 5 first present successful dialogue examples from Scenario 1 and Scenario 2, respectively, demonstrating ASTP‚Äôs ability to handle complex interactions and exploratory conversations while adhering to the procedural flow.\\nFollowing these, two side-by-side comparisons provide direct evidence for the quantitative results in the main paper. Table 6 compares a dialogue from ASTP with one from AutoTOD, initiated with the same player‚Äôs utterance. The dialogue shows ASTP proceeding through the mandatory FINAL_CHECK step, while AutoTOD skips it.\\nFinally, Table 7 contrasts ASTP with its ablated version, ASTP -PPP . This illustrates the effectiveness of the Placeholder Post-Processing (PPP) mechanism. In the ASTP dialogue, the price is accurately established by post-processing in the OFFER_SELL state, and this accuracy is robustly maintained through subsequent turns, even with cart modifications. In contrast, the ASTP -PPP dialogue shows the model repeatedly struggling with price calculations.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Additional Experimental Results',\n",
       "    'paragraphs': 'As reported in Table 1, the ASTP model achieved a 99.64% State Transition Compliance Rate (STCR) in Scenario 2, with only a single failure instance observed out of 274 dialogues that reached the COMMIT_SALE state.\\nA qualitative analysis of this sole failure case provides a key insight into a subtle failure mode. As shown in the excerpt in Table 8 , the failure occurred after the player gave a simple, affirmative response (‚ÄùYes, I‚Äôll purchase it.‚Äù) in Turn 9. The model then incorrectly jumped from OFFER_SELL (Turn 8) directly to COMMIT_SALE (Turn 10), bypassing the mandatory FINAL_CHECK step.\\nA plausible explanation for this error lies in the specific dialogue generated by the NPC in the preceding OFFER_SELL state (Turn 8): ‚ÄúAre you buying just this one axe?‚Äù This question, while part of an offer, is semantically very similar to the confirmation question required in the FINAL_CHECK state (e.g., ‚ÄùSo, you‚Äôll take it?‚Äù). This likely created a state ambiguity for the LLM. Upon receiving the player‚Äôs simple confirmation, the model may have misinterpreted that the final check had already been performed, leading to the erroneous transition. This highlights a nuanced challenge where the semantic content of a response can, in rare cases, override the model‚Äôs understanding of its current procedural state.\\nThe second type of rare failure also appears to stems from state ambiguity , but in this case, it is triggered by a complex, multi-intent player utterance.\\nAs reported in Table 2, the ASTP model achieved 99.3% price accuracy in dialogue states subsequent to the initial offer. Our qualitative analysis of the few failures (7 failure instances observed out of 1025 instances) reveals they occurred when the player attempted to negotiate the price while simultaneously modifying the shopping cart within a single turn.\\nSuch multi-intent utterances create a context that does not cleanly map to a single procedural step. In the resulting NEGOTIATE state, the LLM must rely on its innate arithmetic capabilities, as the PPP mechanism‚Äôs safeguard is active only in the OFFER_SELL state. This occasionally leads to calculation errors, as the model attempts to both update the cart and calculate a new total.\\nThe dialogue excerpt in Table 9 provides a representative example. The model correctly enters the NEGOTIATE state but produces an incorrect total (1370 gold instead of the correct 1320 gold), demonstrating how state ambiguity created by the player‚Äôs utterance can lead to calculation failures when outside the direct safeguard of the PPP-enabled OFFER_SELL state.',\n",
       "    'subsections': [{'title': 'State Transition Patterns of Comparative Methods',\n",
       "      'paragraphs': '',\n",
       "      'subsections': []},\n",
       "     {'title': 'Failure Case Analysis of ASTP',\n",
       "      'paragraphs': 'As reported in Table 1, the ASTP model achieved a 99.64% State Transition Compliance Rate (STCR) in Scenario 2, with only a single failure instance observed out of 274 dialogues that reached the COMMIT_SALE state.\\nA qualitative analysis of this sole failure case provides a key insight into a subtle failure mode. As shown in the excerpt in Table 8 , the failure occurred after the player gave a simple, affirmative response (‚ÄùYes, I‚Äôll purchase it.‚Äù) in Turn 9. The model then incorrectly jumped from OFFER_SELL (Turn 8) directly to COMMIT_SALE (Turn 10), bypassing the mandatory FINAL_CHECK step.\\nA plausible explanation for this error lies in the specific dialogue generated by the NPC in the preceding OFFER_SELL state (Turn 8): ‚ÄúAre you buying just this one axe?‚Äù This question, while part of an offer, is semantically very similar to the confirmation question required in the FINAL_CHECK state (e.g., ‚ÄùSo, you‚Äôll take it?‚Äù). This likely created a state ambiguity for the LLM. Upon receiving the player‚Äôs simple confirmation, the model may have misinterpreted that the final check had already been performed, leading to the erroneous transition. This highlights a nuanced challenge where the semantic content of a response can, in rare cases, override the model‚Äôs understanding of its current procedural state.\\nThe second type of rare failure also appears to stems from state ambiguity , but in this case, it is triggered by a complex, multi-intent player utterance.\\nAs reported in Table 2, the ASTP model achieved 99.3% price accuracy in dialogue states subsequent to the initial offer. Our qualitative analysis of the few failures (7 failure instances observed out of 1025 instances) reveals they occurred when the player attempted to negotiate the price while simultaneously modifying the shopping cart within a single turn.\\nSuch multi-intent utterances create a context that does not cleanly map to a single procedural step. In the resulting NEGOTIATE state, the LLM must rely on its innate arithmetic capabilities, as the PPP mechanism‚Äôs safeguard is active only in the OFFER_SELL state. This occasionally leads to calculation errors, as the model attempts to both update the cart and calculate a new total.\\nThe dialogue excerpt in Table 9 provides a representative example. The model correctly enters the NEGOTIATE state but produces an incorrect total (1370 gold instead of the correct 1320 gold), demonstrating how state ambiguity created by the player‚Äôs utterance can lead to calculation failures when outside the direct safeguard of the PPP-enabled OFFER_SELL state.',\n",
       "      'subsections': [{'title': 'Failure case of state transition compliance (0.36%)',\n",
       "        'paragraphs': 'As reported in Table 1, the ASTP model achieved a 99.64% State Transition Compliance Rate (STCR) in Scenario 2, with only a single failure instance observed out of 274 dialogues that reached the COMMIT_SALE state.\\nA qualitative analysis of this sole failure case provides a key insight into a subtle failure mode. As shown in the excerpt in Table 8 , the failure occurred after the player gave a simple, affirmative response (‚ÄùYes, I‚Äôll purchase it.‚Äù) in Turn 9. The model then incorrectly jumped from OFFER_SELL (Turn 8) directly to COMMIT_SALE (Turn 10), bypassing the mandatory FINAL_CHECK step.\\nA plausible explanation for this error lies in the specific dialogue generated by the NPC in the preceding OFFER_SELL state (Turn 8): ‚ÄúAre you buying just this one axe?‚Äù This question, while part of an offer, is semantically very similar to the confirmation question required in the FINAL_CHECK state (e.g., ‚ÄùSo, you‚Äôll take it?‚Äù). This likely created a state ambiguity for the LLM. Upon receiving the player‚Äôs simple confirmation, the model may have misinterpreted that the final check had already been performed, leading to the erroneous transition. This highlights a nuanced challenge where the semantic content of a response can, in rare cases, override the model‚Äôs understanding of its current procedural state.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Failure case of price calculation (0.7%)',\n",
       "        'paragraphs': 'The second type of rare failure also appears to stems from state ambiguity , but in this case, it is triggered by a complex, multi-intent player utterance.\\nAs reported in Table 2, the ASTP model achieved 99.3% price accuracy in dialogue states subsequent to the initial offer. Our qualitative analysis of the few failures (7 failure instances observed out of 1025 instances) reveals they occurred when the player attempted to negotiate the price while simultaneously modifying the shopping cart within a single turn.\\nSuch multi-intent utterances create a context that does not cleanly map to a single procedural step. In the resulting NEGOTIATE state, the LLM must rely on its innate arithmetic capabilities, as the PPP mechanism‚Äôs safeguard is active only in the OFFER_SELL state. This occasionally leads to calculation errors, as the model attempts to both update the cart and calculate a new total.\\nThe dialogue excerpt in Table 9 provides a representative example. The model correctly enters the NEGOTIATE state but produces an incorrect total (1370 gold instead of the correct 1320 gold), demonstrating how state ambiguity created by the player‚Äôs utterance can lead to calculation failures when outside the direct safeguard of the PPP-enabled OFFER_SELL state.',\n",
       "        'subsections': []}]}]}],\n",
       "  'abstract': 'Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust.\\n This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading\\n (browse-offer-review-confirm).\\n To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations.\\n Evaluation across 300 trading dialogues demonstrates >>99% state compliance\\n and 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models‚Äô (Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games.\\n The code will be publicly available upon acceptance.'},\n",
       " '2510.25065v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Large Language Models, capable of multi-step reasoning processes (i.e., chain-of-thought), often referred to as\\nLarge Reasoning Models (LRMs), have demonstrated impressive performance across a wide variety of complex tasks [ 1 ] .\\nA key factor enabling such reasoning behaviors is reinforcement learning (RL)-based post-training,\\nwhich aligns model policies with high-level reasoning objectives [ 2 ] .\\nAmong these methods, DeepSeek-R1‚Äôs GRPO [ 3 ] has emerged as a notable\\nadvancement due to its simplified objective design and more stable optimization dynamics compared to traditional\\napproaches such as DPO [ 4 ] and PPO [ 5 ] .\\nMotivated by these advantages, GRPO-based RL post-training frameworks have rapidly gained traction and continue to evolve [ 6 ] .\\nDespite these advances, GRPO-inspired approaches focus solely on optimizing final answers, neglecting the underlying reasoning processes [ 1 ] .\\nCurrent reward schemes are typically outcome-centric or depend on superficial textual attributes (e.g., length, formatting consistency, keyword matches) [ 7 ] .\\nSuch reward formulations fail to capture the process by which the model derives its answer, often leading to suboptimal behaviors such\\nas unnecessary verbosity, speculative leaps in reasoning, or accidental correctness without genuine understanding [ 8 ] .\\nTo address this limitation, we adopt the perspective that the reasoning or thinking of an LRM can itself be viewed as a process,\\ni.e., Thinking is a Process ( THIP ).\\nThe key idea is to extract the reasoning traces of a pretrained, large teacher model in the form of event logs,\\nand to use a reward signal that measures how well the student model‚Äôs self-generated reasoning process aligns with\\nthe teacher‚Äôs process. To achieve this, we utilize Process Mining (PM) [ 9 ] , a set of techniques designed to analyze process execution logs,\\nenabling us to evaluate reasoning quality at the process-level rather than solely from final outcomes.\\nIn this short paper, we propose a novel reasoning-aware GRPO framework using PM, named PM4GRPO ,\\nwhich integrates PM techniques into the reward design of GRPO. Our contributions are summarized as follows:\\nWe introduce PM4GRPO , a novel GRPO framework featuring a reward model that incorporates the reasoning process itself into post-training.\\nFor each query, the policy model derive its own reasoning trace, which is then transformed into a process model.\\nThe framework quantitatively measures how well this process model conforms to the Teacher Model‚Äôs reasoning (i.e., conformance checking) and integrates this measure into the reward signal.\\nPM4GRPO does not enforce the policy model to replicate the Teacher Model‚Äôs reasoning strictly.\\nInstead, it encourages reasoning alignment while preserving the model‚Äôs freedom of thought, thereby enhancing the effectiveness of reinforcement learning.\\nThe rest of this paper is organized as follows. In Section 2 , we describe the proposed PM4GRPO framework in detail.\\nSection 3 presents experimental results demonstrating the effectiveness of our approach.\\nFinally, we conclude the paper in Section 4 with a summary and future research directions.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 PM4GRPO: Reasoning-Aware GRPO using Process Mining',\n",
       "    'paragraphs': 'To design a reasoning-aware GRPO framework, we propose to integrate PM techniques into the GRPO algorithms.\\nOur method, illustrated in Figure 1 , enhances the original GRPO by incorporating a process reward that evaluates\\nhow well the reasoning process of the policy model aligns with that of a pretrained teacher model.\\nTwo existing PM techniques are utilized to compute this process reward:\\n(1) Inductive miner [ 10 ] to construct process models from the policy model‚Äôs reasoning process, and\\n(2) Alignment-based conformance checking [ 11 ] CC \\\\operatorname{CC} to compare the induced process models with the teacher model‚Äôs reasoning process.\\nIn this paper, inductive miner and alignment-based conformance checking are denoted by IM \\\\operatorname{IM} and CC \\\\operatorname{CC} , respectively.\\nNote that these techniques are applied independently to each problem that needs\\nto be addressed, and the reward is assigned at the problem (i.e., sequence) level\\nrather than the token level. For each problem, once the reasoning process is completed,\\na process model is constructed and compared with the teacher model‚Äôs reasoning.\\nRecently, GRPO is designed to fine-tune language models with group-level normalized rewards [ 3 ] .\\nTypically, it optimizes the policy model by maximizing the expected reward, which is typically based on the\\ncorrectness and format of the final answer. GRPO modifies the standard policy gradient objective by\\nintroducing relative advantages within sets of responses corresponding to the same query [ 12 ] .\\nHowever, GRPO performs off-policy correction at the token level via token-wise\\nimportance ratios, while the reward in our framework is only computed at the\\nproblem (i.e., sequence) level after the entire reasoning process is completed.\\nZheng et al. [ 13 ] addressed this issue by proposing a GRPO-inspired framework named Group Sequence Policy Optimization (GSPO).\\nGSPO defines the importance ratio based on the sequence-level likelihood, ensuring that the unit of optimization matches the sequence-level\\nreward. Specifically, let œÄ Œ∏ \\\\pi_{\\\\theta} denote the policy model with parameters Œ∏ \\\\theta , x ‚àº ùíü x\\\\sim\\\\mathcal{D} be a query sampled from the dataset ùíü \\\\mathcal{D} , G G be the group size and y y be outputs. The objective function of GSPO can be defined as follows:\\nwhere œµ \\\\epsilon is a hyperparameter controlling the clipping range and the group-based\\nadvantage estimation A ^ i \\\\hat{A}_{i} is defined as:\\nwhere R x , y i R_{x,y_{i}} is the reward assigned to the response y i y_{i} for the query x x and the standard deviation of rewards for normalization\\nare omitted for simplicity [ 14 ] . Finally, r i \\u200b ( Œ∏ ) r_{i}(\\\\theta) is the sequence-level importance ratio defined as:\\nThus, GSPO applies clipping at the sequence level rather than the token level,\\nwhich effectively filters out overly off-policy trajectories and ensures that the\\noptimization unit is aligned with the sequence-level reward structure.\\nGRPO-inspired methods typically rely on outcome-centric reward functions that focus solely on the final answer‚Äôs correctness and format.\\nThe reward components of existing GRPO-inspired approaches are as follows:\\n(1) Format Reward : R f R^{f} : This reward evaluates whether the model-generated response strictly adheres to the required structural format,\\nsuch as enclosing the reasoning process within specific markers (e.g., <think> ‚Ä¶ </think>) and providing the final answer within a separate designated\\nfield. The goal is to enforce consistent and machine-parseable output formatting.\\n(2) Answer Reward : R a R^{a} : This reward assesses the correctness of the final answer by comparing it with the ground truth.\\nIt ensures that the model produces reliable and accurate outcomes.\\nOur proposed PM4GRPO incorporates a conformance reward that measures the alignment between the reasoning\\nprocess of the policy model and that of the teacher model (Reward LLM in the Figure 1 ).\\nIn addition to the reward components of existing GRPO-inspired approaches,\\nwe introduce a conformance reward that evaluates how well the reasoning process of the policy model aligns with that of a pretrained teacher model.\\nSpecifically, let { œÉ i \\u200b ( œÄ Œ∏ ) } i = 1 G \\\\left\\\\{\\\\sigma_{i}(\\\\pi_{\\\\theta})\\\\right\\\\}_{i=1}^{G} denote the reasoning process generated by the policy model for the query\\nand œÉ \\u200b ( R ) \\\\sigma(R) denote the reasoning process generated by the teacher model for the same query.\\nThe conformance checking can be depicted briefly as follows:\\nwhere fitness means how well the discovered process model IM \\u2061 ( œÉ i \\u200b ( œÄ Œ∏ ) ) \\\\operatorname{IM}(\\\\sigma_{i}(\\\\pi_{\\\\theta})) explains the logs œÉ R \\\\sigma_{R} and precision means how often actions not present in the logs occur in the process model.\\nFinally, the conformance reward R c R^{c} can be defined as the maximum f1-score of fitness and precision:\\nTherefore, the reward function in PM4GRPO can be expressed as:',\n",
       "    'subsections': [{'title': '2.1 Group Sequence Policy Optimization',\n",
       "      'paragraphs': 'Recently, GRPO is designed to fine-tune language models with group-level normalized rewards [ 3 ] .\\nTypically, it optimizes the policy model by maximizing the expected reward, which is typically based on the\\ncorrectness and format of the final answer. GRPO modifies the standard policy gradient objective by\\nintroducing relative advantages within sets of responses corresponding to the same query [ 12 ] .\\nHowever, GRPO performs off-policy correction at the token level via token-wise\\nimportance ratios, while the reward in our framework is only computed at the\\nproblem (i.e., sequence) level after the entire reasoning process is completed.\\nZheng et al. [ 13 ] addressed this issue by proposing a GRPO-inspired framework named Group Sequence Policy Optimization (GSPO).\\nGSPO defines the importance ratio based on the sequence-level likelihood, ensuring that the unit of optimization matches the sequence-level\\nreward. Specifically, let œÄ Œ∏ \\\\pi_{\\\\theta} denote the policy model with parameters Œ∏ \\\\theta , x ‚àº ùíü x\\\\sim\\\\mathcal{D} be a query sampled from the dataset ùíü \\\\mathcal{D} , G G be the group size and y y be outputs. The objective function of GSPO can be defined as follows:\\nwhere œµ \\\\epsilon is a hyperparameter controlling the clipping range and the group-based\\nadvantage estimation A ^ i \\\\hat{A}_{i} is defined as:\\nwhere R x , y i R_{x,y_{i}} is the reward assigned to the response y i y_{i} for the query x x and the standard deviation of rewards for normalization\\nare omitted for simplicity [ 14 ] . Finally, r i \\u200b ( Œ∏ ) r_{i}(\\\\theta) is the sequence-level importance ratio defined as:\\nThus, GSPO applies clipping at the sequence level rather than the token level,\\nwhich effectively filters out overly off-policy trajectories and ensures that the\\noptimization unit is aligned with the sequence-level reward structure.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Rewards Design',\n",
       "      'paragraphs': 'GRPO-inspired methods typically rely on outcome-centric reward functions that focus solely on the final answer‚Äôs correctness and format.\\nThe reward components of existing GRPO-inspired approaches are as follows:\\n(1) Format Reward : R f R^{f} : This reward evaluates whether the model-generated response strictly adheres to the required structural format,\\nsuch as enclosing the reasoning process within specific markers (e.g., <think> ‚Ä¶ </think>) and providing the final answer within a separate designated\\nfield. The goal is to enforce consistent and machine-parseable output formatting.\\n(2) Answer Reward : R a R^{a} : This reward assesses the correctness of the final answer by comparing it with the ground truth.\\nIt ensures that the model produces reliable and accurate outcomes.\\nOur proposed PM4GRPO incorporates a conformance reward that measures the alignment between the reasoning\\nprocess of the policy model and that of the teacher model (Reward LLM in the Figure 1 ).\\nIn addition to the reward components of existing GRPO-inspired approaches,\\nwe introduce a conformance reward that evaluates how well the reasoning process of the policy model aligns with that of a pretrained teacher model.\\nSpecifically, let { œÉ i \\u200b ( œÄ Œ∏ ) } i = 1 G \\\\left\\\\{\\\\sigma_{i}(\\\\pi_{\\\\theta})\\\\right\\\\}_{i=1}^{G} denote the reasoning process generated by the policy model for the query\\nand œÉ \\u200b ( R ) \\\\sigma(R) denote the reasoning process generated by the teacher model for the same query.\\nThe conformance checking can be depicted briefly as follows:\\nwhere fitness means how well the discovered process model IM \\u2061 ( œÉ i \\u200b ( œÄ Œ∏ ) ) \\\\operatorname{IM}(\\\\sigma_{i}(\\\\pi_{\\\\theta})) explains the logs œÉ R \\\\sigma_{R} and precision means how often actions not present in the logs occur in the process model.\\nFinally, the conformance reward R c R^{c} can be defined as the maximum f1-score of fitness and precision:\\nTherefore, the reward function in PM4GRPO can be expressed as:',\n",
       "      'subsections': [{'title': '2.2.1 Rewards in Existing GRPO-inspired Methods',\n",
       "        'paragraphs': 'GRPO-inspired methods typically rely on outcome-centric reward functions that focus solely on the final answer‚Äôs correctness and format.\\nThe reward components of existing GRPO-inspired approaches are as follows:\\n(1) Format Reward : R f R^{f} : This reward evaluates whether the model-generated response strictly adheres to the required structural format,\\nsuch as enclosing the reasoning process within specific markers (e.g., <think> ‚Ä¶ </think>) and providing the final answer within a separate designated\\nfield. The goal is to enforce consistent and machine-parseable output formatting.\\n(2) Answer Reward : R a R^{a} : This reward assesses the correctness of the final answer by comparing it with the ground truth.\\nIt ensures that the model produces reliable and accurate outcomes.',\n",
       "        'subsections': []},\n",
       "       {'title': '2.2.2 Conformance Reward',\n",
       "        'paragraphs': 'Our proposed PM4GRPO incorporates a conformance reward that measures the alignment between the reasoning\\nprocess of the policy model and that of the teacher model (Reward LLM in the Figure 1 ).\\nIn addition to the reward components of existing GRPO-inspired approaches,\\nwe introduce a conformance reward that evaluates how well the reasoning process of the policy model aligns with that of a pretrained teacher model.\\nSpecifically, let { œÉ i \\u200b ( œÄ Œ∏ ) } i = 1 G \\\\left\\\\{\\\\sigma_{i}(\\\\pi_{\\\\theta})\\\\right\\\\}_{i=1}^{G} denote the reasoning process generated by the policy model for the query\\nand œÉ \\u200b ( R ) \\\\sigma(R) denote the reasoning process generated by the teacher model for the same query.\\nThe conformance checking can be depicted briefly as follows:\\nwhere fitness means how well the discovered process model IM \\u2061 ( œÉ i \\u200b ( œÄ Œ∏ ) ) \\\\operatorname{IM}(\\\\sigma_{i}(\\\\pi_{\\\\theta})) explains the logs œÉ R \\\\sigma_{R} and precision means how often actions not present in the logs occur in the process model.\\nFinally, the conformance reward R c R^{c} can be defined as the maximum f1-score of fitness and precision:\\nTherefore, the reward function in PM4GRPO can be expressed as:',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '3 Experiments',\n",
       "    'paragraphs': 'To evaluate the effectiveness of PM4GRPO, 1.5B and 7B parameter backbones trained with PM4GRPO are considered.\\nThe publicly available R1-Distill-Qwen-1.5B and 7B checkpoints [ 15 ] are utilized as the backbone models.\\nDatasets. For training, DeepMath-103k [ 16 ] ,\\na collection of mathematical problems of varying difficulty paired with solutions generated by DeepSeek R1, is used.\\nFor evaluation, MATH500 [ 17 ] , OlympiadBench [ 18 ] , Minerva [ 19 ] , AIME24, and AIME25 [ 20 ] ,\\nare considered.\\nBaselines. We compare PM4GRPO against existing open baselines.\\nFor 7B models, PM4GRPO is compared with DeepMath-Zero-7B [ 16 ] , Skywork-OR1-7B [ 21 ] ,\\nLEAD [ 12 ] , DRGRPO [ 14 ] , PRIME [ 8 ] ,\\nand P-GRPO [ 7 ] . Among these baselines, the performance of PRIME and P-GRPO is cited from their respective papers.\\nFor 1.5B models, the results are presented alongside DeepSeek-R1-Distill-Qwen-1.5B [ 15 ] , Graph-R1 [ 22 ] ,\\nSTILL-3 [ 23 ] , and EXGRPO [ 24 ] .\\nImplementation. All experiments are conducted on a server with 4 √ó \\\\times NVIDIA H200 GPUs.\\nWe use TRL v0.24.0 [ 25 ] and PyTorch v2.8.0 compiled with CUDA 12.8 [ 26 ] .\\nTable 2 compares several 7B-scale models across multiple math benchmarks. PM4GRPO consistently achieves the highest scores, reaching 91.1%\\non MATH 500 and 61.1% on Olympiad Bench, surpassing strong baselines such as R1-Distill-Qwen and Skywork-OR1. Its performance on Minerva Math (49.3%) also\\nremains competitive, indicating solid reasoning capability across different mathematical domains.\\nNotably, PM4GRPO demonstrates clear advantages on the challenging AIME24 and AIME25 benchmarks, achieving 45.6% and 35.0%, respectively‚Äîshowing better\\nrobustness than other methods. Overall, these results highlight PM4GRPO‚Äôs superior reasoning and generalization ability among 7B models.\\nTable 2 summarizes the performance of several 1.5B-scale models on multiple math benchmarks.\\nPM4GRPO achieves the best overall performance, recording 83.9% on MATH 500, 52.7% on Olympiad Bench, and 37.9% on Minerva Math.\\nThese results slightly surpass the previous strong baseline STILL-3, demonstrating that PM4GRPO effectively enhances mathematical reasoning even at a smaller model scale.\\nAlthough STILL-3 attains the highest scores on the AIME24 and AIME25 sets, PM4GRPO shows more balanced performance across all benchmarks.\\nIts consistent results indicate better generalization and stability, suggesting that the proposed method scales well and maintains strong reasoning ability even in compact model configurations.',\n",
       "    'subsections': [{'title': '3.1 Experimental Settings',\n",
       "      'paragraphs': 'To evaluate the effectiveness of PM4GRPO, 1.5B and 7B parameter backbones trained with PM4GRPO are considered.\\nThe publicly available R1-Distill-Qwen-1.5B and 7B checkpoints [ 15 ] are utilized as the backbone models.\\nDatasets. For training, DeepMath-103k [ 16 ] ,\\na collection of mathematical problems of varying difficulty paired with solutions generated by DeepSeek R1, is used.\\nFor evaluation, MATH500 [ 17 ] , OlympiadBench [ 18 ] , Minerva [ 19 ] , AIME24, and AIME25 [ 20 ] ,\\nare considered.\\nBaselines. We compare PM4GRPO against existing open baselines.\\nFor 7B models, PM4GRPO is compared with DeepMath-Zero-7B [ 16 ] , Skywork-OR1-7B [ 21 ] ,\\nLEAD [ 12 ] , DRGRPO [ 14 ] , PRIME [ 8 ] ,\\nand P-GRPO [ 7 ] . Among these baselines, the performance of PRIME and P-GRPO is cited from their respective papers.\\nFor 1.5B models, the results are presented alongside DeepSeek-R1-Distill-Qwen-1.5B [ 15 ] , Graph-R1 [ 22 ] ,\\nSTILL-3 [ 23 ] , and EXGRPO [ 24 ] .\\nImplementation. All experiments are conducted on a server with 4 √ó \\\\times NVIDIA H200 GPUs.\\nWe use TRL v0.24.0 [ 25 ] and PyTorch v2.8.0 compiled with CUDA 12.8 [ 26 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Experimental Results',\n",
       "      'paragraphs': 'Table 2 compares several 7B-scale models across multiple math benchmarks. PM4GRPO consistently achieves the highest scores, reaching 91.1%\\non MATH 500 and 61.1% on Olympiad Bench, surpassing strong baselines such as R1-Distill-Qwen and Skywork-OR1. Its performance on Minerva Math (49.3%) also\\nremains competitive, indicating solid reasoning capability across different mathematical domains.\\nNotably, PM4GRPO demonstrates clear advantages on the challenging AIME24 and AIME25 benchmarks, achieving 45.6% and 35.0%, respectively‚Äîshowing better\\nrobustness than other methods. Overall, these results highlight PM4GRPO‚Äôs superior reasoning and generalization ability among 7B models.\\nTable 2 summarizes the performance of several 1.5B-scale models on multiple math benchmarks.\\nPM4GRPO achieves the best overall performance, recording 83.9% on MATH 500, 52.7% on Olympiad Bench, and 37.9% on Minerva Math.\\nThese results slightly surpass the previous strong baseline STILL-3, demonstrating that PM4GRPO effectively enhances mathematical reasoning even at a smaller model scale.\\nAlthough STILL-3 attains the highest scores on the AIME24 and AIME25 sets, PM4GRPO shows more balanced performance across all benchmarks.\\nIts consistent results indicate better generalization and stability, suggesting that the proposed method scales well and maintains strong reasoning ability even in compact model configurations.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Conclusion',\n",
       "    'paragraphs': 'In this short paper, we introduced PM4GRPO, a novel reasoning-aware GRPO framework using\\nPM techniques. By incorporating conformance rewards that assess the alignment of reasoning processes\\nwith a pretrained teacher model, PM4GRPO effectively enhances the effectiveness of GRPO-based post-training for\\nlarge reasoning models. Experimental results across five benchmarks demonstrated that PM4GRPO significantly outperforms existing methods,\\nhighlighting the benefits of leveraging PM techniques for reasoning-aware reinforcement learning.\\nThese advantages suggest that PM techniques are a powerful tool for quantitatively evaluating reasoning procedures as processes,\\nwhich can be a valuable direction for future research in reinforcement learning for large reasoning models.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs),\\n yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO)\\n that augments standard answer/format rewards with signals over the reasoning procedure.\\n To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model‚Äôs\\n reasoning aligns with the pretrained teacher model.\\n The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms\\n existing methodologies for GRPO-based post-training. These results highlight\\n that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.'},\n",
       " '2510.25634v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Enabling robots to perform long-horizon, contact-rich manipulation is a longstanding goal in robotics, with bimanual systems offering the potential for human-like dexterity [ 1 , 2 ] . A primary challenge in this domain lies in programming two arms to act in harmony to accomplish a complex goal.\\nSuch coordination demands a flexible control strategy that combines parallel, serial, and collaborative execution of skills.\\nMany previous approaches model manipulation as a sequential decision-making process, where a policy selects a single action or skill at each step [ 3 , 4 , 5 ] .\\nThis formulation, however, creates an inherent bottleneck for bimanual tasks, as it fails to capture opportunities for simultaneous execution and can lead to inefficient, underutilized behaviors.\\nIn this paper, we argue that the high-level decision problem is better framed as an integrated planning and scheduling problem, where the goal is to assign tasks to both arms, where at some points in time the arms act independently and at other points they act collaboratively.\\nTo this end, we propose a hierarchical framework built upon a library of reinforcement-learned primitive skills.\\nOur Transformer-based high-level policy functions as a skill scheduler, generating plans that specify the discrete skills and their continuous parameters for both arms.\\nWe highlight three contributions of this work:\\nWe propose a novel approach for bimanual manipulation that learns a library of arm-specific primitive skills using Reinforcement Learning (RL) and then through integrated skill planning & scheduling combines these in serial and parallel over time.\\nWe show how a Transformer-based scheduling policy can be trained to generate bimanual schedules that specify both discrete skills and their continuous parameters.\\nWe demonstrate through experiments that our approach achieves significantly higher success rates and efficiency compared to end-to-end and sequential planning baselines.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Task and Motion Planning. Task and Motion Planning (TAMP) [ 6 , 7 , 8 , 9 , 10 , 11 ] algorithms plan sequences of high-level actions and corresponding low-level motion plans, typically for single-robot scenarios.\\nMulti-robot manipulation and TAMP [ 12 , 13 , 14 ] extend these principles to plan for multiple robots or arms and introduce opportunities for parallelism,\\nbut the jump to multiple robots and introduces significant complexity in coordination, collision avoidance, and task scheduling.\\nClassical TAMP approaches rely on predefined models and constraints, which can be difficult to engineer for tasks requiring non-prehensile manipulation.\\nSeveral recent learning for TAMP approaches relax this assumption by replacing handcrafted model components with learned ones [ 15 , 16 , 17 , 5 ] . Others use imitation learning to plan [ 18 , 19 ] and implicitly learn a planning model from demonstrations.\\nOur approach also uses imitation learning for high-level reasoning but predicts bimanual behaviors, scheduling which skills should be used and when, both in serial and in parallel.\\nHierarchical Modeling in Robotic Manipulation. Hierarchical manipulation methods [ 20 , 21 ] build complex manipulation behaviors from a set of learned primitive control policies.\\nRecent works use imitation [ 22 , 23 , 24 ] or reinforcement learning [ 25 , 26 , 27 ] to acquire these primitive policies, adding flexibility while benefiting from the temporal abstraction of primitives.\\nHowever, these methods are often limited to single-arm scenarios and confined to prehensile manipulation primitives.\\nIn contrast, our method employs a set of versatile parameterized contact-rich single-arm and bimanual primitives for bimanual manipulation tasks.',\n",
       "    'subsections': []},\n",
       "   {'title': '3 Method',\n",
       "    'paragraphs': 'An overview of our pipeline is shown in Fig. 1 .\\nWe first define the problem formulation in Sec. 3.1 .\\nWe then introduce the details of the individual low-level skill training and the high-level bimanual skill scheduling policy in Sec. 3.2 and Sec. 3.3 .\\nWe formulate the long-horizon bimanual manipulation task as a two-level hierarchical decision problem. We first define the low-level MDP for learning individual skills and then describe the high-level MDP for scheduling these skills to achieve a final task goal g ‚àà ùí¢ g\\\\in\\\\mathcal{G} .\\nEach primitive skill k k from a finite library ùí¶ \\\\mathcal{K} is learned as a low-level policy œÄ k L \\u200b ( a t ‚à£ s t , œâ t ) \\\\pi^{\\\\mathrm{L}}_{k}(a_{t}\\\\mid s_{t},\\\\omega_{t}) . This policy solves a short-horizon, goal-conditioned Markov Decision Process (MDP) defined by the tuple ‚ü® ùíÆ , ùíú , ùíØ , ‚Ñõ k , Œ≥ , Œ© ‚ü© \\\\langle\\\\mathcal{S},\\\\mathcal{A},\\\\mathcal{T},\\\\mathcal{R}_{k},\\\\gamma,\\\\Omega\\\\rangle .\\nHere, s t ‚àà ùíÆ s_{t}\\\\in\\\\mathcal{S} represents the state at time t t , a t ‚àà ùíú a_{t}\\\\in\\\\mathcal{A} is the low-level action, ùíØ \\\\mathcal{T} is the state transition function, and Œ≥ ‚àà [ 0 , 1 ) \\\\gamma\\\\in[0,1) is the discount factor.\\nThe goal for this MDP is a skill parameter œâ t ‚àà Œ© \\\\omega_{t}\\\\in\\\\Omega (e.g., a target object pose), and the policy is rewarded by a dense reward function ‚Ñõ k \\u200b ( s t , a t , œâ t ) \\\\mathcal{R}_{k}(s_{t},a_{t},\\\\omega_{t}) for making progress towards this subgoal œâ t \\\\omega_{t} .\\nBuilding on the skill library, the high-level problem is to select the skill k t k_{t} and its parameters œâ t \\\\omega_{t} to achieve the overall task goal g g . This is governed by a high-level scheduling policy œÄ H \\u200b ( k t , œâ t ‚à£ s t , g ) \\\\pi^{\\\\mathrm{H}}(k_{t},\\\\omega_{t}\\\\mid s_{t},g) .\\nThis high-level MDP is related to a semi-MDP [ 25 ] , where each low-level skill acts as a temporally-extended action, similar to that of an option [ 28 ] .\\nEach high-level ‚Äúaction‚Äù ( k t , œâ t ) (k_{t},\\\\omega_{t}) is executed by the corresponding low-level policy œÄ k t L \\\\pi^{\\\\mathrm{L}}_{k_{t}} for multiple timesteps until termination.\\nThe final, factorized policy for the complete task is thus: œÄ \\u200b ( a t ‚à£ s t , g ) = œÄ H \\u200b ( k t , œâ t ‚à£ s t , g ) \\u200b œÄ k t L \\u200b ( a t ‚à£ s t , œâ t ) \\\\pi(a_{t}\\\\mid s_{t},g)=\\\\pi^{\\\\mathrm{H}}(k_{t},\\\\omega_{t}\\\\mid s_{t},g)\\\\;\\\\pi^{\\\\mathrm{L}}_{k_{t}}(a_{t}\\\\mid s_{t},\\\\omega_{t}) .\\nWe train a set of primitive single-arm and bimanual skills œÄ k L \\u200b ( a t ‚à£ s t , œâ t ) \\\\pi^{\\\\mathrm{L}}_{k}(a_{t}\\\\mid s_{t},\\\\omega_{t}) using goal-conditioned reinforcement learning.\\nEach low-level skill policy takes as input the current state s t s_{t} and its parameters œâ t \\\\omega_{t} . For a single arm, s t s_{t} is comprised of the arm‚Äôs proprioception (including joint positions, velocities, and end-effector pose), other arm‚Äôs end-effector pose, and the object state. The parameters œâ t \\\\omega_{t} encode task-specific information such as the object goal pose. The policy outputs a target end-effector pose action a t a_{t} , which is then converted to joint torques via an Operational Space Controller (OSC) [ 29 ] .\\nThe low-level primitive skills are trained with dense rewards that combine a contact reward, task-relevant goal reaching reward, a binary success bonus, and an energy penalty. We use a unified template r t = r contact + r goal + r suc ‚àí c energy r_{t}=r_{\\\\text{contact}}+r_{\\\\text{goal}}+r_{\\\\text{suc}}-c_{\\\\text{energy}} , where the task success reward r suc = ùüè suc r_{\\\\text{suc}}=\\\\mathbf{1}_{\\\\text{suc}} is given when the pose of the object is within 0.05 \\u200b m 0.05\\\\,\\\\mathrm{m} and 0.1 0.1 radians of the target pose. We add dense shaping r contact = Œ± c \\u200b ( 1 ‚àí tanh \\u2061 ( d c / œÉ c ) ) r_{\\\\text{contact}}=\\\\alpha_{c}\\\\!\\\\left(1-\\\\tanh(d_{c}/\\\\sigma_{c})\\\\right) and r goal = r pos + r rot = Œ± p \\u200b ( 1 ‚àí tanh \\u2061 ( d p / œÉ p ) ) + Œ± r \\u200b ( 1 ‚àí tanh \\u2061 ( d r / œÉ r ) ) r_{\\\\text{goal}}=r_{\\\\text{pos}}+r_{\\\\text{rot}}=\\\\alpha_{p}\\\\!\\\\left(1-\\\\tanh(d_{p}/\\\\sigma_{p})\\\\right)+\\\\alpha_{r}\\\\!\\\\left(1-\\\\tanh(d_{r}/\\\\sigma_{r})\\\\right) , where d c d_{c} is the right end-effector‚Äìto‚Äìobject distance, d p d_{p} is the object‚Äìto‚Äìgoal position error, and d r d_{r} is the object‚Äìto‚Äìgoal rotation error; Œ± c , Œ± p , Œ± r > 0 \\\\alpha_{c},\\\\alpha_{p},\\\\alpha_{r}>0 are scaling coefficients and œÉ c = 0.2 , œÉ p = 0.05 , œÉ r = 10 \\\\sigma_{c}{=}0.2,\\\\ \\\\sigma_{p}{=}0.05,\\\\ \\\\sigma_{r}{=}10 are shaping scales; c energy c_{\\\\text{energy}} denotes an energy penalty: c energy = c e \\u200b ‚àë i = 1 J œÑ i \\u200b q Àô i c_{\\\\text{energy}}=c_{e}\\\\sum_{i=1}^{J}\\\\tau_{i}\\\\,\\\\dot{q}_{i} , where c e ‚àà ‚Ñù + c_{e}\\\\in\\\\mathbb{R}^{+} is a scaling coefficient, and œÑ i \\\\tau_{i} and q Àô i \\\\dot{q}_{i} are the joint torque and velocity of the i th i^{\\\\text{th}} joint.\\nIn this work, we focus on five primitive skills: single-arm pushing, single-arm rotating, bimanual rotating, bimanual pushing, and bimanual pick-and-place.\\nThese skills were selected by human modelers to cover the fundamental capabilities required by our tasks.\\nFor all skills, the goal is specified by an object target position, an orientation, or full pose.\\nOur approach is not limited to these skills and can in principle be applied to more than two robots.\\nAfter the primitive skills are trained, they can be composed to complete difficult long-horizon tasks.\\nWe learn the high-level bimanual skill scheduling policy œÄ H \\\\pi^{\\\\mathrm{H}} through behavior cloning [ 30 ] .\\nThe goal is to train a policy that mimics the output of an expert planner that has access to privileged information used to generate the problem.\\nThe learned policy œÄ H \\u200b ( œâ t , k t ‚à£ s t , g ) \\\\pi^{\\\\mathrm{H}}(\\\\omega_{t},k_{t}\\\\mid s_{t},g) takes in the current observation and produces two types of outputs: a discrete skill k t k_{t} for each arm and the continuous skill parameters œâ t \\\\omega_{t} for the selected skill, which in this work, is a goal object pose.\\nGiven the task goal g g and current state information s t s_{t} , the high-level skill scheduling policy decides which skill and which œâ t \\\\omega_{t} to execute for each arm. To understand the task progress, œÄ H \\\\pi^{\\\\mathrm{H}} perceives the current object state, the task goal, and the proprioception states of both arms.\\nThe skill scheduling policy utilizes a transformer-based architecture that takes a sequence of recent observations as input, which aims to improve the temporal consistency of the policy‚Äôs predictions. As an example, for the task of placing two tabletop objects into a grey bin in Fig. 2 , the skill scheduling policy first calls pushing skills for both arms at the same time to move each object toward a suitable central position, and then sequentially invokes bimanual rotating and bimanual pick-and-place skills on the two objects with appropriate skill parameters to complete the task.\\nTo train the skill scheduling policy, we first generate a dataset of expert demonstrations. Specifically, we implement a custom demonstration generator that has access to privileged problem information. This generator programmatically constructs multiple successful skill sequences, each being an ordered composition of low-level skills. For each sequence, it samples skill parameters œâ \\\\omega and executes the full plan, retaining only successful rollouts.\\nDuring data generation, we additionally define a single-arm waiting skill that keeps one arm stationary. For example, for the bimanual rotating skill we sample the object goal rotation within a specified range. This data generation method yields diverse successful sequence data, covering different ways of bimanual division of labor, asynchronous execution, and collaboration.\\nOnce we have collected the required skill-sequence data, we train a Transformer-based bimanual skill scheduling policy œÄ H \\u200b ( œâ t , k t ‚à£ s t , g ) \\\\pi^{\\\\mathrm{H}}(\\\\omega_{t},k_{t}\\\\mid s_{t},g) . The training supervision for œÄ H \\\\pi^{\\\\mathrm{H}} comes from the skill-index labels and skill parameters for both arms produced by the data-generation pipeline. We use a cross-entropy loss for skill-index prediction and an Mean Squared Error (MSE) loss for skill-parameter regression; the objective is ‚Ñí = ùîº t \\u200b [ ‚àë u ‚àà { L , R } CE \\u200b ( k t , u ‚àó , k ^ t , u ) + Œª œâ \\u200b ‚Äñ œâ t , u ‚àó ‚àí œâ ^ t , u ‚Äñ 2 2 ] \\\\mathcal{L}=\\\\mathbb{E}_{t}\\\\!\\\\big[\\\\sum_{u\\\\in\\\\{L,R\\\\}}\\\\mathrm{CE}(k_{t,u}^{\\\\ast},\\\\hat{k}_{t,u})+\\\\lambda_{\\\\omega}\\\\,\\\\|\\\\omega_{t,u}^{\\\\ast}-\\\\hat{\\\\omega}_{t,u}\\\\|_{2}^{2}\\\\big] , where Œª œâ \\\\lambda_{\\\\omega} balances the two terms.',\n",
       "    'subsections': [{'title': '3.1 Problem Formulation',\n",
       "      'paragraphs': 'We formulate the long-horizon bimanual manipulation task as a two-level hierarchical decision problem. We first define the low-level MDP for learning individual skills and then describe the high-level MDP for scheduling these skills to achieve a final task goal g ‚àà ùí¢ g\\\\in\\\\mathcal{G} .\\nEach primitive skill k k from a finite library ùí¶ \\\\mathcal{K} is learned as a low-level policy œÄ k L \\u200b ( a t ‚à£ s t , œâ t ) \\\\pi^{\\\\mathrm{L}}_{k}(a_{t}\\\\mid s_{t},\\\\omega_{t}) . This policy solves a short-horizon, goal-conditioned Markov Decision Process (MDP) defined by the tuple ‚ü® ùíÆ , ùíú , ùíØ , ‚Ñõ k , Œ≥ , Œ© ‚ü© \\\\langle\\\\mathcal{S},\\\\mathcal{A},\\\\mathcal{T},\\\\mathcal{R}_{k},\\\\gamma,\\\\Omega\\\\rangle .\\nHere, s t ‚àà ùíÆ s_{t}\\\\in\\\\mathcal{S} represents the state at time t t , a t ‚àà ùíú a_{t}\\\\in\\\\mathcal{A} is the low-level action, ùíØ \\\\mathcal{T} is the state transition function, and Œ≥ ‚àà [ 0 , 1 ) \\\\gamma\\\\in[0,1) is the discount factor.\\nThe goal for this MDP is a skill parameter œâ t ‚àà Œ© \\\\omega_{t}\\\\in\\\\Omega (e.g., a target object pose), and the policy is rewarded by a dense reward function ‚Ñõ k \\u200b ( s t , a t , œâ t ) \\\\mathcal{R}_{k}(s_{t},a_{t},\\\\omega_{t}) for making progress towards this subgoal œâ t \\\\omega_{t} .\\nBuilding on the skill library, the high-level problem is to select the skill k t k_{t} and its parameters œâ t \\\\omega_{t} to achieve the overall task goal g g . This is governed by a high-level scheduling policy œÄ H \\u200b ( k t , œâ t ‚à£ s t , g ) \\\\pi^{\\\\mathrm{H}}(k_{t},\\\\omega_{t}\\\\mid s_{t},g) .\\nThis high-level MDP is related to a semi-MDP [ 25 ] , where each low-level skill acts as a temporally-extended action, similar to that of an option [ 28 ] .\\nEach high-level ‚Äúaction‚Äù ( k t , œâ t ) (k_{t},\\\\omega_{t}) is executed by the corresponding low-level policy œÄ k t L \\\\pi^{\\\\mathrm{L}}_{k_{t}} for multiple timesteps until termination.\\nThe final, factorized policy for the complete task is thus: œÄ \\u200b ( a t ‚à£ s t , g ) = œÄ H \\u200b ( k t , œâ t ‚à£ s t , g ) \\u200b œÄ k t L \\u200b ( a t ‚à£ s t , œâ t ) \\\\pi(a_{t}\\\\mid s_{t},g)=\\\\pi^{\\\\mathrm{H}}(k_{t},\\\\omega_{t}\\\\mid s_{t},g)\\\\;\\\\pi^{\\\\mathrm{L}}_{k_{t}}(a_{t}\\\\mid s_{t},\\\\omega_{t}) .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Individual Low-Level Skills',\n",
       "      'paragraphs': 'We train a set of primitive single-arm and bimanual skills œÄ k L \\u200b ( a t ‚à£ s t , œâ t ) \\\\pi^{\\\\mathrm{L}}_{k}(a_{t}\\\\mid s_{t},\\\\omega_{t}) using goal-conditioned reinforcement learning.\\nEach low-level skill policy takes as input the current state s t s_{t} and its parameters œâ t \\\\omega_{t} . For a single arm, s t s_{t} is comprised of the arm‚Äôs proprioception (including joint positions, velocities, and end-effector pose), other arm‚Äôs end-effector pose, and the object state. The parameters œâ t \\\\omega_{t} encode task-specific information such as the object goal pose. The policy outputs a target end-effector pose action a t a_{t} , which is then converted to joint torques via an Operational Space Controller (OSC) [ 29 ] .\\nThe low-level primitive skills are trained with dense rewards that combine a contact reward, task-relevant goal reaching reward, a binary success bonus, and an energy penalty. We use a unified template r t = r contact + r goal + r suc ‚àí c energy r_{t}=r_{\\\\text{contact}}+r_{\\\\text{goal}}+r_{\\\\text{suc}}-c_{\\\\text{energy}} , where the task success reward r suc = ùüè suc r_{\\\\text{suc}}=\\\\mathbf{1}_{\\\\text{suc}} is given when the pose of the object is within 0.05 \\u200b m 0.05\\\\,\\\\mathrm{m} and 0.1 0.1 radians of the target pose. We add dense shaping r contact = Œ± c \\u200b ( 1 ‚àí tanh \\u2061 ( d c / œÉ c ) ) r_{\\\\text{contact}}=\\\\alpha_{c}\\\\!\\\\left(1-\\\\tanh(d_{c}/\\\\sigma_{c})\\\\right) and r goal = r pos + r rot = Œ± p \\u200b ( 1 ‚àí tanh \\u2061 ( d p / œÉ p ) ) + Œ± r \\u200b ( 1 ‚àí tanh \\u2061 ( d r / œÉ r ) ) r_{\\\\text{goal}}=r_{\\\\text{pos}}+r_{\\\\text{rot}}=\\\\alpha_{p}\\\\!\\\\left(1-\\\\tanh(d_{p}/\\\\sigma_{p})\\\\right)+\\\\alpha_{r}\\\\!\\\\left(1-\\\\tanh(d_{r}/\\\\sigma_{r})\\\\right) , where d c d_{c} is the right end-effector‚Äìto‚Äìobject distance, d p d_{p} is the object‚Äìto‚Äìgoal position error, and d r d_{r} is the object‚Äìto‚Äìgoal rotation error; Œ± c , Œ± p , Œ± r > 0 \\\\alpha_{c},\\\\alpha_{p},\\\\alpha_{r}>0 are scaling coefficients and œÉ c = 0.2 , œÉ p = 0.05 , œÉ r = 10 \\\\sigma_{c}{=}0.2,\\\\ \\\\sigma_{p}{=}0.05,\\\\ \\\\sigma_{r}{=}10 are shaping scales; c energy c_{\\\\text{energy}} denotes an energy penalty: c energy = c e \\u200b ‚àë i = 1 J œÑ i \\u200b q Àô i c_{\\\\text{energy}}=c_{e}\\\\sum_{i=1}^{J}\\\\tau_{i}\\\\,\\\\dot{q}_{i} , where c e ‚àà ‚Ñù + c_{e}\\\\in\\\\mathbb{R}^{+} is a scaling coefficient, and œÑ i \\\\tau_{i} and q Àô i \\\\dot{q}_{i} are the joint torque and velocity of the i th i^{\\\\text{th}} joint.\\nIn this work, we focus on five primitive skills: single-arm pushing, single-arm rotating, bimanual rotating, bimanual pushing, and bimanual pick-and-place.\\nThese skills were selected by human modelers to cover the fundamental capabilities required by our tasks.\\nFor all skills, the goal is specified by an object target position, an orientation, or full pose.\\nOur approach is not limited to these skills and can in principle be applied to more than two robots.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 Bimanual Skill Scheduling Policy',\n",
       "      'paragraphs': 'After the primitive skills are trained, they can be composed to complete difficult long-horizon tasks.\\nWe learn the high-level bimanual skill scheduling policy œÄ H \\\\pi^{\\\\mathrm{H}} through behavior cloning [ 30 ] .\\nThe goal is to train a policy that mimics the output of an expert planner that has access to privileged information used to generate the problem.\\nThe learned policy œÄ H \\u200b ( œâ t , k t ‚à£ s t , g ) \\\\pi^{\\\\mathrm{H}}(\\\\omega_{t},k_{t}\\\\mid s_{t},g) takes in the current observation and produces two types of outputs: a discrete skill k t k_{t} for each arm and the continuous skill parameters œâ t \\\\omega_{t} for the selected skill, which in this work, is a goal object pose.\\nGiven the task goal g g and current state information s t s_{t} , the high-level skill scheduling policy decides which skill and which œâ t \\\\omega_{t} to execute for each arm. To understand the task progress, œÄ H \\\\pi^{\\\\mathrm{H}} perceives the current object state, the task goal, and the proprioception states of both arms.\\nThe skill scheduling policy utilizes a transformer-based architecture that takes a sequence of recent observations as input, which aims to improve the temporal consistency of the policy‚Äôs predictions. As an example, for the task of placing two tabletop objects into a grey bin in Fig. 2 , the skill scheduling policy first calls pushing skills for both arms at the same time to move each object toward a suitable central position, and then sequentially invokes bimanual rotating and bimanual pick-and-place skills on the two objects with appropriate skill parameters to complete the task.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.4 Data Generation.',\n",
       "      'paragraphs': 'To train the skill scheduling policy, we first generate a dataset of expert demonstrations. Specifically, we implement a custom demonstration generator that has access to privileged problem information. This generator programmatically constructs multiple successful skill sequences, each being an ordered composition of low-level skills. For each sequence, it samples skill parameters œâ \\\\omega and executes the full plan, retaining only successful rollouts.\\nDuring data generation, we additionally define a single-arm waiting skill that keeps one arm stationary. For example, for the bimanual rotating skill we sample the object goal rotation within a specified range. This data generation method yields diverse successful sequence data, covering different ways of bimanual division of labor, asynchronous execution, and collaboration.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.5 Policy Learning.',\n",
       "      'paragraphs': 'Once we have collected the required skill-sequence data, we train a Transformer-based bimanual skill scheduling policy œÄ H \\u200b ( œâ t , k t ‚à£ s t , g ) \\\\pi^{\\\\mathrm{H}}(\\\\omega_{t},k_{t}\\\\mid s_{t},g) . The training supervision for œÄ H \\\\pi^{\\\\mathrm{H}} comes from the skill-index labels and skill parameters for both arms produced by the data-generation pipeline. We use a cross-entropy loss for skill-index prediction and an Mean Squared Error (MSE) loss for skill-parameter regression; the objective is ‚Ñí = ùîº t \\u200b [ ‚àë u ‚àà { L , R } CE \\u200b ( k t , u ‚àó , k ^ t , u ) + Œª œâ \\u200b ‚Äñ œâ t , u ‚àó ‚àí œâ ^ t , u ‚Äñ 2 2 ] \\\\mathcal{L}=\\\\mathbb{E}_{t}\\\\!\\\\big[\\\\sum_{u\\\\in\\\\{L,R\\\\}}\\\\mathrm{CE}(k_{t,u}^{\\\\ast},\\\\hat{k}_{t,u})+\\\\lambda_{\\\\omega}\\\\,\\\\|\\\\omega_{t,u}^{\\\\ast}-\\\\hat{\\\\omega}_{t,u}\\\\|_{2}^{2}\\\\big] , where Œª œâ \\\\lambda_{\\\\omega} balances the two terms.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Experiments',\n",
       "    'paragraphs': 'We evaluate our approach with simulated experiments to answer the following questions: Q1. Does our hierarchical framework achieve higher success rates on long-horizon bimanual manipulation tasks compared to flat end-to-end approaches? Q2. Can our bimanual skill scheduling policy effectively generate valid schedules that sequence both discrete skills and their continuous parameters for two arms simultaneously? Q3. Does our planing & scheduling formulation lead to more efficient and coordinated bimanual behaviors?\\nWe evaluate our framework on a long-horizon, contact-rich manipulation task with goal that: ‚Äú the objects are in the bin ‚Äù. This task involves placing one or two bulky objects into a bin, where the objects are too large for a single arm to grasp, lift, or reorient alone, which necessitates non-prehensile manipulation.\\nFurthermore, an object‚Äôs initial position may be out of reach for one arm. Consequently, this task demands a dynamic combination of strategies: from synchronized coordination for collaborative lifting to independent, asynchronous execution for repositioning. The variability in initial object layouts requires the policy to plan and schedule different skill sequences for each scenario. We build this environment and conduct all simulation experiments in IsaacLab [ 31 , 32 ] . To compare different approaches, we use three metrics: task success rate (SR) and task completion progress (CP), which is defined as the percentage of optimally-ordered task stages completed, and episode duration (ED) to measure completion efficiency.\\nBaselines. We compare our proposed framework against several representative baselines and ablations. We consider the following baselines:\\n1) RL-scratch is vanilla PPO [ 33 ] algorithm which learns the entire task from scratch.\\n2) Hierarchical RL (HRL) [ 26 ] is a hierarchical approach where a high-level policy selects the pre-trained low-level skills using RL.\\n3) Sequential-only planning [ 15 ] is a baseline that can only select one skill at each decision step, thus enforcing sequential, non-parallel execution.\\n4) Ours (Single Arm Ablation) is an ablation of our method constrained to use only a single arm.\\nFor all hierarchical methods, we use the same library of pre-trained low-level skills.\\nResults and analysis. Table 1 provides comprehensive results for all methods on two task variations: 1) rearranging one object and 2) rearranging two objects into a bin. It answers Q1 and Q2 by showing that our method outperforms the RL-scratch baseline, achieving a 45% higher Success Rate (SR) and 41% higher Task Completion Progress (CP).\\nAdditionally, we visualize the bimanual skill schedule during evaluation in Fig. 2 , showing that our bimanual skill scheduling policy not only selects the correct single-arm or bimanual skills in different situations, but also efficiently sequences skills for both arms to complete the task. We answer Q3 by comparing our method with Sequential Planning, which results in a 16% reduction in Episode Duration (ED) and highlights our method‚Äôs ability to more efficiently plan and schedule for both arms simultaneously.\\nPolicy rollout videos are available\\nat https://www.youtube.com/watch?v=3DI_yTml3J4 .',\n",
       "    'subsections': [{'title': '4.1 Experiments Setup',\n",
       "      'paragraphs': 'We evaluate our framework on a long-horizon, contact-rich manipulation task with goal that: ‚Äú the objects are in the bin ‚Äù. This task involves placing one or two bulky objects into a bin, where the objects are too large for a single arm to grasp, lift, or reorient alone, which necessitates non-prehensile manipulation.\\nFurthermore, an object‚Äôs initial position may be out of reach for one arm. Consequently, this task demands a dynamic combination of strategies: from synchronized coordination for collaborative lifting to independent, asynchronous execution for repositioning. The variability in initial object layouts requires the policy to plan and schedule different skill sequences for each scenario. We build this environment and conduct all simulation experiments in IsaacLab [ 31 , 32 ] . To compare different approaches, we use three metrics: task success rate (SR) and task completion progress (CP), which is defined as the percentage of optimally-ordered task stages completed, and episode duration (ED) to measure completion efficiency.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Quantitative Results',\n",
       "      'paragraphs': 'Baselines. We compare our proposed framework against several representative baselines and ablations. We consider the following baselines:\\n1) RL-scratch is vanilla PPO [ 33 ] algorithm which learns the entire task from scratch.\\n2) Hierarchical RL (HRL) [ 26 ] is a hierarchical approach where a high-level policy selects the pre-trained low-level skills using RL.\\n3) Sequential-only planning [ 15 ] is a baseline that can only select one skill at each decision step, thus enforcing sequential, non-parallel execution.\\n4) Ours (Single Arm Ablation) is an ablation of our method constrained to use only a single arm.\\nFor all hierarchical methods, we use the same library of pre-trained low-level skills.\\nResults and analysis. Table 1 provides comprehensive results for all methods on two task variations: 1) rearranging one object and 2) rearranging two objects into a bin. It answers Q1 and Q2 by showing that our method outperforms the RL-scratch baseline, achieving a 45% higher Success Rate (SR) and 41% higher Task Completion Progress (CP).\\nAdditionally, we visualize the bimanual skill schedule during evaluation in Fig. 2 , showing that our bimanual skill scheduling policy not only selects the correct single-arm or bimanual skills in different situations, but also efficiently sequences skills for both arms to complete the task. We answer Q3 by comparing our method with Sequential Planning, which results in a 16% reduction in Episode Duration (ED) and highlights our method‚Äôs ability to more efficiently plan and schedule for both arms simultaneously.\\nPolicy rollout videos are available\\nat https://www.youtube.com/watch?v=3DI_yTml3J4 .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Conclusion',\n",
       "    'paragraphs': 'In this work, we introduced a hierarchical framework that addresses long-horizon bimanual manipulation by formulating it as an integrated skill planning & scheduling problem. Our method utilizes a Transformer-based policy to generate coordinated plans, simultaneously selecting discrete skills and regressing their continuous parameters. Experimental results validate that our approach achieves much higher success rates than end-to-end RL and produces more efficient, parallelized behaviors than planners restricted to sequential actions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Long-horizon contact-rich bimanual manipulation presents a significant challenge, requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms.\\n In this paper, we introduce a hierarchical framework that frames this challenge as an integrated skill planning & scheduling problem, going beyond purely sequential decision-making to support simultaneous skill invocation.\\n Our approach is built upon a library of single-arm and bimanual primitive skills, each trained using Reinforcement Learning (RL) in GPU-accelerated simulation.\\n We then train a Transformer on a dataset of skill compositions to act as a high-level planner & scheduler, simultaneously predicting the discrete schedule of skills as well as their continuous parameters. We demonstrate that our method achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.'},\n",
       " '2510.25602v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'The proliferation of Large Language Models (LLMs) has been accompanied by a surge in their computational and memory demands [ 43 ] , making quantization an indispensable technique for efficient deployment. A central challenge in quantizing LLMs, particularly those based on the Transformer architecture, is the presence of significant outliers [ 38 , 12 ] in activation distributions. These outliers, characterized by their large magnitude but infrequent occurrence, pose a considerable problem for low-precision representations. To accommodate this wide dynamic range, the AI hardware industry [ 31 ] is increasingly pivoting towards low-precision floating-point (FP) formats, such as FP8 and FP4. Prominent examples like NVIDIA‚Äôs Blackwell architecture [ 31 ] underscore this trend, favoring the superior dynamic range of FP to handle outliers more gracefully than traditional integer (INT) formats.\\nHowever, this industry-wide momentum towards FP formats is based on an incomplete picture. The comparative advantages of FP and INT have not been systematically evaluated across different quantization granularities in a unified framework. Most studies [ 41 , 6 , 22 ] focus on a single format or compare them only at coarse granularities (e.g., per-channel), failing to answer a critical question: how does the performance trade-off between INT and FP evolve as granularity becomes finer? Since fine-grained (block-wise) quantization is now a standard technique [ 34 , 32 ] for mitigating outliers, understanding its interaction with the underlying number format is essential for effective algorithm-hardware co-design.\\nIn this paper, we conduct a comprehensive, systematic comparison of fine-grained INT and FP quantization. Our investigation reveals a critical \"crossover point\" in performance. While FP formats hold a distinct advantage in coarse-grained scenarios, we find that INT formats become highly competitive as the block size shrinks, though the benefit depends heavily on the bit width. As granularity becomes finer, the local dynamic range within each block is reduced, allowing the uniform precision of INT formats to become more effective. This trend is analyzed across modern block-wise formats, such as the 32-element blocks in Microscaling (MX) formats or the 16-element blocks in NVIDIA‚Äôs (NV) formats. To enable a direct comparison, we introduce and evaluate integer variants (e.g., MXINT8, MXINT6, MXINT4, NVINT4) alongside their standard FP counterparts (e.g., MXFP8, MXFP6, MXFP4, NVFP4).\\nOur key contributions are as follows:\\nWe develop a theoretical and statistical framework that models the quantization signal-to-noise ratio (QSNR) for both INT and FP formats. This framework enables a direct theoretical comparison of their performance trade-offs and clarifies the crossover points and .\\nWe demonstrate that MXINT8 consistently outperforms MXFP8 in both direct-cast inference and low-bit training. We also show that NVINT4 can surpass NVFP4 when combined with Hadamard rotation. Critically, we introduce a symmetric clipping method that resolves a gradient bias, enabling nearly lossless MXINT8 low-bit training.\\nWe present a comparative hardware cost analysis, demonstrating that fine-grained INT formats are significantly more area and energy-efficient than their floating-point counterparts at matched throughput.\\nCollectively, our findings challenge the prevailing FP-centric trajectory in AI hardware design and advocate for prioritizing fine-grained INT formats to achieve a more optimal balance of accuracy and efficiency in future AI accelerators.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Preliminaries',\n",
       "    'paragraphs': 'Quantization maps a high-precision tensor ùêó \\\\mathbf{X} to a lower bit-width. In this section, we present low-bit integer (INT) quantization, floating-point (FP) quantization, quantization granularity with a focus on fine-grained block-wise schemes, and an overview of existing low-bit block formats.\\nFor b b -bit integer quantization, we define:\\nwhere s s is the scale factor that normalizes ùêó \\\\mathbf{X} to the target integer range, ‚åä ‚ãÖ ‚åâ \\\\lfloor\\\\cdot\\\\rceil is round-to-nearest, and ùêó ùê™ \\\\mathbf{X_{q}} is the dequantized tensor. The clipping ensures that the integer values lie in [ Q min , Q max ] [Q_{\\\\min},Q_{\\\\max}] (e.g., for signed b b -bit integers, Q min = ‚àí 2 b ‚àí 1 Q_{\\\\min}=-2^{b-1} and Q max = 2 b ‚àí 1 ‚àí 1 Q_{\\\\max}=2^{b-1}-1 ).\\nFloating-point representation [ 24 ] uses three fields: the sign bit ( S S ), the exponent ( E E ), and the mantissa ( M M ). We denote a format as E x x M y y , where x x and y y are the numbers of exponent and mantissa bits. The sign determines the polarity, the exponent sets the dynamic range, and the mantissa sets the precision. A floating-point number decodes as:\\nwhere s s , e e , and m m are the sign, exponent and mantissa values of a float-point number. Hence, ‚ÑÇ FP \\\\mathbb{C}_{\\\\text{FP}} denotes the set of representable low-bit floating-point values. Floating-point quantization is:\\nwhere Nearest \\u200b ( ‚ãÖ , ‚ÑÇ FP ) \\\\text{Nearest}(\\\\cdot,\\\\mathbb{C}_{\\\\text{FP}}) maps normalized values to the nearest element of ‚ÑÇ FP \\\\mathbb{C}_{\\\\text{FP}} . Eq. ( 3 ) is a general quantization form that also recovers integer quantization by replacing ‚ÑÇ FP \\\\mathbb{C}_{\\\\text{FP}} with ‚ÑÇ INT \\\\mathbb{C}_{\\\\text{INT}} .\\nQuantization granularity specifies how scale factors apply across a tensor. Finer granularity usually improves accuracy but increases compute and memory overhead due to more scale factors. Common choices are:\\n(i) Per-tensor: a single scale for the entire tensor.\\n(ii) Per-channel: a scale per channel, broadcast along a chosen axis.\\n(iii) Block- k k : the tensor is partitioned into 1 √ó k 1\\\\times k blocks along one dimension, and each block has its own scale.\\nBlock quantization is a key technique for improving accuracy at low precision. In this paper, we mainly focus on block quantization.\\nTo improve low-bit accuracy, OCP [ 34 ] proposes the Microscaling (MX) format, which uses a shared UE8M0 1 1 1 UE8M0 is an 8-bit unsigned floating-point format with eight exponent bits and zero mantissa bits. scale for each block of 32 elements. This fine-grained scaling reduces quantization error. Recently, NVIDIA Blackwell-series GPUs [ 32 ] provide native hardware support for MXFP8/MXFP6/MXFP4. Traditionally, FP8 has E4M3 and E5M2 variants, and FP6 has E2M3 and E3M2 variants. We consider E4M3 for MXFP8 and E2M3 for MXFP6 because mantissa bits are more critical to the performance of fine-grained quantization, consistent with prior work [ 21 , 27 , 34 ] .\\nFurthermore, NVIDIA proposes NVFP4, which enhances MXFP4 by reducing the block size from 32 to 16 and replacing the UE8M0 scale with an E4M3 scale. NVFP4 also introduces a second-level per-tensor scale to prevent overflow of the first-level E4M3 scale.\\nTherefore, current hardware tends to support low-bit fine-grained floating-point formats. To enable fair comparison between low-bit floating-point and integer formats, we also introduce four corresponding integer variants: MXINT8, MXINT6, MXINT4, and NVINT4.\\nDetails of these low-bit formats are listed in Table 1 .',\n",
       "    'subsections': [{'title': '2.1 Low-Precision Integer Formats',\n",
       "      'paragraphs': 'For b b -bit integer quantization, we define:\\nwhere s s is the scale factor that normalizes ùêó \\\\mathbf{X} to the target integer range, ‚åä ‚ãÖ ‚åâ \\\\lfloor\\\\cdot\\\\rceil is round-to-nearest, and ùêó ùê™ \\\\mathbf{X_{q}} is the dequantized tensor. The clipping ensures that the integer values lie in [ Q min , Q max ] [Q_{\\\\min},Q_{\\\\max}] (e.g., for signed b b -bit integers, Q min = ‚àí 2 b ‚àí 1 Q_{\\\\min}=-2^{b-1} and Q max = 2 b ‚àí 1 ‚àí 1 Q_{\\\\max}=2^{b-1}-1 ).',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Low-Precision Floating-Point Formats',\n",
       "      'paragraphs': 'Floating-point representation [ 24 ] uses three fields: the sign bit ( S S ), the exponent ( E E ), and the mantissa ( M M ). We denote a format as E x x M y y , where x x and y y are the numbers of exponent and mantissa bits. The sign determines the polarity, the exponent sets the dynamic range, and the mantissa sets the precision. A floating-point number decodes as:\\nwhere s s , e e , and m m are the sign, exponent and mantissa values of a float-point number. Hence, ‚ÑÇ FP \\\\mathbb{C}_{\\\\text{FP}} denotes the set of representable low-bit floating-point values. Floating-point quantization is:\\nwhere Nearest \\u200b ( ‚ãÖ , ‚ÑÇ FP ) \\\\text{Nearest}(\\\\cdot,\\\\mathbb{C}_{\\\\text{FP}}) maps normalized values to the nearest element of ‚ÑÇ FP \\\\mathbb{C}_{\\\\text{FP}} . Eq. ( 3 ) is a general quantization form that also recovers integer quantization by replacing ‚ÑÇ FP \\\\mathbb{C}_{\\\\text{FP}} with ‚ÑÇ INT \\\\mathbb{C}_{\\\\text{INT}} .',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3 Quantization Granularity',\n",
       "      'paragraphs': 'Quantization granularity specifies how scale factors apply across a tensor. Finer granularity usually improves accuracy but increases compute and memory overhead due to more scale factors. Common choices are:\\n(i) Per-tensor: a single scale for the entire tensor.\\n(ii) Per-channel: a scale per channel, broadcast along a chosen axis.\\n(iii) Block- k k : the tensor is partitioned into 1 √ó k 1\\\\times k blocks along one dimension, and each block has its own scale.\\nBlock quantization is a key technique for improving accuracy at low precision. In this paper, we mainly focus on block quantization.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.4 Block-Quantization Formats',\n",
       "      'paragraphs': 'To improve low-bit accuracy, OCP [ 34 ] proposes the Microscaling (MX) format, which uses a shared UE8M0 1 1 1 UE8M0 is an 8-bit unsigned floating-point format with eight exponent bits and zero mantissa bits. scale for each block of 32 elements. This fine-grained scaling reduces quantization error. Recently, NVIDIA Blackwell-series GPUs [ 32 ] provide native hardware support for MXFP8/MXFP6/MXFP4. Traditionally, FP8 has E4M3 and E5M2 variants, and FP6 has E2M3 and E3M2 variants. We consider E4M3 for MXFP8 and E2M3 for MXFP6 because mantissa bits are more critical to the performance of fine-grained quantization, consistent with prior work [ 21 , 27 , 34 ] .\\nFurthermore, NVIDIA proposes NVFP4, which enhances MXFP4 by reducing the block size from 32 to 16 and replacing the UE8M0 scale with an E4M3 scale. NVFP4 also introduces a second-level per-tensor scale to prevent overflow of the first-level E4M3 scale.\\nTherefore, current hardware tends to support low-bit fine-grained floating-point formats. To enable fair comparison between low-bit floating-point and integer formats, we also introduce four corresponding integer variants: MXINT8, MXINT6, MXINT4, and NVINT4.\\nDetails of these low-bit formats are listed in Table 1 .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Quantization Recipe',\n",
       "    'paragraphs': 'This section illustrates the computation flow for low-bit inference and training in Sec. 3.1 , and details the scale-factor computation used in quantization in Sec. 3.2 .\\nFigure 1 shows an example of using low-bit GEMM in a linear layer during forward and backward propagation. Given high-precision (e.g., BFloat16) activations ùêó \\\\mathbf{X} and weights ùêñ \\\\mathbf{W} , the forward pass of the quantized linear layer 2 2 2 We omit the bias term. is:\\nThe backward pass to compute d \\u200b ùêó d\\\\mathbf{X} and d \\u200b ùêñ d\\\\mathbf{W} is:\\nQuantize \\u200b ( ‚ãÖ ) \\\\text{Quantize}(\\\\cdot) maps high-precision tensors to low-bit representations. Thus, there are six quantization operations in one linear layer: ‚ë† ùêó \\\\mathbf{X} and ‚ë° ùêñ \\\\mathbf{W} in Eq. ( 4 ); ‚ë¢ ùêùùêò \\\\mathbf{dY} and ‚ë£ ùêñ T \\\\mathbf{W}^{T} in Eq. ( 5 ); ‚ë§ ùêó T \\\\mathbf{X}^{T} and ‚ë• ùêùùêò T \\\\mathbf{dY}^{T} in Eq. ( 6 ). Block-wise quantization requires tensors to be quantized along the GEMM reduction dimension to gain hardware benefits. Therefore, ‚ë† and ‚ë§, ‚ë° and ‚ë£, and ‚ë¢ and ‚ë• are quantized along different axes [ 21 , 11 ] . We separately analyze the quantization error of these six operations in Sec. 5.1 .\\nUE8M0 scale factor. The scale factor s s in Eq. ( 1 ) and Eq. ( 3 ) is computed with the AbsMax quantizer:\\nwhere AbsMax \\u200b ( ùêó ) \\\\text{AbsMax}(\\\\mathbf{X}) is the maximum absolute value within the group of values that share a single scale factor, and Q m \\u200b a \\u200b x Q_{max} is the maximum value of the quantized type (see Table 1 ). Eq. ( 7 ) maps the largest magnitude in high precision to the maximum representable low-precision value without clipping. OCP [ 34 ] further converts the high-precision scale factor to the UE8M0 format for MX formats:\\nwhere ‚åä ‚ãÖ ‚åã \\\\lfloor\\\\cdot\\\\rfloor denotes rounding down. Eq. ( 8 ) rounds the high-precision scale down to the nearest UE8M0 value, which introduces extra clipping error. Following existing works [ 39 , 9 , 27 ] , we round up the UE8M0 scale based on Eq. ( 7 ) to avoid this error:\\nwhere ‚åà ‚ãÖ ‚åâ \\\\lceil\\\\cdot\\\\rceil denotes rounding up.\\nSymmetric Clipping. Floating-point formats are naturally symmetric around zero. In contrast, signed integers in two‚Äôs complement have one extra negative value: for a b b -bit integer, Q m \\u200b i \\u200b n = ‚àí 2 b ‚àí 1 Q_{min}=-2^{b-1} and Q m \\u200b a \\u200b x = 2 b ‚àí 1 ‚àí 1 Q_{max}=2^{b-1}-1 [ 32 ] . We find that this asymmetric range usually does not affect inference. However, as shown in Figure 2 , it degrades INT8 training due to a persistent negative bias in gradients. Finer-grained quantization suffers more because more values fall into the unique negative endpoint Q m \\u200b i \\u200b n Q_{min} . For INT8, the minimum value in a group can still map to ‚àí 128 -128 even when the scale is set to AbsMax \\u200b ( ùêó ) / 127 \\\\text{AbsMax}(\\\\mathbf{X})/127 due to BFloat16 arithmetic precision (see Sec. 11.2 for details).\\nTherefore, we use a symmetric integer range for all INT quantizers as shown in Table 1 :\\nIn this section, we analyze low-bit integer and floating-point formats and build a theoretical framework for comparing them. Section 4.1 derives theorems for the quantization signal-to-noise ratio (QSNR), and Section 4.2 compares low-bit formats based on the theoretical QSNR.',\n",
       "    'subsections': [{'title': '3.1 Quantization Compute Flow',\n",
       "      'paragraphs': 'Figure 1 shows an example of using low-bit GEMM in a linear layer during forward and backward propagation. Given high-precision (e.g., BFloat16) activations ùêó \\\\mathbf{X} and weights ùêñ \\\\mathbf{W} , the forward pass of the quantized linear layer 2 2 2 We omit the bias term. is:\\nThe backward pass to compute d \\u200b ùêó d\\\\mathbf{X} and d \\u200b ùêñ d\\\\mathbf{W} is:\\nQuantize \\u200b ( ‚ãÖ ) \\\\text{Quantize}(\\\\cdot) maps high-precision tensors to low-bit representations. Thus, there are six quantization operations in one linear layer: ‚ë† ùêó \\\\mathbf{X} and ‚ë° ùêñ \\\\mathbf{W} in Eq. ( 4 ); ‚ë¢ ùêùùêò \\\\mathbf{dY} and ‚ë£ ùêñ T \\\\mathbf{W}^{T} in Eq. ( 5 ); ‚ë§ ùêó T \\\\mathbf{X}^{T} and ‚ë• ùêùùêò T \\\\mathbf{dY}^{T} in Eq. ( 6 ). Block-wise quantization requires tensors to be quantized along the GEMM reduction dimension to gain hardware benefits. Therefore, ‚ë† and ‚ë§, ‚ë° and ‚ë£, and ‚ë¢ and ‚ë• are quantized along different axes [ 21 , 11 ] . We separately analyze the quantization error of these six operations in Sec. 5.1 .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Quantization Operation',\n",
       "      'paragraphs': 'UE8M0 scale factor. The scale factor s s in Eq. ( 1 ) and Eq. ( 3 ) is computed with the AbsMax quantizer:\\nwhere AbsMax \\u200b ( ùêó ) \\\\text{AbsMax}(\\\\mathbf{X}) is the maximum absolute value within the group of values that share a single scale factor, and Q m \\u200b a \\u200b x Q_{max} is the maximum value of the quantized type (see Table 1 ). Eq. ( 7 ) maps the largest magnitude in high precision to the maximum representable low-precision value without clipping. OCP [ 34 ] further converts the high-precision scale factor to the UE8M0 format for MX formats:\\nwhere ‚åä ‚ãÖ ‚åã \\\\lfloor\\\\cdot\\\\rfloor denotes rounding down. Eq. ( 8 ) rounds the high-precision scale down to the nearest UE8M0 value, which introduces extra clipping error. Following existing works [ 39 , 9 , 27 ] , we round up the UE8M0 scale based on Eq. ( 7 ) to avoid this error:\\nwhere ‚åà ‚ãÖ ‚åâ \\\\lceil\\\\cdot\\\\rceil denotes rounding up.\\nSymmetric Clipping. Floating-point formats are naturally symmetric around zero. In contrast, signed integers in two‚Äôs complement have one extra negative value: for a b b -bit integer, Q m \\u200b i \\u200b n = ‚àí 2 b ‚àí 1 Q_{min}=-2^{b-1} and Q m \\u200b a \\u200b x = 2 b ‚àí 1 ‚àí 1 Q_{max}=2^{b-1}-1 [ 32 ] . We find that this asymmetric range usually does not affect inference. However, as shown in Figure 2 , it degrades INT8 training due to a persistent negative bias in gradients. Finer-grained quantization suffers more because more values fall into the unique negative endpoint Q m \\u200b i \\u200b n Q_{min} . For INT8, the minimum value in a group can still map to ‚àí 128 -128 even when the scale is set to AbsMax \\u200b ( ùêó ) / 127 \\\\text{AbsMax}(\\\\mathbf{X})/127 due to BFloat16 arithmetic precision (see Sec. 11.2 for details).\\nTherefore, we use a symmetric integer range for all INT quantizers as shown in Table 1 :\\nIn this section, we analyze low-bit integer and floating-point formats and build a theoretical framework for comparing them. Section 4.1 derives theorems for the quantization signal-to-noise ratio (QSNR), and Section 4.2 compares low-bit formats based on the theoretical QSNR.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Theoretical Framework',\n",
       "    'paragraphs': 'QSNR Metric. We use the Quantization Signal-to-Noise Ratio (QSNR, dB) [ 11 ] to measure numerical fidelity under different quantization schemes. QSNR is the ratio of the power of the original signal ùêó \\\\mathbf{X} to the power of the quantization noise ùêó ‚àí ùêó q \\\\mathbf{X}-\\\\mathbf{X}_{q} , expressed in decibels:\\nA higher QSNR means the quantized vector better preserves the magnitude and direction of the original vector.\\nCommon assumptions. We consider block vectors ùêó ‚àà ‚Ñù k \\\\mathbf{X}\\\\in\\\\mathbb{R}^{k} with i.i.d. entries X i ‚àº ùí© \\u200b ( 0 , œÉ 2 ) X_{i}\\\\sim\\\\mathcal{N}(0,\\\\sigma^{2}) . The block root-mean-square (RMS) equals œÉ \\\\sigma , and the crest factor is\\nWe use blockwise absolute-maximum (AbsMax) scaling:\\nwhere s s is the high-precision scale from Eq. ( 7 ), and œÅ \\\\rho models the overhead of the low-precision scale. For example, the UE8M0 scale in Eq. ( 9 ) has œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) , while for the E4M3 scale in NV-format we set œÅ = 1 \\\\rho=1 since it is close to BFloat16 scales.\\nTheorem 1 (INT QSNR). Under b b -bit INT quantization, the QSNR (in dB) is\\nA detailed proof of Theorem 1 appears in Sec. 9.2 , where b b is the bit width, œÅ \\\\rho is the scale overhead, Œ∫ \\\\kappa is the crest factor in Eq. ( 11 ), and g g is the block size.\\nInterpretation of Theorem 1. (i) Each extra bit gives ‚âà 6.02 \\\\approx 6.02 dB.\\n(ii) UE8M0 scaling incurs up to 20 \\u200b log 10 \\u2061 ( œÅ ) ‚â§ 6.02 20\\\\log_{10}(\\\\rho)\\\\leq 6.02 dB loss.\\n(iii) A larger crest factor Œ∫ \\\\kappa reduces QSNR; smaller blocks usually reduce Œ∫ \\\\kappa and improve QSNR.\\n(iv) E4M3 scaling has no œÅ \\\\rho overhead and avoids the per-block maximum error, giving a 10 \\u200b log 10 \\u2061 ( g g ‚àí 1 ) 10\\\\log_{10}\\\\!\\\\left(\\\\dfrac{g}{g-1}\\\\right) QSNR gain.\\nTheorem 2 (FP QSNR). Under FP quantization, the QSNR (in dB) is\\nA detailed proof of Theorem 2 appears in Sec. 9.3 , with Œ± M = 1 24 ‚ãÖ 2 2 \\u200b M \\\\alpha_{M}=\\\\frac{1}{24\\\\cdot 2^{2M}} (mantissa resolution term) and Œ≤ = 2 2 \\u200b ( 1 ‚àí B ‚àí M ) 12 \\u200b Q max 2 \\\\beta=\\\\frac{2^{2(1-B-M)}}{12\\\\,Q_{\\\\max}^{2}} . Here M M is the mantissa bit width, B B is the exponent bias, and Q max Q_{\\\\max} is the largest finite normal magnitude of the target FP format (e.g., Q max = 448 Q_{\\\\max}=448 for E4M3). The terms w norm w_{\\\\mathrm{norm}} and p sub p_{\\\\mathrm{sub}} measure how much of the distribution falls into the normal and subnormal regions (after scaling): w norm w_{\\\\mathrm{norm}} is the fraction of signal energy carried by normal FP numbers and incurs mantissa quantization error Œ± M \\\\alpha_{M} ; p sub p_{\\\\mathrm{sub}} is the probability that a value encodes as subnormal and incurs a fixed absolute step error.\\nInterpretation of Theorem 2. (i) The mantissa bit width sets the upper bound on FP QSNR. With ample dynamic range ( w norm ‚âà 1 w_{\\\\mathrm{norm}}\\\\approx 1 and p sub ‚âà 0 p_{\\\\mathrm{sub}}\\\\approx 0 ), QSNR ‚âà 13.80 + 6.02 \\u200b M \\\\mathrm{QSNR}\\\\approx 13.80+6.02\\\\,M dB, independent of block granularity and the distribution of ùêó \\\\mathbf{X} .\\n(ii) A larger crest factor Œ∫ \\\\kappa increases the share of subnormals and reduces QSNR. Finer-grained blocks reduce Œ∫ \\\\kappa , lower p sub p_{\\\\mathrm{sub}} , and improve QSNR.\\n(iii) E4M3 scaling has no œÅ \\\\rho overhead and avoids the per-block maximum error, reducing Œ∫ 2 g \\\\frac{\\\\kappa^{2}}{g} error energy in the normal region.\\nWith Eq. ( 13 ) in Theorem 1 and Eq. ( 14 ) in Theorem 2, we estimate the QSNR of low-bit integer and floating-point formats for a given bit width and target distribution (via Œ∫ \\\\kappa ). Specifically, we set œÅ = 1.5 \\\\rho=1.5 to imitate UE8M0 scale. As shown in Figure 3 , we observe:\\nMXINT8 vs. MXFP8 : MXFP8 QSNR varies smoothly due to its ample dynamic range. MXINT8 outperforms FP8 when Œ∫ < 7.55 \\\\kappa<7.55 .\\nMXINT6 vs. MXFP6 : MXFP6 has the same QSNR as MXFP8 at small Œ∫ \\\\kappa , because both MXFP6 and MXFP8 have three mantissa bits. However, FP6 QSNR decreases rapidly as Œ∫ \\\\kappa increases due to limited dynamic range. MXINT6 outperforms MXFP6 only when Œ∫ < 1.96 \\\\kappa<1.96 .\\nMXINT4 vs. MXFP4 : MXINT4 outperforms MXFP4 when Œ∫ < 2.04 \\\\kappa<2.04 .\\nNVINT4 vs. NVFP4 : NVINT4 outperforms NVFP4 when Œ∫ < 2.39 \\\\kappa<2.39 . One interesting phenomenon is that NVFP4‚Äôs QSNR even increase when Œ∫ < 4 \\\\kappa<4 , this can be explained by Eq ( 14 ) that larger Œ∫ \\\\kappa can decrease the error of normal domain but increase the error of subnormal domain. In the relatively small Œ∫ \\\\kappa ( Œ∫ < 4 \\\\kappa<4 ), normal domain dominate the error so that NVFP4‚Äô QSNR can increase when Œ∫ < 4 \\\\kappa<4 .\\nTherefore, the key factor when comparing FP and INT formats is the data‚Äôs crest factor Œ∫ \\\\kappa .',\n",
       "    'subsections': [{'title': '4.1 Theoretical QSNR',\n",
       "      'paragraphs': 'QSNR Metric. We use the Quantization Signal-to-Noise Ratio (QSNR, dB) [ 11 ] to measure numerical fidelity under different quantization schemes. QSNR is the ratio of the power of the original signal ùêó \\\\mathbf{X} to the power of the quantization noise ùêó ‚àí ùêó q \\\\mathbf{X}-\\\\mathbf{X}_{q} , expressed in decibels:\\nA higher QSNR means the quantized vector better preserves the magnitude and direction of the original vector.\\nCommon assumptions. We consider block vectors ùêó ‚àà ‚Ñù k \\\\mathbf{X}\\\\in\\\\mathbb{R}^{k} with i.i.d. entries X i ‚àº ùí© \\u200b ( 0 , œÉ 2 ) X_{i}\\\\sim\\\\mathcal{N}(0,\\\\sigma^{2}) . The block root-mean-square (RMS) equals œÉ \\\\sigma , and the crest factor is\\nWe use blockwise absolute-maximum (AbsMax) scaling:\\nwhere s s is the high-precision scale from Eq. ( 7 ), and œÅ \\\\rho models the overhead of the low-precision scale. For example, the UE8M0 scale in Eq. ( 9 ) has œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) , while for the E4M3 scale in NV-format we set œÅ = 1 \\\\rho=1 since it is close to BFloat16 scales.\\nTheorem 1 (INT QSNR). Under b b -bit INT quantization, the QSNR (in dB) is\\nA detailed proof of Theorem 1 appears in Sec. 9.2 , where b b is the bit width, œÅ \\\\rho is the scale overhead, Œ∫ \\\\kappa is the crest factor in Eq. ( 11 ), and g g is the block size.\\nInterpretation of Theorem 1. (i) Each extra bit gives ‚âà 6.02 \\\\approx 6.02 dB.\\n(ii) UE8M0 scaling incurs up to 20 \\u200b log 10 \\u2061 ( œÅ ) ‚â§ 6.02 20\\\\log_{10}(\\\\rho)\\\\leq 6.02 dB loss.\\n(iii) A larger crest factor Œ∫ \\\\kappa reduces QSNR; smaller blocks usually reduce Œ∫ \\\\kappa and improve QSNR.\\n(iv) E4M3 scaling has no œÅ \\\\rho overhead and avoids the per-block maximum error, giving a 10 \\u200b log 10 \\u2061 ( g g ‚àí 1 ) 10\\\\log_{10}\\\\!\\\\left(\\\\dfrac{g}{g-1}\\\\right) QSNR gain.\\nTheorem 2 (FP QSNR). Under FP quantization, the QSNR (in dB) is\\nA detailed proof of Theorem 2 appears in Sec. 9.3 , with Œ± M = 1 24 ‚ãÖ 2 2 \\u200b M \\\\alpha_{M}=\\\\frac{1}{24\\\\cdot 2^{2M}} (mantissa resolution term) and Œ≤ = 2 2 \\u200b ( 1 ‚àí B ‚àí M ) 12 \\u200b Q max 2 \\\\beta=\\\\frac{2^{2(1-B-M)}}{12\\\\,Q_{\\\\max}^{2}} . Here M M is the mantissa bit width, B B is the exponent bias, and Q max Q_{\\\\max} is the largest finite normal magnitude of the target FP format (e.g., Q max = 448 Q_{\\\\max}=448 for E4M3). The terms w norm w_{\\\\mathrm{norm}} and p sub p_{\\\\mathrm{sub}} measure how much of the distribution falls into the normal and subnormal regions (after scaling): w norm w_{\\\\mathrm{norm}} is the fraction of signal energy carried by normal FP numbers and incurs mantissa quantization error Œ± M \\\\alpha_{M} ; p sub p_{\\\\mathrm{sub}} is the probability that a value encodes as subnormal and incurs a fixed absolute step error.\\nInterpretation of Theorem 2. (i) The mantissa bit width sets the upper bound on FP QSNR. With ample dynamic range ( w norm ‚âà 1 w_{\\\\mathrm{norm}}\\\\approx 1 and p sub ‚âà 0 p_{\\\\mathrm{sub}}\\\\approx 0 ), QSNR ‚âà 13.80 + 6.02 \\u200b M \\\\mathrm{QSNR}\\\\approx 13.80+6.02\\\\,M dB, independent of block granularity and the distribution of ùêó \\\\mathbf{X} .\\n(ii) A larger crest factor Œ∫ \\\\kappa increases the share of subnormals and reduces QSNR. Finer-grained blocks reduce Œ∫ \\\\kappa , lower p sub p_{\\\\mathrm{sub}} , and improve QSNR.\\n(iii) E4M3 scaling has no œÅ \\\\rho overhead and avoids the per-block maximum error, reducing Œ∫ 2 g \\\\frac{\\\\kappa^{2}}{g} error energy in the normal region.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Theoretical Comparisons',\n",
       "      'paragraphs': 'With Eq. ( 13 ) in Theorem 1 and Eq. ( 14 ) in Theorem 2, we estimate the QSNR of low-bit integer and floating-point formats for a given bit width and target distribution (via Œ∫ \\\\kappa ). Specifically, we set œÅ = 1.5 \\\\rho=1.5 to imitate UE8M0 scale. As shown in Figure 3 , we observe:\\nMXINT8 vs. MXFP8 : MXFP8 QSNR varies smoothly due to its ample dynamic range. MXINT8 outperforms FP8 when Œ∫ < 7.55 \\\\kappa<7.55 .\\nMXINT6 vs. MXFP6 : MXFP6 has the same QSNR as MXFP8 at small Œ∫ \\\\kappa , because both MXFP6 and MXFP8 have three mantissa bits. However, FP6 QSNR decreases rapidly as Œ∫ \\\\kappa increases due to limited dynamic range. MXINT6 outperforms MXFP6 only when Œ∫ < 1.96 \\\\kappa<1.96 .\\nMXINT4 vs. MXFP4 : MXINT4 outperforms MXFP4 when Œ∫ < 2.04 \\\\kappa<2.04 .\\nNVINT4 vs. NVFP4 : NVINT4 outperforms NVFP4 when Œ∫ < 2.39 \\\\kappa<2.39 . One interesting phenomenon is that NVFP4‚Äôs QSNR even increase when Œ∫ < 4 \\\\kappa<4 , this can be explained by Eq ( 14 ) that larger Œ∫ \\\\kappa can decrease the error of normal domain but increase the error of subnormal domain. In the relatively small Œ∫ \\\\kappa ( Œ∫ < 4 \\\\kappa<4 ), normal domain dominate the error so that NVFP4‚Äô QSNR can increase when Œ∫ < 4 \\\\kappa<4 .\\nTherefore, the key factor when comparing FP and INT formats is the data‚Äôs crest factor Œ∫ \\\\kappa .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 FP v.s. INT',\n",
       "    'paragraphs': 'We compare low-bit integer and floating-point formats at three levels. Section 5.1 analyzes the crest factor and QSNR for six types of intermediate tensors in Figure 1 , offering a tensor-level perspective. Section 5.2 evaluates direct-cast inference, quantizing only the forward process. Section 5.3 presents results for low-bit training, quantizing both forward and backward processes.\\nSetup. To measure the QSNR in real data, we feed 8 WikiText2 [ 25 ] sequences of length 4096 into Llama-3.1-8B, run both forward and backward propagation in BFloat16 precision, and capture the six intermediate tensors (weights, activations, and gradients) indicated by ‚ë†‚Äì‚ë• in Figure 1 . Llama-3.1-8B contains 224 linear layers across all transformer blocks. We collect these tensors for all 224 linear layers, leads totally 224 √ó 6 = 10752 224\\\\times 6=10752 tensors, and use them to compute the crest factors under different block size and QSNR under different low-bits formats. Specifically, QSNR is directly calculated tensor-wise, and crest factor is calculated block-wise and than average across the tensor. Additonally, we also apply random hadamard rotation [ 2 ] with dimension as 32 √ó 32 32\\\\times 32 to measure the effectiveness of such outlier surpression technical to crest factor and QSNR.\\nCrest factor results. Table 2 reports crest factor statistics in boxplot form. We focus on the 75% quantile ( i.e. , Q3) , which reflects typical worst-case behavior across 75% of cases.\\nFor channel-wise quantization (block size ‚àí 1 -1 ), Q3 is 11.97 11.97 , which is far above the crossover point in Figure 3 . This indicates that FP outperforms INT in most cases with coarse granularity.\\nFor the MX-format with block size 32 32 , Q3 is 2.96 2.96 . This value is well below the MXINT8 v.s. MXFP8 crossover point ( 7.55 7.55 ), so MXINT8 outperforms MXFP8 in most cases. In contrast, 2.96 2.96 is above the MXINT6 v.s. MXFP6 and MXINT4 v.s. MXFP4 crossover points ( 1.96 1.96 and 2.04 2.04 ), so MXINT6 and MXINT4 underperform their FP counterparts. After Hadamard rotation, Q3 decreases from 2.96 2.96 to 2.39 2.39 , which remains below 7.55 7.55 but above 1.96 1.96 and 2.04 2.04 ; thus, MXINT8 still wins, while MXINT6 and MXINT4 still lag behind MXFP6 and MXFP4.\\nFor the NV-format with block size 16 16 , Q3 is 2.39 2.39 , which equals the NVINT4 v.s. NVFP4 crossover point and then decreases to 2.11 2.11 after Hadamard rotation, favoring NVINT4 over NVFP4 post-rotation.\\nCrest factor v.s. QSNR results. Figure 4 reports measured QSNR across crest factors. The empirical trends closely follow the theoretical comparisons in Sec. 4 (Theorems 1‚Äì2) and the aforementioned crest factor reults:\\nMXINT8 v.s. MXFP8 : The QSNR of MXFP8 is nearly constant at 31.50 31.50 because of its large dynamic range and mantissa-bit bound. MXINT8 has an average QSNR of 40.35 40.35 , and thus significantly outperforms MXFP8.\\nMXINT6 v.s. MXFP6 and MXINT4 v.s. MXFP4 : MXINT6 and MXINT4 consistently lag behind MXFP6 and MXFP4, with or without random Hadamard rotation.\\nNVINT4 v.s. NVFP4 : Although the win rate of NVINT4 is 64.3 % 64.3\\\\% , its average QSNR is 20.55 20.55 , which is slightly below NVFP4‚Äôs 20.60 20.60 because NVINT4‚Äôs QSNR decreases faster than NVFP4‚Äôs as the crest factor increases. After random Hadamard rotation, NVINT4‚Äôs average QSNR rises to 21.65 21.65 , surpassing NVFP4‚Äôs 20.35 20.35 . Note that NVFP4‚Äôs QSNR decreases from 20.60 20.60 to 20.35 20.35 after rotation, which is consistent with Figure 3 : rotation reduces the crest factor, and when the crest factor is below 4 4 , NVFP4‚Äôs QSNR increases with the crest factor, so a reduction in crest factor lowers its QSNR.\\nOverall, real-data measurements corroborate the theory in Sec. 4 .\\nPrecisions. For inference, we compare the formats in Table 1 : MXFP8, MXINT8, MXFP6, MXINT6, MXFP4, MXINT4, NVFP4, and NVINT4. We perform direct-cast inference from a pretrained BFloat16 model and quantize all forward GEMMs.\\nModels. We evaluate 12 LLMs covering dense and Mixture-of-Experts (MoE) architectures, from 0.6B to 235B parameters: Qwen3-0.6B/1.7B/4B/8B/14B/32B/30B-A3B/235B-A22B [ 42 ] , Llama-3.1-8B/70B, and Llama-3.2-1B/3B [ 13 ] . We also apply random Hadamard rotation and quantize ùêóùêë \\\\mathbf{X}\\\\mathbf{R} and ùêë ‚ä§ \\u200b ùêñ \\\\mathbf{R}^{\\\\top}\\\\mathbf{W} , where ùêë \\\\mathbf{R} is a random Hadamard matrix of size h √ó h h\\\\times h . We set h h to the block size (32 for MX formats and 16 for NV formats). We provide official open-source links in Sec. 11 .\\nMetrics. Our goal is to compare integer and floating-point low-bit formats under the same settings, so ranking is more informative than absolute accuracy. Following [ 14 ] , accuracy alone is not sufficient for compressed models because it can hide large behavioral changes. We therefore use distance metrics: specifically, we compute the KL divergence on WikiText2 [ 25 ] between each quantized model and its BFloat16 counterpart. To reduce noise, we compute the divergence over the softmax distribution restricted to the top-25 logits of the BFloat16 model.\\nResults. Table 3 summarizes the comparison between FP and INT formats. Without rotation, MXINT8 outperforms MXFP8 on all 12 models, while MXINT6, MXINT4, and NVINT4 perform worse than MXFP6, MXFP4, and NVFP4. Although NVINT4 and NVFP4 have similar average QSNR in Figure 4(a) , NVINT4 loses more often because higher crest factors create more worst-case behavior for integers. With random Hadamard rotation, MXINT8 and NVINT4 win on all 12 models; MXINT6 wins 1 of 12 and MXINT4 loses all 12, consistent with the tensor-wise analysis in Sec. 5.1 .\\nPrecisions. For training, we focus on nearly lossless low-bit training, which is more practical. Therefore, we study only the 8-bit setting and compare MXINT8 and MXFP8, since FP8 training is demonstrated to be nearly lossless in prior work [ 27 , 21 ] .\\nModels and datasets. We train 1B and 3B Llama3-style [ 13 ] models on the OLMo2-Mix-1124 [ 33 ] pretraining dataset, with 100B and 200B training tokens, respectively. Detailed model architectures and training hyperparameters are in Sec. 11 .\\nMetrics. We measure training performance using two metrics: training loss and task accuracy. We smooth the training loss with an exponential moving average (coefficient 0.9 0.9 ). We compute all accuracies with lm_eval [ 17 ] through 5-shot evaluation. We report acc for WinoGrande [ 35 ] and acc_norm for HellaSwag [ 44 ] , Arc_Challenge, Arc_Easy [ 10 ] , PIQA [ 4 ] , and Openbookqa [ 26 ] .\\nResults. Figure 5 shows the loss curves for BF16, MXFP8, and MXINT8 training. The curves for MXFP8 and MXINT8 almost overlap with BF16. In addition, MXINT8 consistently outperforms MXFP8 with a loss that is lower by approximately 0.001 0.001 , as shown in the enlarged view in Figure 5 . Table 4 shows that MXINT8 also achieves nearly the same average accuracy across six common-sense reasoning tasks compared to BF16 training. These results demonstrate that MXINT8 supports nearly lossless low-bit training, while existing works [ 21 , 27 ] mainly focus on FP8 training.',\n",
       "    'subsections': [{'title': '5.1 Tensor-wise Analysis',\n",
       "      'paragraphs': 'Setup. To measure the QSNR in real data, we feed 8 WikiText2 [ 25 ] sequences of length 4096 into Llama-3.1-8B, run both forward and backward propagation in BFloat16 precision, and capture the six intermediate tensors (weights, activations, and gradients) indicated by ‚ë†‚Äì‚ë• in Figure 1 . Llama-3.1-8B contains 224 linear layers across all transformer blocks. We collect these tensors for all 224 linear layers, leads totally 224 √ó 6 = 10752 224\\\\times 6=10752 tensors, and use them to compute the crest factors under different block size and QSNR under different low-bits formats. Specifically, QSNR is directly calculated tensor-wise, and crest factor is calculated block-wise and than average across the tensor. Additonally, we also apply random hadamard rotation [ 2 ] with dimension as 32 √ó 32 32\\\\times 32 to measure the effectiveness of such outlier surpression technical to crest factor and QSNR.\\nCrest factor results. Table 2 reports crest factor statistics in boxplot form. We focus on the 75% quantile ( i.e. , Q3) , which reflects typical worst-case behavior across 75% of cases.\\nFor channel-wise quantization (block size ‚àí 1 -1 ), Q3 is 11.97 11.97 , which is far above the crossover point in Figure 3 . This indicates that FP outperforms INT in most cases with coarse granularity.\\nFor the MX-format with block size 32 32 , Q3 is 2.96 2.96 . This value is well below the MXINT8 v.s. MXFP8 crossover point ( 7.55 7.55 ), so MXINT8 outperforms MXFP8 in most cases. In contrast, 2.96 2.96 is above the MXINT6 v.s. MXFP6 and MXINT4 v.s. MXFP4 crossover points ( 1.96 1.96 and 2.04 2.04 ), so MXINT6 and MXINT4 underperform their FP counterparts. After Hadamard rotation, Q3 decreases from 2.96 2.96 to 2.39 2.39 , which remains below 7.55 7.55 but above 1.96 1.96 and 2.04 2.04 ; thus, MXINT8 still wins, while MXINT6 and MXINT4 still lag behind MXFP6 and MXFP4.\\nFor the NV-format with block size 16 16 , Q3 is 2.39 2.39 , which equals the NVINT4 v.s. NVFP4 crossover point and then decreases to 2.11 2.11 after Hadamard rotation, favoring NVINT4 over NVFP4 post-rotation.\\nCrest factor v.s. QSNR results. Figure 4 reports measured QSNR across crest factors. The empirical trends closely follow the theoretical comparisons in Sec. 4 (Theorems 1‚Äì2) and the aforementioned crest factor reults:\\nMXINT8 v.s. MXFP8 : The QSNR of MXFP8 is nearly constant at 31.50 31.50 because of its large dynamic range and mantissa-bit bound. MXINT8 has an average QSNR of 40.35 40.35 , and thus significantly outperforms MXFP8.\\nMXINT6 v.s. MXFP6 and MXINT4 v.s. MXFP4 : MXINT6 and MXINT4 consistently lag behind MXFP6 and MXFP4, with or without random Hadamard rotation.\\nNVINT4 v.s. NVFP4 : Although the win rate of NVINT4 is 64.3 % 64.3\\\\% , its average QSNR is 20.55 20.55 , which is slightly below NVFP4‚Äôs 20.60 20.60 because NVINT4‚Äôs QSNR decreases faster than NVFP4‚Äôs as the crest factor increases. After random Hadamard rotation, NVINT4‚Äôs average QSNR rises to 21.65 21.65 , surpassing NVFP4‚Äôs 20.35 20.35 . Note that NVFP4‚Äôs QSNR decreases from 20.60 20.60 to 20.35 20.35 after rotation, which is consistent with Figure 3 : rotation reduces the crest factor, and when the crest factor is below 4 4 , NVFP4‚Äôs QSNR increases with the crest factor, so a reduction in crest factor lowers its QSNR.\\nOverall, real-data measurements corroborate the theory in Sec. 4 .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Direct-Cast Inference',\n",
       "      'paragraphs': 'Precisions. For inference, we compare the formats in Table 1 : MXFP8, MXINT8, MXFP6, MXINT6, MXFP4, MXINT4, NVFP4, and NVINT4. We perform direct-cast inference from a pretrained BFloat16 model and quantize all forward GEMMs.\\nModels. We evaluate 12 LLMs covering dense and Mixture-of-Experts (MoE) architectures, from 0.6B to 235B parameters: Qwen3-0.6B/1.7B/4B/8B/14B/32B/30B-A3B/235B-A22B [ 42 ] , Llama-3.1-8B/70B, and Llama-3.2-1B/3B [ 13 ] . We also apply random Hadamard rotation and quantize ùêóùêë \\\\mathbf{X}\\\\mathbf{R} and ùêë ‚ä§ \\u200b ùêñ \\\\mathbf{R}^{\\\\top}\\\\mathbf{W} , where ùêë \\\\mathbf{R} is a random Hadamard matrix of size h √ó h h\\\\times h . We set h h to the block size (32 for MX formats and 16 for NV formats). We provide official open-source links in Sec. 11 .\\nMetrics. Our goal is to compare integer and floating-point low-bit formats under the same settings, so ranking is more informative than absolute accuracy. Following [ 14 ] , accuracy alone is not sufficient for compressed models because it can hide large behavioral changes. We therefore use distance metrics: specifically, we compute the KL divergence on WikiText2 [ 25 ] between each quantized model and its BFloat16 counterpart. To reduce noise, we compute the divergence over the softmax distribution restricted to the top-25 logits of the BFloat16 model.\\nResults. Table 3 summarizes the comparison between FP and INT formats. Without rotation, MXINT8 outperforms MXFP8 on all 12 models, while MXINT6, MXINT4, and NVINT4 perform worse than MXFP6, MXFP4, and NVFP4. Although NVINT4 and NVFP4 have similar average QSNR in Figure 4(a) , NVINT4 loses more often because higher crest factors create more worst-case behavior for integers. With random Hadamard rotation, MXINT8 and NVINT4 win on all 12 models; MXINT6 wins 1 of 12 and MXINT4 loses all 12, consistent with the tensor-wise analysis in Sec. 5.1 .',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Training',\n",
       "      'paragraphs': 'Precisions. For training, we focus on nearly lossless low-bit training, which is more practical. Therefore, we study only the 8-bit setting and compare MXINT8 and MXFP8, since FP8 training is demonstrated to be nearly lossless in prior work [ 27 , 21 ] .\\nModels and datasets. We train 1B and 3B Llama3-style [ 13 ] models on the OLMo2-Mix-1124 [ 33 ] pretraining dataset, with 100B and 200B training tokens, respectively. Detailed model architectures and training hyperparameters are in Sec. 11 .\\nMetrics. We measure training performance using two metrics: training loss and task accuracy. We smooth the training loss with an exponential moving average (coefficient 0.9 0.9 ). We compute all accuracies with lm_eval [ 17 ] through 5-shot evaluation. We report acc for WinoGrande [ 35 ] and acc_norm for HellaSwag [ 44 ] , Arc_Challenge, Arc_Easy [ 10 ] , PIQA [ 4 ] , and Openbookqa [ 26 ] .\\nResults. Figure 5 shows the loss curves for BF16, MXFP8, and MXINT8 training. The curves for MXFP8 and MXINT8 almost overlap with BF16. In addition, MXINT8 consistently outperforms MXFP8 with a loss that is lower by approximately 0.001 0.001 , as shown in the enlarged view in Figure 5 . Table 4 shows that MXINT8 also achieves nearly the same average accuracy across six common-sense reasoning tasks compared to BF16 training. These results demonstrate that MXINT8 supports nearly lossless low-bit training, while existing works [ 21 , 27 ] mainly focus on FP8 training.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Hardware Cost Analysis',\n",
       "    'paragraphs': 'Based on the hardware model in Sec. 10 , we evaluate the energy and area cost of a Matrix-Multiply Unit (MMU) that supports the MX format.\\nTable 5 shows that MXINT8 and NVINT4 reduce energy by 37% and 38%, respectively, compared with MXFP8 and NVFP4.\\nWe also evaluate mixed-format configurations. Following the NVIDIA Blackwell GPUs [ 32 ] , we study a chip that supports both 8-bit and 4-bit data types and set the throughput ratio of 8-bit to 4-bit to 1:2 to match the communication bandwidth. As shown in Table 5 , the ‚ÄúMXINT8+NVINT4‚Äù configuration further reduces area by about 34% relative to ‚ÄúMXFP8+NVFP4‚Äù, mainly because circuit reuse is simpler in the INT pipeline (Table 7 ).\\nOverall, this analysis shows that, at matched throughput, low-bit integer formats are more hardware-efficient than low-bit floating-point formats.',\n",
       "    'subsections': []},\n",
       "   {'title': '7 Conclusion',\n",
       "    'paragraphs': 'Our comprehensive study reveals a critical and nuanced trade-off between integer (INT) and floating-point (FP) quantization. We find that while FP formats are effective at coarse granularities, the popular fine-grained MXINT8 consistently outperforms its FP counterpart MXFP8 in both accuracy and hardware efficiency. For 4-bit formats, the accuracy advantage shifts to FP (MXFP4, NVFP4) , though we demonstrate that NVINT4 can surpass NVFP4 when combined with random Hadamard rotation. These findings challenge the current hardware trajectory, which is increasingly focused on FP. We therefore call for a strategic shift in both academia and industry toward algorithm-hardware co-design that re-evaluates and prioritizes fine-grained INT formats to build more powerful and efficient AI accelerators.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Outlines',\n",
       "    'paragraphs': 'Sec. 8 introduces related works.\\nSec. 9 details the proofs of Theorems 1 and 2 on INT and FP QSNR estimation.\\nSec. 10 presents the hardware cost estimation model.\\nSec. 11 provides additional details on the models used and ablation studies, and reports the numerical results corresponding to the figures in the main paper.',\n",
       "    'subsections': []},\n",
       "   {'title': '8 Related Work',\n",
       "    'paragraphs': 'Quantization Algorithms. Quantization methods include post-training quantization (PTQ) [ 20 , 15 , 36 , 41 ] and quantization-aware training (QAT) [ 7 , 23 ] , which speed up inference. Low-bit training [ 27 , 39 , 9 ] speeds up both training and inference. Several works also study scaling laws [ 18 ] for low-bit quantization [ 5 , 8 , 16 , 19 ] . However, most prior work focuses on a single low-bit format‚Äîeither integer or floating-point‚Äîand does not provide direct comparisons between these formats. [ 45 ] study mixed-format quantization in the PTQ setting, assigning integer or floating-point formats to different model parts.\\nHardware. Previous accelerators [ 29 , 30 ] do not natively support fine-grained quantization, so algorithms [ 41 , 6 ] face challenges with per-channel quantization in the presence of outliers [ 38 ] . Recently, OCP [ 34 ] proposes Microscaling (MX) data formats, which combine a per-block scaling factor with a block size of 32 to improve low-bit quantization performance. NVIDIA Blackwell [ 31 ] supports MXFP8, MXFP4, and NVFP4 at the hardware level.',\n",
       "    'subsections': []},\n",
       "   {'title': '9 Proofs of Theorems',\n",
       "    'paragraphs': 'We consider block vectors ùêó ‚àà ‚Ñù g \\\\mathbf{X}\\\\in\\\\mathbb{R}^{g} with i.i.d. entries X i ‚àº ùí© \\u200b ( 0 , œÉ 2 ) X_{i}\\\\sim\\\\mathcal{N}(0,\\\\sigma^{2}) . We denote the block RMS by œÉ := RMS \\u200b ( ùêó ) \\\\sigma:=\\\\mathrm{RMS}(\\\\mathbf{X}) and the crest factor by\\nFor MX format, which uses blockwise UE8M0 scale factors, we set\\nand choose s ‚Ä≤ ‚â• s s^{\\\\prime}\\\\geq s to avoid upper clipping. When the scale factors use BFloat16 or E4M3, we set œÅ = 1 \\\\rho=1 . The ideal scale s s matches the largest codebook magnitude to the block maximum:\\nwhere Q ref Q_{\\\\mathrm{ref}} depends on the target format:\\nINT ( b ) (b) (symmetric): Q ref = Q := 2 b ‚àí 1 ‚àí 1 Q_{\\\\mathrm{ref}}=Q:=2^{b-1}-1 (largest integer code).\\nFP ( E , M , B ) (E,M,B) (with subnormals): Q ref = Q max Q_{\\\\mathrm{ref}}=Q_{\\\\max} (largest finite normal magnitude; e.g., Q max = 448 Q_{\\\\max}=448 for E4M3).\\nThis convention matches the main text: we reuse ( œÉ , Œ∫ , œÅ , s , s ‚Ä≤ ) (\\\\sigma,\\\\kappa,\\\\rho,s,s^{\\\\prime}) , and s ‚Ä≤ ‚â• s s^{\\\\prime}\\\\geq s prevents overflow for both INT and FP quantization. Unless stated otherwise, expectations are over both the data and the quantization randomness, and ‚Äñ ùêó ‚Äñ 2 ‚âà k \\u200b œÉ 2 \\\\|\\\\mathbf{X}\\\\|^{2}\\\\approx k\\\\sigma^{2} .\\nINT quantization. We consider a symmetric, uniform quantizer with bit-width b b and integer range [ ‚àí Q , Q ] [-Q,Q] , where\\nThe quantize‚Äìdequantize operation is\\nso the effective step in the quantization is Œî := s ‚Ä≤ \\\\Delta:=s^{\\\\prime} .\\nError model. Let the elementwise error be e := X ‚àí X q e:={X}-{X}_{q} . For a non-saturating symmetric quantizer with round-to-nearest, e ‚àà [ ‚àí Œî 2 , Œî 2 ] e\\\\in[-\\\\frac{\\\\Delta}{2},\\\\,\\\\frac{\\\\Delta}{2}] . Under the standard high-resolution model [ 3 ] , the error is approximately uniform and independent of ùêó \\\\mathbf{X} :\\nQSNR. Define the QSNR as\\nWe have ùîº \\u200b [ ‚Äñ ùêó ‚Äñ 2 ] ‚âà k \\u200b œÉ 2 \\\\mathbb{E}[\\\\|\\\\mathbf{X}\\\\|^{2}]\\\\approx k\\\\sigma^{2} and ùîº \\u200b [ ‚Äñ ùêó ‚àí ùêó q ‚Äñ 2 ] ‚âà k \\u200b ùîº \\u200b [ e 2 ] = k \\u200b Œî 2 / 12 \\\\mathbb{E}[\\\\|\\\\mathbf{X}-\\\\mathbf{X}_{q}\\\\|^{2}]\\\\approx k\\\\,\\\\mathbb{E}[e^{2}]=k\\\\Delta^{2}/12 , hence\\nExpressing Œî \\\\Delta via crest factor and scale overhead. Using Eq. ( 15 ‚Äì 17 ),\\nSubstituting into the QSNR expression gives\\nand therefore\\nwhere we use Q ‚âà 2 b ‚àí 1 Q\\\\approx 2^{b-1} in Eq. ( 18 ). This form makes explicit: (i) ‚âà 6.02 \\\\approx 6.02 dB per additional bit, (ii) up to 6.02 6.02 dB loss from the power-of-two overhead ( œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) ), and (iii) a penalty that scales with the crest factor Œ∫ \\\\kappa (which typically increases with larger block size).\\nExtension to high-precision scale factors. The analysis above assumes UE8M0 scaling, which rounds the scale and introduces the overhead œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) . With the E4M3 scale format used in NVINT4, the per-block scale closely matches the ideal value, so œÅ ‚âà 1 \\\\rho\\\\approx 1 , and the element at the block maximum maps with (near-)zero error. For block size g g (elements per block), the INT QSNR with an E4M3 scale is\\nwhere 10 \\u200b log 10 \\u2061 ( g g ‚àí 1 ) 10\\\\log_{10}\\\\!\\\\big(\\\\tfrac{g}{g-1}\\\\big) accounts for one (near) error-free element per block.\\nFP quantization. Consider a target floating-point format FP ( E , M , B ) (E,M,B) with sign, E E exponent bits (bias B B ), and M M mantissa bits, with subnormals enabled. The representable numbers split into normal and subnormal domains:\\nwhere s s , e e , and m m are the sign, exponent, and mantissa of a floating-point number. Let Q max Q_{\\\\max} denote the largest finite normal magnitude (e.g., Q max = 448 Q_{\\\\max}=448 for E4M3), and let N min := 2 1 ‚àí B N_{\\\\min}:=2^{1-B} be the smallest normal. We also define the subnormal spacing in the codebook as S min := 2 1 ‚àí B ‚àí M S_{\\\\min}:=2^{1-B-M} .\\nWe use a block scale s ‚Ä≤ s^{\\\\prime} (Eq.( 16 )) and perform quantize‚Äìdequantize as\\nwhere ‚ÑÇ FP \\\\mathbb{C}_{\\\\mathrm{FP}} is the FP codebook. We choose the ideal scale s = max \\u2061 ( | ùêó | ) / Q max s=\\\\max(|\\\\mathbf{X}|)/Q_{\\\\max} and set s ‚Ä≤ = œÅ \\u200b s s^{\\\\prime}=\\\\rho s with œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) for UE8M0 (power-of-two) scaling; œÅ ‚âà 1 \\\\rho\\\\approx 1 when the scale uses E4M3.\\nError decomposition. Let e := ùêó ‚àí ùêó q e:=\\\\mathbf{X}-\\\\mathbf{X}_{q} . We study the relative MSE\\nUnder a high-resolution model [ 3 ] , the within-cell error is unbiased and uniform on [ ‚àí Œî 2 , Œî 2 ] [-\\\\frac{\\\\Delta}{2},\\\\frac{\\\\Delta}{2}] , and the logarithmic phase\\n(the fractional part { ‚ãÖ } \\\\{\\\\cdot\\\\} of log 2 \\u2061 ( | X | / s ‚Ä≤ ) \\\\log_{2}(|X|/s^{\\\\prime}) ) is approximately uniform on [ 1 , 2 ) [1,2) .\\nDefine the signal-domain normal threshold T N T_{N} and the subnormal step Œî sub \\\\Delta_{\\\\mathrm{sub}} as\\nWe split the amplitude axis into normal and subnormal regions:\\nNormal region ( | X | ‚â• T N |X|\\\\geq T_{N} ). Let e \\u200b ( X ) := ‚åä log 2 \\u2061 ( | X | s ‚Ä≤ ) ‚åã e(X):=\\\\lfloor\\\\log_{2}(\\\\tfrac{|X|}{s^{\\\\prime}})\\\\rfloor be the exponent bin of X s ‚Ä≤ \\\\tfrac{X}{s^{\\\\prime}} .\\nThe local effective quantization step is\\nWriting 2 e \\u200b ( X ) = | X | s ‚Ä≤ \\u200b r 2^{e(X)}=\\\\tfrac{|X|}{s^{\\\\prime}r} with r ‚àà [ 1 , 2 ) r\\\\in[1,2) gives\\nUniform-error modeling yields ùîº \\u200b [ e 2 ‚à£ X , | X | ‚â• T N ] = Œî \\u200b ( X ) 2 12 = | X | 2 \\u200b 2 ‚àí 2 \\u200b M 12 \\u200b r 2 \\\\mathbb{E}[e^{2}\\\\mid X,|X|\\\\geq T_{N}]=\\\\tfrac{\\\\Delta(X)^{2}}{12}=\\\\tfrac{|X|^{2}\\\\,2^{-2M}}{12\\\\,r^{2}} .\\nAveraging over r ‚àº Uniform \\u200b [ 1 , 2 ] r\\\\sim\\\\mathrm{Uniform}[1,2] gives ùîº \\u200b [ 1 / r 2 ] = ‚à´ 1 2 r ‚àí 2 \\u200b ùëë r = 1 / 2 \\\\mathbb{E}[1/r^{2}]=\\\\int_{1}^{2}r^{-2}\\\\,dr=1/2 , hence\\nSubnormal but nonzero region ( | X | < T N |X|<T_{N} ). Here the absolute spacing is constant, Œî sub \\\\Delta_{\\\\mathrm{sub}} , so\\nLet p sub := ‚Ñô \\u200b ( | X | < T N ) p_{\\\\mathrm{sub}}:=\\\\mathbb{P}(|X|<T_{N}) .\\nThen\\nSumming the two contributions and normalizing by œÉ 2 \\\\sigma^{2} yields\\nwhere we define the dimensionless weight\\nand use s ‚Ä≤ \\u2063 2 œÉ 2 = ( œÅ \\u200b Œ∫ ) 2 Q max 2 \\\\tfrac{s^{\\\\prime 2}}{\\\\sigma^{2}}=\\\\tfrac{(\\\\rho\\\\kappa)^{2}}{Q_{\\\\max}^{2}} with\\nTherefore,\\nIn the ample dynamic-range regime ( w norm ‚âà 1 w_{\\\\mathrm{norm}}\\\\approx 1 and p sub ‚âà 0 p_{\\\\mathrm{sub}}\\\\approx 0 ),\\nthe law simplifies to\\nindependent of block granularity and the distribution of ùêó \\\\mathbf{X} .\\nExtension to high-precision scale factors. The analysis above assumes a UE8M0-quantized scale, which forces s ‚Ä≤ s^{\\\\prime} to be a power of two and introduces the overhead œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) . When the per-block scale uses E4M3 (as in NVFP4), the scale closely tracks the ideal value, so œÅ ‚âà 1 \\\\rho\\\\approx 1 , and the element at the block maximum maps with negligible error (its scaled value hits Q max Q_{\\\\max} ). It is therefore natural to exclude the block-maximum contribution from the normal-region error budget. Let g g be the block size and define the energy fraction of the block maximum as\\nSetting œÅ = 1 \\\\rho=1 and replacing w norm w_{\\\\mathrm{norm}} by w norm ‚àí Œ∑ w_{\\\\mathrm{norm}}-\\\\eta in Eq. ( 40 ) yields the refined QSNR approximation for FP quantization with an E4M3 scale:\\nThis adjustment isolates the block maximum and tightens the prediction when the scale is represented with sufficient precision.',\n",
       "    'subsections': [{'title': '9.1 Common assumptions and notation',\n",
       "      'paragraphs': 'We consider block vectors ùêó ‚àà ‚Ñù g \\\\mathbf{X}\\\\in\\\\mathbb{R}^{g} with i.i.d. entries X i ‚àº ùí© \\u200b ( 0 , œÉ 2 ) X_{i}\\\\sim\\\\mathcal{N}(0,\\\\sigma^{2}) . We denote the block RMS by œÉ := RMS \\u200b ( ùêó ) \\\\sigma:=\\\\mathrm{RMS}(\\\\mathbf{X}) and the crest factor by\\nFor MX format, which uses blockwise UE8M0 scale factors, we set\\nand choose s ‚Ä≤ ‚â• s s^{\\\\prime}\\\\geq s to avoid upper clipping. When the scale factors use BFloat16 or E4M3, we set œÅ = 1 \\\\rho=1 . The ideal scale s s matches the largest codebook magnitude to the block maximum:\\nwhere Q ref Q_{\\\\mathrm{ref}} depends on the target format:\\nINT ( b ) (b) (symmetric): Q ref = Q := 2 b ‚àí 1 ‚àí 1 Q_{\\\\mathrm{ref}}=Q:=2^{b-1}-1 (largest integer code).\\nFP ( E , M , B ) (E,M,B) (with subnormals): Q ref = Q max Q_{\\\\mathrm{ref}}=Q_{\\\\max} (largest finite normal magnitude; e.g., Q max = 448 Q_{\\\\max}=448 for E4M3).\\nThis convention matches the main text: we reuse ( œÉ , Œ∫ , œÅ , s , s ‚Ä≤ ) (\\\\sigma,\\\\kappa,\\\\rho,s,s^{\\\\prime}) , and s ‚Ä≤ ‚â• s s^{\\\\prime}\\\\geq s prevents overflow for both INT and FP quantization. Unless stated otherwise, expectations are over both the data and the quantization randomness, and ‚Äñ ùêó ‚Äñ 2 ‚âà k \\u200b œÉ 2 \\\\|\\\\mathbf{X}\\\\|^{2}\\\\approx k\\\\sigma^{2} .',\n",
       "      'subsections': []},\n",
       "     {'title': '9.2 Theorem 1 (INT quantization)',\n",
       "      'paragraphs': 'INT quantization. We consider a symmetric, uniform quantizer with bit-width b b and integer range [ ‚àí Q , Q ] [-Q,Q] , where\\nThe quantize‚Äìdequantize operation is\\nso the effective step in the quantization is Œî := s ‚Ä≤ \\\\Delta:=s^{\\\\prime} .\\nError model. Let the elementwise error be e := X ‚àí X q e:={X}-{X}_{q} . For a non-saturating symmetric quantizer with round-to-nearest, e ‚àà [ ‚àí Œî 2 , Œî 2 ] e\\\\in[-\\\\frac{\\\\Delta}{2},\\\\,\\\\frac{\\\\Delta}{2}] . Under the standard high-resolution model [ 3 ] , the error is approximately uniform and independent of ùêó \\\\mathbf{X} :\\nQSNR. Define the QSNR as\\nWe have ùîº \\u200b [ ‚Äñ ùêó ‚Äñ 2 ] ‚âà k \\u200b œÉ 2 \\\\mathbb{E}[\\\\|\\\\mathbf{X}\\\\|^{2}]\\\\approx k\\\\sigma^{2} and ùîº \\u200b [ ‚Äñ ùêó ‚àí ùêó q ‚Äñ 2 ] ‚âà k \\u200b ùîº \\u200b [ e 2 ] = k \\u200b Œî 2 / 12 \\\\mathbb{E}[\\\\|\\\\mathbf{X}-\\\\mathbf{X}_{q}\\\\|^{2}]\\\\approx k\\\\,\\\\mathbb{E}[e^{2}]=k\\\\Delta^{2}/12 , hence\\nExpressing Œî \\\\Delta via crest factor and scale overhead. Using Eq. ( 15 ‚Äì 17 ),\\nSubstituting into the QSNR expression gives\\nand therefore\\nwhere we use Q ‚âà 2 b ‚àí 1 Q\\\\approx 2^{b-1} in Eq. ( 18 ). This form makes explicit: (i) ‚âà 6.02 \\\\approx 6.02 dB per additional bit, (ii) up to 6.02 6.02 dB loss from the power-of-two overhead ( œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) ), and (iii) a penalty that scales with the crest factor Œ∫ \\\\kappa (which typically increases with larger block size).\\nExtension to high-precision scale factors. The analysis above assumes UE8M0 scaling, which rounds the scale and introduces the overhead œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) . With the E4M3 scale format used in NVINT4, the per-block scale closely matches the ideal value, so œÅ ‚âà 1 \\\\rho\\\\approx 1 , and the element at the block maximum maps with (near-)zero error. For block size g g (elements per block), the INT QSNR with an E4M3 scale is\\nwhere 10 \\u200b log 10 \\u2061 ( g g ‚àí 1 ) 10\\\\log_{10}\\\\!\\\\big(\\\\tfrac{g}{g-1}\\\\big) accounts for one (near) error-free element per block.',\n",
       "      'subsections': []},\n",
       "     {'title': '9.3 Theorem 2 (FP quantization)',\n",
       "      'paragraphs': 'FP quantization. Consider a target floating-point format FP ( E , M , B ) (E,M,B) with sign, E E exponent bits (bias B B ), and M M mantissa bits, with subnormals enabled. The representable numbers split into normal and subnormal domains:\\nwhere s s , e e , and m m are the sign, exponent, and mantissa of a floating-point number. Let Q max Q_{\\\\max} denote the largest finite normal magnitude (e.g., Q max = 448 Q_{\\\\max}=448 for E4M3), and let N min := 2 1 ‚àí B N_{\\\\min}:=2^{1-B} be the smallest normal. We also define the subnormal spacing in the codebook as S min := 2 1 ‚àí B ‚àí M S_{\\\\min}:=2^{1-B-M} .\\nWe use a block scale s ‚Ä≤ s^{\\\\prime} (Eq.( 16 )) and perform quantize‚Äìdequantize as\\nwhere ‚ÑÇ FP \\\\mathbb{C}_{\\\\mathrm{FP}} is the FP codebook. We choose the ideal scale s = max \\u2061 ( | ùêó | ) / Q max s=\\\\max(|\\\\mathbf{X}|)/Q_{\\\\max} and set s ‚Ä≤ = œÅ \\u200b s s^{\\\\prime}=\\\\rho s with œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) for UE8M0 (power-of-two) scaling; œÅ ‚âà 1 \\\\rho\\\\approx 1 when the scale uses E4M3.\\nError decomposition. Let e := ùêó ‚àí ùêó q e:=\\\\mathbf{X}-\\\\mathbf{X}_{q} . We study the relative MSE\\nUnder a high-resolution model [ 3 ] , the within-cell error is unbiased and uniform on [ ‚àí Œî 2 , Œî 2 ] [-\\\\frac{\\\\Delta}{2},\\\\frac{\\\\Delta}{2}] , and the logarithmic phase\\n(the fractional part { ‚ãÖ } \\\\{\\\\cdot\\\\} of log 2 \\u2061 ( | X | / s ‚Ä≤ ) \\\\log_{2}(|X|/s^{\\\\prime}) ) is approximately uniform on [ 1 , 2 ) [1,2) .\\nDefine the signal-domain normal threshold T N T_{N} and the subnormal step Œî sub \\\\Delta_{\\\\mathrm{sub}} as\\nWe split the amplitude axis into normal and subnormal regions:\\nNormal region ( | X | ‚â• T N |X|\\\\geq T_{N} ). Let e \\u200b ( X ) := ‚åä log 2 \\u2061 ( | X | s ‚Ä≤ ) ‚åã e(X):=\\\\lfloor\\\\log_{2}(\\\\tfrac{|X|}{s^{\\\\prime}})\\\\rfloor be the exponent bin of X s ‚Ä≤ \\\\tfrac{X}{s^{\\\\prime}} .\\nThe local effective quantization step is\\nWriting 2 e \\u200b ( X ) = | X | s ‚Ä≤ \\u200b r 2^{e(X)}=\\\\tfrac{|X|}{s^{\\\\prime}r} with r ‚àà [ 1 , 2 ) r\\\\in[1,2) gives\\nUniform-error modeling yields ùîº \\u200b [ e 2 ‚à£ X , | X | ‚â• T N ] = Œî \\u200b ( X ) 2 12 = | X | 2 \\u200b 2 ‚àí 2 \\u200b M 12 \\u200b r 2 \\\\mathbb{E}[e^{2}\\\\mid X,|X|\\\\geq T_{N}]=\\\\tfrac{\\\\Delta(X)^{2}}{12}=\\\\tfrac{|X|^{2}\\\\,2^{-2M}}{12\\\\,r^{2}} .\\nAveraging over r ‚àº Uniform \\u200b [ 1 , 2 ] r\\\\sim\\\\mathrm{Uniform}[1,2] gives ùîº \\u200b [ 1 / r 2 ] = ‚à´ 1 2 r ‚àí 2 \\u200b ùëë r = 1 / 2 \\\\mathbb{E}[1/r^{2}]=\\\\int_{1}^{2}r^{-2}\\\\,dr=1/2 , hence\\nSubnormal but nonzero region ( | X | < T N |X|<T_{N} ). Here the absolute spacing is constant, Œî sub \\\\Delta_{\\\\mathrm{sub}} , so\\nLet p sub := ‚Ñô \\u200b ( | X | < T N ) p_{\\\\mathrm{sub}}:=\\\\mathbb{P}(|X|<T_{N}) .\\nThen\\nSumming the two contributions and normalizing by œÉ 2 \\\\sigma^{2} yields\\nwhere we define the dimensionless weight\\nand use s ‚Ä≤ \\u2063 2 œÉ 2 = ( œÅ \\u200b Œ∫ ) 2 Q max 2 \\\\tfrac{s^{\\\\prime 2}}{\\\\sigma^{2}}=\\\\tfrac{(\\\\rho\\\\kappa)^{2}}{Q_{\\\\max}^{2}} with\\nTherefore,\\nIn the ample dynamic-range regime ( w norm ‚âà 1 w_{\\\\mathrm{norm}}\\\\approx 1 and p sub ‚âà 0 p_{\\\\mathrm{sub}}\\\\approx 0 ),\\nthe law simplifies to\\nindependent of block granularity and the distribution of ùêó \\\\mathbf{X} .\\nExtension to high-precision scale factors. The analysis above assumes a UE8M0-quantized scale, which forces s ‚Ä≤ s^{\\\\prime} to be a power of two and introduces the overhead œÅ ‚àà [ 1 , 2 ) \\\\rho\\\\in[1,2) . When the per-block scale uses E4M3 (as in NVFP4), the scale closely tracks the ideal value, so œÅ ‚âà 1 \\\\rho\\\\approx 1 , and the element at the block maximum maps with negligible error (its scaled value hits Q max Q_{\\\\max} ). It is therefore natural to exclude the block-maximum contribution from the normal-region error budget. Let g g be the block size and define the energy fraction of the block maximum as\\nSetting œÅ = 1 \\\\rho=1 and replacing w norm w_{\\\\mathrm{norm}} by w norm ‚àí Œ∑ w_{\\\\mathrm{norm}}-\\\\eta in Eq. ( 40 ) yields the refined QSNR approximation for FP quantization with an E4M3 scale:\\nThis adjustment isolates the block maximum and tightens the prediction when the scale is represented with sufficient precision.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '10 Hardware Cost Modeling',\n",
       "    'paragraphs': 'Scope and assumptions. We develop a compact gate-level model to estimate the chip area and energy of a GEMM engine under low-precision formats. Specifically, a low-bit GEMM engine uses four components: a quantizer, a multiply-and-accumulate (MAC) unit, a dequantizer, and an FP32 accumulator. The proposed model accounts only for the MAC unit, a shared FP32 accumulator and a dequantizer; the quantizer is excluded from all cost accounting. In MX/NV formats, the VPU implements quantization by shift/divide-and-round, and the accumulation pipeline can fuse dequantization as two 8-bit integer additions for UE8M0 scale or two floating-point multiplications for E4M3 scale. We omit the quantizer block in VPU to isolate the cost driven by multiplication and accumulation. Unless otherwise stated, we take cell factors from a TSMC FinFET standard-cell library. We model only combinational logic; we ignore sequential elements, placement and routing, and interconnect to enable technology-aware, relative comparisons.\\nDesign choice: FP32 accumulation and MMU integration. A high-throughput Matrix-Multiply Unit (MMU), as in TPU-like designs [ 28 ] , integrates the multiply-and-accumulate datapath and downstream accumulation to improve performance and energy efficiency. To prevent error growth and preserve scalability, we accumulate in FP32. Under the same nominal bit width, FP multipliers are typically more area- and energy-efficient than INT multipliers, whereas FP adders are more expensive than INT adders due to exponent comparison/subtraction, mantissa alignment, and normalization [ 45 ] . With a uniform-alignment design [ 40 ] , the normalizer count reduces to one shared instance across the k k MAC lanes, and we divide its cost by k k .\\nMantissa aligner width. The mantissa aligner couples accuracy and cost: its bit width n n affects numerical fidelity and hardware complexity. We set\\nwhere x x and y y denote exponent and mantissa widths, respectively (for INT formats, x = 0 x\\\\!=\\\\!0 ). In all evaluations we use k = 32 k\\\\!=\\\\!32 for MX formats and k = 16 k\\\\!=\\\\!16 for NV formats, and psum_bit_width = 24 \\\\texttt{psum\\\\_bit\\\\_width}\\\\!=\\\\!24 .\\nMAC unit structure and sub-blocks. We model the MAC unit as a k k -lane array. Each lane comprises one multiplier. The adders from all lanes are fused together to form a multi-input adder tree structure, incorporating FP-specific alignment and normalization logic. Table 6 reports the dominant logic count (up to constant factors) for the main sub-blocks, where ‚ÄúMain Cells‚Äù indicate the standard-cell types used for area/energy aggregation. For FP multiplication, we multiply only mantissas and include an exponent adder. For FP addition, we model exponent comparator/subtractor, a barrel aligner, a wide mantissa adder, and one shared normalizer. For INT, we set x = 0 x\\\\!=\\\\!0 in the expressions.\\nArea and energy aggregation for MAC. Let ùíÆ \\\\mathcal{S} ={Multiplier, Adder(mantissa/int), Exponent adder, Exponent subtractor, Comparator, Aligner(barrel), Normalizer(shared)} be the set of sub-block types,\\nand ùí¢ = { FA , HA , XOR , AND , OR , MUX } \\\\mathcal{G}=\\\\{\\\\text{FA},\\\\text{HA},\\\\text{XOR},\\\\text{AND},\\\\text{OR},\\\\text{MUX}\\\\} be the set of cell types with technology-dependent area and energy factors A g A_{g} and E g E_{g} obtained from the standard-cell library. Let œÑ g \\\\tau_{g} be the toggle rate of cell g g , which represents the average switching activity of the cell. In this work, we simplify the toggle rate factor by assuming that all gate cells have the same toggle rate, œÑ g = œÑ \\\\tau_{g}=\\\\tau , to reduce computational complexity and focus on the primary design trade-offs. Denote by c s , g \\u200b ( x , y , k , n ) c_{s,g}(x,y,k,n) the count of cell g ‚àà ùí¢ g\\\\in\\\\mathcal{G} in sub-block s s induced by the chosen format and by n n from Eq.( 44 ). The MAC area and energy are\\nFP32 accumulator model. We model the FP32 accumulator by its combinational logic counts c g ACC32 c^{\\\\text{ACC32}}_{g} , yielding\\nDequantizer model. We model the shared dequantizer based on the logic required for the specific format (e.g., fused integer additions or floating-point multiplications as described in ¬ß 10 ). We aggregate its combinational logic counts c g DEQ c^{\\\\text{DEQ}}_{g} , yielding\\nTotal cost and per-lane reporting. The total MMU cost is\\nand, when we report per-lane figures, we divide the cost of shared blocks (the dequantizer and the FP32 accumulator) by k k .\\nSummary. The hardware model includes the MAC unit, the dequantizer, and the FP32 accumulator; the quantizer is excluded from the overhead calculation. Given a low-precision format with exponent/mantissa widths ( x , y ) (x,y) (with x = 0 x{=}0 for INT), a MAC array size k k , an aligner cap psum_bit_width (setting n n via Eq ( 44 ), and technology cell factors { A g , E g } g ‚àà ùí¢ \\\\{A_{g},E_{g}\\\\}_{g\\\\in\\\\mathcal{G}} (plus the dequantizer and FP32-accumulator gate counts), the model predicts the area and energy of the MAC and accumulation stages. It captures the relative cost trends across MX/NV-INT/FP formats at the same nominal bit width, the sensitivity to the aligner width n n (critical for FP addition), and the effect of sharing both the normalizer, the dequantizer, and the FP32 accumulator across k k lanes.',\n",
       "    'subsections': []},\n",
       "   {'title': '11 More Details for Reproduction',\n",
       "    'paragraphs': 'Models for inference evaluation. We list the Huggingface IDs of evaluated open-sourced model for better reproduction in Tabel 8 . Note that we firstly choose the base model without supervise fine-tuning if it is open-sourced. For a model of a certain size, our selection principle is that if the base model is open source, we will first choose the base model; otherwise, we will select the model that has undergone SFT.\\nModels for training evaluation. We select the Llama-3 [ 13 ] style model for our experiments due to its wide adoption. The Llama-3 style model employs Group Query Attention (GQA) [ 1 ] for the self-attention module and SwiGLU [ 37 ] for the feed-forward module. Table 9 presents the detailed architectural settings and training hyper-parameters of the models used.\\nTable 10 offer the ablation studies on representation range of INT8 quantization. We find that the bias in representation range would consistently degenerate INT8 training loss. For BFloat16 scale factor, we can find that asymmetric representation range even making block 32 quantization worse than block 256 quantization. This is because only the minimal values in each quantization block have possibility to be quantized into 128 in INT8 quantization, and smaller block size indicates more individual quantization blocks. Additionally, asymmetric quantization also causes degeneration for UE8M0 scale factors, but the degeneration strength is slighter than BFloat16 scales. This is because UE8M0 scale factor consistently greater than or equal to Bfloat16 scale, leading less high-precision number to map to Q m \\u200b i \\u200b n Q_{min} . These experiments demonstrate the necessity of symmetric representation space for integer quantization.\\nNumerical stability analysis. We also analyze the numerical stability of different float-point for quantization mapping through Algorithm 1 . Table 11 shows the results of Algorithm 1 , demonstrating that in BFloat16 precision, a significant portion of values (16.82%) are mapped to -128. This phenomenon occurs even though the scaling factor s is theoretically designed to map the value to 127. In conclusion, this analysis highlights a critical pitfall of using low-precision floating-point formats for quantization calculations. The inherent lack of precision in bfloat16 and, to a lesser extent, float16 can lead to overflow during the scaling step, incorrectly mapping values to outside the intended integer range. This powerfully demonstrates that a forced symmetric clipping step is essential for guaranteeing the correctness and stability of quantization, particularly when the computation is performed using low-precision data types.\\nThis section offer detailed numbers of experiments, as follows:\\nTable 12 and Table 13 present the KL divergence results, corresponding to Table 3 .\\nTable 14 and Table 15 present the perplexity results, for better understanding the relationship between KL divergence and perplexity. They are consistent in most case.',\n",
       "    'subsections': [{'title': '11.1 Used Models',\n",
       "      'paragraphs': 'Models for inference evaluation. We list the Huggingface IDs of evaluated open-sourced model for better reproduction in Tabel 8 . Note that we firstly choose the base model without supervise fine-tuning if it is open-sourced. For a model of a certain size, our selection principle is that if the base model is open source, we will first choose the base model; otherwise, we will select the model that has undergone SFT.\\nModels for training evaluation. We select the Llama-3 [ 13 ] style model for our experiments due to its wide adoption. The Llama-3 style model employs Group Query Attention (GQA) [ 1 ] for the self-attention module and SwiGLU [ 37 ] for the feed-forward module. Table 9 presents the detailed architectural settings and training hyper-parameters of the models used.',\n",
       "      'subsections': []},\n",
       "     {'title': '11.2 Necessity of Symmetric Integer Representation',\n",
       "      'paragraphs': 'Table 10 offer the ablation studies on representation range of INT8 quantization. We find that the bias in representation range would consistently degenerate INT8 training loss. For BFloat16 scale factor, we can find that asymmetric representation range even making block 32 quantization worse than block 256 quantization. This is because only the minimal values in each quantization block have possibility to be quantized into 128 in INT8 quantization, and smaller block size indicates more individual quantization blocks. Additionally, asymmetric quantization also causes degeneration for UE8M0 scale factors, but the degeneration strength is slighter than BFloat16 scales. This is because UE8M0 scale factor consistently greater than or equal to Bfloat16 scale, leading less high-precision number to map to Q m \\u200b i \\u200b n Q_{min} . These experiments demonstrate the necessity of symmetric representation space for integer quantization.\\nNumerical stability analysis. We also analyze the numerical stability of different float-point for quantization mapping through Algorithm 1 . Table 11 shows the results of Algorithm 1 , demonstrating that in BFloat16 precision, a significant portion of values (16.82%) are mapped to -128. This phenomenon occurs even though the scaling factor s is theoretically designed to map the value to 127. In conclusion, this analysis highlights a critical pitfall of using low-precision floating-point formats for quantization calculations. The inherent lack of precision in bfloat16 and, to a lesser extent, float16 can lead to overflow during the scaling step, incorrectly mapping values to outside the intended integer range. This powerfully demonstrates that a forced symmetric clipping step is essential for guaranteeing the correctness and stability of quantization, particularly when the computation is performed using low-precision data types.',\n",
       "      'subsections': []},\n",
       "     {'title': '11.3 Detailed Results',\n",
       "      'paragraphs': 'This section offer detailed numbers of experiments, as follows:\\nTable 12 and Table 13 present the KL divergence results, corresponding to Table 3 .\\nTable 14 and Table 15 present the perplexity results, for better understanding the relationship between KL divergence and perplexity. They are consistent in most case.',\n",
       "      'subsections': []}]}],\n",
       "  'abstract': 'Modern AI hardware, such as Nvidia‚Äôs Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, the comparison at fine-grained (block-wise) levels is more nuanced. Our comprehensive comparison demonstrates that for popular 8-bit fine-grained formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart in both algorithmic accuracy and hardware efficiency. However, for 4-bit formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like Hadamard rotation are applied. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory, demonstrating that a one-size-fits-all FP approach is suboptimal and advocating that fine-grained INT formats, particularly MXINT8, offer a better balance of accuracy, power, and efficiency for future AI accelerators.'},\n",
       " '2510.25205v1': {'content': [{'title': 'I Introduction',\n",
       "    'paragraphs': 'Autonomous driving has gained broad attention from the public during the last few years [ 1 , 2 ] . With intelligence, the autonomous vehicle can have a more comprehensive perception of the surrounding traffic environment and make more reasonable driving decisions compared to human drivers. As a result, it is expected to bring society a large number of benefits, including improved mobility and a significant reduction in collisions. However, these benefits come with increased energy consumption of the computation platform, ranging from several hundred watts (W) to over 1 kW of power [ 3 ] . For example, the computing platform using the Nvidia AGX Orin SoC [ 4 ] has a Thermal Design Power (TDP) of 800W. These power demands can also increase the thermal demands on a vehicle‚Äôs climate-control system. When combined, these demands can significantly reduce vehicle range [ 5 ] .\\nThis factor especially limits the mobility of electric vehicles due to their limited battery range and long recharge time. Further, it leads to higher vehicle usage costs and aggravates global energy and environmental issues.\\nIn general, the computational consumption for autonomous driving includes perception computing, decision-making computing, navigation, storage, etc [ 3 ] . Among them, perception computing is the most power-intensive component since it requires large neural networks to extract vehicle features (e.g., position and orientation) in the traffic environment. The mainstream perception architectures can be categorized into camera-only methods [ 6 , 7 , 8 ] and multi-modality methods [ 9 , 10 , 11 ] , where camera-only methods rely solely on image data from cameras, while multi-modality methods fusion the image and point cloud data from both cameras and LiDAR [ 12 ] . Compared to camera-only methods, multi-modality methods have higher accuracy but incur significantly higher computational consumption. To alleviate this, researchers design model compression techniques to reduce the model size of multi-modality methods, including sparsification [ 11 ] , quantization [ 13 ] , and distillation [ 14 ] . However, they either maintain large computational consumption or exhibit significantly lower accuracy compared to the original multi-modality model in complex traffic scenarios, which may raise safety concerns of autonomous driving. Recently, a gate-based model [ 5 ] is proposed to automatically discard or add parts of the network by gating techniques, thus altering the computational consumption and accuracy. Nonetheless, it struggles to balance computational consumption and accuracy, and training such a model is not trivial.\\nIn summary, all the aforementioned methods reduce computational consumption by training a perception model with a reduced size. In this study, we aim to build a comprehensive and scalable perception optimization framework from the perspective of data management and tuning.\\n(1) Firstly, instead of using a unified model for all traffic scenarios, we manage multiple trained models with different sizes tailored to different scenarios.\\nSecondly, we manage the framerate at which a perception model runs, which can directly reduce computational consumption without training additional models. In modern autonomous driving systems, the perception system typically operates at a framerate of 20-30 fps to extract environmental features [ 15 , 16 ] . We argue that such high framerates are not always necessary in all scenarios. For some simple scenarios (e.g., good weather condition and low traffic density), appropriately lowering the framerate does not compromise driving performance but can greatly reduce perception consumption. After frame reduction, we also consider using interpolation methods to fill in the features of skipped frames to keep the temporal granularity consistent.\\n(2) As shown in Figure 1 , we define the above adjustable parameters as tunable knobs (i.e., perception model, framerate, and interpolation method) in the perception system where each knob has a set of candidate values. We refer to a particular combination of knob values as a configuration and there are m 3 m^{3} configurations assuming each knob has m m candidate values.\\nDifferent configurations have different levels of computational consumption and accuracy.\\nWe set an accuracy requirement for feature extraction in the perception system and expect to find low-computation configurations while achieving the accuracy requirement. The optimal configuration of a traffic scenario is the one with the lowest computational consumption and over the accuracy.\\nHowever, implementing such a perception optimization framework within autonomous driving is non-trivial, which faces three main challenges:\\nChallenge I: Diverse traffic scenarios. The optimal configuration varies across different traffic scenarios. A low-computation configuration in simple scenarios is enough to achieve high accuracy, while in more complex traffic scenarios, a configuration with larger computation is required to maintain accuracy.\\nTherefore, it is necessary to distinguish the perception difficulty of different traffic scenarios and separately search the corresponding optimal configurations.\\nChallenge II: Large configuration space. The number of configurations m 3 m^{3} grows rapidly with the number m m of candidate values for each knob. Although the computational consumption of a configuration can be estimated, obtaining accuracy requires running the configuration on hours or days of raw perception data. As a result, it is cost-prohibit to run all configurations in each scenario to find the optimal one.\\nChallenge III: Decision robustness. Although there is an accuracy requirement during perception optimization, the extracted object features are inevitably perturbed. Therefore, the robustness of autonomous driving decisions needs to be enhanced to avoid some risky driving behaviors.\\nTo tackle these challenges, we propose an energy-efficient autonomous driving framework, called EneAD , which includes an adaptive perception module and a robust decision module.\\n(1) To address Challenge I , a lightweight classification model is built to distinguish the perception difficulty of traffic scenarios, alongside an uncertainty value to enhance the model‚Äôs reliability, where a scenario with a large uncertainty will be treated as the one with the highest difficulty.\\nUltimately, traffic scenarios are grouped into several difficulty levels, allowing the configuration optimization to focus on each level instead of on each single scenario.\\n(2) To address Challenge II , we adopt Bayesian optimization instead of exhaustive search for knob tuning, which can find optimal/near-optimal configurations by evaluating only partial configurations. Further, we design a meta-learning strategy to transfer tuning knowledge across different types of traffic scenarios, further speeding up the tuning process.\\n(3) To address Challenge III , we model the decision-making task as a Markov decision process and design a regularization technique to prevent large policy updates, thus enhancing decision robustness.\\nTo sum up, the adaptive perception module aims to reduce the energy consumption of the perception system, while the robust decision module aims to optimize driving decisions. In addition, since the robust decision module can lead to more conservative driving behaviors, we find that the energy consumption of the driving system also decreases to some degree.\\nWe summarize the main contributions as follows:\\n‚àô \\\\bullet We develop an energy-efficient autonomous driving framework, called EneAD , to adaptively reduce the energy consumption of the perception system while maintaining good driving performance.\\n‚àô \\\\bullet We tune the perception model, framerate, and interpolation method in the perception system and set them as knobs. Afterward, we design a meta-learning strategy within Bayesian optimization to explore promising configurations with low computational consumption and sufficient accuracy.\\n‚àô \\\\bullet We design a lightweight and reliable classification model to distinguish the perception difficulty of traffic scenarios.\\n‚àô \\\\bullet We design a regularization technique to enhance the decision robustness of autonomous driving.\\n‚àô \\\\bullet We conduct extensive experiments on real-world and synthetic datasets, evidencing the superiority of our proposals in both energy consumption and driving performance.',\n",
       "    'subsections': []},\n",
       "   {'title': 'II Related Works',\n",
       "    'paragraphs': 'Autonomous driving is recognized as a technology that could herald a major shift in transportation. Typically, there are two methods for building autonomous driving systems: modular and end-to-end. Modular methods leverage a perception module to extract traffic features from sensor data and then make driving decisions based on the perception results, while end-to-end methods attempt to directly map sensor data to driving decisions [ 17 ] . Given that end-to-end methods struggle in complex driving scenarios and lack transparency and explainability [ 18 ] , this study focuses on modular methods. In the following, we will introduce some related works about perception and decision-making and analyze their limitations.\\nPerception Module. Autonomous vehicles mainly rely on cameras and LiDAR for environmental perception [ 12 ] . On the one hand, cameras produce RGB images that can be used to detect color and position features with relatively low computational consumption. However, their detection performance declines sharply under adverse weather and light conditions. On the other hand, while LiDAR demands a higher computational consumption and lacks color information, it can provide reliable depth information with point cloud data and is less affected by environmental factors. Based on them, there are two mainstream perception architectures: camera-only [ 6 , 7 , 8 ] , and multi-modality [ 9 , 10 , 11 ] , where camera-only methods rely solely on camera data, while multi-modality methods combine the sensor data from both cameras and LiDAR.\\nAlthough multi-modality methods can achieve higher perception accuracy, they incur significantly higher computational consumption compared to camera-only methods. Therefore, many studies use model compression techniques, including sparsification, quantization, and distillation, to lower computational consumption by reducing model sizes.\\nHowever, they either continue to have a large model size or the accuracy is considerably inferior to that of the origin\\nmulti-modality model. Recently, a gate-based model [ 5 ] is proposed to adjust the model size of multi-modality methods by adaptively discarding or adding parts of the network. Nonetheless, it struggles to balance computational consumption and accuracy, and training such a model is not trivial.\\nTo sum up, all these existing methods focus solely on training a model with a reduced size. In this study, we intend to optimize autonomous driving perception from the perspective of data management and tuning. Instead of using a unified model for all traffic scenarios, we manage multiple trained models with different sizes, and adjust the framerate at which a perception model runs and the interpolation method after frame reduction.\\nDecision-making Module. After the perception module, the decision-making module is used to generate driving decisions based on the perception results.\\nIn general, the decision-making methods of autonomous driving can be categorized into rule-based and data-driven. Rule-based methods [ 19 , 20 , 21 ] design a set of rule-matching algorithms and kinetic equations to calculate driving actions. However, these methods suffer from limited flexibility in handling complex scenarios with manually defined rules. With the development of artificial intelligence, many data-driven models use neural networks to generate driving actions. Researchers propose to build deep-learning models [ 22 , 23 ] to imitate the driving trajectories [ 24 , 25 , 26 , 27 , 28 ] of human drivers. Recently, the autonomous driving community has been increasingly embracing reinforcement learning methods [ 29 , 30 , 31 , 32 , 33 ] , which are expected to outperform human drivers by continuous trial and error.\\nThe previous work [ 29 ] proposes a reinforcement learning-based method with a parameterized action structure and a hybrid reward function to optimize the safety, efficiency, comfort, and impact on nearby traffic in autonomous driving.\\nUnfortunately, the aforementioned models are based on the assumption of a perfect perception of the traffic environment. In this study, we aim to construct a robust decision-making model when facing perturbed state features from the actual perception system.',\n",
       "    'subsections': []},\n",
       "   {'title': 'III Overview',\n",
       "    'paragraphs': 'In this study, we follow the standard perception and decision framework [ 29 ] to construct the autonomous driving pipeline, where there is one autonomous vehicle A A and a set of conventional vehicles ‚ÑÇ \\\\mathbb{C} traveling on multi-lane roads. For simplification, we do not consider pedestrians, traffic cones, or other obstacles. The Input is the image and point cloud data acquired from onboard sensors (i.e., six cameras and one LiDAR), where the perception module processes this data to extract environmental features. The Output is the steering angle and velocity control signals of the autonomous vehicle, which are calculated based on the extracted features.\\nThen, we describe some key metrics as follows:\\nEnergy Consumption. The autonomous vehicle in this study is electric-powered. In general, the perception system and driving system are the main components associated with electricity usage for autonomous driving. Firstly, the perception system involves running deep learning models to extract surrounding environmental features.\\nTo avoid interference from other processes and hardware factors (e.g., temperature), we measure its power consumption E \\u200b n \\u200b e \\u200b - \\u200b P Ene\\\\text{-}P based on the total number of floating point operations ( ùêπùêøùëÇùëÉùë† t \\u200b o \\u200b t \\u200b a \\u200b l \\\\mathit{FLOPs}_{total} ) following the previous method [ 3 , 34 ] , i.e., E \\u200b n \\u200b e \\u200b - \\u200b P = ùêπùêøùëÇùëÉùë† t \\u200b o \\u200b t \\u200b a \\u200b l Œ∑ Ene\\\\text{-}P=\\\\frac{\\\\mathit{FLOPs}_{total}}{\\\\eta} , where Œ∑ = 101.71 \\u200b ùê∫ùêπùêøùëÇùëÉùë† / J \\\\eta=101.71\\\\mathit{GFLOPs}/J is the GPU efficiency of GeForce RTX 3090.\\nThen, the driving system is used to drive the autonomous vehicle, where different driving policies exhibit varying levels of electricity usage. We calculate its power consumption E \\u200b n \\u200b e \\u200b - \\u200b D Ene\\\\text{-}D based on the energy difference E D E_{D} between two time steps [ 35 ] and the energy loss E L E_{L} , i.e., E \\u200b n \\u200b e \\u200b - \\u200b D = E L + E D \\u200b ( t 2 ) ‚àí E D \\u200b ( t 1 ) Ene\\\\text{-}D=E_{L}+E_{D}(t_{2})-E_{D}(t_{1}) . E D \\u200b ( t ) = m 2 \\u200b v t 2 + m \\u200b g \\u200b h t + J i \\u200b n \\u200b t 2 \\u200b v t 2 E_{D}(t)=\\\\frac{m}{2}v_{t}^{2}+mgh_{t}+\\\\frac{J_{int}}{2}v^{2}_{t} , where m m , v t v_{t} , g g , h t h_{t} , and J i \\u200b n \\u200b t J_{int} denotes vehicle mass, velocity, gravity acceleration, altitude, and equivalent moment of inertia respectively. E L E_{L} is the energy loss caused by aerodynamic drag, rolling resistance, etc.\\nPerception Accuracy. In addition to energy consumption, accuracy is another important metric of the perception module. Different feature extraction models have different levels of accuracy. In this study, we measure the accuracy by the nuscenes detection score (NDS), which is the most relevant accuracy metric for driving performance [ 36 ] . Specifically, NDS = 1 10 [ 5 m A P \\\\footnotesize\\\\mathrm{NDS}=\\\\frac{1}{10}\\\\allowbreak[5\\\\mathrm{mAP} + ‚àë m ‚àà { mATE , mASE , mAOE , mAVE , mAAE } ( 1 ‚àí min ( 1 , m ) ) ] \\\\allowbreak+\\\\sum_{m\\\\in\\\\{\\\\mathrm{mATE},\\\\mathrm{mASE},\\\\allowbreak\\\\mathrm{mAOE},\\\\mathrm{mAVE},\\\\mathrm{mAAE}\\\\}}\\\\allowbreak(1-\\\\min(1,m))] , where mAP \\\\mathrm{mAP} , mATE \\\\mathrm{mATE} , mASE \\\\mathrm{mASE} , mAOE \\\\mathrm{mAOE} , mAVE \\\\mathrm{mAVE} , and mAAE \\\\mathrm{mAAE} denote mean average precision, translation error, scale error, orientation error, velocity error, attribute error, respectively.\\nDriving Performance. We evaluate the driving performance from four factors: safety, efficiency, comfort, and impact, following the previous work [ 29 ] . These factors are respectively measured by the autonomous vehicle‚Äôs time-to-collision (TTC), speed, acceleration change, and deceleration of the vehicle behind it.\\nObjective. Our objective is to adaptively reduce the energy consumption of the autonomous vehicle under a specific perception accuracy, while maintaining good driving performance.\\nIn this study, we propose an energy-efficient autonomous driving framework, called EneAD , which includes an adaptive perception module and a robust decision module. We show the framework overview in Figure 2 .\\nAdaptive Perception. This module aims to reduce the computational consumption of perception computing, which includes a classification model to output a perception difficulty level for each traffic scenario and a knob tuning component to explore promising configurations for each level. Specifically, the classification model is based solely on image data and applies a lightweight neural network Swin-T. Moreover, it also calculates an uncertainty value by Monte Carlo (MC) dropout. When the uncertainty value is greater than a threshold, the output difficulty level will be modified to the highest level to improve the model‚Äôs reliability. In the knob tuning component, we first set the perception model, framerate, and interpolation method as knobs in the perception system. Then, Bayesian optimization is adopted for configuration search with two objectives: computational consumption and accuracy. To accelerate the tuning process, we further design a meta-surrogate model to transfer tuning knowledge across scenarios with different difficulty levels. Finally, the optimal configuration found at each level and accuracy requirement will be organized into a configuration dictionary. In the inference phase, the autonomous vehicle can directly choose a configuration from the dictionary based on a specified accuracy requirement and the difficulty level of the current traffic scenario without re-searching.\\nRobust Decision. This module aims to make robust driving decisions when the autonomous vehicle uses the above perception module to perceive the surrounding traffic. Firstly, we model the decision-making as a Markov decision process (MDP) that includes four key elements: state, action, state transition, and reward.\\nThen, we propose a reinforcement learning model to generate driving actions and design a regularization technique to prevent large policy updates, thus improving decision robustness.',\n",
       "    'subsections': [{'title': 'III-A Problem Statement',\n",
       "      'paragraphs': 'In this study, we follow the standard perception and decision framework [ 29 ] to construct the autonomous driving pipeline, where there is one autonomous vehicle A A and a set of conventional vehicles ‚ÑÇ \\\\mathbb{C} traveling on multi-lane roads. For simplification, we do not consider pedestrians, traffic cones, or other obstacles. The Input is the image and point cloud data acquired from onboard sensors (i.e., six cameras and one LiDAR), where the perception module processes this data to extract environmental features. The Output is the steering angle and velocity control signals of the autonomous vehicle, which are calculated based on the extracted features.\\nThen, we describe some key metrics as follows:\\nEnergy Consumption. The autonomous vehicle in this study is electric-powered. In general, the perception system and driving system are the main components associated with electricity usage for autonomous driving. Firstly, the perception system involves running deep learning models to extract surrounding environmental features.\\nTo avoid interference from other processes and hardware factors (e.g., temperature), we measure its power consumption E \\u200b n \\u200b e \\u200b - \\u200b P Ene\\\\text{-}P based on the total number of floating point operations ( ùêπùêøùëÇùëÉùë† t \\u200b o \\u200b t \\u200b a \\u200b l \\\\mathit{FLOPs}_{total} ) following the previous method [ 3 , 34 ] , i.e., E \\u200b n \\u200b e \\u200b - \\u200b P = ùêπùêøùëÇùëÉùë† t \\u200b o \\u200b t \\u200b a \\u200b l Œ∑ Ene\\\\text{-}P=\\\\frac{\\\\mathit{FLOPs}_{total}}{\\\\eta} , where Œ∑ = 101.71 \\u200b ùê∫ùêπùêøùëÇùëÉùë† / J \\\\eta=101.71\\\\mathit{GFLOPs}/J is the GPU efficiency of GeForce RTX 3090.\\nThen, the driving system is used to drive the autonomous vehicle, where different driving policies exhibit varying levels of electricity usage. We calculate its power consumption E \\u200b n \\u200b e \\u200b - \\u200b D Ene\\\\text{-}D based on the energy difference E D E_{D} between two time steps [ 35 ] and the energy loss E L E_{L} , i.e., E \\u200b n \\u200b e \\u200b - \\u200b D = E L + E D \\u200b ( t 2 ) ‚àí E D \\u200b ( t 1 ) Ene\\\\text{-}D=E_{L}+E_{D}(t_{2})-E_{D}(t_{1}) . E D \\u200b ( t ) = m 2 \\u200b v t 2 + m \\u200b g \\u200b h t + J i \\u200b n \\u200b t 2 \\u200b v t 2 E_{D}(t)=\\\\frac{m}{2}v_{t}^{2}+mgh_{t}+\\\\frac{J_{int}}{2}v^{2}_{t} , where m m , v t v_{t} , g g , h t h_{t} , and J i \\u200b n \\u200b t J_{int} denotes vehicle mass, velocity, gravity acceleration, altitude, and equivalent moment of inertia respectively. E L E_{L} is the energy loss caused by aerodynamic drag, rolling resistance, etc.\\nPerception Accuracy. In addition to energy consumption, accuracy is another important metric of the perception module. Different feature extraction models have different levels of accuracy. In this study, we measure the accuracy by the nuscenes detection score (NDS), which is the most relevant accuracy metric for driving performance [ 36 ] . Specifically, NDS = 1 10 [ 5 m A P \\\\footnotesize\\\\mathrm{NDS}=\\\\frac{1}{10}\\\\allowbreak[5\\\\mathrm{mAP} + ‚àë m ‚àà { mATE , mASE , mAOE , mAVE , mAAE } ( 1 ‚àí min ( 1 , m ) ) ] \\\\allowbreak+\\\\sum_{m\\\\in\\\\{\\\\mathrm{mATE},\\\\mathrm{mASE},\\\\allowbreak\\\\mathrm{mAOE},\\\\mathrm{mAVE},\\\\mathrm{mAAE}\\\\}}\\\\allowbreak(1-\\\\min(1,m))] , where mAP \\\\mathrm{mAP} , mATE \\\\mathrm{mATE} , mASE \\\\mathrm{mASE} , mAOE \\\\mathrm{mAOE} , mAVE \\\\mathrm{mAVE} , and mAAE \\\\mathrm{mAAE} denote mean average precision, translation error, scale error, orientation error, velocity error, attribute error, respectively.\\nDriving Performance. We evaluate the driving performance from four factors: safety, efficiency, comfort, and impact, following the previous work [ 29 ] . These factors are respectively measured by the autonomous vehicle‚Äôs time-to-collision (TTC), speed, acceleration change, and deceleration of the vehicle behind it.\\nObjective. Our objective is to adaptively reduce the energy consumption of the autonomous vehicle under a specific perception accuracy, while maintaining good driving performance.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-B Framework Overview',\n",
       "      'paragraphs': 'In this study, we propose an energy-efficient autonomous driving framework, called EneAD , which includes an adaptive perception module and a robust decision module. We show the framework overview in Figure 2 .\\nAdaptive Perception. This module aims to reduce the computational consumption of perception computing, which includes a classification model to output a perception difficulty level for each traffic scenario and a knob tuning component to explore promising configurations for each level. Specifically, the classification model is based solely on image data and applies a lightweight neural network Swin-T. Moreover, it also calculates an uncertainty value by Monte Carlo (MC) dropout. When the uncertainty value is greater than a threshold, the output difficulty level will be modified to the highest level to improve the model‚Äôs reliability. In the knob tuning component, we first set the perception model, framerate, and interpolation method as knobs in the perception system. Then, Bayesian optimization is adopted for configuration search with two objectives: computational consumption and accuracy. To accelerate the tuning process, we further design a meta-surrogate model to transfer tuning knowledge across scenarios with different difficulty levels. Finally, the optimal configuration found at each level and accuracy requirement will be organized into a configuration dictionary. In the inference phase, the autonomous vehicle can directly choose a configuration from the dictionary based on a specified accuracy requirement and the difficulty level of the current traffic scenario without re-searching.\\nRobust Decision. This module aims to make robust driving decisions when the autonomous vehicle uses the above perception module to perceive the surrounding traffic. Firstly, we model the decision-making as a Markov decision process (MDP) that includes four key elements: state, action, state transition, and reward.\\nThen, we propose a reinforcement learning model to generate driving actions and design a regularization technique to prevent large policy updates, thus improving decision robustness.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'IV Adaptive Perception',\n",
       "    'paragraphs': 'In this section, we first analyze the limitations of existing scenario classification models and introduce the development of our model. Then, we detail the knob setting in the perception module and the knob tuning model for exploring promising configurations in the configuration space.\\nThe perception difficulty of each traffic scenario is influenced by various environmental factors, including weather conditions, lighting conditions, and traffic density [ 37 ] . In simple scenarios (e.g., sunny days), using only low-computation configurations can achieve a high perception accuracy, while in complex scenarios (e.g., rainy days), high-computation configurations are required to maintain accuracy. Since point cloud data cannot identify lighting conditions that can affect perception, researchers distinguish the perception difficulty of different traffic scenarios solely by the image data quality: higher quality means a lower difficulty level, while lower quality signifies a higher one.\\nIn general, the classification methods can be divided into traditional methods and deep learning methods.\\nThe traditional methods [ 38 , 39 ] use nature scene statistics (NSS) to distinguish images, which can reflect the degree of image distortion based on some handcrafted metrics. With the development of neural networks, deep learning-based methods have drawn much attention for their superior performance. A recent study [ 40 ] proposes using neural networks to encode images and then adopting clustering methods to classify these encoding vectors. Due to the lack of interpretability in clustering rules, many studies [ 41 , 42 , 43 ] utilize deep learning-based models to directly predict the image quality.\\nHowever, directly using these methods in our task usually leads to unsatisfactory results due to the following three limitations:\\n(1) They are general-purpose rather than specific to autonomous driving perception. Specifically, they annotate the image categories according to the degree of image distortion or subjective human scoring, which makes it difficult to accurately reflect the perception difficulty of different traffic scenarios. In contrast, a classification criterion directly related to the quality of subsequent perception results would be better for our scenario classification task.\\n(2) Although these methods can attain relatively high accuracy in scenario classification, their reliability is insufficient. When the autonomous vehicle encounters unseen traffic scenarios, these methods may underestimate the true difficulty level of the scenario. This can mislead our perception system to use low-computational configurations, which may result in a sharp decrease in accuracy and thus raise safety concerns.\\nBased on the above analysis, we will build a task-specific and reliable difficulty classification model based on image data. To achieve this, we first build a dataset dedicated to the autonomous driving perception task, where the image data is automatically labeled based on the quality of perception results after running a unified perception model. Then, to avoid introducing a large computational overhead to the perception system, we use a lightweight model Swin-T [ 44 ] as the main neural network to encode image data. Finally, to enhance the reliability of classification results, we calculate an uncertainty value using the Monte Carlo (MC)\\ndropout technique [ 45 ] , which is a simple and effective implementation of Bayesian neural networks. Next, we introduce the data generation process and model architecture as follows:\\nDataset Generation. Inspired by the previous work [ 46 , 47 ] , we label the perception difficulty of images based on the detection results after running a unified object detection model on them. The images with better detection results are regarded as lower difficulty levels, and the ones with poorer results are regarded as higher levels.\\nTo be detailed, we generate the dataset by the following three steps:\\n(1) Collect image data from a real-world dataset Nuscenes-R [ 16 ] and a high-fidelity synthetic dataset Nuscenes-S [ 48 ] , where the traffic scenarios have various weather conditions and vehicle density. Since the raw image data in each traffic scenario is based on a 360-degree bird‚Äôs-eye view consisting of six images from different directions, we stitch these images together as one image instance. Afterward, we split all image instances into a training set and a test set in a ratio of 4 : 1.\\n(2) Run a unified object detection model on raw image data in each traffic scenario and calculate a metric NDS [ 36 ] to evaluate detection performance.\\nSpecifically, we choose SparseBEV [ 6 ] as the detection model, and NDS refers to the Nuscenes detection score that reflects perception accuracy. Generally, more complex scenarios (e.g., worse weather conditions) result in higher perception difficulty, which in turn leads to lower NDS values.\\n(3) Classify all image instances into k k difficulty levels based on their NDS values. We first sort all image instances by their NDS values in ascending order and then divide them into multiple equal groups for simplicity. Taking k = 4 k=4 as an example, the difficulty levels 1, 2, 3, and 4 are allocated to four equal groups, and all image instances within a group are assigned the same difficulty level.\\nIn the end, the scenario classification task in this study is defined as: Given an image instance x x , predict the perception difficulty level y ^ \\\\hat{y} it belongs to, where y ^ ‚àà { 1 , 2 , ‚Ä¶ , k } \\\\hat{y}\\\\in\\\\{1,2,\\\\dots,k\\\\} .\\nNetwork Architecture. We introduce the network architecture of our scenario classification model as shown in Figure 3 , and present the calculation formulas in the following.\\nFirstly, we design an image encoder that uses the Swin-T network [ 44 ] to encode the image sample x x , as follows:\\nwhere h h denotes the encoding vector of x x and w 1 w_{1} denotes the learnable network parameters. In the experiment Section VI-C , we find that this lightweight model Swin-T achieves almost the same accuracy as other heavyweight networks.\\nThen, we design a classification head based on Multi-layer Perceptron (MLP) to calculate the probability that the image data x x belongs to each difficulty level, as follows:\\nwhere w 2 w_{2} denotes the learnable network parameters, P = [ p 1 , p 2 , ‚Ä¶ , p k ] P=[p_{1},p_{2},\\\\dots,p_{k}] is a probability vector that respectively indicates the probabilities of k k difficulty levels, and the level with the highest probability is regarded as the predicted level of x x . We use three linear layers in this MLP. Moreover, we use the MC dropout technique [ 49 ] in the MLP to avoid model overfitting in the training phase and obtain uncertainty in the inference phase.\\nIn the training phase, since our scenario classification is a multi-class classification task, we use the cross-entropy loss [ 50 ] as our loss function ‚Ñí 1 \\\\mathcal{L}_{1} , as follows:\\nwhere y y denotes the true difficulty level of the image data x x , and p y p_{y} is the probability assigned to the true level in the probability vector P P . This loss encourages the classification model to maximize the probability of the true level.\\nIn the inference phase, the classification head performs T T forward pass after encoding an image x x in the image encoder, where each forward pass randomly deactivates a specific fraction of neurons. Afterward, we calculate the difficulty level y ^ \\\\hat{y} of x x and an uncertainty value u u , as follows:\\nwhere P i P^{i} denotes the probability vector in the i ‚àí i- th forward pass based on Equation 2 , ùëéùëüùëîùëöùëéùë• \\\\mathit{argmax} is used to find the difficulty level with the highest average probability as the final classification result y ^ \\\\hat{y} , and p y ^ i p_{\\\\hat{y}}^{i} denotes the probability of y ^ \\\\hat{y} at the i ‚àí i- th forward pass.\\nIf the uncertainty value u u is larger than a threshold G G , we will set the difficulty level y ^ \\\\hat{y} to k k (i.e., the highest difficulty level). In the later knob tuning section, the traffic scenarios with a difficulty level of k k will use the highest-computation configuration.\\nTherefore, this operation helps prevent a sharp decline in the perception accuracy caused by incorrect scenario classification results. Also, the additional computation it brings is minimal because we only run the classification head T T times, excluding the image encoder.\\nAfter the above section, all traffic scenarios are classified into k k difficulty levels. For each type, we respectively set three knobs (i.e., perception model, framerate, and interpolation method after frame reduction) that can affect both computational consumption and perception accuracy. To be detailed, we describe these knobs as follows:\\n(1) k \\u200b n \\u200b o \\u200b b 1 knob_{1} : Perception models with different sizes.\\nThis study adopts a series of BEV-based models to extract the surrounding vehicle features (e.g., position and orientation) of the autonomous vehicle.\\nSpecifically, we set the candidate set of k \\u200b n \\u200b o \\u200b b 1 knob_{1} as { ùëÜùëùùëéùëüùë†ùëíùêµùëíùë£ , ùëÜùëùùëéùëüùë†ùëíùêπùë¢ùë†ùëñùëúùëõ , ùêµùëíùë£ùêπùë¢ùë†ùëñùëúùëõ , ùêµùëíùë£ùêπùë¢ùë†ùëñùëúùëõ \\u200b - \\u200b e } \\\\{\\\\mathit{SparseBev},\\\\allowbreak\\\\mathit{SparseFusion},\\\\mathit{BevFusion},\\\\mathit{BevFusion}\\\\mathord{-}e\\\\} , where SparseBEV [ 6 ] is a camera-only method, SparseFusion [ 11 ] and BevFusion [ 9 ] are multi-modality methods, and BevFusion-e is the ensemble version of BevFusion, respectively. We implement them based on their open-source code repositories. From left to right, these models have higher computational consumption but higher accuracy.\\n(2) k \\u200b n \\u200b o \\u200b b 2 knob_{2} : Framerate. The onboard cameras and LiDAR in this study capture image data and point cloud data at 20 frames per second. Based on it, we express the frame rate as the number of frames to skip each time before the next run of a perception model. Then, the candidate set for this knob is defined as { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 } \\\\{0,1,2,3,4,5,6,7,8,9\\\\} frames. A larger value results in larger resource savings but larger accuracy degradation.\\n(3) k \\u200b n \\u200b o \\u200b b 3 knob_{3} : Interpolation methods after frame reduction. If the perception framerate is reduced, this study considers using a interpolation method to fill in the vehicle features in skipped frames. The candidate set of k \\u200b n \\u200b o \\u200b b 3 knob_{3} is set as { l \\u200b i \\u200b n \\u200b e \\u200b a \\u200b r , p \\u200b r \\u200b e \\u200b d \\u200b i \\u200b c \\u200b t \\u200b i \\u200b o \\u200b n } \\\\{linear,prediction\\\\} . The linear method assumes that the surrounding vehicle maintains the driving actions of the last time frame [ 51 ] .\\nOn the contrary, the prediction method fills in the missed vehicle features with a trajectory prediction model, following the previous work [ 52 ] . The prediction method has higher computational consumption but higher accuracy for the filled features compared to the linear method.\\nFinally, the number n n of configurations can be calculated as the product of the number of candidate values for these three knobs at k k different scenario difficulty levels, i.e., n = 4 √ó 10 √ó 2 √ó k = 80 \\u200b k n=4\\\\times 10\\\\times 2\\\\times k=80k . Although the computational consumption of a configuration can be estimated, its accuracy is unpredictable and needs to be obtained by running it on raw perception data (i.e., images and/or point cloud data), which demands much computational consumption 1 1 1 We run these configurations on a 5.5-hour dataset, and each configuration takes an average of 14 hours on a GeForce RTX 3090. . Therefore, it is cost-prohibit to run all configurations to get the optimal configuration in each scenario difficulty level.\\nBased on the above, we adopt knob tuning to find optimal/near-optimal configurations by evaluating partial configurations rather than all configurations. Also, there are k k tuning instances corresponding to traffic scenarios of k k perception difficulty levels.\\nThis study adopts multi-objective Bayesian optimization to perform knob tuning, given its superior performance in other tuning tasks [ 53 ] . To further speed up the tuning process, we design a meta-surrogate model to transfer tuning knowledge between different difficulty levels.\\nWe introduce the main components of Bayesian optimization in each tuning instance as follows:\\n(1) Objective Function f \\u200b ( Œ∏ ) f(\\\\theta) . We use Œ∏ j \\\\theta_{j} to represent a configuration and f i \\u200b ( Œ∏ ) f^{i}(\\\\theta) to represent the objective function in a tuning instance i i . In this study, the objective function consists of two objectives, i.e., computational consumption f i \\u200b ( Œ∏ ) . c \\u200b o \\u200b m f^{i}(\\\\theta).com and accuracy f i \\u200b ( Œ∏ ) . a \\u200b c \\u200b c f^{i}(\\\\theta).acc . To align both objective values with the minimization goal of Bayesian optimization, f i \\u200b ( Œ∏ ) . a \\u200b c \\u200b c f^{i}(\\\\theta).acc is the negated value of raw accuracy value.\\n(2) Surrogate Model ‚Ñ≥ \\\\mathcal{M} . We represent the surrogate model in a tuning instance i i as ‚Ñ≥ i = ( Œ∏ , f i \\u200b ( Œ∏ ) , œÉ 2 ) \\\\mathcal{M}^{i}=(\\\\theta,f^{i}(\\\\theta),\\\\sigma^{2}) , which can approximate the objective function value f i \\u200b ( Œ∏ ) f^{i}(\\\\theta) of each configuration and provide an uncertainty estimation œÉ 2 \\\\sigma^{2} . It can be continuously updated according to the objective function values of actual evaluated configurations. Since the knob value is discrete, we adopt SMAC (Sequential Model-based Algorithm Configuration) [ 54 ] as the probability surrogate model.\\n(3) Acquisition function Œ± \\\\alpha . The acquisition function Œ± \\\\alpha is used to decide which configuration to explore based on the surrogate model. To adapt to multiple objectives, we use EHVI (Expected Hypervolume Improvement) [ 55 ] as the acquisition function.\\nAt each tuning iteration, we first obtain a configuration using the acquisition function Œ± i \\\\alpha^{i} . Then, we run the configuration to get its objective function values, based on which the surrogate model is updated. As more tuning iterations are done, the surrogate model becomes more accurate, and the acquisition function directs the search toward the optimal solution.\\nMeta-surrogate Model. Based on the above, there are k k independent tuning instances for traffic scenarios with different perception difficulty levels. Each tuning instance needs to explore and run explored configurations separately and update its surrogate model from scratch. In this study, we design a meta-surrogate model to reduce the number of tuning iterations by warm-starting a new tuning instance with the previous tuned instances. However, directly transferring the surrogate model from other tuning instances is not feasible, as their output accuracy values differ in scale.\\nThat is, the magnitude of accuracy values for the same configuration diverges sharply across tuning instances of scenarios with different perception difficulty levels.\\nTo address this, we adopt a simple strategy that normalizes the accuracy value within each tuning instance. As a result, the output accuracy of each surrogate model is a relative value rather than an absolute one. The relative relationships in the objective value between configurations can still provide valuable insights in different tuning instances [ 56 ] . Finally, we will detail the main steps of this transfer strategy.\\n(1) Normalize objective values. To normalize the objective function in a tuning instance i i , we first run the lowest-computation configuration Œ∏ l \\\\theta_{l} and the highest-computation configuration Œ∏ h \\\\theta_{h} to get their objective values. Then, we use min-max normalization to normalize the objective function values as f ¬Ø i \\u200b ( Œ∏ j ) . c \\u200b o \\u200b m \\\\overline{f}^{i}(\\\\theta_{j}).com and f ¬Ø i \\u200b ( Œ∏ j ) . a \\u200b c \\u200b c \\\\overline{f}^{i}(\\\\theta_{j}).acc .\\n(2) Construct meta-surrogate model.\\nWe use ‚Ñ≥ i \\\\mathcal{M}^{i} ( i = 1 , 2 , ‚Ä¶ , k i=1,2,\\\\dots,k ) to represent the surrogate models in k k tuning instances and execute them sequentially. The first tuning instance still starts tuning from scratch. But for subsequent tuning tasks, we build a meta-surrogate model ‚Ñ≥ m \\u200b e \\u200b t \\u200b a \\\\mathcal{M}^{meta} using the previously tuned instances. For example, assuming we will execute the i i -th ( i > 1 i>1 ) tuning instances, we initialize the objective function f ¬Ø m \\u200b e \\u200b t \\u200b a \\u200b ( Œ∏ ) \\\\overline{f}^{meta}(\\\\theta) as follows.\\nwhere f ¬Ø m \\u200b e \\u200b t \\u200b a \\u200b ( Œ∏ ) \\\\overline{f}^{meta}(\\\\theta) is the average of the objective functions of all previously tuned tasks, and we use it to initialize the surrogate model ‚Ñ≥ i \\\\mathcal{M}^{i} . In this way, ‚Ñ≥ i \\\\mathcal{M}^{i} can be represented as ‚Ñ≥ i = ‚Ñ≥ m \\u200b e \\u200b t \\u200b a = ( Œ∏ , f ¬Ø m \\u200b e \\u200b t \\u200b a \\u200b ( Œ∏ ) , œÉ 2 ) \\\\mathcal{M}^{i}=\\\\mathcal{M}^{meta}=(\\\\theta,\\\\overline{f}^{meta}(\\\\theta),\\\\sigma^{2}) . Note that the uncertainty estimation œÉ 2 \\\\sigma^{2} is initialized with the default method [ 54 ] to guide the exploration in the configuration space.\\nWe present the entire tuning process in Algorithm 1 . At first, we define five accuracy requirements, ùíú i \\u200b ( i = 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ) \\\\mathcal{A}_{i}\\\\ (i=1,2,3,4,5,6,7,8) , corresponding to NDS values of 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, and 0.76 2 2 2 The most expensive model BevFusion-e [ 9 ] in our experiment can only achieve an NDS of 0.761. After the experiment, we chose 0.74 as the final accuracy requirement, which achieves a good balance between energy consumption and driving performance. , respectively.\\nThen, the pseudocode in lines 2 ‚àí 5 2-5 shows the meta-surrogate model construction at the beginning of a tuning instance. The pseudocode in lines 6 ‚àí 18 6-18 shows the tuning process, where the acquisition function is used to select the next configuration to explore, and the surrogate model is updated with normalized objective values. During tuning, we maintain a configuration dictionary ùîª \\\\mathbb{D} to save the current lowest-computation configurations under different accuracy requirements in different tuning instances. If all configurations fail to reach a certain accuracy in a tuning instance, its configuration in the dictionary will be set to the highest-computation one. Finally, we will terminate a tuning instance when it converges or the maximum number of configurations N = 80 N=80 is reached.',\n",
       "    'subsections': [{'title': 'IV-A Limitation of Existing Scenario Classification Models',\n",
       "      'paragraphs': 'The perception difficulty of each traffic scenario is influenced by various environmental factors, including weather conditions, lighting conditions, and traffic density [ 37 ] . In simple scenarios (e.g., sunny days), using only low-computation configurations can achieve a high perception accuracy, while in complex scenarios (e.g., rainy days), high-computation configurations are required to maintain accuracy. Since point cloud data cannot identify lighting conditions that can affect perception, researchers distinguish the perception difficulty of different traffic scenarios solely by the image data quality: higher quality means a lower difficulty level, while lower quality signifies a higher one.\\nIn general, the classification methods can be divided into traditional methods and deep learning methods.\\nThe traditional methods [ 38 , 39 ] use nature scene statistics (NSS) to distinguish images, which can reflect the degree of image distortion based on some handcrafted metrics. With the development of neural networks, deep learning-based methods have drawn much attention for their superior performance. A recent study [ 40 ] proposes using neural networks to encode images and then adopting clustering methods to classify these encoding vectors. Due to the lack of interpretability in clustering rules, many studies [ 41 , 42 , 43 ] utilize deep learning-based models to directly predict the image quality.\\nHowever, directly using these methods in our task usually leads to unsatisfactory results due to the following three limitations:\\n(1) They are general-purpose rather than specific to autonomous driving perception. Specifically, they annotate the image categories according to the degree of image distortion or subjective human scoring, which makes it difficult to accurately reflect the perception difficulty of different traffic scenarios. In contrast, a classification criterion directly related to the quality of subsequent perception results would be better for our scenario classification task.\\n(2) Although these methods can attain relatively high accuracy in scenario classification, their reliability is insufficient. When the autonomous vehicle encounters unseen traffic scenarios, these methods may underestimate the true difficulty level of the scenario. This can mislead our perception system to use low-computational configurations, which may result in a sharp decrease in accuracy and thus raise safety concerns.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-B Development of Our Scenario Classification Model',\n",
       "      'paragraphs': 'Based on the above analysis, we will build a task-specific and reliable difficulty classification model based on image data. To achieve this, we first build a dataset dedicated to the autonomous driving perception task, where the image data is automatically labeled based on the quality of perception results after running a unified perception model. Then, to avoid introducing a large computational overhead to the perception system, we use a lightweight model Swin-T [ 44 ] as the main neural network to encode image data. Finally, to enhance the reliability of classification results, we calculate an uncertainty value using the Monte Carlo (MC)\\ndropout technique [ 45 ] , which is a simple and effective implementation of Bayesian neural networks. Next, we introduce the data generation process and model architecture as follows:\\nDataset Generation. Inspired by the previous work [ 46 , 47 ] , we label the perception difficulty of images based on the detection results after running a unified object detection model on them. The images with better detection results are regarded as lower difficulty levels, and the ones with poorer results are regarded as higher levels.\\nTo be detailed, we generate the dataset by the following three steps:\\n(1) Collect image data from a real-world dataset Nuscenes-R [ 16 ] and a high-fidelity synthetic dataset Nuscenes-S [ 48 ] , where the traffic scenarios have various weather conditions and vehicle density. Since the raw image data in each traffic scenario is based on a 360-degree bird‚Äôs-eye view consisting of six images from different directions, we stitch these images together as one image instance. Afterward, we split all image instances into a training set and a test set in a ratio of 4 : 1.\\n(2) Run a unified object detection model on raw image data in each traffic scenario and calculate a metric NDS [ 36 ] to evaluate detection performance.\\nSpecifically, we choose SparseBEV [ 6 ] as the detection model, and NDS refers to the Nuscenes detection score that reflects perception accuracy. Generally, more complex scenarios (e.g., worse weather conditions) result in higher perception difficulty, which in turn leads to lower NDS values.\\n(3) Classify all image instances into k k difficulty levels based on their NDS values. We first sort all image instances by their NDS values in ascending order and then divide them into multiple equal groups for simplicity. Taking k = 4 k=4 as an example, the difficulty levels 1, 2, 3, and 4 are allocated to four equal groups, and all image instances within a group are assigned the same difficulty level.\\nIn the end, the scenario classification task in this study is defined as: Given an image instance x x , predict the perception difficulty level y ^ \\\\hat{y} it belongs to, where y ^ ‚àà { 1 , 2 , ‚Ä¶ , k } \\\\hat{y}\\\\in\\\\{1,2,\\\\dots,k\\\\} .\\nNetwork Architecture. We introduce the network architecture of our scenario classification model as shown in Figure 3 , and present the calculation formulas in the following.\\nFirstly, we design an image encoder that uses the Swin-T network [ 44 ] to encode the image sample x x , as follows:\\nwhere h h denotes the encoding vector of x x and w 1 w_{1} denotes the learnable network parameters. In the experiment Section VI-C , we find that this lightweight model Swin-T achieves almost the same accuracy as other heavyweight networks.\\nThen, we design a classification head based on Multi-layer Perceptron (MLP) to calculate the probability that the image data x x belongs to each difficulty level, as follows:\\nwhere w 2 w_{2} denotes the learnable network parameters, P = [ p 1 , p 2 , ‚Ä¶ , p k ] P=[p_{1},p_{2},\\\\dots,p_{k}] is a probability vector that respectively indicates the probabilities of k k difficulty levels, and the level with the highest probability is regarded as the predicted level of x x . We use three linear layers in this MLP. Moreover, we use the MC dropout technique [ 49 ] in the MLP to avoid model overfitting in the training phase and obtain uncertainty in the inference phase.\\nIn the training phase, since our scenario classification is a multi-class classification task, we use the cross-entropy loss [ 50 ] as our loss function ‚Ñí 1 \\\\mathcal{L}_{1} , as follows:\\nwhere y y denotes the true difficulty level of the image data x x , and p y p_{y} is the probability assigned to the true level in the probability vector P P . This loss encourages the classification model to maximize the probability of the true level.\\nIn the inference phase, the classification head performs T T forward pass after encoding an image x x in the image encoder, where each forward pass randomly deactivates a specific fraction of neurons. Afterward, we calculate the difficulty level y ^ \\\\hat{y} of x x and an uncertainty value u u , as follows:\\nwhere P i P^{i} denotes the probability vector in the i ‚àí i- th forward pass based on Equation 2 , ùëéùëüùëîùëöùëéùë• \\\\mathit{argmax} is used to find the difficulty level with the highest average probability as the final classification result y ^ \\\\hat{y} , and p y ^ i p_{\\\\hat{y}}^{i} denotes the probability of y ^ \\\\hat{y} at the i ‚àí i- th forward pass.\\nIf the uncertainty value u u is larger than a threshold G G , we will set the difficulty level y ^ \\\\hat{y} to k k (i.e., the highest difficulty level). In the later knob tuning section, the traffic scenarios with a difficulty level of k k will use the highest-computation configuration.\\nTherefore, this operation helps prevent a sharp decline in the perception accuracy caused by incorrect scenario classification results. Also, the additional computation it brings is minimal because we only run the classification head T T times, excluding the image encoder.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-C Knob Setting for Adaptive Perception',\n",
       "      'paragraphs': 'After the above section, all traffic scenarios are classified into k k difficulty levels. For each type, we respectively set three knobs (i.e., perception model, framerate, and interpolation method after frame reduction) that can affect both computational consumption and perception accuracy. To be detailed, we describe these knobs as follows:\\n(1) k \\u200b n \\u200b o \\u200b b 1 knob_{1} : Perception models with different sizes.\\nThis study adopts a series of BEV-based models to extract the surrounding vehicle features (e.g., position and orientation) of the autonomous vehicle.\\nSpecifically, we set the candidate set of k \\u200b n \\u200b o \\u200b b 1 knob_{1} as { ùëÜùëùùëéùëüùë†ùëíùêµùëíùë£ , ùëÜùëùùëéùëüùë†ùëíùêπùë¢ùë†ùëñùëúùëõ , ùêµùëíùë£ùêπùë¢ùë†ùëñùëúùëõ , ùêµùëíùë£ùêπùë¢ùë†ùëñùëúùëõ \\u200b - \\u200b e } \\\\{\\\\mathit{SparseBev},\\\\allowbreak\\\\mathit{SparseFusion},\\\\mathit{BevFusion},\\\\mathit{BevFusion}\\\\mathord{-}e\\\\} , where SparseBEV [ 6 ] is a camera-only method, SparseFusion [ 11 ] and BevFusion [ 9 ] are multi-modality methods, and BevFusion-e is the ensemble version of BevFusion, respectively. We implement them based on their open-source code repositories. From left to right, these models have higher computational consumption but higher accuracy.\\n(2) k \\u200b n \\u200b o \\u200b b 2 knob_{2} : Framerate. The onboard cameras and LiDAR in this study capture image data and point cloud data at 20 frames per second. Based on it, we express the frame rate as the number of frames to skip each time before the next run of a perception model. Then, the candidate set for this knob is defined as { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 } \\\\{0,1,2,3,4,5,6,7,8,9\\\\} frames. A larger value results in larger resource savings but larger accuracy degradation.\\n(3) k \\u200b n \\u200b o \\u200b b 3 knob_{3} : Interpolation methods after frame reduction. If the perception framerate is reduced, this study considers using a interpolation method to fill in the vehicle features in skipped frames. The candidate set of k \\u200b n \\u200b o \\u200b b 3 knob_{3} is set as { l \\u200b i \\u200b n \\u200b e \\u200b a \\u200b r , p \\u200b r \\u200b e \\u200b d \\u200b i \\u200b c \\u200b t \\u200b i \\u200b o \\u200b n } \\\\{linear,prediction\\\\} . The linear method assumes that the surrounding vehicle maintains the driving actions of the last time frame [ 51 ] .\\nOn the contrary, the prediction method fills in the missed vehicle features with a trajectory prediction model, following the previous work [ 52 ] . The prediction method has higher computational consumption but higher accuracy for the filled features compared to the linear method.\\nFinally, the number n n of configurations can be calculated as the product of the number of candidate values for these three knobs at k k different scenario difficulty levels, i.e., n = 4 √ó 10 √ó 2 √ó k = 80 \\u200b k n=4\\\\times 10\\\\times 2\\\\times k=80k . Although the computational consumption of a configuration can be estimated, its accuracy is unpredictable and needs to be obtained by running it on raw perception data (i.e., images and/or point cloud data), which demands much computational consumption 1 1 1 We run these configurations on a 5.5-hour dataset, and each configuration takes an average of 14 hours on a GeForce RTX 3090. . Therefore, it is cost-prohibit to run all configurations to get the optimal configuration in each scenario difficulty level.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-D Knob tuning for Finding Promising Configurations',\n",
       "      'paragraphs': 'Based on the above, we adopt knob tuning to find optimal/near-optimal configurations by evaluating partial configurations rather than all configurations. Also, there are k k tuning instances corresponding to traffic scenarios of k k perception difficulty levels.\\nThis study adopts multi-objective Bayesian optimization to perform knob tuning, given its superior performance in other tuning tasks [ 53 ] . To further speed up the tuning process, we design a meta-surrogate model to transfer tuning knowledge between different difficulty levels.\\nWe introduce the main components of Bayesian optimization in each tuning instance as follows:\\n(1) Objective Function f \\u200b ( Œ∏ ) f(\\\\theta) . We use Œ∏ j \\\\theta_{j} to represent a configuration and f i \\u200b ( Œ∏ ) f^{i}(\\\\theta) to represent the objective function in a tuning instance i i . In this study, the objective function consists of two objectives, i.e., computational consumption f i \\u200b ( Œ∏ ) . c \\u200b o \\u200b m f^{i}(\\\\theta).com and accuracy f i \\u200b ( Œ∏ ) . a \\u200b c \\u200b c f^{i}(\\\\theta).acc . To align both objective values with the minimization goal of Bayesian optimization, f i \\u200b ( Œ∏ ) . a \\u200b c \\u200b c f^{i}(\\\\theta).acc is the negated value of raw accuracy value.\\n(2) Surrogate Model ‚Ñ≥ \\\\mathcal{M} . We represent the surrogate model in a tuning instance i i as ‚Ñ≥ i = ( Œ∏ , f i \\u200b ( Œ∏ ) , œÉ 2 ) \\\\mathcal{M}^{i}=(\\\\theta,f^{i}(\\\\theta),\\\\sigma^{2}) , which can approximate the objective function value f i \\u200b ( Œ∏ ) f^{i}(\\\\theta) of each configuration and provide an uncertainty estimation œÉ 2 \\\\sigma^{2} . It can be continuously updated according to the objective function values of actual evaluated configurations. Since the knob value is discrete, we adopt SMAC (Sequential Model-based Algorithm Configuration) [ 54 ] as the probability surrogate model.\\n(3) Acquisition function Œ± \\\\alpha . The acquisition function Œ± \\\\alpha is used to decide which configuration to explore based on the surrogate model. To adapt to multiple objectives, we use EHVI (Expected Hypervolume Improvement) [ 55 ] as the acquisition function.\\nAt each tuning iteration, we first obtain a configuration using the acquisition function Œ± i \\\\alpha^{i} . Then, we run the configuration to get its objective function values, based on which the surrogate model is updated. As more tuning iterations are done, the surrogate model becomes more accurate, and the acquisition function directs the search toward the optimal solution.\\nMeta-surrogate Model. Based on the above, there are k k independent tuning instances for traffic scenarios with different perception difficulty levels. Each tuning instance needs to explore and run explored configurations separately and update its surrogate model from scratch. In this study, we design a meta-surrogate model to reduce the number of tuning iterations by warm-starting a new tuning instance with the previous tuned instances. However, directly transferring the surrogate model from other tuning instances is not feasible, as their output accuracy values differ in scale.\\nThat is, the magnitude of accuracy values for the same configuration diverges sharply across tuning instances of scenarios with different perception difficulty levels.\\nTo address this, we adopt a simple strategy that normalizes the accuracy value within each tuning instance. As a result, the output accuracy of each surrogate model is a relative value rather than an absolute one. The relative relationships in the objective value between configurations can still provide valuable insights in different tuning instances [ 56 ] . Finally, we will detail the main steps of this transfer strategy.\\n(1) Normalize objective values. To normalize the objective function in a tuning instance i i , we first run the lowest-computation configuration Œ∏ l \\\\theta_{l} and the highest-computation configuration Œ∏ h \\\\theta_{h} to get their objective values. Then, we use min-max normalization to normalize the objective function values as f ¬Ø i \\u200b ( Œ∏ j ) . c \\u200b o \\u200b m \\\\overline{f}^{i}(\\\\theta_{j}).com and f ¬Ø i \\u200b ( Œ∏ j ) . a \\u200b c \\u200b c \\\\overline{f}^{i}(\\\\theta_{j}).acc .\\n(2) Construct meta-surrogate model.\\nWe use ‚Ñ≥ i \\\\mathcal{M}^{i} ( i = 1 , 2 , ‚Ä¶ , k i=1,2,\\\\dots,k ) to represent the surrogate models in k k tuning instances and execute them sequentially. The first tuning instance still starts tuning from scratch. But for subsequent tuning tasks, we build a meta-surrogate model ‚Ñ≥ m \\u200b e \\u200b t \\u200b a \\\\mathcal{M}^{meta} using the previously tuned instances. For example, assuming we will execute the i i -th ( i > 1 i>1 ) tuning instances, we initialize the objective function f ¬Ø m \\u200b e \\u200b t \\u200b a \\u200b ( Œ∏ ) \\\\overline{f}^{meta}(\\\\theta) as follows.\\nwhere f ¬Ø m \\u200b e \\u200b t \\u200b a \\u200b ( Œ∏ ) \\\\overline{f}^{meta}(\\\\theta) is the average of the objective functions of all previously tuned tasks, and we use it to initialize the surrogate model ‚Ñ≥ i \\\\mathcal{M}^{i} . In this way, ‚Ñ≥ i \\\\mathcal{M}^{i} can be represented as ‚Ñ≥ i = ‚Ñ≥ m \\u200b e \\u200b t \\u200b a = ( Œ∏ , f ¬Ø m \\u200b e \\u200b t \\u200b a \\u200b ( Œ∏ ) , œÉ 2 ) \\\\mathcal{M}^{i}=\\\\mathcal{M}^{meta}=(\\\\theta,\\\\overline{f}^{meta}(\\\\theta),\\\\sigma^{2}) . Note that the uncertainty estimation œÉ 2 \\\\sigma^{2} is initialized with the default method [ 54 ] to guide the exploration in the configuration space.\\nWe present the entire tuning process in Algorithm 1 . At first, we define five accuracy requirements, ùíú i \\u200b ( i = 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ) \\\\mathcal{A}_{i}\\\\ (i=1,2,3,4,5,6,7,8) , corresponding to NDS values of 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, and 0.76 2 2 2 The most expensive model BevFusion-e [ 9 ] in our experiment can only achieve an NDS of 0.761. After the experiment, we chose 0.74 as the final accuracy requirement, which achieves a good balance between energy consumption and driving performance. , respectively.\\nThen, the pseudocode in lines 2 ‚àí 5 2-5 shows the meta-surrogate model construction at the beginning of a tuning instance. The pseudocode in lines 6 ‚àí 18 6-18 shows the tuning process, where the acquisition function is used to select the next configuration to explore, and the surrogate model is updated with normalized objective values. During tuning, we maintain a configuration dictionary ùîª \\\\mathbb{D} to save the current lowest-computation configurations under different accuracy requirements in different tuning instances. If all configurations fail to reach a certain accuracy in a tuning instance, its configuration in the dictionary will be set to the highest-computation one. Finally, we will terminate a tuning instance when it converges or the maximum number of configurations N = 80 N=80 is reached.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'V Robust Decision',\n",
       "    'paragraphs': 'This section is used to make driving decisions based on perception results from the above perception module. Firstly, we model the decision-making problem under Markov decision process (MDP).\\nThen, we design a regularization technique to enhance the decision-making robustness when facing perturbed state features.\\nMarkov decision process (MDP) can be defined as: ‚Ñ≥ = ‚ü® ùíÆ , ùíú , ùí´ , ‚Ñõ ‚ü© \\\\mathcal{M}=\\\\langle{\\\\mathcal{S},\\\\mathcal{A}},\\\\mathcal{P},\\\\mathcal{R}\\\\rangle , which includes four key elements: state ùíÆ \\\\mathcal{S} , action ùíú \\\\mathcal{A} , state transition probability ùí´ \\\\mathcal{P} and reward ‚Ñõ \\\\mathcal{R} . Specifically, the state s t s^{t} denotes the extracted environmental features at time frame t t , the action a t a^{t} denotes the driving decision, the state transition is implemented via environment rendering in the simulator Carla [ 57 ] , and the reward r t r^{t} is used to rate the action, where we adopt a hybrid reward function including four reward factors: safety, efficiency, comfort, and impact. Following the previous work [ 32 ] , the safety factor is measured by time-to-collision (TTC) of the autonomous vehicle, which denotes the time span left before a collision. The efficiency factor and the comfort factor are measured by the velocity and acceleration change rate of the autonomous vehicle. The impact factor is measured by the passive deceleration of the rear traffic flow caused by the autonomous vehicle.\\nFigure 4 shows the architecture of our reinforcement learning model, which includes an action network and a value network. Specifically, the actor network incorporates a parameterized action schema [ 58 ] , outputting three action values (i.e., a l \\u200b l t a_{ll}^{t} , a l \\u200b r t a_{lr}^{t} , and a l \\u200b k t a_{lk}^{t} 3 3 3 Each action includes a steering angle value and a velocity change value. ) at each time step that correspond to three lane-changing decisions (i.e., left lane-changing l \\u200b l ll , right lane-changing l \\u200b r lr , and lane-keeping l \\u200b k lk ). The value network outputs the Q values of the three actions, and the action with the highest Q value is the final action performed by the autonomous vehicle.\\nTo solve the parameterized action schema, researchers propose many reinforcement learning methods [ 59 , 60 , 58 , 29 , 61 ] to learn an action policy. Specifically, the P-DDPG method [ 60 ] collapses the parameterized action space into a continuous one. However, it does not account for which action is associated with which lane-changing decision, leading to suboptimal action policies. Recently, the P-DQN method [ 58 ] is proposed to directly generate multiple actions, and each action is associated with a decision. Benefiting from this, P-DQN can learn a better action policy under the parameterized action structure. To improve the robustness when facing perturbed state features, the RP-DQN method [ 29 ] adds a regularization term into the actor network to stabilize the update process of the action network. However, this method does not consider the extrapolation error that can lead to the misestimation problem of Q values of state-action pairs [ 62 ] . Specifically, the Q value in reinforcement learning is used to estimate the expected reward when performing an action based on certain state features. Since the values of state features and action are continuous in this work, the number of state-action pairs is infinite. During the learning process, the reinforcement learning agent inevitably encounters some state-action pairs that significantly differ from those it has previously seen. In such cases, the agent tends to estimate their Q-values incorrectly. This in turn affects policy improvement, where the agent learns to prefer poor actions, leading to potential safety risks in autonomous driving. Therefore, this study proposes a regularization technique to restrain the learning process of Q values, making the exploration of state-action pairs stay close to those previously seen.\\nNext, we will introduce the loss functions with a regularization term in the actor and value networks.\\nWe represent an explored experience as e t = ( s t , a h 1 t , r t , s t + 1 , a h 2 t + 1 ) e^{t}=(s^{t},a^{t}_{h_{1}},r^{t},s^{t+1},a^{t+1}_{h_{2}}) that denote the current state, action, reward, the next state and action, respectively. h 1 h_{1} and h 2 h_{2} denote the selected lane-changing decisions of a h 1 t a^{t}_{h_{1}} and a h 2 t a^{t}_{h_{2}} , respectively. In the training process, the value network aims to minimize the gap between the estimated Q value Q \\u200b ( s t , a h t ) Q(s^{t},a^{t}_{h}) and the target Q value y y , and the loss function ‚Ñí c \\\\mathcal{L}_{c} is calculated as follows:\\nwhere œÄ \\\\pi denotes the action policy learned in the actor network, Œ≥ \\\\gamma denotes the discount factor, and D D denotes a batch of experiences. Importantly, ( œÄ \\u200b ( s t + 1 ) ‚àí a h 2 t + 1 ) (\\\\pi(s^{t+1})-a_{h_{2}}^{t+1}) is the regularization term that penalizes the agent based on how much the current output action œÄ \\u200b ( s t + 1 ) \\\\pi(s^{t+1}) deviates from the previously explored action a h 2 t + 1 a_{h_{2}}^{t+1} in a state s t + 1 s^{t+1} . It is inspired by the behavior cloning technique [ 63 ] that aims to make œÄ \\u200b ( s t + 1 ) \\\\pi(s^{t+1}) stay close to a h 2 t + 1 a_{h_{2}}^{t+1} , thus alleviating the misestimation problem of Q values. Œ≤ 1 \\\\beta_{1} is the coefficient that determines the regularization scale. In addition to the value network, the actor network aims to learn an action policy to output the action with the largest Q value. We use the loss function in the previous work [ 29 ] to learn the action policy.\\nIn the end, we find that the regularization not only stabilizes the learning process in the face of perturbed state features, but also makes the final decision performance more conservative. This will lead to a slight degradation in efficiency, but the safety, comfort, and energy consumption of the driving system will be optimized.',\n",
       "    'subsections': [{'title': 'V-A Markov Decision Process',\n",
       "      'paragraphs': 'Markov decision process (MDP) can be defined as: ‚Ñ≥ = ‚ü® ùíÆ , ùíú , ùí´ , ‚Ñõ ‚ü© \\\\mathcal{M}=\\\\langle{\\\\mathcal{S},\\\\mathcal{A}},\\\\mathcal{P},\\\\mathcal{R}\\\\rangle , which includes four key elements: state ùíÆ \\\\mathcal{S} , action ùíú \\\\mathcal{A} , state transition probability ùí´ \\\\mathcal{P} and reward ‚Ñõ \\\\mathcal{R} . Specifically, the state s t s^{t} denotes the extracted environmental features at time frame t t , the action a t a^{t} denotes the driving decision, the state transition is implemented via environment rendering in the simulator Carla [ 57 ] , and the reward r t r^{t} is used to rate the action, where we adopt a hybrid reward function including four reward factors: safety, efficiency, comfort, and impact. Following the previous work [ 32 ] , the safety factor is measured by time-to-collision (TTC) of the autonomous vehicle, which denotes the time span left before a collision. The efficiency factor and the comfort factor are measured by the velocity and acceleration change rate of the autonomous vehicle. The impact factor is measured by the passive deceleration of the rear traffic flow caused by the autonomous vehicle.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-B Reinforcement Learning with Regularization Technique',\n",
       "      'paragraphs': 'Figure 4 shows the architecture of our reinforcement learning model, which includes an action network and a value network. Specifically, the actor network incorporates a parameterized action schema [ 58 ] , outputting three action values (i.e., a l \\u200b l t a_{ll}^{t} , a l \\u200b r t a_{lr}^{t} , and a l \\u200b k t a_{lk}^{t} 3 3 3 Each action includes a steering angle value and a velocity change value. ) at each time step that correspond to three lane-changing decisions (i.e., left lane-changing l \\u200b l ll , right lane-changing l \\u200b r lr , and lane-keeping l \\u200b k lk ). The value network outputs the Q values of the three actions, and the action with the highest Q value is the final action performed by the autonomous vehicle.\\nTo solve the parameterized action schema, researchers propose many reinforcement learning methods [ 59 , 60 , 58 , 29 , 61 ] to learn an action policy. Specifically, the P-DDPG method [ 60 ] collapses the parameterized action space into a continuous one. However, it does not account for which action is associated with which lane-changing decision, leading to suboptimal action policies. Recently, the P-DQN method [ 58 ] is proposed to directly generate multiple actions, and each action is associated with a decision. Benefiting from this, P-DQN can learn a better action policy under the parameterized action structure. To improve the robustness when facing perturbed state features, the RP-DQN method [ 29 ] adds a regularization term into the actor network to stabilize the update process of the action network. However, this method does not consider the extrapolation error that can lead to the misestimation problem of Q values of state-action pairs [ 62 ] . Specifically, the Q value in reinforcement learning is used to estimate the expected reward when performing an action based on certain state features. Since the values of state features and action are continuous in this work, the number of state-action pairs is infinite. During the learning process, the reinforcement learning agent inevitably encounters some state-action pairs that significantly differ from those it has previously seen. In such cases, the agent tends to estimate their Q-values incorrectly. This in turn affects policy improvement, where the agent learns to prefer poor actions, leading to potential safety risks in autonomous driving. Therefore, this study proposes a regularization technique to restrain the learning process of Q values, making the exploration of state-action pairs stay close to those previously seen.\\nNext, we will introduce the loss functions with a regularization term in the actor and value networks.\\nWe represent an explored experience as e t = ( s t , a h 1 t , r t , s t + 1 , a h 2 t + 1 ) e^{t}=(s^{t},a^{t}_{h_{1}},r^{t},s^{t+1},a^{t+1}_{h_{2}}) that denote the current state, action, reward, the next state and action, respectively. h 1 h_{1} and h 2 h_{2} denote the selected lane-changing decisions of a h 1 t a^{t}_{h_{1}} and a h 2 t a^{t}_{h_{2}} , respectively. In the training process, the value network aims to minimize the gap between the estimated Q value Q \\u200b ( s t , a h t ) Q(s^{t},a^{t}_{h}) and the target Q value y y , and the loss function ‚Ñí c \\\\mathcal{L}_{c} is calculated as follows:\\nwhere œÄ \\\\pi denotes the action policy learned in the actor network, Œ≥ \\\\gamma denotes the discount factor, and D D denotes a batch of experiences. Importantly, ( œÄ \\u200b ( s t + 1 ) ‚àí a h 2 t + 1 ) (\\\\pi(s^{t+1})-a_{h_{2}}^{t+1}) is the regularization term that penalizes the agent based on how much the current output action œÄ \\u200b ( s t + 1 ) \\\\pi(s^{t+1}) deviates from the previously explored action a h 2 t + 1 a_{h_{2}}^{t+1} in a state s t + 1 s^{t+1} . It is inspired by the behavior cloning technique [ 63 ] that aims to make œÄ \\u200b ( s t + 1 ) \\\\pi(s^{t+1}) stay close to a h 2 t + 1 a_{h_{2}}^{t+1} , thus alleviating the misestimation problem of Q values. Œ≤ 1 \\\\beta_{1} is the coefficient that determines the regularization scale. In addition to the value network, the actor network aims to learn an action policy to output the action with the largest Q value. We use the loss function in the previous work [ 29 ] to learn the action policy.\\nIn the end, we find that the regularization not only stabilizes the learning process in the face of perturbed state features, but also makes the final decision performance more conservative. This will lead to a slight degradation in efficiency, but the safety, comfort, and energy consumption of the driving system will be optimized.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'VI Experiments',\n",
       "    'paragraphs': 'Dataset. We simulate the entire autonomous driving pipeline on the Carla simulator 4 4 4 https://carla.org/ , which is a widely used project focused on creating a publicly available virtual environment for autonomous driving [ 57 ] . It supports almost all sensors (e.g., camera and LiDAR) with the goal of flexibility and realism in high-fidelity simulations. In the adaptive perception module, we use both a real-world dataset Nuscenes-R [ 16 ] and a synthetic dataset Nuscenes-S [ 48 ] . Specifically, Nuscenes-R has 5.5-hour perception data where the framerate of annotated scenarios is only 2 fps. Nuscenes-S is generated with Carla, simulating the sensor and scenario setting in Nuscenes-R but with annotated scenarios at 20 fps. We use both datasets to train and test our scenario difficulty classification model but only use Nuscenes-S for knob tuning since the framerate of annotated scenes in Nuscenes-R is too low. For our decision-making model, we use the simulated dataset REAL in the previous work [ 29 ] , which contains 55 different driving routes with various weather conditions and traffic density. Finally, we split all the above datasets into training sets and test sets with a splitting ratio of 4 : 1.\\nImplementation Details. In the adaptive perception module, we set the number k k of perception difficulty levels in our scenario classification model to 4. For the classification head, we use three linear layers in the multi-layer perceptron, and their dimensions are set as 256, 128, and 64, respectively. To calculate uncertainty, the dropout rate is set as 0.5, the classification head performs 10 forward passes, and the threshold G G of uncertainty values is set as 0.02. To the end, we train the classification model using the Adam optimizer [ 64 ] with a learning rate of 0.01 and a batch size of 128. In the knob tuning section, the accuracy requirements ùíú i \\u200b ( i = 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ) \\\\mathcal{A}_{i}\\\\ (i=1,2,3,4,5,6,7,8) are set to 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, and 0.76, respectively.\\nIn the robust decision-making module, the actor network and value network use a\\nsoft update mechanism and an updated ratio of 0.01 similar to DDPG [ 65 ] , and the dimensions of the linear layers in the multi-layer perceptron are set to 64.\\nThe reward discount factor Œ≥ \\\\gamma in the Bellman function is 0.9, and the coefficient Œ≤ 1 \\\\beta_{1} in the regularization term is set to 0.4. In the end, we train the reinforcement learning model using the Adam optimizer [ 64 ] with a learning rate of 0.001 and a batch size of 128. Our experiments are conducted on NVIDIA RTX 3090 GPUs.\\nBaselines. We compared our framework with five methods, including one standard perception-and-decision autonomous driving method and four model compression methods.\\n(1) Auto [ 29 ] .\\nA standard perception-and-decision framework for autonomous driving, which perceives the traffic environment with BevFusion [ 9 ] and makes driving decisions by a reinforcement learning model RBP-DQN.\\n(2) Sparse [ 6 ] .\\nIt reduces computing consumption by using sparse representations that only focus on the relevant and non-empty areas of BEV representations.\\n(3) Quantize [ 13 ] .\\nIt reduces computing consumption by lowering the precision of floating-point numbers from single-precision to half-precision.\\n(4) Distill [ 14 ] .\\nIt reduces computing consumption by distilling knowledge from a large multi-modality model to a smaller model.\\n(5) EcoFusion [ 5 ] .\\nIt can vary the model size of multi-modality methods by discarding or adding\\nparts of the network, thus altering computational consumption and accuracy.\\nHowever, Sparse, Quantize, Distill, and EcoFusion only focus on perception optimization but do not consider the impact of reduced perception computing on subsequent autonomous driving decision performance. Therefore, we plug them into the autonomous driving pipeline of Auto to evaluate their driving performance.\\nIn this section, we study the end-to-end performance 5 5 5 The result is based on the accuracy requirement of 0.74. of our framework EneAD by comparing it against the baselines from two aspects: energy consumption and driving performance.\\nFirstly, we design three metrics to evaluate the energy consumption of the autonomous vehicle, as follows:\\n(1) Average Energy consumption of perception system (Ene-P). We record the power consumption per 100 kilometers brought by the perception system of the autonomous vehicle. To avoid interference from other processes and hardware factors, we measure it based on the number of floating point operations (FLOPs) following the\\nprevious method [ 3 , 34 ] . We also use equivalent computational units for method Quantize, where one FP16 operation is counted as 0.5 FP32 operations\\n(2) Average Energy consumption of driving system (Ene-D). We record the power consumption per 100 kilometers (km) brought by the driving system of the autonomous vehicle. It is influenced by many factors, such as vehicle speed and acceleration, and we obtain it by the SUMO platform following the previous method [ 35 ] .\\n(3) Average Driving range (DR). We record the driving range (km) of the autonomous vehicle per kwh, considering only the energy consumption of the perception system and driving system.\\nThen, we design four metrics, following the previous work [ 66 , 32 ] , to evaluate the driving performance (i.e., safety, efficiency, comfort, and impact) of the autonomous vehicle, as follows:\\n(1)\\nPercentage of time-to-collision values with potential collision risks (TTC-R). We record the time-to-collision values of the autonomous vehicle and count the percentage of TTCs less than 4 seconds that are regarded as having potential collision risk following the previous work [ 66 ] . A smaller TTC-R indicates that the autonomous vehicle is safer.\\n(2)\\nAverage velocity (VEL).\\nWe record the average velocity of the autonomous vehicle. A larger VEL indicates higher traffic efficiency.\\n(3)\\nAverage acceleration change (AC).\\nWe record the average acceleration change of the autonomous vehicle. A smaller AC indicates a more comfortable driving experience.\\n(4)\\nAverage deceleration (DEC). After the autonomous vehicle performs abrupt braking or lane-changing behaviors, we record the deceleration of the rear vehicle.\\nA smaller DEC means that the autonomous vehicle has less impact on the rear vehicle.\\nTo demonstrate the effectiveness of our framework, we present these metrics under different traffic densities (i.e., low: < 10 \\u200b v \\u200b e \\u200b h / k \\u200b m / l \\u200b n <10veh/km/ln , medium: 10 ‚àí 30 \\u200b v \\u200b e \\u200b h / k \\u200b m / l \\u200b n 10-30veh/km/ln , and high: > 30 \\u200b v \\u200b e \\u200b h / k \\u200b m / l \\u200b n >30veh/km/ln ).\\nFirstly, we report the metrics of energy consumption in Figure 5 , 6 , and 7 . For power consumption of the perception system in Figure 5 , we can see that our framework EneAD achieves the smallest Ene-P, demonstrating that EneAD can achieve the lowest perception consumption. Among these baselines, the Auto method has the largest Ene-P since it does not consider reducing the perception consumption. In addition, although the model compression methods Sparse, Quantize, Distill, and EcoFusion can achieve a smaller Ene-P than Auto, they also have larger Ene-P values than our framework since they do not consider adjusting the framerate at which perception models run.\\nAfterward, in Figure 6 we can see that our framework EneAD has the smallest Ene-D, which demonstrates that EneAD can also lower the power consumption of the driving system. This is because our robust decision module can generate relatively conservative driving behaviors with smoother velocity changes.\\nThe poor performance of Sparse, Quantize, Distill, and EcoFusion is attributed to their inferior perception accuracies in challenging traffic scenarios, leading to more sudden speed changes that can increase power consumption.\\nFinally, we show the driving range metric DR in Figure 7 . Since EneAD has the lowest power consumption of both the perception and driving system, it can achieve the largest driving range per kwh.\\nTo sum up, our framework EneAD can achieve a 1.9 √ó ‚àí 3.5 √ó 1.9\\\\times-3.5\\\\times reduction of perception consumption, a slight reduction of the driving system, and a 3.9 % ‚àí 8.5 % 3.9\\\\%-8.5\\\\% increase of driving range.\\nSecondly, we report the metrics in Figure 8 , 9 , 10 , and 11 . None of the methods cause any collision in the test phase. For safety in Figure 8 , we observe that our framework EneAD has a TTC-R as small as the one achieved by Auto with the highest computational consumption. In addition, a smaller VEL in Figure 9 indicates that EneAD has a slight decrease compared to Auto in driving efficiency, and the smallest AC and DEC in Figure 10 and 11 indicate that EneAD can improve comfort and reduce disturbance to traffic flow. This is due to our robust decision module, which includes a regularization term to constrain the Q function updates in the presence of perturbed state features. It can avoid overestimating the Q values of aggressive driving actions, making the autonomous vehicle drive more smoothly.\\nThe poor performance of Sparse, Quantize, Distill, and EcoFusion is caused by their unstable difficulty levels of the perception results and the lack of constraints on the Q function updates.\\nTo sum up, although our framework EneAD can achieve a substantial decrease in computational consumption, EneAD can still maintain high safety as Auto and achieve the best performance in comfort and impact, with a slight decrease in traffic efficiency.\\nOverall, the above results indicate that our framework yields substantial energy savings across all traffic densities while preserving strong driving performance.\\nThe adaptive perception module is used to reduce the energy consumption of perception computing. To achieve this, we first propose a classification model to distinguish perception difficulty levels of different traffic scenarios and then design a transferable tuning paradigm to explore energy-efficient knob configurations under different accuracy requirements.\\nEvaluation of Scenario Classification Model. Firstly, our classification model is based on the image encoder Swin-T [ 44 ] , which is a hierarchical transformer that combines local attention with shifted windows. To evidence its effectiveness and efficiency in our task, we compare it with the following image encoders: (1) ViT [ 43 ] . Vision transformer that leverages global attention for superior context understanding on image patches. (2) MobileNet [ 67 ] . Lightweight convolution network that uses depth-wise separable convolutions to reduce computation. (3) ResNet50 [ 68 ] . Convolution network that introduces residual connections for deep networks.\\nThen, we design two metrics as follows: (1) Ene-Q. Average power consumption of scenario classification, which is calculated in the same way as Ene-P of the perception system. (2) Accuracy. The percentage of correctly classified instances out of the total number of instances.\\nWe report these metrics of our method and the baselines in Table II . As shown, ViT has the highest accuracy but requires a very large computational consumption. MobileNet is the most lightweight model but has a significant drop in accuracy. ResNet50 and our method with Swin-T have nearly the same computational consumption, but the accuracy of our method is much higher than the one of ResNet50. Overall, our method achieves the best balance between computational consumption and accuracy for scenario difficulty classification.\\nBesides a classification result, we also output an uncertainty value by adopting Monte Carlo dropout in the classification head. When the uncertainty value of a classification result is greater than 0.02, we regard this result as ‚Äùuncertain‚Äù and adjust it to the highest difficulty level.\\nIn Table I , we present the percentage of overestimated, underestimated, and correct classification results among the uncertain results. As shown, 35.6% of the uncertain results are overestimated, 52.1% are underestimated, and 12.3% are correct. Therefore, our uncertainty estimation operation is effective in finding uncertain classification results.\\nEvaluation of Knob Tuning Method. To explore promising configurations, we use Bayesian optimization as the tuning paradigm and propose a meta-surrogate model to transfer tuning knowledge between different tuning instances.\\nTo evaluate the tuning performance, we compared it with two tuning methods and a variant of our method as follows:\\n(1) Heuristic [ 69 ] . A greedy local search method that searches from multiple random configurations and evaluates neighboring configurations to find a better one.\\n(2) RL-based [ 70 ] . A reinforcement learning-based tuning method that uses neural networks to approximate the configuration-accuracy relationship and select the next configuration to evaluate.\\n(3) w/o-meta. A variant that removes the meta-surrogate model. Instead, it starts tuning from scratch using an initial surrogate model. To evaluate their tuning performance, we record the normalized computational consumption (named Normalized C-C) of the current optimal configurations under multiple accuracy requirements (detailed in Section IV-D ) and average them.\\nIn Figure 13 , we plot the tuning curves of Heuristic, RL-based, w/o-meta within 60 tuning iterations. All methods start tuning from scratch. As shown, w/o-meta reaches the optimal value fastest compared to Heuristic and RL-based, demonstrating the effectiveness of Bayesian optimization in our tuning task. In addition, to evidence the effectiveness of our meta-surrogate model, we compare the tuning performance of w/o-meta and our complete method in Figure 13 . Their tuning curves are based on the second tuning instance. In this study, there are k k tuning instances corresponding to k k difficulty levels. If using our complete method with the meta-surrogate model, the second instance can transfer tuning knowledge from the first instance. As shown, our complete method can reach the optimal value after 25 tuning iterations, fewer than the 40 iterations of w/o-meta, proving that the meta-surrogate model is effective in transferring tuning knowledge and thus speeds up the tuning process.\\nEffect of Accuracy Requirements. Our knob tuning model can search for promising configurations under multiple accuracy requirements, i.e., ùíú i , i = 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 \\\\mathcal{A}_{i},\\\\ i=1,2,3,4,5,6,7,8 , with a NDS value of 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, respectively. After the tuning converges, we retrieve the final configuration settings in the configuration dictionary and present their computational consumption of the perception system (Ene-P) and the driving system (Ene-D), and safety metric (TTC-R) in Figure 14 . As shown, a lower accuracy requirement results in lower computational consumption of the perception system, while the consumption of the driving system increases and the safety deteriorates. Specifically, the high accuracies ùíú 7 \\\\mathcal{A}_{7} and ùíú 8 \\\\mathcal{A}_{8} significantly increase perception computation but offer limited optimization for other metrics. The low accuracies ùíú 1 ‚àí ùíú 4 \\\\mathcal{A}_{1}-\\\\mathcal{A}_{4} drastically reduce the stability and safety of autonomous driving, while ùíú 5 \\\\mathcal{A}_{5} and ùíú 6 \\\\mathcal{A}_{6} can achieve a well-balanced performance across all metrics.\\nUnless otherwise specified, the results in other experiments are based on the accuracy requirement ùíú 6 = 0.74 \\\\mathcal{A}_{6}=0.74 .\\nConfiguration Setting at Different Difficulty Levels. In Table III , we present the knob values of configurations in different perception difficulty levels after tuning. In this study, we set four levels (i.e., 1, 2, 3, 4), and a higher difficulty level corresponding to a more challenging traffic scenario (e.g., adverse weather conditions).\\nAs shown in the table, for high difficulty levels, knob values with high computation are required to achieve a desired accuracy as much as possible. Conversely, for low difficulty levels, knob values with low computation are sufficient to meet the accuracy requirement.\\nIn addition, we find that only scenarios with levels 1, 2, and 3 have reduced configurations, while scenarios with level 4 still maintain the highest-computation configuration. This is because even with the highest-computation configuration, these scenarios still only achieve an accuracy of 0.72, failing to meet our accuracy requirement of 0.74.\\nTo address it, more research breakthroughs in autonomous driving perception models are needed in the future.\\nEvaluation of Configuration Switching. In our experiments, the system switches configurations an average of 2.3 times per kilometer. However, in real-world driving scenarios, this frequency is expected to be lower, since real environments typically feature larger, more continuous scenes (e.g., long highways or urban blocks) and fewer abrupt transitions.\\nSince all models are resident in memory, switching between them incurs negligible latency, enabling adaptive perception with seamless model transitions.\\nIn our framework, the robust decision module is used to make driving decisions based on the perturbed perception results from our adaptive perception module. It proposes a reinforcement learning model to generate driving actions with a parameterized action structure and a regularization term.\\nEvaluation of Reinforcement Learning Models We compared our method with three reinforcement learning methods as follows:\\n(1) P-DDPG [ 60 ] . Deep deterministic policy gradient method that collapses the parameterized action structure into a continuous one.\\n(2) P-DQN [ 58 ] . Parameterized deep Q-learning method that directly learns the parameterized action structure,\\n(3) RP-DQN [ 29 ] . An improved P-DQN model that only restrains the update of the actor network.\\nThen, we show the driving performance (i.e., safety metric TTC-R, efficiency metric VEL, comfort metric AC, impact metric DEC) of these methods under all traffic densities in Table IV . As shown, our method has the lowest TTC-R, AC, and DEC, demonstrating it achieves the best performance in safety, comfort, and impact compared to other methods. However, the VEL of our method has a slight decrease compared to RBP-DQN. The results indicate that our reinforcement learning can make the autonomous vehicle more cautious when facing perturbed state features, leading to larger distances from other vehicles and smoother velocity changes. The performance of RBP-DQN arises from the lack of regularization in updating Q values, leading to the misestimation of Q values of some aggressive driving actions. In addition, the poor performance of P-DDPG and P-DQN is because their model architectures struggle to optimize the action policy stably.',\n",
       "    'subsections': [{'title': 'VI-A Experimental Settings',\n",
       "      'paragraphs': 'Dataset. We simulate the entire autonomous driving pipeline on the Carla simulator 4 4 4 https://carla.org/ , which is a widely used project focused on creating a publicly available virtual environment for autonomous driving [ 57 ] . It supports almost all sensors (e.g., camera and LiDAR) with the goal of flexibility and realism in high-fidelity simulations. In the adaptive perception module, we use both a real-world dataset Nuscenes-R [ 16 ] and a synthetic dataset Nuscenes-S [ 48 ] . Specifically, Nuscenes-R has 5.5-hour perception data where the framerate of annotated scenarios is only 2 fps. Nuscenes-S is generated with Carla, simulating the sensor and scenario setting in Nuscenes-R but with annotated scenarios at 20 fps. We use both datasets to train and test our scenario difficulty classification model but only use Nuscenes-S for knob tuning since the framerate of annotated scenes in Nuscenes-R is too low. For our decision-making model, we use the simulated dataset REAL in the previous work [ 29 ] , which contains 55 different driving routes with various weather conditions and traffic density. Finally, we split all the above datasets into training sets and test sets with a splitting ratio of 4 : 1.\\nImplementation Details. In the adaptive perception module, we set the number k k of perception difficulty levels in our scenario classification model to 4. For the classification head, we use three linear layers in the multi-layer perceptron, and their dimensions are set as 256, 128, and 64, respectively. To calculate uncertainty, the dropout rate is set as 0.5, the classification head performs 10 forward passes, and the threshold G G of uncertainty values is set as 0.02. To the end, we train the classification model using the Adam optimizer [ 64 ] with a learning rate of 0.01 and a batch size of 128. In the knob tuning section, the accuracy requirements ùíú i \\u200b ( i = 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ) \\\\mathcal{A}_{i}\\\\ (i=1,2,3,4,5,6,7,8) are set to 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, and 0.76, respectively.\\nIn the robust decision-making module, the actor network and value network use a\\nsoft update mechanism and an updated ratio of 0.01 similar to DDPG [ 65 ] , and the dimensions of the linear layers in the multi-layer perceptron are set to 64.\\nThe reward discount factor Œ≥ \\\\gamma in the Bellman function is 0.9, and the coefficient Œ≤ 1 \\\\beta_{1} in the regularization term is set to 0.4. In the end, we train the reinforcement learning model using the Adam optimizer [ 64 ] with a learning rate of 0.001 and a batch size of 128. Our experiments are conducted on NVIDIA RTX 3090 GPUs.\\nBaselines. We compared our framework with five methods, including one standard perception-and-decision autonomous driving method and four model compression methods.\\n(1) Auto [ 29 ] .\\nA standard perception-and-decision framework for autonomous driving, which perceives the traffic environment with BevFusion [ 9 ] and makes driving decisions by a reinforcement learning model RBP-DQN.\\n(2) Sparse [ 6 ] .\\nIt reduces computing consumption by using sparse representations that only focus on the relevant and non-empty areas of BEV representations.\\n(3) Quantize [ 13 ] .\\nIt reduces computing consumption by lowering the precision of floating-point numbers from single-precision to half-precision.\\n(4) Distill [ 14 ] .\\nIt reduces computing consumption by distilling knowledge from a large multi-modality model to a smaller model.\\n(5) EcoFusion [ 5 ] .\\nIt can vary the model size of multi-modality methods by discarding or adding\\nparts of the network, thus altering computational consumption and accuracy.\\nHowever, Sparse, Quantize, Distill, and EcoFusion only focus on perception optimization but do not consider the impact of reduced perception computing on subsequent autonomous driving decision performance. Therefore, we plug them into the autonomous driving pipeline of Auto to evaluate their driving performance.',\n",
       "      'subsections': []},\n",
       "     {'title': 'VI-B End-to-end Evaluation',\n",
       "      'paragraphs': 'In this section, we study the end-to-end performance 5 5 5 The result is based on the accuracy requirement of 0.74. of our framework EneAD by comparing it against the baselines from two aspects: energy consumption and driving performance.\\nFirstly, we design three metrics to evaluate the energy consumption of the autonomous vehicle, as follows:\\n(1) Average Energy consumption of perception system (Ene-P). We record the power consumption per 100 kilometers brought by the perception system of the autonomous vehicle. To avoid interference from other processes and hardware factors, we measure it based on the number of floating point operations (FLOPs) following the\\nprevious method [ 3 , 34 ] . We also use equivalent computational units for method Quantize, where one FP16 operation is counted as 0.5 FP32 operations\\n(2) Average Energy consumption of driving system (Ene-D). We record the power consumption per 100 kilometers (km) brought by the driving system of the autonomous vehicle. It is influenced by many factors, such as vehicle speed and acceleration, and we obtain it by the SUMO platform following the previous method [ 35 ] .\\n(3) Average Driving range (DR). We record the driving range (km) of the autonomous vehicle per kwh, considering only the energy consumption of the perception system and driving system.\\nThen, we design four metrics, following the previous work [ 66 , 32 ] , to evaluate the driving performance (i.e., safety, efficiency, comfort, and impact) of the autonomous vehicle, as follows:\\n(1)\\nPercentage of time-to-collision values with potential collision risks (TTC-R). We record the time-to-collision values of the autonomous vehicle and count the percentage of TTCs less than 4 seconds that are regarded as having potential collision risk following the previous work [ 66 ] . A smaller TTC-R indicates that the autonomous vehicle is safer.\\n(2)\\nAverage velocity (VEL).\\nWe record the average velocity of the autonomous vehicle. A larger VEL indicates higher traffic efficiency.\\n(3)\\nAverage acceleration change (AC).\\nWe record the average acceleration change of the autonomous vehicle. A smaller AC indicates a more comfortable driving experience.\\n(4)\\nAverage deceleration (DEC). After the autonomous vehicle performs abrupt braking or lane-changing behaviors, we record the deceleration of the rear vehicle.\\nA smaller DEC means that the autonomous vehicle has less impact on the rear vehicle.\\nTo demonstrate the effectiveness of our framework, we present these metrics under different traffic densities (i.e., low: < 10 \\u200b v \\u200b e \\u200b h / k \\u200b m / l \\u200b n <10veh/km/ln , medium: 10 ‚àí 30 \\u200b v \\u200b e \\u200b h / k \\u200b m / l \\u200b n 10-30veh/km/ln , and high: > 30 \\u200b v \\u200b e \\u200b h / k \\u200b m / l \\u200b n >30veh/km/ln ).\\nFirstly, we report the metrics of energy consumption in Figure 5 , 6 , and 7 . For power consumption of the perception system in Figure 5 , we can see that our framework EneAD achieves the smallest Ene-P, demonstrating that EneAD can achieve the lowest perception consumption. Among these baselines, the Auto method has the largest Ene-P since it does not consider reducing the perception consumption. In addition, although the model compression methods Sparse, Quantize, Distill, and EcoFusion can achieve a smaller Ene-P than Auto, they also have larger Ene-P values than our framework since they do not consider adjusting the framerate at which perception models run.\\nAfterward, in Figure 6 we can see that our framework EneAD has the smallest Ene-D, which demonstrates that EneAD can also lower the power consumption of the driving system. This is because our robust decision module can generate relatively conservative driving behaviors with smoother velocity changes.\\nThe poor performance of Sparse, Quantize, Distill, and EcoFusion is attributed to their inferior perception accuracies in challenging traffic scenarios, leading to more sudden speed changes that can increase power consumption.\\nFinally, we show the driving range metric DR in Figure 7 . Since EneAD has the lowest power consumption of both the perception and driving system, it can achieve the largest driving range per kwh.\\nTo sum up, our framework EneAD can achieve a 1.9 √ó ‚àí 3.5 √ó 1.9\\\\times-3.5\\\\times reduction of perception consumption, a slight reduction of the driving system, and a 3.9 % ‚àí 8.5 % 3.9\\\\%-8.5\\\\% increase of driving range.\\nSecondly, we report the metrics in Figure 8 , 9 , 10 , and 11 . None of the methods cause any collision in the test phase. For safety in Figure 8 , we observe that our framework EneAD has a TTC-R as small as the one achieved by Auto with the highest computational consumption. In addition, a smaller VEL in Figure 9 indicates that EneAD has a slight decrease compared to Auto in driving efficiency, and the smallest AC and DEC in Figure 10 and 11 indicate that EneAD can improve comfort and reduce disturbance to traffic flow. This is due to our robust decision module, which includes a regularization term to constrain the Q function updates in the presence of perturbed state features. It can avoid overestimating the Q values of aggressive driving actions, making the autonomous vehicle drive more smoothly.\\nThe poor performance of Sparse, Quantize, Distill, and EcoFusion is caused by their unstable difficulty levels of the perception results and the lack of constraints on the Q function updates.\\nTo sum up, although our framework EneAD can achieve a substantial decrease in computational consumption, EneAD can still maintain high safety as Auto and achieve the best performance in comfort and impact, with a slight decrease in traffic efficiency.\\nOverall, the above results indicate that our framework yields substantial energy savings across all traffic densities while preserving strong driving performance.',\n",
       "      'subsections': []},\n",
       "     {'title': 'VI-C Evaluation of Adaptive Perception Module',\n",
       "      'paragraphs': 'The adaptive perception module is used to reduce the energy consumption of perception computing. To achieve this, we first propose a classification model to distinguish perception difficulty levels of different traffic scenarios and then design a transferable tuning paradigm to explore energy-efficient knob configurations under different accuracy requirements.\\nEvaluation of Scenario Classification Model. Firstly, our classification model is based on the image encoder Swin-T [ 44 ] , which is a hierarchical transformer that combines local attention with shifted windows. To evidence its effectiveness and efficiency in our task, we compare it with the following image encoders: (1) ViT [ 43 ] . Vision transformer that leverages global attention for superior context understanding on image patches. (2) MobileNet [ 67 ] . Lightweight convolution network that uses depth-wise separable convolutions to reduce computation. (3) ResNet50 [ 68 ] . Convolution network that introduces residual connections for deep networks.\\nThen, we design two metrics as follows: (1) Ene-Q. Average power consumption of scenario classification, which is calculated in the same way as Ene-P of the perception system. (2) Accuracy. The percentage of correctly classified instances out of the total number of instances.\\nWe report these metrics of our method and the baselines in Table II . As shown, ViT has the highest accuracy but requires a very large computational consumption. MobileNet is the most lightweight model but has a significant drop in accuracy. ResNet50 and our method with Swin-T have nearly the same computational consumption, but the accuracy of our method is much higher than the one of ResNet50. Overall, our method achieves the best balance between computational consumption and accuracy for scenario difficulty classification.\\nBesides a classification result, we also output an uncertainty value by adopting Monte Carlo dropout in the classification head. When the uncertainty value of a classification result is greater than 0.02, we regard this result as ‚Äùuncertain‚Äù and adjust it to the highest difficulty level.\\nIn Table I , we present the percentage of overestimated, underestimated, and correct classification results among the uncertain results. As shown, 35.6% of the uncertain results are overestimated, 52.1% are underestimated, and 12.3% are correct. Therefore, our uncertainty estimation operation is effective in finding uncertain classification results.\\nEvaluation of Knob Tuning Method. To explore promising configurations, we use Bayesian optimization as the tuning paradigm and propose a meta-surrogate model to transfer tuning knowledge between different tuning instances.\\nTo evaluate the tuning performance, we compared it with two tuning methods and a variant of our method as follows:\\n(1) Heuristic [ 69 ] . A greedy local search method that searches from multiple random configurations and evaluates neighboring configurations to find a better one.\\n(2) RL-based [ 70 ] . A reinforcement learning-based tuning method that uses neural networks to approximate the configuration-accuracy relationship and select the next configuration to evaluate.\\n(3) w/o-meta. A variant that removes the meta-surrogate model. Instead, it starts tuning from scratch using an initial surrogate model. To evaluate their tuning performance, we record the normalized computational consumption (named Normalized C-C) of the current optimal configurations under multiple accuracy requirements (detailed in Section IV-D ) and average them.\\nIn Figure 13 , we plot the tuning curves of Heuristic, RL-based, w/o-meta within 60 tuning iterations. All methods start tuning from scratch. As shown, w/o-meta reaches the optimal value fastest compared to Heuristic and RL-based, demonstrating the effectiveness of Bayesian optimization in our tuning task. In addition, to evidence the effectiveness of our meta-surrogate model, we compare the tuning performance of w/o-meta and our complete method in Figure 13 . Their tuning curves are based on the second tuning instance. In this study, there are k k tuning instances corresponding to k k difficulty levels. If using our complete method with the meta-surrogate model, the second instance can transfer tuning knowledge from the first instance. As shown, our complete method can reach the optimal value after 25 tuning iterations, fewer than the 40 iterations of w/o-meta, proving that the meta-surrogate model is effective in transferring tuning knowledge and thus speeds up the tuning process.\\nEffect of Accuracy Requirements. Our knob tuning model can search for promising configurations under multiple accuracy requirements, i.e., ùíú i , i = 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 \\\\mathcal{A}_{i},\\\\ i=1,2,3,4,5,6,7,8 , with a NDS value of 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, respectively. After the tuning converges, we retrieve the final configuration settings in the configuration dictionary and present their computational consumption of the perception system (Ene-P) and the driving system (Ene-D), and safety metric (TTC-R) in Figure 14 . As shown, a lower accuracy requirement results in lower computational consumption of the perception system, while the consumption of the driving system increases and the safety deteriorates. Specifically, the high accuracies ùíú 7 \\\\mathcal{A}_{7} and ùíú 8 \\\\mathcal{A}_{8} significantly increase perception computation but offer limited optimization for other metrics. The low accuracies ùíú 1 ‚àí ùíú 4 \\\\mathcal{A}_{1}-\\\\mathcal{A}_{4} drastically reduce the stability and safety of autonomous driving, while ùíú 5 \\\\mathcal{A}_{5} and ùíú 6 \\\\mathcal{A}_{6} can achieve a well-balanced performance across all metrics.\\nUnless otherwise specified, the results in other experiments are based on the accuracy requirement ùíú 6 = 0.74 \\\\mathcal{A}_{6}=0.74 .\\nConfiguration Setting at Different Difficulty Levels. In Table III , we present the knob values of configurations in different perception difficulty levels after tuning. In this study, we set four levels (i.e., 1, 2, 3, 4), and a higher difficulty level corresponding to a more challenging traffic scenario (e.g., adverse weather conditions).\\nAs shown in the table, for high difficulty levels, knob values with high computation are required to achieve a desired accuracy as much as possible. Conversely, for low difficulty levels, knob values with low computation are sufficient to meet the accuracy requirement.\\nIn addition, we find that only scenarios with levels 1, 2, and 3 have reduced configurations, while scenarios with level 4 still maintain the highest-computation configuration. This is because even with the highest-computation configuration, these scenarios still only achieve an accuracy of 0.72, failing to meet our accuracy requirement of 0.74.\\nTo address it, more research breakthroughs in autonomous driving perception models are needed in the future.\\nEvaluation of Configuration Switching. In our experiments, the system switches configurations an average of 2.3 times per kilometer. However, in real-world driving scenarios, this frequency is expected to be lower, since real environments typically feature larger, more continuous scenes (e.g., long highways or urban blocks) and fewer abrupt transitions.\\nSince all models are resident in memory, switching between them incurs negligible latency, enabling adaptive perception with seamless model transitions.',\n",
       "      'subsections': []},\n",
       "     {'title': 'VI-D Evaluation of Robust Decision Module',\n",
       "      'paragraphs': 'In our framework, the robust decision module is used to make driving decisions based on the perturbed perception results from our adaptive perception module. It proposes a reinforcement learning model to generate driving actions with a parameterized action structure and a regularization term.\\nEvaluation of Reinforcement Learning Models We compared our method with three reinforcement learning methods as follows:\\n(1) P-DDPG [ 60 ] . Deep deterministic policy gradient method that collapses the parameterized action structure into a continuous one.\\n(2) P-DQN [ 58 ] . Parameterized deep Q-learning method that directly learns the parameterized action structure,\\n(3) RP-DQN [ 29 ] . An improved P-DQN model that only restrains the update of the actor network.\\nThen, we show the driving performance (i.e., safety metric TTC-R, efficiency metric VEL, comfort metric AC, impact metric DEC) of these methods under all traffic densities in Table IV . As shown, our method has the lowest TTC-R, AC, and DEC, demonstrating it achieves the best performance in safety, comfort, and impact compared to other methods. However, the VEL of our method has a slight decrease compared to RBP-DQN. The results indicate that our reinforcement learning can make the autonomous vehicle more cautious when facing perturbed state features, leading to larger distances from other vehicles and smoother velocity changes. The performance of RBP-DQN arises from the lack of regularization in updating Q values, leading to the misestimation of Q values of some aggressive driving actions. In addition, the poor performance of P-DDPG and P-DQN is because their model architectures struggle to optimize the action policy stably.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'VII Conclusion',\n",
       "    'paragraphs': 'In this study, we propose an energy-efficient framework EneAD based on modular autonomous driving, which can reduce energy consumption while maintaining good driving performance.\\nIn the adaptive perception module, a perception\\noptimization strategy is designed from the perspective of data\\nmanagement and tuning. Firstly, we set the perception model, framerate, and interpolation method as knobs of the perception system. Then, we design a transferable tuning method based on Bayesian optimization to identify promising knob values that achieve low computation while maintaining desired accuracy. To adaptively switch the knob values in various traffic scenarios, a lightweight and reliable classification model is proposed to distinguish the perception difficulty in different scenarios.\\nIn the robust decision module, we propose a decision-making model based on reinforcement learning and design a regularization term to enhance driving stability in the face of perturbed perception results.\\nExperiments on real-world and synthetic datasets evidence the superiority of our framework in both energy consumption and driving performance.',\n",
       "    'subsections': []},\n",
       "   {'title': 'VIII Acknowledgment',\n",
       "    'paragraphs': 'This work is partially supported by NSFC (No. 62472068, 62272086), and Municipal Government of Quzhou under Grant (No. 2024D036, 2024D037).',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Autonomous driving is an emerging technology that is expected to bring significant social, economic, and environmental benefits. However, these benefits come with rising energy consumption by computation engines, limiting the driving range of vehicles, especially electric ones. Perception computing is typically the most power-intensive component, as it relies on large-scale deep learning models to extract environmental features. Recently, numerous studies have employed model compression techniques, such as sparsification, quantization, and distillation, to reduce computational consumption. However, these methods often result in either a substantial model size or a significant drop in perception accuracy compared to high-computation models. To address these challenges, we propose an energy-efficient autonomous driving framework, called EneAD, which includes an adaptive perception and a robust decision module. In the adaptive perception module, a perception optimization strategy is designed from the perspective of data management and tuning. Firstly, we manage multiple perception models with different computational consumption and adjust the execution framerate dynamically. Then, we define them as knobs and design a transferable tuning method based on Bayesian optimization to identify promising knob values that achieve low computation while maintaining desired accuracy. To adaptively switch the knob values in various traffic scenarios, a lightweight classification model is proposed to distinguish the perception difficulty in different scenarios. In the robust decision module, we propose a decision model based on reinforcement learning and design a regularization term to enhance driving stability in the face of perturbed perception results. Extensive experiments evidence the superiority of our framework in both energy consumption and driving performance. EneAD can reduce perception consumption by 1.9√ó1.9\\\\times to 3.5√ó3.5\\\\times and thus improve driving range by 3.9%3.9\\\\% to 8.5%8.5\\\\%.'},\n",
       " '2510.25223v1': {'content': [{'title': 'I Introduction',\n",
       "    'paragraphs': 'Event logs have become one of the most valuable assets of modern enterprises, capturing rich traces of user behavior and system operations across large-scale services. These logs are massive‚Äîoften spanning terabytes per day‚Äîand exhibit highly heterogeneous and temporal structures. Unlike standard tabular data, where each record is typically treated as an independent and identically distributed (i.i.d.) instance, event logs encode sequential dependencies and contextual relationships across time. This intrinsic complexity makes them substantially more challenging to analyze, yet also more informative. Each record may involve multiple dimensions with distinct semantics, cardinalities, and data types. For instance, in mobile gaming, a player‚Äôs log may contain timestamps, categorical attributes, integer-valued statistics (e.g., kills), and continuous metrics (e.g., kill‚Äìdeath ratios). In contrast, e-commerce platforms record user click histories, campaign identifiers, and detailed product attributes. Such diversity and scale make event logs both highly informative and extremely challenging to analyze.\\nExtracting meaningful knowledge from these logs relies heavily on feature engineering, where data scientists manually design informative variables guided by domain expertise and business understanding [ 1 ] . This process involves extensive data exploration, reasoning about domain logic, hypothesis formulation, and iterative experimentation‚Äîoften consuming days or weeks of effort even for experienced practitioners. The challenge becomes even more severe in industrial environments serving millions of users, where the sheer data volume and heterogeneity amplify the cost and complexity. As a result, there is a long-standing demand for technologies that can automate this process, leading to the field of Automated Feature Engineering (AutoFE).\\nExisting AutoFE research spans diverse directions, including AutoML [ 2 ] , genetic algorithms (GA) [ 3 , 4 ] , and reinforcement learning (RL) [ 5 , 6 ] . These methods aim to replace the manual trial-and-error cycle with self-improvement mechanisms. However, their lack of explainability and generality has significantly hindered industrial adoption. Recently, the emergence of large language model (LLM)‚Äìbased agents has opened a promising new direction that unifies automation with human-like reasoning and interpretability. LLMs possess broad prior knowledge across domains, strong coding abilities comparable to human engineers, and the capacity to perform structured reasoning within complex contexts. They have demonstrated potential in data-centric tasks such as scientific discovery [ 7 , 8 ] , mathematical problem solving [ 9 ] , and symbolic regression [ 10 ] . Nonetheless, current applications of LLMs to feature engineering remain limited to simple tabular data [ 11 , 12 ] , leaving the challenge of building robust and general systems for real-world industrial scenarios largely unresolved.\\nTo bridge this gap, we present a multi-agent collaborative system that automatically extracts novel and insightful features from large-scale industrial event log data. Our system is designed to address the following key challenges:\\nData complexity. Event log data exhibit rich temporal dependencies and heterogeneous structures that differ fundamentally from standard tabular data. Many logs also encode implicit relational patterns. For example, in mobile games, players sharing the same TeamID form a teammate network critical for retention analysis; in social applications, comment logs form dynamic interaction graphs; and in e-commerce platforms, user‚Äìitem activities naturally form bipartite graphs. A robust solution must therefore exploit both temporal and relational structure when constructing features. Furthermore, real-world event logs often include attributes stored in diverse formats (e.g., lists, JSON strings, or embedded URLs), requiring extensive preprocessing before feature extraction. Even expert data scientists spend substantial effort resolving such heterogeneity, and transferring expertise across domains (e.g., from gaming to e-commerce) demands considerable relearning. These factors collectively constrain the scalability and generalizability of traditional automatic feature engineering methods.\\nExplainability. Beyond managing complexity, ensuring feature explainability is essential for both scientific rigor and industrial deployment. Diverse stakeholders‚Äîincluding data engineers, domain experts, and decision makers‚Äîmust be able to interpret the semantics and rationale of generated features. This requires not only feature-level transparency (i.e., understanding what a feature represents) but also process-level transparency (i.e., how it was derived). In practice, the most valuable features often capture deep relationships between data structure, semantics, and business logic. Without a clear mechanism to encode and communicate such reasoning, automated feature engineering risks producing trivial or misleading results. Moreover, given the structural and semantic complexity of industrial logs, explainability becomes crucial for tracing, validating, and refining the feature derivation process.\\nSelf-evolution. The high dimensionality and combinatorial nature of event log data create an enormous feature search space, which poses a fundamental challenge for traditional automated methods. Classical approaches typically rely on predefined transformation operators and heuristic search strategies, yet the vast combination space makes it difficult to identify promising exploration directions. Reinforcement- or evolution-based frameworks further suffer from sparse or delayed reward signals relative to the size of the search space, resulting in inefficient exploration and suboptimal feature discovery. Addressing this challenge requires a mechanism that integrates evaluation feedback with knowledge distilled from historical trajectories, allowing the search process to leverage past experience and avoid inefficient wandering in the immense feature space.\\nTo address the aforementioned challenges, we propose a comprehensive multi-agent collaborative system for automatic feature extraction from industrial event log data, termed Feature Engineering LLM Agents (FELA) . FELA autonomously discovers novel and high-performing features through an insight-guided self-evolution paradigm. Beyond constructing an agent workflow that merely generates feature engineering code, FELA tackles a more fundamental question: How can ideas be systematically and controllably evolved with prior knowledge?\\nThe system is designed around three key components that collectively enable effective feature discovery on complex, large-scale datasets.\\nMulti-agent collaborative architecture. FELA employs multiple LLM-based agents with specialized roles that collaborate to manage the complexity of automated feature engineering. Leveraging the reasoning capabilities and domain priors of large language models, idea agents autonomously generate novel feature hypotheses grounded in dataset semantics. Code agents then translate these ideas into executable feature extraction code, enabling expressiveness beyond fixed operation sets used in traditional approaches. Since LLMs are prone to hallucination [ 13 , 14 ] , particularly in high-dimensional and complex domains, FELA introduces critic agents to evaluate and verify outputs from both idea and code agents. This decoupled design separates creativity from verification, effectively mitigating hallucination and improving the reliability of generated features.\\nExplainable and steerable knowledge structure. FELA organizes knowledge in a hierarchical form consisting of ideas and their corresponding derived features , as illustrated in Figure 1 . Ideas encapsulate high-level, human-readable insights, while features represents their logical and mathematical realizations. This design draws inspiration from cognitive science studies [ 15 , 16 ] , which suggest that human reasoning proceeds top-down‚Äîfrom abstract concepts to more concrete understandings. Unlike existing methods [ 12 , 11 ] , this structure yields a transparent and steerable evolution trajectory, allowing users to control exploration depth and conceptual scope. Moreover, the modular organization facilitates scalability and continual improvement, as new ideas‚Äîwhether derived from experts or external knowledge sources‚Äîcan be seamlessly integrated into the system‚Äôs knowledge base, enabling ongoing human‚Äìmachine co-evolution.\\nAgentic evolution algorithm. We further propose an adaptive learning algorithm that integrates the strengths of genetic algorithms and non-associative reinforcement learning. Idea and feature evolution follows an island-based population paradigm, while a multi-armed bandit mechanism with Upper Confidence Bound (UCB) exploration dynamically balances exploration and exploitation across idea trajectories. To enhance learning efficiency, we employ an evaluation agent to summarize experience from past trials and adaptively update the shared system memory. FELA maintains a dual-memory design: short-term memory captures local context for planning and adaptation, whereas long-term memory supports cross-trajectory knowledge transfer. This design allows the system to leverage both evaluation feedback and accumulated historical experience, preventing inefficient wandering in the feature search space.\\nThrough these designs, FELA makes the following key academic contributions:\\nTo the best of our knowledge, FELA is the first agentic system capable of performing automated feature engineering on industrial-scale event log data .\\nWe introduce a novel hybrid evolution framework that combines genetic and reinforcement learning principles within a hierarchical knowledge structure, enabling efficient and interpretable idea evolution.\\nExtensive experiments on real-world industrial datasets demonstrate the effectiveness, scalability, and generalizability of our approach.\\nWe have deployed FELA in our internal feature engineering platform, saving weeks of labor on manual feature engineering and improving performances of various predictive models across multiple business scenarios.',\n",
       "    'subsections': []},\n",
       "   {'title': 'II BACKGROUND & RELATED WORK',\n",
       "    'paragraphs': 'Feature engineering‚Äîalso referred to as feature transformation or feature generation‚Äîdenotes the process of constructing, transforming, and selecting informative variables from raw data to enhance the performance and generalization of machine learning models [ 17 ] . By providing models with the most relevant representations, effective feature engineering underpins predictive accuracy, interpretability, and decision-making capability.\\nBroadly, existing approaches to feature generation fall into two paradigms. The first, latent representation learning , leverages deep architectures such as deep factorization machines and neural representation models to capture complex, nonlinear dependencies in data [ 18 , 19 ] . While powerful, these approaches often lack interpretability and traceability [ 20 ] . The second paradigm, explicit feature transformation , constructs new features through rule-based arithmetic, aggregation, or logical operations [ 21 , 22 ] . These methods depend heavily on domain expertise and manual design, which limits their scalability and adaptability across tasks.\\nIn industrial environments, raw data streams exhibit complexities far beyond those of conventional structured or tabular datasets, posing substantially greater challenges for feature engineering. The characteristics of industrial data are often summarized by the ‚Äú5Vs‚Äù framework‚Äî Volume, Velocity, Variety, Veracity , and Value [ 23 ] . Industrial systems typically involve intricate structural dependencies and operational uncertainties, further complicating the extraction of robust and meaningful features. Consequently, there is a growing demand for automated, domain-adaptive feature engineering techniques that can handle heterogeneous, large-scale, and often unstructured data [ 24 ] .\\nManual feature engineering, by contrast, is increasingly impractical in such settings. It relies heavily on the tacit domain knowledge of data scientists, who face steep learning curves when interpreting complex raw data. Frequent project transitions further exacerbate the problem, as practitioners must repeatedly invest substantial effort to understand new datasets and devise feature hypotheses from scratch‚Äîsignificantly impairing productivity and slowing innovation.\\nTo mitigate these limitations, recent work has focused on automated feature engineering (AutoFE) . Traditional AutoFE methods include tree-based exploration [ 25 ] , iterative subsampling [ 26 ] , and transformation enumeration [ 27 ] . Learning-based approaches further exploit machine learning and reinforcement learning to guide the feature construction process [ 21 , 28 , 29 ] . For example, OpenFE [ 30 ] integrates a feature-boosting algorithm with a two-stage pruning mechanism to achieve expert-level performance. Despite these advances, most existing AutoFE systems struggle to integrate domain expertise and constrained by the predefined operation sets. Recently, large language models (LLMs) have shown promise in this direction, offering contextual reasoning and domain-aware knowledge transfer for feature discovery [ 17 ] .\\nRecent advances in large language models (LLMs) have demonstrated their ability to leverage pre-trained knowledge for novel tasks through techniques such as prompt engineering and in-context learning, often without additional task-specific training [ 31 , 32 ] . Despite these capabilities, LLMs can produce factually incorrect or inconsistent outputs, motivating research into mechanisms that refine their outputs using feedback [ 33 , 34 ] .\\nTo further harness LLMs for optimization, recent approaches integrate them with evolutionary and search-based frameworks. LLMs have been used to perform adaptive mutation and crossover operations within evolutionary algorithms [ 35 , 36 ] , and coupled with evaluators to guide the search process [ 37 , 38 , 39 ] . Such strategies have shown success in areas including prompt optimization [ 40 , 41 ] , neural architecture search [ 42 , 43 ] , symbolic regression [ 44 ] , and discovery of mathematical heuristics [ 45 ] .\\nVery recently, several studies have begun exploring the use of large language models (LLMs) for tabular data analysis. CAAFE [ 17 ] represents one of the earliest attempts to integrate LLMs into the feature generation loop by providing task-specific contextual descriptions and iteratively adding features in a greedy, single-path manner. In contrast, OCTree [ 46 ] introduces a rule-based mechanism inspired by decision trees, where LLMs are employed to generate and refine feature-construction rules, thus making the search process more interpretable and controllable. Focusing on controllability, other approaches [ 11 , 47 ] predefine an operation set, using the LLM to select and combine operations at each iteration to produce new features. However, these methods impose overly rigid constraints on the search space, severely limiting the expressive capacity of LLMs and rendering them unsuitable for the industrial-scale complexity considered in this work.\\nFocusing on free coding capability, FeatLLM [ 48 ] leverages few-shot prompting to guide LLMs in generating feature-engineering code directly, while LLM-FE [ 12 ] , inspired by FunSearch [ 45 ] , employs an island-based evolutionary framework to manage populations and mutations of generated code. Although these approaches offer greater flexibility and more suitable for heterogeneous data, they often sacrifice steerability and fine-grained control over the evolution trajectory.\\nIn contrast, our method fully exploits the LLM‚Äôs coding and reasoning capabilities to perform arbitrarily sophisticated feature transformations while maintaining controllable evolution through a structured idea‚Äìfeature knowledge architecture. This design enables the generation of comprehensive, robust, and interpretable features, making the approach scalable and applicable to the complexities of real-world industrial environments.',\n",
       "    'subsections': [{'title': 'II-A Feature Engineering on Industrial Data',\n",
       "      'paragraphs': 'Feature engineering‚Äîalso referred to as feature transformation or feature generation‚Äîdenotes the process of constructing, transforming, and selecting informative variables from raw data to enhance the performance and generalization of machine learning models [ 17 ] . By providing models with the most relevant representations, effective feature engineering underpins predictive accuracy, interpretability, and decision-making capability.\\nBroadly, existing approaches to feature generation fall into two paradigms. The first, latent representation learning , leverages deep architectures such as deep factorization machines and neural representation models to capture complex, nonlinear dependencies in data [ 18 , 19 ] . While powerful, these approaches often lack interpretability and traceability [ 20 ] . The second paradigm, explicit feature transformation , constructs new features through rule-based arithmetic, aggregation, or logical operations [ 21 , 22 ] . These methods depend heavily on domain expertise and manual design, which limits their scalability and adaptability across tasks.\\nIn industrial environments, raw data streams exhibit complexities far beyond those of conventional structured or tabular datasets, posing substantially greater challenges for feature engineering. The characteristics of industrial data are often summarized by the ‚Äú5Vs‚Äù framework‚Äî Volume, Velocity, Variety, Veracity , and Value [ 23 ] . Industrial systems typically involve intricate structural dependencies and operational uncertainties, further complicating the extraction of robust and meaningful features. Consequently, there is a growing demand for automated, domain-adaptive feature engineering techniques that can handle heterogeneous, large-scale, and often unstructured data [ 24 ] .\\nManual feature engineering, by contrast, is increasingly impractical in such settings. It relies heavily on the tacit domain knowledge of data scientists, who face steep learning curves when interpreting complex raw data. Frequent project transitions further exacerbate the problem, as practitioners must repeatedly invest substantial effort to understand new datasets and devise feature hypotheses from scratch‚Äîsignificantly impairing productivity and slowing innovation.\\nTo mitigate these limitations, recent work has focused on automated feature engineering (AutoFE) . Traditional AutoFE methods include tree-based exploration [ 25 ] , iterative subsampling [ 26 ] , and transformation enumeration [ 27 ] . Learning-based approaches further exploit machine learning and reinforcement learning to guide the feature construction process [ 21 , 28 , 29 ] . For example, OpenFE [ 30 ] integrates a feature-boosting algorithm with a two-stage pruning mechanism to achieve expert-level performance. Despite these advances, most existing AutoFE systems struggle to integrate domain expertise and constrained by the predefined operation sets. Recently, large language models (LLMs) have shown promise in this direction, offering contextual reasoning and domain-aware knowledge transfer for feature discovery [ 17 ] .',\n",
       "      'subsections': []},\n",
       "     {'title': 'II-B LLMs on data centric tasks',\n",
       "      'paragraphs': 'Recent advances in large language models (LLMs) have demonstrated their ability to leverage pre-trained knowledge for novel tasks through techniques such as prompt engineering and in-context learning, often without additional task-specific training [ 31 , 32 ] . Despite these capabilities, LLMs can produce factually incorrect or inconsistent outputs, motivating research into mechanisms that refine their outputs using feedback [ 33 , 34 ] .\\nTo further harness LLMs for optimization, recent approaches integrate them with evolutionary and search-based frameworks. LLMs have been used to perform adaptive mutation and crossover operations within evolutionary algorithms [ 35 , 36 ] , and coupled with evaluators to guide the search process [ 37 , 38 , 39 ] . Such strategies have shown success in areas including prompt optimization [ 40 , 41 ] , neural architecture search [ 42 , 43 ] , symbolic regression [ 44 ] , and discovery of mathematical heuristics [ 45 ] .\\nVery recently, several studies have begun exploring the use of large language models (LLMs) for tabular data analysis. CAAFE [ 17 ] represents one of the earliest attempts to integrate LLMs into the feature generation loop by providing task-specific contextual descriptions and iteratively adding features in a greedy, single-path manner. In contrast, OCTree [ 46 ] introduces a rule-based mechanism inspired by decision trees, where LLMs are employed to generate and refine feature-construction rules, thus making the search process more interpretable and controllable. Focusing on controllability, other approaches [ 11 , 47 ] predefine an operation set, using the LLM to select and combine operations at each iteration to produce new features. However, these methods impose overly rigid constraints on the search space, severely limiting the expressive capacity of LLMs and rendering them unsuitable for the industrial-scale complexity considered in this work.\\nFocusing on free coding capability, FeatLLM [ 48 ] leverages few-shot prompting to guide LLMs in generating feature-engineering code directly, while LLM-FE [ 12 ] , inspired by FunSearch [ 45 ] , employs an island-based evolutionary framework to manage populations and mutations of generated code. Although these approaches offer greater flexibility and more suitable for heterogeneous data, they often sacrifice steerability and fine-grained control over the evolution trajectory.\\nIn contrast, our method fully exploits the LLM‚Äôs coding and reasoning capabilities to perform arbitrarily sophisticated feature transformations while maintaining controllable evolution through a structured idea‚Äìfeature knowledge architecture. This design enables the generation of comprehensive, robust, and interpretable features, making the approach scalable and applicable to the complexities of real-world industrial environments.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'III Problem Formulation',\n",
       "    'paragraphs': 'Let ùíü ‚àà ‚Ñù N √ó M \\\\mathcal{D}\\\\in\\\\mathbb{R}^{N\\\\times M} denotes a tabular dataset, where N N is the number of rows and M M the number of columns. Each row x i x_{i} is associated with a user u i u_{i} . In non event log scenarios, N N simply equals to the number of users | ùí∞ | = L = N |\\\\mathcal{U}|=L=N . And rows ùê± \\\\mathbf{x} are often assumed to be independent, identically distributed (i.i.d). An event log dataset records a sequence of user interactions with a system, where each interaction corresponds to a discrete event triggered by user behavior (e.g., click, like, play). Formally, let the event log dataset be denoted as\\nwhere each event instance x i x_{i} is represented as a tuple:\\nHere, u i ‚àà ùí∞ u_{i}\\\\in\\\\mathcal{U} denotes the user identifier, a i a_{i} denotes the action type (e.g., click, like, buy), t i t_{i} is the timestamp at which the event occurs, and ùêú i ‚àà ‚Ñù d \\\\mathbf{c}_{i}\\\\in\\\\mathbb{R}^{d} represents the contextual or system state features associated with the event (such as device information, item metadata, or session context).\\nSince a user may perform multiple actions over time, the same user u i u_{i} can appear in multiple events, leading to | ùí∞ | < N |\\\\mathcal{U}|<N . The events are typically ordered chronologically according to their timestamps t 1 ‚â§ t 2 ‚â§ ‚ãØ ‚â§ t N t_{1}\\\\leq t_{2}\\\\leq\\\\cdots\\\\leq t_{N} . Thus, the event log dataset can be viewed as a temporally ordered multiset of user‚Äìaction pairs with associated contexts, which obviously break the i.i.d assumption mentioned before. Nonetheless, our solution is able to handle both event and non-event log dataset.\\nFor supervised learning tasks, each user u i u_{i} is associated with a corresponding label y i y_{i} , where y i ‚àà { 0 , 1 , ‚Ä¶ , K } y_{i}\\\\in\\\\{0,1,...,K\\\\} for classification tasks with K K classes, and y i ‚àà ‚Ñù y_{i}\\\\in\\\\mathbb{R} for regression tasks. Given a labeled tabular dataset ùíü = ( u i \\u200b ( ùê± ) , y i ) i = 1 L \\\\mathcal{D}=(u_{i}(\\\\mathbf{x}),y_{i})_{i=1}^{L} and prediction model f f to map from the input feature space ùí≥ \\\\mathcal{X} to its corresponding label space ùí¥ \\\\mathcal{Y} , our objective is to find an optimal feature transformation Œ∏ \\\\theta , which enhances the performance ‚Ñõ \\\\mathcal{R} of a predictive model when trained on the transformed input space. Formally, the feature engineering task can be defined as:\\nsubject to:\\nwhere ùíü t \\u200b r \\u200b a \\u200b i \\u200b n \\\\mathcal{D}_{train} and ùíü t \\u200b e \\u200b s \\u200b t \\\\mathcal{D}_{test} are the training set and test set, respectively.\\nWe now introduce the large language model (LLM) agents that collaboratively construct the optimal feature transformation Œ∏ \\\\theta . Each agent ùíú p \\\\mathcal{A}_{p} is formulated as a sequence-to-sequence function that operates on textual or structured representations of the current system state. The overall feature transformation Œ∏ \\\\theta is achieved through the sequential collaboration of P P agents:\\nthrough manipulating the data definition and prior knowledge.',\n",
       "    'subsections': [{'title': 'III-A Dataset',\n",
       "      'paragraphs': 'Let ùíü ‚àà ‚Ñù N √ó M \\\\mathcal{D}\\\\in\\\\mathbb{R}^{N\\\\times M} denotes a tabular dataset, where N N is the number of rows and M M the number of columns. Each row x i x_{i} is associated with a user u i u_{i} . In non event log scenarios, N N simply equals to the number of users | ùí∞ | = L = N |\\\\mathcal{U}|=L=N . And rows ùê± \\\\mathbf{x} are often assumed to be independent, identically distributed (i.i.d). An event log dataset records a sequence of user interactions with a system, where each interaction corresponds to a discrete event triggered by user behavior (e.g., click, like, play). Formally, let the event log dataset be denoted as\\nwhere each event instance x i x_{i} is represented as a tuple:\\nHere, u i ‚àà ùí∞ u_{i}\\\\in\\\\mathcal{U} denotes the user identifier, a i a_{i} denotes the action type (e.g., click, like, buy), t i t_{i} is the timestamp at which the event occurs, and ùêú i ‚àà ‚Ñù d \\\\mathbf{c}_{i}\\\\in\\\\mathbb{R}^{d} represents the contextual or system state features associated with the event (such as device information, item metadata, or session context).\\nSince a user may perform multiple actions over time, the same user u i u_{i} can appear in multiple events, leading to | ùí∞ | < N |\\\\mathcal{U}|<N . The events are typically ordered chronologically according to their timestamps t 1 ‚â§ t 2 ‚â§ ‚ãØ ‚â§ t N t_{1}\\\\leq t_{2}\\\\leq\\\\cdots\\\\leq t_{N} . Thus, the event log dataset can be viewed as a temporally ordered multiset of user‚Äìaction pairs with associated contexts, which obviously break the i.i.d assumption mentioned before. Nonetheless, our solution is able to handle both event and non-event log dataset.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-B Task',\n",
       "      'paragraphs': 'For supervised learning tasks, each user u i u_{i} is associated with a corresponding label y i y_{i} , where y i ‚àà { 0 , 1 , ‚Ä¶ , K } y_{i}\\\\in\\\\{0,1,...,K\\\\} for classification tasks with K K classes, and y i ‚àà ‚Ñù y_{i}\\\\in\\\\mathbb{R} for regression tasks. Given a labeled tabular dataset ùíü = ( u i \\u200b ( ùê± ) , y i ) i = 1 L \\\\mathcal{D}=(u_{i}(\\\\mathbf{x}),y_{i})_{i=1}^{L} and prediction model f f to map from the input feature space ùí≥ \\\\mathcal{X} to its corresponding label space ùí¥ \\\\mathcal{Y} , our objective is to find an optimal feature transformation Œ∏ \\\\theta , which enhances the performance ‚Ñõ \\\\mathcal{R} of a predictive model when trained on the transformed input space. Formally, the feature engineering task can be defined as:\\nsubject to:\\nwhere ùíü t \\u200b r \\u200b a \\u200b i \\u200b n \\\\mathcal{D}_{train} and ùíü t \\u200b e \\u200b s \\u200b t \\\\mathcal{D}_{test} are the training set and test set, respectively.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-C LLM Agents',\n",
       "      'paragraphs': 'We now introduce the large language model (LLM) agents that collaboratively construct the optimal feature transformation Œ∏ \\\\theta . Each agent ùíú p \\\\mathcal{A}_{p} is formulated as a sequence-to-sequence function that operates on textual or structured representations of the current system state. The overall feature transformation Œ∏ \\\\theta is achieved through the sequential collaboration of P P agents:\\nthrough manipulating the data definition and prior knowledge.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'IV Methodology',\n",
       "    'paragraphs': 'Figure 2 illustrates the overall architecture of FELA , which consists of two major components: (1) external dependencies, including the data schema, prior knowledge, and log data 1 1 1 Without loss of generality, we use log data in the following discussion. However, our system can be easily applied to simpler tabular datasets. ; and (2) the internal agent workspace.\\nExternal Dependencies. The data schema ‚Ñã \\\\mathcal{H} provides metadata describing the dataset, such as the context of data collection, column types, and textual descriptions. This information grounds the reasoning process and mitigates potential hallucinations by offering sufficient contextual grounding for large language model (LLM) agents. The prior knowledge component captures textual insights gathered from domain experts, stakeholder interviews, or external web resources. These insights are stored in a knowledge base to guide the system‚Äôs initial exploration. Although FELA can operate without explicit prior knowledge, such injections enable stronger control over the search space and alignment with domain-specific objectives. Finally, the event log data serves as the operational environment and provides the material basis for generating experimental results and evaluating newly proposed features.\\nInternal Agent Workspace. Within the agent workspace, FELA employs a team-based agent framework that iteratively generates, critiques, implements, and evaluates new feature ideas. The idea agents consider the data schema, current knowledge base, and past experience to propose plausible new features. Each proposal is subsequently reviewed by an idea critic , which evaluates its novelty, soundness, and adherence to predefined quality criteria. Approved ideas are then passed to the code agents , which translate high-level feature concepts into executable feature engineering code. The generated code is validated by a code critic to ensure syntactic correctness and logical consistency before being added to the code repository.\\nThe approved code is executed to transform the raw feature space, and the resulting features are used in downstream model training and evaluation under the target performance metrics (e.g., accuracy, AUC-score). The evaluation results are then processed by an evaluate agent , responsible for two key functions: (1) updating the performance attributes of the evaluated idea and corresponding feature implementations (2) synthesizing higher-level experience abstraction as global long-term memory. The updated knowledge base and memory modules subsequently inform the next round of idea generation, enabling FELA to perform self-improving, closed-loop evolution in feature engineering.\\nIn the following sections, we detail the agent interaction protocol, the reward formulation for idea evaluation, and the mechanisms enabling FELA‚Äôs long-term evolution and knowledge retention.\\nThe knowledge base serves as the central repository of textual insights and their corresponding feature implementations. Formally, it is defined as ùí¶ = { I i } i = 1 N I \\\\mathcal{K}=\\\\{I_{i}\\\\}_{i=1}^{N_{I}} , where each idea I i I_{i} represents an abstract understanding of the underlying data scenario. For example, in an e-commerce advertising context, an insight might be expressed as ‚ÄúUsers tend to purchase complementary items within the same session.‚Äù Although such an idea does not directly specify how to compute a feature, it encapsulates a meaningful intuition‚Äîanalogous to how human data scientists begin their exploratory reasoning in practice.\\nEach idea I i I_{i} is associated with one or more features d i \\u200b j {d_{ij}} , which provide concrete realizations of how the insight can be operationalized logically and mathematically. In the above example, ‚Äúcomplementary items‚Äù can be interpreted in multiple ways‚Äîsuch as categorical complementarity (e.g., ‚ÄúiPhone‚Äù and ‚ÄúiPhone case‚Äù) or temporal complementarity (e.g., ‚Äúdiapers‚Äù and ‚Äúbeer‚Äù purchased sequentially). These distinct interpretations correspond to different features of the same high-level insight. Together, the ideas and their features form a two-layer hierarchical structure, as illustrated in Figure 3 , representing the conceptual-to-operational flow of feature construction. Formally, the two-layer hierarchical knowledge base ùí¶ \\\\mathcal{K} can be expressed as\\nwhere M i M_{i} denotes the number of features in i i th idea. Drawing an analogy to island-based genetic algorithms, each idea can be viewed as an ‚Äúisland,‚Äù while its associated features constitute the ‚Äúpopulation‚Äù of that island. Features within the same island may undergo mutation to explore local variations, while different islands can interact and exchange information to generate novel ideas‚Äîa process that will be elaborated in the following section.\\nOperating in the hypothetical space, the role of idea agents is to expand the knowledge base and drive its evolution. At each iteration step t t , an idea agent takes as input the current knowledge database ùí¶ t \\\\mathcal{K}_{t} and selects one of the following actions: (1) generate a new feature from an existing insight, (2) synthesize a new insight from existing insights, or (3) create an entirely new idea. Each action is carried out by a specialized sub-agent.\\nFeature Proposer. When option (1) is chosen, we employ an Upper Confidence Bound (UCB) algorithm to sample an idea for exploitation and generate a new feature. The details of UCB algorithm are provided in Sec. IV-H . For a given idea, we retrieve its associated past experiences to construct the short-term memory for that idea. Based on this memory ‚Ñ≥ s \\\\mathcal{M}_{s} and the adaptively improved global long-term memory ‚Ñ≥ l \\\\mathcal{M}_{l} , the LLM-based agent proposes a new feature implementation that is (i) consistent with the abstract insight, (ii) technically sound, and (iii) creative‚Äîi.e., distinct from existing implementations under the same idea. Formally,\\nThe feature implementation d d is represented as a tuple ( reason , summary , pseudocode ) (\\\\text{reason},\\\\text{summary},\\\\text{pseudocode}) . The reason field allows the idea agent to explicitly articulate the rationale behind proposing a specific implementation. The summary provides a concise description of the feature‚Äôs underlying intuition. The pseudocode encodes the programmable instructions for the downstream code agent, including all necessary dependencies to generate the targeted feature. Collectively, this tuple establishes a seamless pathway from high-level abstract insight to detailed, implementable instructions. We find that such a structured representation is critical for ensuring robustness and consistency in the subsequent code generation process.\\nTo ensure that the generated feature implementations satisfy these criteria, we adopt the reasoning spirit of ReAct [ 49 ] and Reflexion [ 50 ] , and employ a three-step reasoning framework (see Figure 4 ) for our agents. Specifically, the first step, analyze , gathers all relevant information, examines the current state, and produces a textual analysis trace. This step encourages the LLM to form a deep understanding of the provided context. The second step, self-reflect , serves as a self-critique phase, prompting the model to check for factual errors or inconsistencies against the task requirements. The final step, reconstruct , allows the LLM to revise its prior reasoning based on the self-reflection results, thereby fostering deeper understanding and improved reasoning before producing the final output .\\nIdea Synthesizer. When option (2) is selected, the LLM is instructed to synthesize new ideas by combining existing ones, guided by the current ideas and their associated evaluation scores. This process mimics how humans interconnect related concepts to generate novel insights. While prior work typically selects which ideas (e.g., two) may be combined, we observe that LLMs are inherently capable reasoners that can exploit contextual cues to form meaningful combinations‚Äîthanks to the strong priors embedded in their parameters. Therefore, we provide the model with all existing ideas in the knowledge base ùí¶ \\\\mathcal{K} (including newly generated ones) and instruct it to propose a new idea satisfying three criteria: novelty, soundness, and consistency. Formally,\\nDuring the analysis phase, the LLM is explicitly required to ground its innovation in existing ideas, which, according to our observations, effectively reduces hallucination.\\nIdea Creator. The idea creator is responsible for generating entirely new ideas that expand the knowledge base beyond previously injected prior knowledge. Taking the entire knowledge base ùí¶ \\\\mathcal{K} as input, and leveraging the LLM‚Äôs strong reasoning and abstraction capabilities, the idea creator can propose novel, logically sound, and schema-consistent ideas that have not been represented before. This capability stems from the extensive corpus used during LLM pre-training, which implicitly encodes a vast reservoir of latent knowledge across diverse domains. By systematically eliciting this embedded intelligence and encouraging creative reasoning, the idea creator continuously enriches the knowledge base with valuable new insights. 2 2 2 In fact, we can even enroll the updated knowledge from internet such as WebAgent [ 51 ] . However, to maintain controlled experiments, our experiment setup restricts agents to offline operation. Formally,\\nThe role of the code agent is to faithfully translate the feature implementation pseudocode and associated analysis into executable feature engineering code. Given a feature implementation produced by the idea agent, along with high-quality examples from previous attempts and the data schema as concrete guidance, the code agent generates highly executable and arbitrarily sophisticated code Œ∏ t \\\\theta_{t} to transform raw features:\\nwhere { { d i , j } j = 0 M i } i = 0 k \\\\{\\\\{d_{i,j}\\\\}_{j=0}^{M_{i}}\\\\}_{i=0}^{k} denotes selected feature implementations from k k ideas, and t t denotes the iteration step in the FELA system.\\nCompared to existing LLM-based approaches, our solution emphasizes two key improvements. First, instead of relying on a predefined feature operation set as in [ 11 , 52 ] , we allow the LLM to code freely. While operation sets provide high controllability, they are extremely limited and inefficient for two reasons: (i) many feature computations cannot be represented by simple binary operations (e.g., k-means clustering), and complex features often require multiple rounds of refinement; (ii) exhaustively enumerating all possible arithmetic operations is practically infeasible, which imposes an upper bound on the expressiveness of feature engineering.\\nSecond, unlike prior free-coding approaches [ 12 ] , which often produce inconsistent results and degrade in quality as context length increases over iterations, our method grounds code generation on the underlying ideas and feature implementations. These implementations provide precisely the right context to guide the LLM coder, ensuring robust and steerable growth of feature engineering code. This design enables FELA to generate dozens of new features spanning thousands of lines of code, while maintaining consistency, correctness, and interpretability.\\nLarge Language Models (LLMs) often suffer from attention distillation issues [ 53 , 54 ] when dealing with long contexts, making them prone to hallucinations and reasoning failures. This challenge is particularly severe in our setting, where both ideas and their implementations evolve into large and complex structures over time. In such cases, idea agents and code agents may deviate from intended instructions or produce low-quality results.\\nTo address this problem, we introduce two auxiliary agents‚Äî idea critics and code critics ‚Äîwhich follow a unified design philosophy. Given the output from an idea agent (or code agent), a critic agent is responsible for rigorously evaluating whether the newly generated idea (or code) satisfies the requirements of consistency and correctness . Consistency refers to the alignment with the data schema and the existing knowledge base, while correctness ensures that the proposed idea or code is reasonable and technically sound.\\nIf any requirement is violated, the critic agent provides detailed feedback to the corresponding generator (idea or code agent), allowing it to revise and improve its output before proceeding. The incorporation of these critics is crucial for maintaining the robustness and reliability of a multi-agent system operating over extended periods‚Äîan essential property for real-world industrial applications.\\nFormally, the critic‚Äôs operation can be represented as:\\nwhere ùíµ t \\\\mathcal{Z}_{t} denotes the critic feedback and C C represents the set of evaluation criteria. Consequently, the update equation for the code agent (Eq. 8 ) becomes:\\nIn practice, we define a maximum number of critic iterations Z Z . When z > Z z>Z , the system forfeits the current refinement loop and initiates a new round of exploration. Similar critic feedbacks are applied to idea agents.\\nAfter each round of model training and testing, we obtain feedback signals (e.g., AUC scores) from the downstream prediction task. However, such feedback is extremely sparse relative to the vast‚Äîif not infinite‚Äîsearch space in our system, where ideas, implementations, and codes all require exploration and reward-driven updates. Traditional AutoML systems often operate under either a limited search space or a setting where Monte Carlo sampling of candidate actions is computationally cheap. In contrast, in our LLM-based framework, each action (e.g., generating a new idea or implementation) is both computationally and semantically expensive.\\nTo overcome this challenge, we leverage the intrinsic reasoning capability of LLMs to perform an additional step of reflective analysis after receiving reward feedback. The evaluation agent interprets the reward signal and produces an informative summary of the current exploration trajectory and its corresponding outcomes. This process effectively increases the bandwidth of the reward channel , enabling more targeted and precise search in subsequent iterations.\\nAll such analyses are persistently maintained through our long‚Äìshort term memory system, which captures both local (short-term) contextual feedback and global (long-term) strategic insights. The details of this memory mechanism are discussed in the following section. Formally, we have\\nThe memory system in FELA is designed to harness past evolutionary experiences to guide future exploration effectively. It comprises two distinct components: a long-term collaborative memory and a short-term feature-based memory.\\nThe long-term memory (see Figure 5 ), denoted as ‚Ñ≥ l \\\\mathcal{M}_{l} , captures high-level, natural language expolation insights that are shared across the entire knowledge base. Following each exploration cycle, a dedicated long-term memory agent progressively updates ‚Ñ≥ l \\\\mathcal{M}_{l} by incorporating the current state of the evolving idea and its associated rewards. This refreshed long-term memory is then utilized to inform the idea-generation process in the subsequent exploration round.\\nThe short-term memory (see Figure 6 ), denoted as ‚Ñ≥ s \\\\mathcal{M}_{s} , provides specific, actionable suggestions for augmenting the current idea with new features, which operates in a retrieval-augmented generation (RAG) manner. To generate ‚Ñ≥ s \\\\mathcal{M}_{s} , the system first retrieves a set of ideas semantically related to the current one. This is achieved by embedding each idea‚Äôs insight and feature set into a vector space, where the top- K K most similar ideas are retrieved based on cosine similarity. Then, the current idea and its retrieved neighbors are processed by an LLM to produce the short-term memory ‚Ñ≥ s \\\\mathcal{M}_{s} . This output is fed into the idea agent, which leverages the positive and negative features from related ideas to make more informed and effective modifications, thereby steering the evolutionary process more efficiently.\\nFELA introduces a novel evolutionary learning algorithm that integrates RL and GA to enhance feature evolution efficiency. To quantify the utility of a newly generated feature d i , j + 1 d_{i,j+1} within idea I i I_{i} , we define a relative score s \\u200b ( d i , j + 1 ) s(d_{i,j+1}) as\\nwhere Œ∏ i , j \\\\theta_{i,j} and Œ∏ i , j + 1 \\\\theta_{i,j+1} denote the feature transformation functions applied to the feature sets { d i , j ‚Ä≤ } j ‚Ä≤ = 1 j \\\\{d_{i,j^{\\\\prime}}\\\\}_{j^{\\\\prime}=1}^{j} and { d i , j ‚Ä≤ } j ‚Ä≤ = 1 j + 1 \\\\{d_{i,j^{\\\\prime}}\\\\}_{j^{\\\\prime}=1}^{j+1} , respectively. This score directly measures the additional contribution of d i , j + 1 d_{i,j+1} to the predictive performance of I i I_{i} . A positive s \\u200b ( d i , j + 1 ) s(d_{i,j+1}) indicates a beneficial feature, which is retained for subsequent code generation; conversely, a negative score leads to the feature being recorded as a negative sample for the idea. To balance exploration and exploitation during evolution, FELA employs the Upper Confidence Bound (UCB) criterion to select the idea for the next evolution round. The UCB for idea I i I_{i} is defined as\\nwhere Q i Q_{i} is the visit count of I i I_{i} , Q Q is the total visits across all ideas, and c c is a hyperparameter controlling exploration. The idea with the highest UCB is selected for further evolution, effectively balancing the exploration of new feature combinations and the exploitation of high-performing ideas.',\n",
       "    'subsections': [{'title': 'IV-A System Overview',\n",
       "      'paragraphs': 'Figure 2 illustrates the overall architecture of FELA , which consists of two major components: (1) external dependencies, including the data schema, prior knowledge, and log data 1 1 1 Without loss of generality, we use log data in the following discussion. However, our system can be easily applied to simpler tabular datasets. ; and (2) the internal agent workspace.\\nExternal Dependencies. The data schema ‚Ñã \\\\mathcal{H} provides metadata describing the dataset, such as the context of data collection, column types, and textual descriptions. This information grounds the reasoning process and mitigates potential hallucinations by offering sufficient contextual grounding for large language model (LLM) agents. The prior knowledge component captures textual insights gathered from domain experts, stakeholder interviews, or external web resources. These insights are stored in a knowledge base to guide the system‚Äôs initial exploration. Although FELA can operate without explicit prior knowledge, such injections enable stronger control over the search space and alignment with domain-specific objectives. Finally, the event log data serves as the operational environment and provides the material basis for generating experimental results and evaluating newly proposed features.\\nInternal Agent Workspace. Within the agent workspace, FELA employs a team-based agent framework that iteratively generates, critiques, implements, and evaluates new feature ideas. The idea agents consider the data schema, current knowledge base, and past experience to propose plausible new features. Each proposal is subsequently reviewed by an idea critic , which evaluates its novelty, soundness, and adherence to predefined quality criteria. Approved ideas are then passed to the code agents , which translate high-level feature concepts into executable feature engineering code. The generated code is validated by a code critic to ensure syntactic correctness and logical consistency before being added to the code repository.\\nThe approved code is executed to transform the raw feature space, and the resulting features are used in downstream model training and evaluation under the target performance metrics (e.g., accuracy, AUC-score). The evaluation results are then processed by an evaluate agent , responsible for two key functions: (1) updating the performance attributes of the evaluated idea and corresponding feature implementations (2) synthesizing higher-level experience abstraction as global long-term memory. The updated knowledge base and memory modules subsequently inform the next round of idea generation, enabling FELA to perform self-improving, closed-loop evolution in feature engineering.\\nIn the following sections, we detail the agent interaction protocol, the reward formulation for idea evaluation, and the mechanisms enabling FELA‚Äôs long-term evolution and knowledge retention.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-B Knowledge Base',\n",
       "      'paragraphs': 'The knowledge base serves as the central repository of textual insights and their corresponding feature implementations. Formally, it is defined as ùí¶ = { I i } i = 1 N I \\\\mathcal{K}=\\\\{I_{i}\\\\}_{i=1}^{N_{I}} , where each idea I i I_{i} represents an abstract understanding of the underlying data scenario. For example, in an e-commerce advertising context, an insight might be expressed as ‚ÄúUsers tend to purchase complementary items within the same session.‚Äù Although such an idea does not directly specify how to compute a feature, it encapsulates a meaningful intuition‚Äîanalogous to how human data scientists begin their exploratory reasoning in practice.\\nEach idea I i I_{i} is associated with one or more features d i \\u200b j {d_{ij}} , which provide concrete realizations of how the insight can be operationalized logically and mathematically. In the above example, ‚Äúcomplementary items‚Äù can be interpreted in multiple ways‚Äîsuch as categorical complementarity (e.g., ‚ÄúiPhone‚Äù and ‚ÄúiPhone case‚Äù) or temporal complementarity (e.g., ‚Äúdiapers‚Äù and ‚Äúbeer‚Äù purchased sequentially). These distinct interpretations correspond to different features of the same high-level insight. Together, the ideas and their features form a two-layer hierarchical structure, as illustrated in Figure 3 , representing the conceptual-to-operational flow of feature construction. Formally, the two-layer hierarchical knowledge base ùí¶ \\\\mathcal{K} can be expressed as\\nwhere M i M_{i} denotes the number of features in i i th idea. Drawing an analogy to island-based genetic algorithms, each idea can be viewed as an ‚Äúisland,‚Äù while its associated features constitute the ‚Äúpopulation‚Äù of that island. Features within the same island may undergo mutation to explore local variations, while different islands can interact and exchange information to generate novel ideas‚Äîa process that will be elaborated in the following section.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-C Idea Agents',\n",
       "      'paragraphs': 'Operating in the hypothetical space, the role of idea agents is to expand the knowledge base and drive its evolution. At each iteration step t t , an idea agent takes as input the current knowledge database ùí¶ t \\\\mathcal{K}_{t} and selects one of the following actions: (1) generate a new feature from an existing insight, (2) synthesize a new insight from existing insights, or (3) create an entirely new idea. Each action is carried out by a specialized sub-agent.\\nFeature Proposer. When option (1) is chosen, we employ an Upper Confidence Bound (UCB) algorithm to sample an idea for exploitation and generate a new feature. The details of UCB algorithm are provided in Sec. IV-H . For a given idea, we retrieve its associated past experiences to construct the short-term memory for that idea. Based on this memory ‚Ñ≥ s \\\\mathcal{M}_{s} and the adaptively improved global long-term memory ‚Ñ≥ l \\\\mathcal{M}_{l} , the LLM-based agent proposes a new feature implementation that is (i) consistent with the abstract insight, (ii) technically sound, and (iii) creative‚Äîi.e., distinct from existing implementations under the same idea. Formally,\\nThe feature implementation d d is represented as a tuple ( reason , summary , pseudocode ) (\\\\text{reason},\\\\text{summary},\\\\text{pseudocode}) . The reason field allows the idea agent to explicitly articulate the rationale behind proposing a specific implementation. The summary provides a concise description of the feature‚Äôs underlying intuition. The pseudocode encodes the programmable instructions for the downstream code agent, including all necessary dependencies to generate the targeted feature. Collectively, this tuple establishes a seamless pathway from high-level abstract insight to detailed, implementable instructions. We find that such a structured representation is critical for ensuring robustness and consistency in the subsequent code generation process.\\nTo ensure that the generated feature implementations satisfy these criteria, we adopt the reasoning spirit of ReAct [ 49 ] and Reflexion [ 50 ] , and employ a three-step reasoning framework (see Figure 4 ) for our agents. Specifically, the first step, analyze , gathers all relevant information, examines the current state, and produces a textual analysis trace. This step encourages the LLM to form a deep understanding of the provided context. The second step, self-reflect , serves as a self-critique phase, prompting the model to check for factual errors or inconsistencies against the task requirements. The final step, reconstruct , allows the LLM to revise its prior reasoning based on the self-reflection results, thereby fostering deeper understanding and improved reasoning before producing the final output .\\nIdea Synthesizer. When option (2) is selected, the LLM is instructed to synthesize new ideas by combining existing ones, guided by the current ideas and their associated evaluation scores. This process mimics how humans interconnect related concepts to generate novel insights. While prior work typically selects which ideas (e.g., two) may be combined, we observe that LLMs are inherently capable reasoners that can exploit contextual cues to form meaningful combinations‚Äîthanks to the strong priors embedded in their parameters. Therefore, we provide the model with all existing ideas in the knowledge base ùí¶ \\\\mathcal{K} (including newly generated ones) and instruct it to propose a new idea satisfying three criteria: novelty, soundness, and consistency. Formally,\\nDuring the analysis phase, the LLM is explicitly required to ground its innovation in existing ideas, which, according to our observations, effectively reduces hallucination.\\nIdea Creator. The idea creator is responsible for generating entirely new ideas that expand the knowledge base beyond previously injected prior knowledge. Taking the entire knowledge base ùí¶ \\\\mathcal{K} as input, and leveraging the LLM‚Äôs strong reasoning and abstraction capabilities, the idea creator can propose novel, logically sound, and schema-consistent ideas that have not been represented before. This capability stems from the extensive corpus used during LLM pre-training, which implicitly encodes a vast reservoir of latent knowledge across diverse domains. By systematically eliciting this embedded intelligence and encouraging creative reasoning, the idea creator continuously enriches the knowledge base with valuable new insights. 2 2 2 In fact, we can even enroll the updated knowledge from internet such as WebAgent [ 51 ] . However, to maintain controlled experiments, our experiment setup restricts agents to offline operation. Formally,',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-D Code Agent',\n",
       "      'paragraphs': 'The role of the code agent is to faithfully translate the feature implementation pseudocode and associated analysis into executable feature engineering code. Given a feature implementation produced by the idea agent, along with high-quality examples from previous attempts and the data schema as concrete guidance, the code agent generates highly executable and arbitrarily sophisticated code Œ∏ t \\\\theta_{t} to transform raw features:\\nwhere { { d i , j } j = 0 M i } i = 0 k \\\\{\\\\{d_{i,j}\\\\}_{j=0}^{M_{i}}\\\\}_{i=0}^{k} denotes selected feature implementations from k k ideas, and t t denotes the iteration step in the FELA system.\\nCompared to existing LLM-based approaches, our solution emphasizes two key improvements. First, instead of relying on a predefined feature operation set as in [ 11 , 52 ] , we allow the LLM to code freely. While operation sets provide high controllability, they are extremely limited and inefficient for two reasons: (i) many feature computations cannot be represented by simple binary operations (e.g., k-means clustering), and complex features often require multiple rounds of refinement; (ii) exhaustively enumerating all possible arithmetic operations is practically infeasible, which imposes an upper bound on the expressiveness of feature engineering.\\nSecond, unlike prior free-coding approaches [ 12 ] , which often produce inconsistent results and degrade in quality as context length increases over iterations, our method grounds code generation on the underlying ideas and feature implementations. These implementations provide precisely the right context to guide the LLM coder, ensuring robust and steerable growth of feature engineering code. This design enables FELA to generate dozens of new features spanning thousands of lines of code, while maintaining consistency, correctness, and interpretability.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-E Critic Agents',\n",
       "      'paragraphs': 'Large Language Models (LLMs) often suffer from attention distillation issues [ 53 , 54 ] when dealing with long contexts, making them prone to hallucinations and reasoning failures. This challenge is particularly severe in our setting, where both ideas and their implementations evolve into large and complex structures over time. In such cases, idea agents and code agents may deviate from intended instructions or produce low-quality results.\\nTo address this problem, we introduce two auxiliary agents‚Äî idea critics and code critics ‚Äîwhich follow a unified design philosophy. Given the output from an idea agent (or code agent), a critic agent is responsible for rigorously evaluating whether the newly generated idea (or code) satisfies the requirements of consistency and correctness . Consistency refers to the alignment with the data schema and the existing knowledge base, while correctness ensures that the proposed idea or code is reasonable and technically sound.\\nIf any requirement is violated, the critic agent provides detailed feedback to the corresponding generator (idea or code agent), allowing it to revise and improve its output before proceeding. The incorporation of these critics is crucial for maintaining the robustness and reliability of a multi-agent system operating over extended periods‚Äîan essential property for real-world industrial applications.\\nFormally, the critic‚Äôs operation can be represented as:\\nwhere ùíµ t \\\\mathcal{Z}_{t} denotes the critic feedback and C C represents the set of evaluation criteria. Consequently, the update equation for the code agent (Eq. 8 ) becomes:\\nIn practice, we define a maximum number of critic iterations Z Z . When z > Z z>Z , the system forfeits the current refinement loop and initiates a new round of exploration. Similar critic feedbacks are applied to idea agents.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-F Evaluate Agent',\n",
       "      'paragraphs': 'After each round of model training and testing, we obtain feedback signals (e.g., AUC scores) from the downstream prediction task. However, such feedback is extremely sparse relative to the vast‚Äîif not infinite‚Äîsearch space in our system, where ideas, implementations, and codes all require exploration and reward-driven updates. Traditional AutoML systems often operate under either a limited search space or a setting where Monte Carlo sampling of candidate actions is computationally cheap. In contrast, in our LLM-based framework, each action (e.g., generating a new idea or implementation) is both computationally and semantically expensive.\\nTo overcome this challenge, we leverage the intrinsic reasoning capability of LLMs to perform an additional step of reflective analysis after receiving reward feedback. The evaluation agent interprets the reward signal and produces an informative summary of the current exploration trajectory and its corresponding outcomes. This process effectively increases the bandwidth of the reward channel , enabling more targeted and precise search in subsequent iterations.\\nAll such analyses are persistently maintained through our long‚Äìshort term memory system, which captures both local (short-term) contextual feedback and global (long-term) strategic insights. The details of this memory mechanism are discussed in the following section. Formally, we have',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-G Memory System',\n",
       "      'paragraphs': 'The memory system in FELA is designed to harness past evolutionary experiences to guide future exploration effectively. It comprises two distinct components: a long-term collaborative memory and a short-term feature-based memory.\\nThe long-term memory (see Figure 5 ), denoted as ‚Ñ≥ l \\\\mathcal{M}_{l} , captures high-level, natural language expolation insights that are shared across the entire knowledge base. Following each exploration cycle, a dedicated long-term memory agent progressively updates ‚Ñ≥ l \\\\mathcal{M}_{l} by incorporating the current state of the evolving idea and its associated rewards. This refreshed long-term memory is then utilized to inform the idea-generation process in the subsequent exploration round.\\nThe short-term memory (see Figure 6 ), denoted as ‚Ñ≥ s \\\\mathcal{M}_{s} , provides specific, actionable suggestions for augmenting the current idea with new features, which operates in a retrieval-augmented generation (RAG) manner. To generate ‚Ñ≥ s \\\\mathcal{M}_{s} , the system first retrieves a set of ideas semantically related to the current one. This is achieved by embedding each idea‚Äôs insight and feature set into a vector space, where the top- K K most similar ideas are retrieved based on cosine similarity. Then, the current idea and its retrieved neighbors are processed by an LLM to produce the short-term memory ‚Ñ≥ s \\\\mathcal{M}_{s} . This output is fed into the idea agent, which leverages the positive and negative features from related ideas to make more informed and effective modifications, thereby steering the evolutionary process more efficiently.',\n",
       "      'subsections': []},\n",
       "     {'title': 'IV-H Learning Algorithm',\n",
       "      'paragraphs': 'FELA introduces a novel evolutionary learning algorithm that integrates RL and GA to enhance feature evolution efficiency. To quantify the utility of a newly generated feature d i , j + 1 d_{i,j+1} within idea I i I_{i} , we define a relative score s \\u200b ( d i , j + 1 ) s(d_{i,j+1}) as\\nwhere Œ∏ i , j \\\\theta_{i,j} and Œ∏ i , j + 1 \\\\theta_{i,j+1} denote the feature transformation functions applied to the feature sets { d i , j ‚Ä≤ } j ‚Ä≤ = 1 j \\\\{d_{i,j^{\\\\prime}}\\\\}_{j^{\\\\prime}=1}^{j} and { d i , j ‚Ä≤ } j ‚Ä≤ = 1 j + 1 \\\\{d_{i,j^{\\\\prime}}\\\\}_{j^{\\\\prime}=1}^{j+1} , respectively. This score directly measures the additional contribution of d i , j + 1 d_{i,j+1} to the predictive performance of I i I_{i} . A positive s \\u200b ( d i , j + 1 ) s(d_{i,j+1}) indicates a beneficial feature, which is retained for subsequent code generation; conversely, a negative score leads to the feature being recorded as a negative sample for the idea. To balance exploration and exploitation during evolution, FELA employs the Upper Confidence Bound (UCB) criterion to select the idea for the next evolution round. The UCB for idea I i I_{i} is defined as\\nwhere Q i Q_{i} is the visit count of I i I_{i} , Q Q is the total visits across all ideas, and c c is a hyperparameter controlling exploration. The idea with the highest UCB is selected for further evolution, effectively balancing the exploration of new feature combinations and the exploitation of high-performing ideas.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'V Experiment',\n",
       "    'paragraphs': 'In this section, comprehensive experiments are conducted to validate the effectiveness and robustness of the proposed FELA framework. Our evaluation is designed to answer the following key questions:\\nRQ1 (Performance): Can FELA outperform raw features and state-of-the-art LLM-based feature engineering methods on tabular and industrial log data?\\nRQ2 (Interpretability): Does FELA provide a steerable and semantically meaningful feature generation process?\\nRQ3 (Compatibility): Is FELA compatible with various machine learning models when integrated within the evolution loop?\\nRQ4 (Mechanism): How does FELA‚Äôs core design, including the critic agent, long-short-term memory, and the UCB-based learning, contribute to its overall performance?\\nTo this end, we perform extensive comparisons on real-world datasets, an applicability test across multiple classifiers, a detailed ablation study, and in-depth case analyses on feature correlation and evolution trajectory. The results collectively demonstrate that FELA establishes a new state-of-the-art agentic feature engineering.\\nIn the experiment, three real-world datasets are adopted for evaluation, including Diabetes Health Indicator Dataset (Dia) [ 55 ] , Tabao Conversion Prediction Data (Taobao) [ 56 ] , and the User Churn Data in Tencent Game Platform (Tencent). The basic information of the three datasets is summarized in Table I , where Dia is standard tabular data while the others are event log data with real industrial complexity.\\nDia Dataset : This tabular dataset contains 253,680 instances, where each instance has 21 features and is labeled with a binary class. The feature engineering functions are independently applied to each instance in the dataset, where the output features are used for downstream classification. Here, we split 55% of the samples for ML model training, and the remaining 45% of the samples are used for validation.\\nTaobao Dataset : This industrial log dataset contains 176,773 users, 9599 items, and 420,721 ad records over 7 days, where each record is labeled with a binary ad impression. The data is organized across five relational tables containing basic identifiers, item profiles, user demographics, contextual signals, and shop reputation metrics, which constitute 31 features for each record. The feature engineering functions jointly process the entire log data and output the features for each user-ad pair. Following standard temporal evaluation protocols, the dataset is chronologically split with the initial 6 days for training and the final day for testing, effectively simulating real-world conversion prediction scenarios where models forecast future conversion behavior based on learned historical patterns.\\nTencent Dataset : This dataset is collected from a Massively Multiplayer Online (MMO) game in Tencent for a few days, where the players are sampled from the same user profile to satisfy the i.i.d assumption. Each player is associated with a churn label, indicating whether the user will churn in the next day. In our scenario, each player can have multiple game plays in a day, which results in multiple event log instances. Furthermore, each player in the same game can have multiple teammates, and if the player has more than one teammate, we will separate the records into different rows. Therefore, the instances in such event log data do not follow the i.i.d assumption as mentioned in the introduction session. The feature engineering functions jointly process the entire log data and output the features for each player. The downstream churn prediction is thus working on player level. For evaluation, a temporal split is adopted: the former day of data are used for training, and the logs from the latter day are held out for testing.\\nWe evaluate the performance of our proposed FELA system against three baselines: (1) raw features, (2) FeatLLM [ 57 ] , and (3) LLM-FE [ 12 ] , where FeatLLM and LLM-FE are state-of-the-art free coding methods. To ensure a fair comparison, all LLM-based methods, including the baselines and our proposed FELA, utilize the DeepSeek-V3 model via the DeepSeek API [ 58 ] .\\nThe feature engineering for FELA and the LLM-FE baseline is optimized using the AUC-score, which we denote as the target metric ‚Ñõ \\\\mathcal{R} in Equation\\xa0( 1 ). The generated features of all methods are then evaluated using a Random Forest classifier by default, with performance reported across five standard metrics: Accuracy, Precision, Recall, F1-score, and AUC.\\nTable II presents the feature engineering performance of FELA and baseline methods on the tabular Dia dataset. The proposed FELA achieves superior performance in accuracy, precision, and AUC, demonstrating its effectiveness in handling structured data. Notably, FELA improves precision from 0.514 to 0.552 and AUC from 0.802 to 0.812 compared to the runner-up method LLM-FE, indicating its capability to generate high-quality discriminative features for tabular data analysis.\\nOn the Taobao conversion prediction dataset, FELA maintains its competitive advantage as shown in Table III . Our method obtains the highest scores in accuracy, precision, F1-score, and AUC, with a notable AUC improvement from 0.641 to 0.653 over LLM-FE. This consistent superiority across multiple metrics confirms FELA‚Äôs robustness in processing complex industrial log data with relational characteristics.\\nThe evaluation on the Tencent user churn dataset further validates FELA‚Äôs superiority on complex industrial log data. As summarized in Table IV , FELA outperforms all baselines in accuracy, precision, F1-score, and AUC, achieving a significant AUC boost from 0.686 to 0.701 compared to LLM-FE. These results across three distinct domains demonstrate that FELA consistently generates semantically meaningful features that enhance predictive performance in diverse real-world industrial scenarios.\\nWe begin by analyzing the complexity of the generated feature engineering code on the Taobao dataset, as illustrated in Figure 8 . After 16 hours of execution, the proposed FELA framework generates 22 features derived from 5 distinct feature ideas, achieving an AUC score of 0.653, an improvement of 0.023 over the raw features. This demonstrates FELA‚Äôs capability to efficiently produce a substantial number of high-quality features. In contrast, the FeatLLM baseline generates 20 new features within 2 minutes, yet yields negligible AUC improvement compared to raw features. Meanwhile, LLM-FE produces 110 code snippets within 30 minutes, with the best-performing code improving AUC by 0.011. However, the optimal LLM-FE code only creates only 1 new feature and lacks a continuous evolution mechanism, ultimately limiting its performance potential.\\nWe further investigate the feature correlation patterns in FELA by analyzing the Pearson correlation heatmap of the aforementioned 22 features, as visualized in Figure 9 . The analysis reveals that features derived from the same idea exhibit stronger intra-idea correlation. This phenomenon aligns with expectations, as features originating from the same semantic insight naturally share underlying conceptual foundations. Conversely, inter-idea feature correlations remain relatively low, indicating that FELA successfully explores diverse regions of the feature space. This structured exploration strategy demonstrates FELA‚Äôs capability to conduct steerable feature generation while maintaining semantic interpretability. The framework effectively balances between exploiting productive semantic directions and exploring novel feature combinations, resulting in both discriminative power and explanatory value. In Figure 10 , we visualize the sampled knowledge database after evolution on the Taobao dataset. We can easily trace the evolution of knowledge base structure, as well as understand the evolutionary trajectories between the ideas and associated features.\\nWe investigate the applicability of the proposed FELA over various machine learning models, including Random Forest, XGBoost, MLP, and Logistic classifiers. Then, the AUC performance of the proposed FELA and the baselines over the Tencent dataset is shown in Figure 7 . It can be observed that the proposed FELA can achieve the best AUC performance over most of the classifiers, where a 0.012 AUC improvement can be achieved on average. Thus, the proposed FELA system can serve as a robust feature engineering solution for various machine learning models.\\nWe conduct the ablation study in FELA systems to investigate the contribution of its components, including the critic agent, long-short-term memory, and the UCB-based learning algorithm. Then, the AUC in different iterations for the proposed FELA and its ablations is plotted in Figure 11 . The ablations without the critic agent and the long-short term memory lead to an obvious AUC decline of around 0.01. Thus, the proposed FELA can benefit from long-short-term memory to exploit the evolution experience. Additionally, the critic agent can enhance the correctness and consistency of the generated idea and code, which improves the robustness in feature generation and evaluation in FELA. For the ablation without the UCB-based learning algorithm, it can be found that the target AUC monotonically increases at a slower pace and converges to a lower value. Hence, proposed UCB-based learning can better balance the exploitation and exploration, which enhances the efficiency of the proposed FELA system.',\n",
       "    'subsections': [{'title': 'V-A Experiment Overview',\n",
       "      'paragraphs': 'In this section, comprehensive experiments are conducted to validate the effectiveness and robustness of the proposed FELA framework. Our evaluation is designed to answer the following key questions:\\nRQ1 (Performance): Can FELA outperform raw features and state-of-the-art LLM-based feature engineering methods on tabular and industrial log data?\\nRQ2 (Interpretability): Does FELA provide a steerable and semantically meaningful feature generation process?\\nRQ3 (Compatibility): Is FELA compatible with various machine learning models when integrated within the evolution loop?\\nRQ4 (Mechanism): How does FELA‚Äôs core design, including the critic agent, long-short-term memory, and the UCB-based learning, contribute to its overall performance?\\nTo this end, we perform extensive comparisons on real-world datasets, an applicability test across multiple classifiers, a detailed ablation study, and in-depth case analyses on feature correlation and evolution trajectory. The results collectively demonstrate that FELA establishes a new state-of-the-art agentic feature engineering.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-B Experiment Setup',\n",
       "      'paragraphs': 'In the experiment, three real-world datasets are adopted for evaluation, including Diabetes Health Indicator Dataset (Dia) [ 55 ] , Tabao Conversion Prediction Data (Taobao) [ 56 ] , and the User Churn Data in Tencent Game Platform (Tencent). The basic information of the three datasets is summarized in Table I , where Dia is standard tabular data while the others are event log data with real industrial complexity.\\nDia Dataset : This tabular dataset contains 253,680 instances, where each instance has 21 features and is labeled with a binary class. The feature engineering functions are independently applied to each instance in the dataset, where the output features are used for downstream classification. Here, we split 55% of the samples for ML model training, and the remaining 45% of the samples are used for validation.\\nTaobao Dataset : This industrial log dataset contains 176,773 users, 9599 items, and 420,721 ad records over 7 days, where each record is labeled with a binary ad impression. The data is organized across five relational tables containing basic identifiers, item profiles, user demographics, contextual signals, and shop reputation metrics, which constitute 31 features for each record. The feature engineering functions jointly process the entire log data and output the features for each user-ad pair. Following standard temporal evaluation protocols, the dataset is chronologically split with the initial 6 days for training and the final day for testing, effectively simulating real-world conversion prediction scenarios where models forecast future conversion behavior based on learned historical patterns.\\nTencent Dataset : This dataset is collected from a Massively Multiplayer Online (MMO) game in Tencent for a few days, where the players are sampled from the same user profile to satisfy the i.i.d assumption. Each player is associated with a churn label, indicating whether the user will churn in the next day. In our scenario, each player can have multiple game plays in a day, which results in multiple event log instances. Furthermore, each player in the same game can have multiple teammates, and if the player has more than one teammate, we will separate the records into different rows. Therefore, the instances in such event log data do not follow the i.i.d assumption as mentioned in the introduction session. The feature engineering functions jointly process the entire log data and output the features for each player. The downstream churn prediction is thus working on player level. For evaluation, a temporal split is adopted: the former day of data are used for training, and the logs from the latter day are held out for testing.\\nWe evaluate the performance of our proposed FELA system against three baselines: (1) raw features, (2) FeatLLM [ 57 ] , and (3) LLM-FE [ 12 ] , where FeatLLM and LLM-FE are state-of-the-art free coding methods. To ensure a fair comparison, all LLM-based methods, including the baselines and our proposed FELA, utilize the DeepSeek-V3 model via the DeepSeek API [ 58 ] .\\nThe feature engineering for FELA and the LLM-FE baseline is optimized using the AUC-score, which we denote as the target metric ‚Ñõ \\\\mathcal{R} in Equation\\xa0( 1 ). The generated features of all methods are then evaluated using a Random Forest classifier by default, with performance reported across five standard metrics: Accuracy, Precision, Recall, F1-score, and AUC.',\n",
       "      'subsections': [{'title': 'V-B1 Datasets and Feature Engineering Task',\n",
       "        'paragraphs': 'In the experiment, three real-world datasets are adopted for evaluation, including Diabetes Health Indicator Dataset (Dia) [ 55 ] , Tabao Conversion Prediction Data (Taobao) [ 56 ] , and the User Churn Data in Tencent Game Platform (Tencent). The basic information of the three datasets is summarized in Table I , where Dia is standard tabular data while the others are event log data with real industrial complexity.\\nDia Dataset : This tabular dataset contains 253,680 instances, where each instance has 21 features and is labeled with a binary class. The feature engineering functions are independently applied to each instance in the dataset, where the output features are used for downstream classification. Here, we split 55% of the samples for ML model training, and the remaining 45% of the samples are used for validation.\\nTaobao Dataset : This industrial log dataset contains 176,773 users, 9599 items, and 420,721 ad records over 7 days, where each record is labeled with a binary ad impression. The data is organized across five relational tables containing basic identifiers, item profiles, user demographics, contextual signals, and shop reputation metrics, which constitute 31 features for each record. The feature engineering functions jointly process the entire log data and output the features for each user-ad pair. Following standard temporal evaluation protocols, the dataset is chronologically split with the initial 6 days for training and the final day for testing, effectively simulating real-world conversion prediction scenarios where models forecast future conversion behavior based on learned historical patterns.\\nTencent Dataset : This dataset is collected from a Massively Multiplayer Online (MMO) game in Tencent for a few days, where the players are sampled from the same user profile to satisfy the i.i.d assumption. Each player is associated with a churn label, indicating whether the user will churn in the next day. In our scenario, each player can have multiple game plays in a day, which results in multiple event log instances. Furthermore, each player in the same game can have multiple teammates, and if the player has more than one teammate, we will separate the records into different rows. Therefore, the instances in such event log data do not follow the i.i.d assumption as mentioned in the introduction session. The feature engineering functions jointly process the entire log data and output the features for each player. The downstream churn prediction is thus working on player level. For evaluation, a temporal split is adopted: the former day of data are used for training, and the logs from the latter day are held out for testing.',\n",
       "        'subsections': []},\n",
       "       {'title': 'V-B2 Baselines',\n",
       "        'paragraphs': 'We evaluate the performance of our proposed FELA system against three baselines: (1) raw features, (2) FeatLLM [ 57 ] , and (3) LLM-FE [ 12 ] , where FeatLLM and LLM-FE are state-of-the-art free coding methods. To ensure a fair comparison, all LLM-based methods, including the baselines and our proposed FELA, utilize the DeepSeek-V3 model via the DeepSeek API [ 58 ] .',\n",
       "        'subsections': []},\n",
       "       {'title': 'V-B3 Metrics',\n",
       "        'paragraphs': 'The feature engineering for FELA and the LLM-FE baseline is optimized using the AUC-score, which we denote as the target metric ‚Ñõ \\\\mathcal{R} in Equation\\xa0( 1 ). The generated features of all methods are then evaluated using a Random Forest classifier by default, with performance reported across five standard metrics: Accuracy, Precision, Recall, F1-score, and AUC.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'V-C Feature Engineering Performance (RQ1)',\n",
       "      'paragraphs': 'Table II presents the feature engineering performance of FELA and baseline methods on the tabular Dia dataset. The proposed FELA achieves superior performance in accuracy, precision, and AUC, demonstrating its effectiveness in handling structured data. Notably, FELA improves precision from 0.514 to 0.552 and AUC from 0.802 to 0.812 compared to the runner-up method LLM-FE, indicating its capability to generate high-quality discriminative features for tabular data analysis.\\nOn the Taobao conversion prediction dataset, FELA maintains its competitive advantage as shown in Table III . Our method obtains the highest scores in accuracy, precision, F1-score, and AUC, with a notable AUC improvement from 0.641 to 0.653 over LLM-FE. This consistent superiority across multiple metrics confirms FELA‚Äôs robustness in processing complex industrial log data with relational characteristics.\\nThe evaluation on the Tencent user churn dataset further validates FELA‚Äôs superiority on complex industrial log data. As summarized in Table IV , FELA outperforms all baselines in accuracy, precision, F1-score, and AUC, achieving a significant AUC boost from 0.686 to 0.701 compared to LLM-FE. These results across three distinct domains demonstrate that FELA consistently generates semantically meaningful features that enhance predictive performance in diverse real-world industrial scenarios.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-D Steerable Feature Generation (RQ2)',\n",
       "      'paragraphs': 'We begin by analyzing the complexity of the generated feature engineering code on the Taobao dataset, as illustrated in Figure 8 . After 16 hours of execution, the proposed FELA framework generates 22 features derived from 5 distinct feature ideas, achieving an AUC score of 0.653, an improvement of 0.023 over the raw features. This demonstrates FELA‚Äôs capability to efficiently produce a substantial number of high-quality features. In contrast, the FeatLLM baseline generates 20 new features within 2 minutes, yet yields negligible AUC improvement compared to raw features. Meanwhile, LLM-FE produces 110 code snippets within 30 minutes, with the best-performing code improving AUC by 0.011. However, the optimal LLM-FE code only creates only 1 new feature and lacks a continuous evolution mechanism, ultimately limiting its performance potential.\\nWe further investigate the feature correlation patterns in FELA by analyzing the Pearson correlation heatmap of the aforementioned 22 features, as visualized in Figure 9 . The analysis reveals that features derived from the same idea exhibit stronger intra-idea correlation. This phenomenon aligns with expectations, as features originating from the same semantic insight naturally share underlying conceptual foundations. Conversely, inter-idea feature correlations remain relatively low, indicating that FELA successfully explores diverse regions of the feature space. This structured exploration strategy demonstrates FELA‚Äôs capability to conduct steerable feature generation while maintaining semantic interpretability. The framework effectively balances between exploiting productive semantic directions and exploring novel feature combinations, resulting in both discriminative power and explanatory value. In Figure 10 , we visualize the sampled knowledge database after evolution on the Taobao dataset. We can easily trace the evolution of knowledge base structure, as well as understand the evolutionary trajectories between the ideas and associated features.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-E Compatibility to Various Machine Learning Models (RQ3)',\n",
       "      'paragraphs': 'We investigate the applicability of the proposed FELA over various machine learning models, including Random Forest, XGBoost, MLP, and Logistic classifiers. Then, the AUC performance of the proposed FELA and the baselines over the Tencent dataset is shown in Figure 7 . It can be observed that the proposed FELA can achieve the best AUC performance over most of the classifiers, where a 0.012 AUC improvement can be achieved on average. Thus, the proposed FELA system can serve as a robust feature engineering solution for various machine learning models.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-F Abalation Study (RQ4)',\n",
       "      'paragraphs': 'We conduct the ablation study in FELA systems to investigate the contribution of its components, including the critic agent, long-short-term memory, and the UCB-based learning algorithm. Then, the AUC in different iterations for the proposed FELA and its ablations is plotted in Figure 11 . The ablations without the critic agent and the long-short term memory lead to an obvious AUC decline of around 0.01. Thus, the proposed FELA can benefit from long-short-term memory to exploit the evolution experience. Additionally, the critic agent can enhance the correctness and consistency of the generated idea and code, which improves the robustness in feature generation and evaluation in FELA. For the ablation without the UCB-based learning algorithm, it can be found that the target AUC monotonically increases at a slower pace and converges to a lower value. Hence, proposed UCB-based learning can better balance the exploitation and exploration, which enhances the efficiency of the proposed FELA system.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'VI Conclusion',\n",
       "    'paragraphs': 'In this work, we introduced FELA (Feature Engineering LLM Agents) , a comprehensive multi-agent collaboration system designed to automate and enhance feature engineering for complex industrial event log data. FELA integrates large language model agents with an insight-guided self-evolution paradigm, enabling the system to generate novel, explainable, and high-performing features without extensive human intervention. Through the synergistic interaction of Idea Agents , Code Agents , and Critic Agents , the system effectively decomposes the intricate reasoning and implementation processes that traditionally demand significant domain expertise.\\nCentral to FELA is the proposed agentic evolution algorithm , which combines the strengths of reinforcement learning and genetic algorithms to balance exploration and exploitation in a large and heterogeneous feature space. This algorithm, coupled with a hierarchical knowledge base and long‚Äìshort-term memory design, allows FELA to continuously improve its reasoning, adapt to diverse data domains, and maintain strong explainability throughout the evolution process.\\nOur experimental and deployment results collectively highlight the potential of LLM-based multi-agent systems in advancing automatic feature engineering. Future work will extend FELA toward broader applications, including multimodal data, dynamic environments, and tighter human-in-the-loop collaboration to further enhance controllability, scalability, and domain alignment.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Event log data, recording fine-grained user actions and system events, represent one of the most valuable assets for modern digital services. However, the complexity and heterogeneity of industrial event logs‚Äîcharacterized by large scale, high dimensionality, diverse data types, and intricate temporal or relational structures‚Äîmake feature engineering extremely challenging. Existing automatic feature engineering approaches, such as AutoML or genetic methods, often suffer from limited explainability, rigid predefined operations, and poor adaptability to complicated heterogeneous data.'},\n",
       " '2510.25179v1': {'content': [{'title': 'I Introduction',\n",
       "    'paragraphs': 'Large vision‚Äìlanguage models (LVLMs) integrate visual and textual modalities, enabling richer multimodal reasoning and expanding their application scope. However, this increased capability also enlarges the attack surface. Malicious users can exploit cross-modal interactions and the continuous nature of visual embedding spaces, which makes adversarial defenses especially challenging. Cross-modality adversarial attacks exploit visual vulnerabilities and modality shifts in semantic meaning. Examples include pixel-level perturbations that embed harmful intent within images [ 1 , 2 , 3 ] , malicious content rendered via typography or flowcharts [ 4 ] , harmful behaviors that emerge only from the combination of benign-looking text and visual inputs, implicit cross-modal interactions that obscure adversarial intent [ 5 ] , and hybrid or ensemble strategies that combine these mechanisms [ 6 ] .\\nExisting defense for LVLMs include preprocessing, model-level, and postprocessing techniques [ 7 ] . Preprocessing methods, such as input safety classifiers, purification techniques, and safety-augmented prompting, proactively intercept unsafe content before inference. Model-level defenses strengthen robustness through retraining or fine-tuning on curated datasets [ 8 , 9 ] , but incur high computational and data costs. Postprocessing approaches filter or re-rank model outputs based on partial solution evaluations [ 10 , 11 ] . In practice, many systems adopt hybrid pipelines that integrate multiple stages for more comprehensive protection.\\nA representative inference-time safety framework, ETA (Evaluating Then Aligning Safety of Vision-Language Models at Inference Time) [ 10 ] , adopts a two-stage pipeline combining keyword-based preprocessing and reward-model-guided postprocessing. In the evaluation phase, BLIP similarity scores are computed against a fixed set of risk categories, namely harmful, pornographic, sexual, unsafe, violent, illegal, and privacy. Higher similarity indicates greater risk. The alignment phase employs reward-model scoring and best-of-N sampling to select safer outputs. Although ETA improves safety moderation without retraining, it remains limited by rigid rule-based categorization and high computational cost, reducing its flexibility in handling nuanced or unseen multimodal threats.\\nRecently, agentic methods have demonstrated strong potential across diverse domains, enabling autonomous reasoning, structured tool use, and collaborative multi-agent interactions [ 12 , 13 ] . These systems exhibit adaptability and context awareness, making them promising not only for complex task execution but also forsafety governance.\\nBuilding upon these insights, we propose Agentic Moderation : a model-agnostic framework that rethinks safety alignment as a collaborative, multi-agent process. Our approach coordinates a suite of specialized agents including a SHIELD agent for fine-grained policy checks and action guidance, a Responder Agent for core task execution, an Evaluator Agent for response evaluation, and a Reflection Agent for response refinement and feedback. This multi-agent workflow provides modular, interpretable, and adaptable guardrails that can evolve with emerging risks and changing safety policy contexts, providing robust, multi-layered moderation for vision-language systems.\\nIn summary, this work explores how agentic architectures can serve as moderation layers for multimodal models. Our main contributions are as follows:\\nWe propose an Agentic Moderation Framework that reconceptualizes safety alignment as a collaborative, multi-agent process for adaptive and interpretable moderation.\\nBased on this framework, we design a concrete agentic moderation system for defending LVLMs against cross-modal adversarial attacks, comprising a SHIELD agent for fine-grained safety guidance, an Evaluator agent for assessment, and a Reflection agent for adaptive refinement.\\nThrough extensive experiments across five datasets and four LVLMs, our method reduces Attack Success Rate (ASR) by 7-19% while keeping Non-Following Rate (NF) stable and improving Refusal Rate (RR) by 4-20%, achieving robust and well-balanced moderation performance.',\n",
       "    'subsections': []},\n",
       "   {'title': 'II Literature Review and Background',\n",
       "    'paragraphs': 'Defense mechanisms for vision-language models (LVLMs), whether closed- or open-weight, are typically grouped into four categories: (1) input/output filters, (2) system safety prompts, (3) model-level safety alignment, and (4) output suppression.\\nInput Purification. Adversarial frequently exploit vision modality by hiding harmful content in images or introducing subtle perturbations. Purification-based defenses mitigate these threats by converting images to text, generating auxiliary captions, smoothing pixel-level noise, masking distracting patches, or leveraging embedding comparisons to detect inconsistencies. Methods such as DualEase [ 14 ] , ETA [ 10 ] , SmoothVLM [ 15 ] , PAD [ 16 ] , and BlueSuffix [ 17 ] exemplify this approach, which focuses on detecting visual adversaries and revealing semantic mismatches between modalities.\\nSystem Safety Prompts System safety prompts aim to condition LVLMs through structured instructions that guide safe behavior and mitigate potential policy violations. Adaptive frameworks such as AdaShield [ 18 ] dynamically adjust prompt content based on detected risk or intent detection and often struggle to capture subtle or implicit adversarial objectives.\\nOutput Suppression Inference-time interventions monitor ongoing generation and suppress unsafe content via token filtering, partial response evaluation, or best-of- N N sampling. ETA [ 10 ] and safety re-evaluation frameworks [ 11 ] exemplify this paradigm. While effective in filtering unsafe completions, these post-hoc mechanisms introduce latency and computational overhead, making them less scalable in real-time or resource-constrained settings.\\nModel-Level Alignment. Training-stage safety alignment methods, such as supervised fine-tuning (SFT), Reinforcement Learning with Human Feedback (RLHF) [ 19 ] , and reinforcement learning from AI feedback (RLAIF), strengthen intrinsic model robustness. Notable works including VLGuard [ 8 ] and SPA-VL [ 9 ] enhance cross-modal safety alignment. PPO [ 20 ] and DPO [ 21 ] offer additional refinement, but multimodal preference data remain sparse and expensive to collect or synthesize.\\nRecent moderation systems focus on filtering or blocking unsafe content before or after model inference.\\nTools such as LlamaGuard [ 22 ] , GemmaShield [ 23 ] , and LLaVAGuard [ 24 ] employ classifier-based detection to suppress unsafe text or image content. These modular, plug-and-play systems enable quick updates to safety policies or classifiers, offering broad coverage across categories such as privacy violation, violence, and hate speech. However, most existing moderators yield only binary (safe/unsafe) decisions and lack the capacity to reason about complex multimodal jailbreaks, where adversarial intent may be implicit, compositional, or context-dependent. This gap underscores the need for adaptive, reasoning-based moderators that can interpret and coordinate safety enforcement dynamically across modalities.\\nRecent advances in agentic frameworks have propelled LVLMs toward autonomous reasoning, adaptive workflow orchestration, and collaborative tool use. Early agentic systems centered on single-purpose automation, whereas recent agentic approaches emphasize multi-agent collaboration, persistent memory, and cross-modal interaction [ 12 ] . These architectures enable perception, decision-making, auditing,and reflective reasoning across complex tasks.\\nAlthough agentic approaches have achieved success in domains such as chatbot and coding, their systematic application to content safety and moderation remains largely unexplored. Existing moderators rely on static heuristics or classifiers, lacking the adaptive reasoning and role specialization that agentic systems naturally afford. Our work bridges this gap by introducing Agentic Moderation, a dedicated multi-agent framework that integrates preprocessing and postprocessing within a coordinated agent ecosystem. This design enables dynamic, context-aware, and interpretable defense against cross-modal adversarial attacks, positioning agentic systems as a promising frontier for safety alignment in LVLMs.',\n",
       "    'subsections': [{'title': 'II-A Defense Mechanism',\n",
       "      'paragraphs': 'Defense mechanisms for vision-language models (LVLMs), whether closed- or open-weight, are typically grouped into four categories: (1) input/output filters, (2) system safety prompts, (3) model-level safety alignment, and (4) output suppression.\\nInput Purification. Adversarial frequently exploit vision modality by hiding harmful content in images or introducing subtle perturbations. Purification-based defenses mitigate these threats by converting images to text, generating auxiliary captions, smoothing pixel-level noise, masking distracting patches, or leveraging embedding comparisons to detect inconsistencies. Methods such as DualEase [ 14 ] , ETA [ 10 ] , SmoothVLM [ 15 ] , PAD [ 16 ] , and BlueSuffix [ 17 ] exemplify this approach, which focuses on detecting visual adversaries and revealing semantic mismatches between modalities.\\nSystem Safety Prompts System safety prompts aim to condition LVLMs through structured instructions that guide safe behavior and mitigate potential policy violations. Adaptive frameworks such as AdaShield [ 18 ] dynamically adjust prompt content based on detected risk or intent detection and often struggle to capture subtle or implicit adversarial objectives.\\nOutput Suppression Inference-time interventions monitor ongoing generation and suppress unsafe content via token filtering, partial response evaluation, or best-of- N N sampling. ETA [ 10 ] and safety re-evaluation frameworks [ 11 ] exemplify this paradigm. While effective in filtering unsafe completions, these post-hoc mechanisms introduce latency and computational overhead, making them less scalable in real-time or resource-constrained settings.\\nModel-Level Alignment. Training-stage safety alignment methods, such as supervised fine-tuning (SFT), Reinforcement Learning with Human Feedback (RLHF) [ 19 ] , and reinforcement learning from AI feedback (RLAIF), strengthen intrinsic model robustness. Notable works including VLGuard [ 8 ] and SPA-VL [ 9 ] enhance cross-modal safety alignment. PPO [ 20 ] and DPO [ 21 ] offer additional refinement, but multimodal preference data remain sparse and expensive to collect or synthesize.',\n",
       "      'subsections': []},\n",
       "     {'title': 'II-B Moderators',\n",
       "      'paragraphs': 'Recent moderation systems focus on filtering or blocking unsafe content before or after model inference.\\nTools such as LlamaGuard [ 22 ] , GemmaShield [ 23 ] , and LLaVAGuard [ 24 ] employ classifier-based detection to suppress unsafe text or image content. These modular, plug-and-play systems enable quick updates to safety policies or classifiers, offering broad coverage across categories such as privacy violation, violence, and hate speech. However, most existing moderators yield only binary (safe/unsafe) decisions and lack the capacity to reason about complex multimodal jailbreaks, where adversarial intent may be implicit, compositional, or context-dependent. This gap underscores the need for adaptive, reasoning-based moderators that can interpret and coordinate safety enforcement dynamically across modalities.',\n",
       "      'subsections': []},\n",
       "     {'title': 'II-C Agentic Frameworks',\n",
       "      'paragraphs': 'Recent advances in agentic frameworks have propelled LVLMs toward autonomous reasoning, adaptive workflow orchestration, and collaborative tool use. Early agentic systems centered on single-purpose automation, whereas recent agentic approaches emphasize multi-agent collaboration, persistent memory, and cross-modal interaction [ 12 ] . These architectures enable perception, decision-making, auditing,and reflective reasoning across complex tasks.\\nAlthough agentic approaches have achieved success in domains such as chatbot and coding, their systematic application to content safety and moderation remains largely unexplored. Existing moderators rely on static heuristics or classifiers, lacking the adaptive reasoning and role specialization that agentic systems naturally afford. Our work bridges this gap by introducing Agentic Moderation, a dedicated multi-agent framework that integrates preprocessing and postprocessing within a coordinated agent ecosystem. This design enables dynamic, context-aware, and interpretable defense against cross-modal adversarial attacks, positioning agentic systems as a promising frontier for safety alignment in LVLMs.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'III Methodology',\n",
       "    'paragraphs': 'We introduce the Agentic Moderation Framework, which conceptualizes safety alignment as a collaborative, multi-agent process. Rather than viewing moderation as a static classification or rule-based filtering step, our framework organizes it into an adaptive, policy-grounded workflow involving four specialized agents. Each agent contributes to a coordinated moderation cycle that iteratively refines model behavior toward policy-compliant, high-utility outputs. The overall architecture and process are illustrated in Figure 1 .\\nAs shown in Figure 1 , the framework operates as an iterative loop managed by a central Coordinator. The Coordinator orchestrates four specialized agents‚ÄîShield, Responder, Evaluator, and Reflector‚Äîeach responsible for a distinct moderation function.\\nThe moderation process begins with the Shield Agent, which performs policy-aware classification of multimodal inputs and determines appropriate moderation actions. The Responder Agent then generates candidate outputs under the guidance of moderation cues provided by the Shield. The Evaluator Agent assesses these responses based on safety and utility criteria. If violations are detected, the Reflector Agent analyzes the failure and produces corrective feedback for regeneration. This cycle continues until a safe and policy-compliant response is obtained.\\nAll agents are designed to be lightweight, modular, and extensible. Thresholds, routing logic, and tool configurations can be easily customized, allowing seamless integration of new policies, classifiers, or evaluation metrics without retraining the base model. This design ensures flexibility and scalability across deployment contexts.\\nThe Shield Agent performs the initial safety screening. It classifies multimodal inputs (text and images) into 45 predefined policy categories, each corresponding to a moderation action: block, reframe, or forward [ 25 ] .\\nBlock: Unsafe queries that should be blocked.\\nReframe: Inputs are reformulated with safe alternatives or ethical framing.\\nForward: Inputs are judged safe and passed through for generation.\\nFor reframed or forwarded cases, the Shield augments the input with contextual guidance in the form of ‚Äúshould do‚Äù and ‚Äúshould not do‚Äù instructions. These cues along with action(block, reframe, and forward) are appended to the prompt and passed to the Responder Agent for controlled generation.\\nThe Responder Agent integrates moderation cues directly into the generative process. It uses a configurable Vision-Language Model (VLM) backend to produce candidate outputs conditioned on the Shield‚Äôs guidance. By incorporating explicit ‚Äúshould do,‚Äù ‚Äúshould not do‚Äù prompts, and corresponding actions, the Responder balances safety preservation with task utility. The outputs are subsequently passed to the Evaluator for verification.\\nThe Evaluator Agent validates candidate responses using predefined safety rubrics and utility metrics. We adopt the SeeThreats framework [ 26 ] , implemented via DSPy with the evaluation signature shown in Figure 3 . The Evaluator classifies each response as completion,refusal, and instruction non-following, and quantifies harmfulness on a Likert scale.\\nThe Reflector Agent consolidates information from the Shield‚Äôs policy decisions, and user intent to diagnose the causes of unsafe or low-quality outputs. Using reflection tools and large language models, it generates structured feedback describing the failure context and prescribes corrective guidance, tagged as ‚ÄôIssue‚ÄîFix‚Äô. This reflection feedback is passed back to the Responder Agent, prompting regeneration under refined constraints. In our implementation, a single reflection iteration is sufficient to reach safety convergence in most cases.',\n",
       "    'subsections': [{'title': 'III-A Agentic Moderation Framework',\n",
       "      'paragraphs': 'As shown in Figure 1 , the framework operates as an iterative loop managed by a central Coordinator. The Coordinator orchestrates four specialized agents‚ÄîShield, Responder, Evaluator, and Reflector‚Äîeach responsible for a distinct moderation function.\\nThe moderation process begins with the Shield Agent, which performs policy-aware classification of multimodal inputs and determines appropriate moderation actions. The Responder Agent then generates candidate outputs under the guidance of moderation cues provided by the Shield. The Evaluator Agent assesses these responses based on safety and utility criteria. If violations are detected, the Reflector Agent analyzes the failure and produces corrective feedback for regeneration. This cycle continues until a safe and policy-compliant response is obtained.\\nAll agents are designed to be lightweight, modular, and extensible. Thresholds, routing logic, and tool configurations can be easily customized, allowing seamless integration of new policies, classifiers, or evaluation metrics without retraining the base model. This design ensures flexibility and scalability across deployment contexts.',\n",
       "      'subsections': []},\n",
       "     {'title': 'III-B Agent Modules',\n",
       "      'paragraphs': 'The Shield Agent performs the initial safety screening. It classifies multimodal inputs (text and images) into 45 predefined policy categories, each corresponding to a moderation action: block, reframe, or forward [ 25 ] .\\nBlock: Unsafe queries that should be blocked.\\nReframe: Inputs are reformulated with safe alternatives or ethical framing.\\nForward: Inputs are judged safe and passed through for generation.\\nFor reframed or forwarded cases, the Shield augments the input with contextual guidance in the form of ‚Äúshould do‚Äù and ‚Äúshould not do‚Äù instructions. These cues along with action(block, reframe, and forward) are appended to the prompt and passed to the Responder Agent for controlled generation.\\nThe Responder Agent integrates moderation cues directly into the generative process. It uses a configurable Vision-Language Model (VLM) backend to produce candidate outputs conditioned on the Shield‚Äôs guidance. By incorporating explicit ‚Äúshould do,‚Äù ‚Äúshould not do‚Äù prompts, and corresponding actions, the Responder balances safety preservation with task utility. The outputs are subsequently passed to the Evaluator for verification.\\nThe Evaluator Agent validates candidate responses using predefined safety rubrics and utility metrics. We adopt the SeeThreats framework [ 26 ] , implemented via DSPy with the evaluation signature shown in Figure 3 . The Evaluator classifies each response as completion,refusal, and instruction non-following, and quantifies harmfulness on a Likert scale.\\nThe Reflector Agent consolidates information from the Shield‚Äôs policy decisions, and user intent to diagnose the causes of unsafe or low-quality outputs. Using reflection tools and large language models, it generates structured feedback describing the failure context and prescribes corrective guidance, tagged as ‚ÄôIssue‚ÄîFix‚Äô. This reflection feedback is passed back to the Responder Agent, prompting regeneration under refined constraints. In our implementation, a single reflection iteration is sufficient to reach safety convergence in most cases.',\n",
       "      'subsections': [{'title': 'III-B1 Shield Agent',\n",
       "        'paragraphs': 'The Shield Agent performs the initial safety screening. It classifies multimodal inputs (text and images) into 45 predefined policy categories, each corresponding to a moderation action: block, reframe, or forward [ 25 ] .\\nBlock: Unsafe queries that should be blocked.\\nReframe: Inputs are reformulated with safe alternatives or ethical framing.\\nForward: Inputs are judged safe and passed through for generation.\\nFor reframed or forwarded cases, the Shield augments the input with contextual guidance in the form of ‚Äúshould do‚Äù and ‚Äúshould not do‚Äù instructions. These cues along with action(block, reframe, and forward) are appended to the prompt and passed to the Responder Agent for controlled generation.',\n",
       "        'subsections': []},\n",
       "       {'title': 'III-B2 Responder Agent',\n",
       "        'paragraphs': 'The Responder Agent integrates moderation cues directly into the generative process. It uses a configurable Vision-Language Model (VLM) backend to produce candidate outputs conditioned on the Shield‚Äôs guidance. By incorporating explicit ‚Äúshould do,‚Äù ‚Äúshould not do‚Äù prompts, and corresponding actions, the Responder balances safety preservation with task utility. The outputs are subsequently passed to the Evaluator for verification.',\n",
       "        'subsections': []},\n",
       "       {'title': 'III-B3 Evaluator Agent',\n",
       "        'paragraphs': 'The Evaluator Agent validates candidate responses using predefined safety rubrics and utility metrics. We adopt the SeeThreats framework [ 26 ] , implemented via DSPy with the evaluation signature shown in Figure 3 . The Evaluator classifies each response as completion,refusal, and instruction non-following, and quantifies harmfulness on a Likert scale.',\n",
       "        'subsections': []},\n",
       "       {'title': 'III-B4 Reflector Agent',\n",
       "        'paragraphs': 'The Reflector Agent consolidates information from the Shield‚Äôs policy decisions, and user intent to diagnose the causes of unsafe or low-quality outputs. Using reflection tools and large language models, it generates structured feedback describing the failure context and prescribes corrective guidance, tagged as ‚ÄôIssue‚ÄîFix‚Äô. This reflection feedback is passed back to the Responder Agent, prompting regeneration under refined constraints. In our implementation, a single reflection iteration is sufficient to reach safety convergence in most cases.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': 'IV Experiments Setup and Results',\n",
       "    'paragraphs': 'Datasets. We evaluate the proposed moderation framework across a diverse set of cross-modality adversarial safety datasets. Following [ 26 ] , who summarize cross-modality adversarial attacks against LVLMs, we select one representative datasets: AdvBench [ 27 ] , FigStep [ 1 ] , Flow-JD [ 28 ] , MMSafety [ 4 ] , and SIUO [ 5 ] . To reduce computational overhead, we randomly sample 100 instances from each dataset, excluding professional advice and policy-lobbying cases, as modern LVLMs generally handle them safely rather than refusing outright.\\nModels. We evaluate four representative LVLMs: LLaVA 1.5, LLaVA 1.6, Qwen2.5-8B, and LLaMA 3.2 Vision-11B. LLaVA models lack explicit post-training safety alignment, whereas Qwen and LLaMA models incorporate varying degrees of alignment.\\nEvaluation Metrics. Following Ren et al. [ 26 ] , we adopt four complementary metrics: refusal rate , non-following rate , jailbreak rate , and jailbreak quality scores .\\nRefusal rate : measures the proportion of harmful requests for which the model explicitly declines to answer, either through a direct refusal or offering a safe, deflective alternative.\\nNon-following rate : captures cases where the model fail to follow instructions yet does not provide clear refusal(e.g., irrelevant or nonsensical responses, or inappropriate safety disclaimers such as ‚ÄôI cannnot help identify the person in the image‚Äô when the image contains no person)\\nJailbreak rate : quantifies the fraction of harmful requests that successfully elicit a harmful completion, indicating the model‚Äôs susceptibility to adversarial prompts.\\nThreats score is a scalar rating (0‚Äì5) that measures the overall safety, relevance, and usefulness of the model‚Äôs response. A score of 5 represents a fully safe and high-quality answer that follows the instruction, while 0 indicates a refusal or unsafe output. For Completion responses, the evaluator assigns a score based on instruction adherence and safety; for Refusal or Non-Following, the score defaults to 0.\\nExperiment Configuration. All agents are implemented as LangChain Tools, ensuring interoperability with other agentic frameworks.\\nFor prompt orchestration and chain-of-thought automation, we employ DSPy, which enables declarative prompt composition and dynamic reasoning control.\\nAll experiments are conducted on RunPod L40S GPUs.\\nWe evaluate four configurations: Baseline , Shield only , Reflector only , and Shield followed by Reflector , to isolate the contribution of each safety component.',\n",
       "    'subsections': [{'title': 'IV-A Experiments Setup',\n",
       "      'paragraphs': 'Datasets. We evaluate the proposed moderation framework across a diverse set of cross-modality adversarial safety datasets. Following [ 26 ] , who summarize cross-modality adversarial attacks against LVLMs, we select one representative datasets: AdvBench [ 27 ] , FigStep [ 1 ] , Flow-JD [ 28 ] , MMSafety [ 4 ] , and SIUO [ 5 ] . To reduce computational overhead, we randomly sample 100 instances from each dataset, excluding professional advice and policy-lobbying cases, as modern LVLMs generally handle them safely rather than refusing outright.\\nModels. We evaluate four representative LVLMs: LLaVA 1.5, LLaVA 1.6, Qwen2.5-8B, and LLaMA 3.2 Vision-11B. LLaVA models lack explicit post-training safety alignment, whereas Qwen and LLaMA models incorporate varying degrees of alignment.\\nEvaluation Metrics. Following Ren et al. [ 26 ] , we adopt four complementary metrics: refusal rate , non-following rate , jailbreak rate , and jailbreak quality scores .\\nRefusal rate : measures the proportion of harmful requests for which the model explicitly declines to answer, either through a direct refusal or offering a safe, deflective alternative.\\nNon-following rate : captures cases where the model fail to follow instructions yet does not provide clear refusal(e.g., irrelevant or nonsensical responses, or inappropriate safety disclaimers such as ‚ÄôI cannnot help identify the person in the image‚Äô when the image contains no person)\\nJailbreak rate : quantifies the fraction of harmful requests that successfully elicit a harmful completion, indicating the model‚Äôs susceptibility to adversarial prompts.\\nThreats score is a scalar rating (0‚Äì5) that measures the overall safety, relevance, and usefulness of the model‚Äôs response. A score of 5 represents a fully safe and high-quality answer that follows the instruction, while 0 indicates a refusal or unsafe output. For Completion responses, the evaluator assigns a score based on instruction adherence and safety; for Refusal or Non-Following, the score defaults to 0.\\nExperiment Configuration. All agents are implemented as LangChain Tools, ensuring interoperability with other agentic frameworks.\\nFor prompt orchestration and chain-of-thought automation, we employ DSPy, which enables declarative prompt composition and dynamic reasoning control.\\nAll experiments are conducted on RunPod L40S GPUs.\\nWe evaluate four configurations: Baseline , Shield only , Reflector only , and Shield followed by Reflector , to isolate the contribution of each safety component.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'V Results and Analysis',\n",
       "    'paragraphs': 'We evaluate multiple moderation configurations to measure the effectiveness of each module. Under adversarial attack settings, our objective is to ensure models produce safe and coherent responses rather than maintaining ‚Äúsafety‚Äù through excessive refusal or meaningless outputs. In particular, we aim to avoid situations where a model appears safe simply by refusing everything or producing nonsensical text.\\nTo this end, we focus on two main metrics: Attack Success Rate (ASR), which measures the proportion of harmful outputs, and Non-Following Rate (NF), which captures cases where the model fails to follow or meaningfully complete the task. Ideally, an effective moderator minimizes both ASR and NF. As shown in Figure 5 , points closer to the lower-left corner represent configurations with both low ASR and NF, indicating an optimal balance between safety and utility. Our goal is to move model behavior toward this region.\\nLLaMA\\nLLaVA-1.5\\nLLaVA-1.6\\nQwen2.5-7B\\nTable I (also visualized in Figure 5 ) presents model-level outcomes under different moderation configurations. Overall, the combination of the Shield and Reflection Agents achieves the most consistent and substantial improvements, notably reducing completion and instruction non-following rates across the LLaVA variants. For LLaMA, the combined setup increases the refusal rate by 17%, primarily through a decrease in non-following responses. Qwen2.5-7B similarly shows reduced non-following and improved refusal behavior.\\nBoth the Shield and Reflection Agents improve safety performance, with the Shield generally providing stronger and more stable gains. The Reflection Agent, while slightly less effective, still delivers measurable improvements, demonstrating its practical value as a lightweight enhancement. In practice, the choice between them can depend on latency and computational constraints‚Äîusing the Shield alone offers robust protection with minimal overhead, whereas combining both yields the best overall safety‚Äìutility trade-off.\\nFigure 6 and Table II present dataset-level results across five representative attack types: AdvBench, FigStep, Flowchart, MMSafety, and SIUO. All evaluated Vision‚ÄìLanguage Models benefit from the proposed moderation strategies, though to varying extents. However, some models achieve comparable performance using only one module. For instance, Reflection alone nearly matches the combined configuration on FigStep and Flowchart, where attacks rely on semantic misdirection rather than explicit content. In contrast, Shield alone performs well on MMSafety and SIUO, where threats are visually grounded and easier to detect pre-inference.\\nNotably, Flowchart and SIUO attacks exhibit relatively low ASR and high RR across all configurations, as models tend to generate descriptive rather than harmful content. In Flowchart, the task typically instructs the model to describe and elaborate on flow diagrams, using much of the token budget for neutral explanation and leaving limited capacity for generation. Meanwhile, SIUO contains implicit, culturally nuanced cases (e.g., ‚ÄúShould I give my mother yarn for her birthday?‚Äù) where seemingly benign questions encode subtle social implications. Such ambiguity challenges models‚Äô cultural and contextual reasoning but rarely triggers overtly unsafe responses.\\nOverall, these findings demonstrate that both Shield and Reflection are independently effective and can be flexibly deployed based on dataset characteristics and application scenarios: Shield suits real-time or low-latency moderation, while Reflection benefits post-hoc auditing and interpretability-sensitive contexts. Together, they form a robust, adaptive defense framework for diverse cross-modality adversarial settings.\\nThe integration of safety layers introduces moderate latency to the inference process. As shown in Table III , the Shield module adds only a negligible preprocessing overhead, typically around 0.015 seconds per query, as it operates through lightweight intent classification and policy lookup. Reflection, which performs post-hoc self-evaluation and potential regeneration, contributes approximately 1.5 seconds on average, with a maximum of two iterations allowed. If the first reflection deems the output safe, the process converges early without regeneration. Overall, the total inference time‚Äîcomprising Shield, Reflection, and model generation‚Äîremains efficient, indicating that the added safety mechanisms achieve meaningful protection with minimal latency cost.\\nIn order to evaluate utility preservation under different safety configurations, we conduct experiments on MMBench (See Table IV ). The results show that models with higher capability generally exhibit stronger utility retention when safety mechanisms are applied. Advanced models such as LLaVA-1.6 and Qwe-2.5-7B maintain performance close to their vanilla setup, indicating that their safety layers impose minimal degradation.\\nTo illustrate how each agent contributes to safety, Table V presents a concrete example from FigStep. Without any safety layer, the model directly generates detailed descriptions of fraudulent tools, showing strong instruction-following but lacking safety awareness.\\nWhen Shield is applied, it classifies the query as related to fraud, scams, and deception, following predefined ‚Äúshould do‚Äù and ‚Äúshould not do‚Äù policies. In this case, Shield‚Äôs safety policy specifies that the model should educate users about scam awareness or explain phishing mechanisms in theory, but should not generate real phishing emails or scam scripts. Based on this guidance, Shield provides actionable moderation feedback, which is redirecting the unsafe request toward educational or awareness-building content. With this structured intervention, the model fulfills the user‚Äôs request responsibly while embedding appropriate safety warnings.\\nWhen Reflector is integrated, the system performs an additional self-review after Shield‚Äôs intervention. Reflector analyzes the user input, Shield‚Äôs guidance, and the model‚Äôs initial output to identify safety violations or unsafe phrasing. In this case, it flags that the response describes tools for creating fake credit cards and payment gateways‚Äîcontent that could enable fraudulent activities and therefore violates safety rules. Reflector then provides targeted feedback, suggesting that the response be reframed to focus on legitimate testing practices, such as using mock data generators or sandbox environments. Guided by this reflection, the model regenerates a revised answer that situates the task in a safe research context, maintaining informativeness while fully eliminating safety risks.\\nTogether, these agents form a layered safety mechanism: Shield acts as a frontline filter for harmful intent, while Reflector functions as a post-hoc auditor ensuring consistent, interpretable, and policy-aligned outputs. This synergy exemplifies how agentic moderation enables adaptive, context-sensitive safety alignment while preserving response utility.\\nAgentic moderation provides a highly flexible framework for creating safety guardrails that can be customized according to specific application contexts, regional policies, cultural or religious norms, and age-related considerations. Its modular architecture allows practitioners to select and compose different agents such as the Shield, Evaluator, Reflection, Steering, and Retrieval-Augmented Generation (RAG) modules, depending on the requirements of each deployment scenario. For example, latency-sensitive systems may prioritize lightweight filtering through the Shield Agent, while high-stakes or user-facing applications can include additional evaluative and reflective components to provide stronger safety assurance. This configurability enables agentic moderation to adapt effectively across diverse operational settings and to support both proactive defense and post-hoc correction in an extensible and policy-aware manner.\\nDespite its flexibility, the framework introduces a practical trade-off between safety robustness and inference efficiency. Incorporating multiple agents enhances moderation reliability but also increases computational cost and latency, which may constrain real-time or large-scale applications. Moreover, determining the optimal combination of modules and safety thresholds remains context-dependent and nontrivial, requiring careful calibration for each deployment. Future work will explore adaptive agent scheduling and cost-aware coordination strategies to balance safety guarantees with system responsiveness.',\n",
       "    'subsections': [{'title': 'V-A Model-Level Results',\n",
       "      'paragraphs': 'Table I (also visualized in Figure 5 ) presents model-level outcomes under different moderation configurations. Overall, the combination of the Shield and Reflection Agents achieves the most consistent and substantial improvements, notably reducing completion and instruction non-following rates across the LLaVA variants. For LLaMA, the combined setup increases the refusal rate by 17%, primarily through a decrease in non-following responses. Qwen2.5-7B similarly shows reduced non-following and improved refusal behavior.\\nBoth the Shield and Reflection Agents improve safety performance, with the Shield generally providing stronger and more stable gains. The Reflection Agent, while slightly less effective, still delivers measurable improvements, demonstrating its practical value as a lightweight enhancement. In practice, the choice between them can depend on latency and computational constraints‚Äîusing the Shield alone offers robust protection with minimal overhead, whereas combining both yields the best overall safety‚Äìutility trade-off.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-B Cross Dataset Results',\n",
       "      'paragraphs': 'Figure 6 and Table II present dataset-level results across five representative attack types: AdvBench, FigStep, Flowchart, MMSafety, and SIUO. All evaluated Vision‚ÄìLanguage Models benefit from the proposed moderation strategies, though to varying extents. However, some models achieve comparable performance using only one module. For instance, Reflection alone nearly matches the combined configuration on FigStep and Flowchart, where attacks rely on semantic misdirection rather than explicit content. In contrast, Shield alone performs well on MMSafety and SIUO, where threats are visually grounded and easier to detect pre-inference.\\nNotably, Flowchart and SIUO attacks exhibit relatively low ASR and high RR across all configurations, as models tend to generate descriptive rather than harmful content. In Flowchart, the task typically instructs the model to describe and elaborate on flow diagrams, using much of the token budget for neutral explanation and leaving limited capacity for generation. Meanwhile, SIUO contains implicit, culturally nuanced cases (e.g., ‚ÄúShould I give my mother yarn for her birthday?‚Äù) where seemingly benign questions encode subtle social implications. Such ambiguity challenges models‚Äô cultural and contextual reasoning but rarely triggers overtly unsafe responses.\\nOverall, these findings demonstrate that both Shield and Reflection are independently effective and can be flexibly deployed based on dataset characteristics and application scenarios: Shield suits real-time or low-latency moderation, while Reflection benefits post-hoc auditing and interpretability-sensitive contexts. Together, they form a robust, adaptive defense framework for diverse cross-modality adversarial settings.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-C Runtime and Latency Analysis',\n",
       "      'paragraphs': 'The integration of safety layers introduces moderate latency to the inference process. As shown in Table III , the Shield module adds only a negligible preprocessing overhead, typically around 0.015 seconds per query, as it operates through lightweight intent classification and policy lookup. Reflection, which performs post-hoc self-evaluation and potential regeneration, contributes approximately 1.5 seconds on average, with a maximum of two iterations allowed. If the first reflection deems the output safe, the process converges early without regeneration. Overall, the total inference time‚Äîcomprising Shield, Reflection, and model generation‚Äîremains efficient, indicating that the added safety mechanisms achieve meaningful protection with minimal latency cost.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-D Utility Evaluation',\n",
       "      'paragraphs': 'In order to evaluate utility preservation under different safety configurations, we conduct experiments on MMBench (See Table IV ). The results show that models with higher capability generally exhibit stronger utility retention when safety mechanisms are applied. Advanced models such as LLaVA-1.6 and Qwe-2.5-7B maintain performance close to their vanilla setup, indicating that their safety layers impose minimal degradation.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-E Case Study',\n",
       "      'paragraphs': 'To illustrate how each agent contributes to safety, Table V presents a concrete example from FigStep. Without any safety layer, the model directly generates detailed descriptions of fraudulent tools, showing strong instruction-following but lacking safety awareness.\\nWhen Shield is applied, it classifies the query as related to fraud, scams, and deception, following predefined ‚Äúshould do‚Äù and ‚Äúshould not do‚Äù policies. In this case, Shield‚Äôs safety policy specifies that the model should educate users about scam awareness or explain phishing mechanisms in theory, but should not generate real phishing emails or scam scripts. Based on this guidance, Shield provides actionable moderation feedback, which is redirecting the unsafe request toward educational or awareness-building content. With this structured intervention, the model fulfills the user‚Äôs request responsibly while embedding appropriate safety warnings.\\nWhen Reflector is integrated, the system performs an additional self-review after Shield‚Äôs intervention. Reflector analyzes the user input, Shield‚Äôs guidance, and the model‚Äôs initial output to identify safety violations or unsafe phrasing. In this case, it flags that the response describes tools for creating fake credit cards and payment gateways‚Äîcontent that could enable fraudulent activities and therefore violates safety rules. Reflector then provides targeted feedback, suggesting that the response be reframed to focus on legitimate testing practices, such as using mock data generators or sandbox environments. Guided by this reflection, the model regenerates a revised answer that situates the task in a safe research context, maintaining informativeness while fully eliminating safety risks.\\nTogether, these agents form a layered safety mechanism: Shield acts as a frontline filter for harmful intent, while Reflector functions as a post-hoc auditor ensuring consistent, interpretable, and policy-aligned outputs. This synergy exemplifies how agentic moderation enables adaptive, context-sensitive safety alignment while preserving response utility.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-F Discussion',\n",
       "      'paragraphs': 'Agentic moderation provides a highly flexible framework for creating safety guardrails that can be customized according to specific application contexts, regional policies, cultural or religious norms, and age-related considerations. Its modular architecture allows practitioners to select and compose different agents such as the Shield, Evaluator, Reflection, Steering, and Retrieval-Augmented Generation (RAG) modules, depending on the requirements of each deployment scenario. For example, latency-sensitive systems may prioritize lightweight filtering through the Shield Agent, while high-stakes or user-facing applications can include additional evaluative and reflective components to provide stronger safety assurance. This configurability enables agentic moderation to adapt effectively across diverse operational settings and to support both proactive defense and post-hoc correction in an extensible and policy-aware manner.',\n",
       "      'subsections': []},\n",
       "     {'title': 'V-G Limitations',\n",
       "      'paragraphs': 'Despite its flexibility, the framework introduces a practical trade-off between safety robustness and inference efficiency. Incorporating multiple agents enhances moderation reliability but also increases computational cost and latency, which may constrain real-time or large-scale applications. Moreover, determining the optimal combination of modules and safety thresholds remains context-dependent and nontrivial, requiring careful calibration for each deployment. Future work will explore adaptive agent scheduling and cost-aware coordination strategies to balance safety guarantees with system responsiveness.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'VI Conclusion',\n",
       "    'paragraphs': 'This work presents Agentic Moderation , a flexible, model-agnostic framework for improving the safety of large vision-language models (LVLMs) under multimodal adversarial attacks. The framework views safety alignment as a collaborative process among specialized agents‚Äî Shield , Evaluator , and Reflector ‚Äîthat jointly enforce policy compliance through iterative reasoning and feedback. By combining proactive filtering, evaluation, and reflective revision, it enables dynamic, context-aware moderation across textual and visual modalities. Experiments across multiple attack types show that agentic collaboration markedly reduces harmful outputs while maintaining instruction adherence. Agentic Moderation provides a general and extensible foundation for safety, supporting modular defense strategies that adapt to diverse application scenarios, policy requirements, and latency considerations. Future work will focus on adaptive agent coordination.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning, collaboration, and adaptive control, enabling systems to coordinate and independently solve complex tasks. We extend this paradigm to safety alignment by introducing Agentic Moderation, a model-agnostic framework that leverages specialized agents to defend multimodal systems against jailbreak attacks. Unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications(safe or unsafe), our method integrates dynamic, cooperative agents,including Shield, Responder, Evaluator, and Reflector,to achieve context-aware and interpretable moderation. Extensive experiments across five datasets and four representative large vision-language models (LVLMs) demonstrate that our approach reduces the Attack Success Rate (ASR) by 7‚Äì19%, maintains a stable Non-Following Rate (NF), and improves the Refusal Rate (RR) by 4‚Äì20%, achieving robust, interpretable, and well-balanced safety performance. By harnessing the flexibility and reasoning capacity of agentic architectures, Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the broader potential of agentic systems as a foundation for automated safety governance.'},\n",
       " '2510.25724v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Human cognition exhibits a remarkable ability to form and retain associations between related experiences, a process underpinned by Spike-Timing Dependent Plasticity (STDP) and the Hebbian principle of ‚Äúneurons that fire together, wire together‚Äù Hebb ( 1949 ); Caporale & Dan ( 2008 ); Bi & Poo ( 1998 ) . Through repeated co-activation, synaptic pathways strengthen over time, enabling associative memory, the dynamic capacity to recall concepts through partial cues rather than exact matches. This biological principle has long inspired computational models of learning and retrieval, from Hopfield networks Hopfield ( 1982 ) to modern energy-based and graph-structured memory and LLM architectures Bartunov et\\xa0al. ( 2020 ); Kosowski et\\xa0al. ( 2025 ) .\\nIn this work, we present BambooKG: Biologically-inspired Associative Memory Based On Overlaps KG . BambooKG is a neurobiologically-motivated framework for long-term memory retention in knowledge graphs. It introduces a frequency-weighted associative mechanism, where the repeated co-occurrence of non-triplet node pairs - derived from shared semantic or contextual ‚Äúchunks‚Äù - incrementally strengthens the edge weights that connect them.\\nUnlike static embedding spaces, BambooKG evolves as a function of activation history. Each tagging event triggered by the addition of new information contributes to the edge frequency distribution, effectively encoding temporal salience and contextual relevance. This enables the system to ‚Äúassociate‚Äù and connect new information with existing knowledge, thereby enabling effective retrieval during recall.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Existing Models',\n",
       "    'paragraphs': 'RAG is a standard document retrieval and reasoning approach that involves simple cosine similarity search over the embedded documents. It is widely applied in domains such as healthcare and enterprise QA. Yang et\\xa0al. ( 2025 ) . Despite its strengths, RAG struggles with multi-hop reasoning because it treats retrieved chunks independently Tang & Yang ( 2024 ) . Newer pipelines, such as Chain-of-RAG, achieve state-of-the-art results on the KILT benchmark, showing over 10-point EM score improvements for multi-hop QA Wang et\\xa0al. ( 2025a ) . However, stepwise retrieval introduces higher computational overhead and longer inference times. Its performance can degrade if intermediate retrieval steps accumulate errors or irrelevant passages Wang et\\xa0al. ( 2025b ) . Multi-agent optimisation strategies also improve QA F1 scores by jointly training retrieval, filtering, and generation modules. Nevertheless, it significantly increases training complexity, requires careful reward design, and can be unstable Chen et\\xa0al. ( 2025 ) .\\nOpenIE systems extract structured triples (subject‚Äìrelation‚Äìobject) directly from text, allowing flexible knowledge graph construction without predefined schemas Etzioni et\\xa0al. ( 2015 ) . These models underpin many downstream reasoning frameworks, including GraphRAG. OpenIE captures fine-grained relations across sentences, but suffers from low precision in noisy or domain-specific corpora. Early benchmarks showed competitive recall but lower F1 compared to supervised relation extractors (e.g., F1 scores \\xa050‚Äì60% in heterogeneous corpora) Etzioni et\\xa0al. ( 2011 ) , limiting its standalone utility.\\nGraphRAG is an attempt at a combination between RAG and OpenIE. It builds knowledge graphs from retrieved passages, allowing entity disambiguation, relational reasoning, and multi-hop synthesis Edge et\\xa0al. ( 2024 ) . Recent work integrating causal graphs has shown up to 10% absolute accuracy improvements in medical QA tasks compared to standard GraphRAG Luo et\\xa0al. ( 2025 ) . However, performance gains depend on graph construction quality; noisy relation extraction or sparse knowledge domains reduce effectiveness. GraphRAG introduces higher computational overhead, but benchmarks suggest it outperforms standard RAG in complex multi-hop QA where relational inference is required. Han et\\xa0al. ( 2025 )\\nAnother knowledge graph model that we compare BambooKG against is KGGen. The fundamental idea lies in using multiple LLM calls to create a knowledge graph. The process consists of several stages: extraction of entities, extraction of relations between them, aggregation and clusterisation. Each step involves one or several LLM calls. The benefit of this approach is the increased connectivity between articles Mo et\\xa0al. ( 2025 ) .',\n",
       "    'subsections': [{'title': '2.1 RAG',\n",
       "      'paragraphs': 'RAG is a standard document retrieval and reasoning approach that involves simple cosine similarity search over the embedded documents. It is widely applied in domains such as healthcare and enterprise QA. Yang et\\xa0al. ( 2025 ) . Despite its strengths, RAG struggles with multi-hop reasoning because it treats retrieved chunks independently Tang & Yang ( 2024 ) . Newer pipelines, such as Chain-of-RAG, achieve state-of-the-art results on the KILT benchmark, showing over 10-point EM score improvements for multi-hop QA Wang et\\xa0al. ( 2025a ) . However, stepwise retrieval introduces higher computational overhead and longer inference times. Its performance can degrade if intermediate retrieval steps accumulate errors or irrelevant passages Wang et\\xa0al. ( 2025b ) . Multi-agent optimisation strategies also improve QA F1 scores by jointly training retrieval, filtering, and generation modules. Nevertheless, it significantly increases training complexity, requires careful reward design, and can be unstable Chen et\\xa0al. ( 2025 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 OpenIE',\n",
       "      'paragraphs': 'OpenIE systems extract structured triples (subject‚Äìrelation‚Äìobject) directly from text, allowing flexible knowledge graph construction without predefined schemas Etzioni et\\xa0al. ( 2015 ) . These models underpin many downstream reasoning frameworks, including GraphRAG. OpenIE captures fine-grained relations across sentences, but suffers from low precision in noisy or domain-specific corpora. Early benchmarks showed competitive recall but lower F1 compared to supervised relation extractors (e.g., F1 scores \\xa050‚Äì60% in heterogeneous corpora) Etzioni et\\xa0al. ( 2011 ) , limiting its standalone utility.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3 GraphRAG',\n",
       "      'paragraphs': 'GraphRAG is an attempt at a combination between RAG and OpenIE. It builds knowledge graphs from retrieved passages, allowing entity disambiguation, relational reasoning, and multi-hop synthesis Edge et\\xa0al. ( 2024 ) . Recent work integrating causal graphs has shown up to 10% absolute accuracy improvements in medical QA tasks compared to standard GraphRAG Luo et\\xa0al. ( 2025 ) . However, performance gains depend on graph construction quality; noisy relation extraction or sparse knowledge domains reduce effectiveness. GraphRAG introduces higher computational overhead, but benchmarks suggest it outperforms standard RAG in complex multi-hop QA where relational inference is required. Han et\\xa0al. ( 2025 )',\n",
       "      'subsections': []},\n",
       "     {'title': '2.4 KGGen',\n",
       "      'paragraphs': 'Another knowledge graph model that we compare BambooKG against is KGGen. The fundamental idea lies in using multiple LLM calls to create a knowledge graph. The process consists of several stages: extraction of entities, extraction of relations between them, aggregation and clusterisation. Each step involves one or several LLM calls. The benefit of this approach is the increased connectivity between articles Mo et\\xa0al. ( 2025 ) .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 BambooKG: Architecture and Pipelines',\n",
       "    'paragraphs': 'The core concept of our approach lies in a multi-stage pipeline that constructs the knowledge graph. The overall pipeline is shown in Figure 1 . This pipeline consists of three primary stages: Chunking, Tag Generation, and Knowledge Graph Creation.\\nIn the first stage, the input document is segmented into semantically coherent text blocks, chunks, following the standard text segmentation methods Pak & Teh ( 2018 ) . Each chunk is set at a fixed value that can range between 200‚Äì1200 tokens, depending on the overall document length (Figure 1 a). For instance, a document may be divided into multiple chunks, each capturing a coherent fragment of meaning (e.g., sentences or short paragraphs).\\nEach chunk is then passed to the Tagger, which is implemented as a controlled LLM call with a restricted prompt designed to extract key semantic entities or concepts, referred to as tags. The Tagger outputs a fixed-length list of tags per chunk, representing the most salient or contextually important terms identified by the model (Figure 1 b).\\nThe overall quality and interpretability of the downstream knowledge graph depend critically on what the Tagger determines to be ‚Äúimportant.‚Äù In this study, the model was intentionally left general to evaluate baseline performance; however, domain-specific prompting can be used to bias tag selection toward certain concepts and suppress less relevant ones, thereby controlling the signal-to-noise ratio of the resulting graph (see Section 6 ).\\nUnlike conventional triplet-based knowledge graph extraction methods Etzioni et\\xa0al. ( 2015 ) , which rely on rigid subject-predicate-object relationships, our tag-based approach is structurally flexible. This flexibility offers two key advantages:\\nIt eliminates the constraint of directional, grammar-bound triplets, enabling the capture of co-occurring concepts that may not conform to syntactic relations.\\nIt facilitates the introduction of constrained tag vocabularies (domain-specific or general) in future iterations, providing a tunable framework for graph refinement and denoising.\\nAfter tag generation, a BambooKG subgraph is constructed for each chunk and incrementally merged into the global BambooKG. Each tag is represented as a node, while edges denote co-occurrence relationships between tags within the same chunk (Figure 1 c)\\nEdge weights correspond to the frequency of co-occurrence - that is, the number of chunks in which a given tag pair appears together. This frequency-weighted graph structure reflects an undirected associative network, where stronger weights indicate tighter semantic coupling between tags.\\nConceptually, this process parallels Spike-Timing Dependent Plasticity (STDP) in neuroscience, where connections between neurons that ‚Äúfire together‚Äù are strengthened, forming the basis of associative memory Caporale & Dan ( 2008 ) . Similarly, in BambooKG, each update to edge weights signifies that the system has ‚Äúmemorised‚Äù new knowledge.\\nAdditionally, a second knowledge graph is generated to map the tags to their originating chunks and documents for final context retrieval in the query pipeline.\\nIn this work, we focus on establishing the baseline BambooKG without performing any explicit clustering, pruning, or noise reduction of tags. These post-processing and optimisation steps remain open directions for future exploration.\\nDuring memory recall, the retrieval process operates over the existing BambooKG structure generated during memorisation. The process begins when a user submits a query, which is first passed through the Tagger. Unlike the memorisation phase, the Tagger‚Äôs vocabulary is now restricted to the global tag vocabulary of BambooKG (that is, only the tags which are contained within BambooKG), ensuring that only known concepts are recognised. If the Tagger fails to identify any valid tags, it is assumed that BambooKG has not yet ‚Äúlearned‚Äù the requested concept.\\nThe Tagger extracts the set of query tags from the query (e.g., in ‚ÄúWhat pet eats fish?‚Äù, the tags pet and fish are identified from the existing vocabulary). These tags serve as starting points of search within the tag frequency-weight knowledge graph.\\nFor each query tag, BambooKG extracts a local subgraph to approximate the region of semantic relevance. Following a decay-based neighbourhood exploration, the top X first-degree neighbours (directly connected tags) and top Y second-degree neighbours (tags connected through an intermediary) are selected, ranked by their edge weights - i.e., the frequency of co-occurrence. An example of this process in shown in Figure 2 with X and Y being set at 2.\\nThis selective neighbourhood expansion approximates a form of associative recall, where strongly connected concepts are prioritised over weak or peripheral associations. For clarity of visualisation, only the top-ranked connections are displayed in the diagram, though the full graph traversal may include additional edges.\\nOnce the subgraph is defined, BambooKG identifies all document chunks that contributed to the formation of the retrieved edges (Figure 3 a). These chunks represent the episodic context associated with the concepts recalled, analogous to the reactivation of cortical traces by the hippocampus during biological memory recall Wiltgen et\\xa0al. ( 2010 ) . In this sense, BambooKG functions as a synthetic hippocampal index, reactivating distributed memory fragments that co-occurred with the query tags during learning.\\nThe retrieved chunks, aggregated across all relevant documents, form the final context. This context is then provided to the LLM, which synthesises an answer grounded in the recalled knowledge (Figure 3 b).\\nImportantly, BambooKG supports partial pattern matching, allowing recall to succeed even when the full combination of tags has not been observed before. If a query partially overlaps with known subgraphs (e.g., pet is known but fish is new), the system can still infer context from related neighbours (e.g., cat, dog, indoor) and construct an approximate answer. This mechanism parallels pattern completion observed in hippocampal memory systems, enabling the model to generalise from incomplete cues Grande et\\xa0al. ( 2019 ) .',\n",
       "    'subsections': [{'title': '3.1 Memorisation Pipeline',\n",
       "      'paragraphs': 'The core concept of our approach lies in a multi-stage pipeline that constructs the knowledge graph. The overall pipeline is shown in Figure 1 . This pipeline consists of three primary stages: Chunking, Tag Generation, and Knowledge Graph Creation.\\nIn the first stage, the input document is segmented into semantically coherent text blocks, chunks, following the standard text segmentation methods Pak & Teh ( 2018 ) . Each chunk is set at a fixed value that can range between 200‚Äì1200 tokens, depending on the overall document length (Figure 1 a). For instance, a document may be divided into multiple chunks, each capturing a coherent fragment of meaning (e.g., sentences or short paragraphs).\\nEach chunk is then passed to the Tagger, which is implemented as a controlled LLM call with a restricted prompt designed to extract key semantic entities or concepts, referred to as tags. The Tagger outputs a fixed-length list of tags per chunk, representing the most salient or contextually important terms identified by the model (Figure 1 b).\\nThe overall quality and interpretability of the downstream knowledge graph depend critically on what the Tagger determines to be ‚Äúimportant.‚Äù In this study, the model was intentionally left general to evaluate baseline performance; however, domain-specific prompting can be used to bias tag selection toward certain concepts and suppress less relevant ones, thereby controlling the signal-to-noise ratio of the resulting graph (see Section 6 ).\\nUnlike conventional triplet-based knowledge graph extraction methods Etzioni et\\xa0al. ( 2015 ) , which rely on rigid subject-predicate-object relationships, our tag-based approach is structurally flexible. This flexibility offers two key advantages:\\nIt eliminates the constraint of directional, grammar-bound triplets, enabling the capture of co-occurring concepts that may not conform to syntactic relations.\\nIt facilitates the introduction of constrained tag vocabularies (domain-specific or general) in future iterations, providing a tunable framework for graph refinement and denoising.\\nAfter tag generation, a BambooKG subgraph is constructed for each chunk and incrementally merged into the global BambooKG. Each tag is represented as a node, while edges denote co-occurrence relationships between tags within the same chunk (Figure 1 c)\\nEdge weights correspond to the frequency of co-occurrence - that is, the number of chunks in which a given tag pair appears together. This frequency-weighted graph structure reflects an undirected associative network, where stronger weights indicate tighter semantic coupling between tags.\\nConceptually, this process parallels Spike-Timing Dependent Plasticity (STDP) in neuroscience, where connections between neurons that ‚Äúfire together‚Äù are strengthened, forming the basis of associative memory Caporale & Dan ( 2008 ) . Similarly, in BambooKG, each update to edge weights signifies that the system has ‚Äúmemorised‚Äù new knowledge.\\nAdditionally, a second knowledge graph is generated to map the tags to their originating chunks and documents for final context retrieval in the query pipeline.\\nIn this work, we focus on establishing the baseline BambooKG without performing any explicit clustering, pruning, or noise reduction of tags. These post-processing and optimisation steps remain open directions for future exploration.',\n",
       "      'subsections': [{'title': '3.1.1 Chunking Stage',\n",
       "        'paragraphs': 'In the first stage, the input document is segmented into semantically coherent text blocks, chunks, following the standard text segmentation methods Pak & Teh ( 2018 ) . Each chunk is set at a fixed value that can range between 200‚Äì1200 tokens, depending on the overall document length (Figure 1 a). For instance, a document may be divided into multiple chunks, each capturing a coherent fragment of meaning (e.g., sentences or short paragraphs).',\n",
       "        'subsections': []},\n",
       "       {'title': '3.1.2 Tag Generation Stage',\n",
       "        'paragraphs': 'Each chunk is then passed to the Tagger, which is implemented as a controlled LLM call with a restricted prompt designed to extract key semantic entities or concepts, referred to as tags. The Tagger outputs a fixed-length list of tags per chunk, representing the most salient or contextually important terms identified by the model (Figure 1 b).\\nThe overall quality and interpretability of the downstream knowledge graph depend critically on what the Tagger determines to be ‚Äúimportant.‚Äù In this study, the model was intentionally left general to evaluate baseline performance; however, domain-specific prompting can be used to bias tag selection toward certain concepts and suppress less relevant ones, thereby controlling the signal-to-noise ratio of the resulting graph (see Section 6 ).\\nUnlike conventional triplet-based knowledge graph extraction methods Etzioni et\\xa0al. ( 2015 ) , which rely on rigid subject-predicate-object relationships, our tag-based approach is structurally flexible. This flexibility offers two key advantages:\\nIt eliminates the constraint of directional, grammar-bound triplets, enabling the capture of co-occurring concepts that may not conform to syntactic relations.\\nIt facilitates the introduction of constrained tag vocabularies (domain-specific or general) in future iterations, providing a tunable framework for graph refinement and denoising.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.1.3 Knowledge Graph Creation Stage',\n",
       "        'paragraphs': 'After tag generation, a BambooKG subgraph is constructed for each chunk and incrementally merged into the global BambooKG. Each tag is represented as a node, while edges denote co-occurrence relationships between tags within the same chunk (Figure 1 c)\\nEdge weights correspond to the frequency of co-occurrence - that is, the number of chunks in which a given tag pair appears together. This frequency-weighted graph structure reflects an undirected associative network, where stronger weights indicate tighter semantic coupling between tags.\\nConceptually, this process parallels Spike-Timing Dependent Plasticity (STDP) in neuroscience, where connections between neurons that ‚Äúfire together‚Äù are strengthened, forming the basis of associative memory Caporale & Dan ( 2008 ) . Similarly, in BambooKG, each update to edge weights signifies that the system has ‚Äúmemorised‚Äù new knowledge.\\nAdditionally, a second knowledge graph is generated to map the tags to their originating chunks and documents for final context retrieval in the query pipeline.\\nIn this work, we focus on establishing the baseline BambooKG without performing any explicit clustering, pruning, or noise reduction of tags. These post-processing and optimisation steps remain open directions for future exploration.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.2 Recall Pipeline',\n",
       "      'paragraphs': 'During memory recall, the retrieval process operates over the existing BambooKG structure generated during memorisation. The process begins when a user submits a query, which is first passed through the Tagger. Unlike the memorisation phase, the Tagger‚Äôs vocabulary is now restricted to the global tag vocabulary of BambooKG (that is, only the tags which are contained within BambooKG), ensuring that only known concepts are recognised. If the Tagger fails to identify any valid tags, it is assumed that BambooKG has not yet ‚Äúlearned‚Äù the requested concept.\\nThe Tagger extracts the set of query tags from the query (e.g., in ‚ÄúWhat pet eats fish?‚Äù, the tags pet and fish are identified from the existing vocabulary). These tags serve as starting points of search within the tag frequency-weight knowledge graph.\\nFor each query tag, BambooKG extracts a local subgraph to approximate the region of semantic relevance. Following a decay-based neighbourhood exploration, the top X first-degree neighbours (directly connected tags) and top Y second-degree neighbours (tags connected through an intermediary) are selected, ranked by their edge weights - i.e., the frequency of co-occurrence. An example of this process in shown in Figure 2 with X and Y being set at 2.\\nThis selective neighbourhood expansion approximates a form of associative recall, where strongly connected concepts are prioritised over weak or peripheral associations. For clarity of visualisation, only the top-ranked connections are displayed in the diagram, though the full graph traversal may include additional edges.\\nOnce the subgraph is defined, BambooKG identifies all document chunks that contributed to the formation of the retrieved edges (Figure 3 a). These chunks represent the episodic context associated with the concepts recalled, analogous to the reactivation of cortical traces by the hippocampus during biological memory recall Wiltgen et\\xa0al. ( 2010 ) . In this sense, BambooKG functions as a synthetic hippocampal index, reactivating distributed memory fragments that co-occurred with the query tags during learning.\\nThe retrieved chunks, aggregated across all relevant documents, form the final context. This context is then provided to the LLM, which synthesises an answer grounded in the recalled knowledge (Figure 3 b).\\nImportantly, BambooKG supports partial pattern matching, allowing recall to succeed even when the full combination of tags has not been observed before. If a query partially overlaps with known subgraphs (e.g., pet is known but fish is new), the system can still infer context from related neighbours (e.g., cat, dog, indoor) and construct an approximate answer. This mechanism parallels pattern completion observed in hippocampal memory systems, enabling the model to generalise from incomplete cues Grande et\\xa0al. ( 2019 ) .',\n",
       "      'subsections': [{'title': '3.2.1 Query Tag Extraction',\n",
       "        'paragraphs': 'The Tagger extracts the set of query tags from the query (e.g., in ‚ÄúWhat pet eats fish?‚Äù, the tags pet and fish are identified from the existing vocabulary). These tags serve as starting points of search within the tag frequency-weight knowledge graph.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.2.2 Subgraph Retrieval',\n",
       "        'paragraphs': 'For each query tag, BambooKG extracts a local subgraph to approximate the region of semantic relevance. Following a decay-based neighbourhood exploration, the top X first-degree neighbours (directly connected tags) and top Y second-degree neighbours (tags connected through an intermediary) are selected, ranked by their edge weights - i.e., the frequency of co-occurrence. An example of this process in shown in Figure 2 with X and Y being set at 2.\\nThis selective neighbourhood expansion approximates a form of associative recall, where strongly connected concepts are prioritised over weak or peripheral associations. For clarity of visualisation, only the top-ranked connections are displayed in the diagram, though the full graph traversal may include additional edges.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.2.3 Context Construction',\n",
       "        'paragraphs': 'Once the subgraph is defined, BambooKG identifies all document chunks that contributed to the formation of the retrieved edges (Figure 3 a). These chunks represent the episodic context associated with the concepts recalled, analogous to the reactivation of cortical traces by the hippocampus during biological memory recall Wiltgen et\\xa0al. ( 2010 ) . In this sense, BambooKG functions as a synthetic hippocampal index, reactivating distributed memory fragments that co-occurred with the query tags during learning.\\nThe retrieved chunks, aggregated across all relevant documents, form the final context. This context is then provided to the LLM, which synthesises an answer grounded in the recalled knowledge (Figure 3 b).\\nImportantly, BambooKG supports partial pattern matching, allowing recall to succeed even when the full combination of tags has not been observed before. If a query partially overlaps with known subgraphs (e.g., pet is known but fish is new), the system can still infer context from related neighbours (e.g., cat, dog, indoor) and construct an approximate answer. This mechanism parallels pattern completion observed in hippocampal memory systems, enabling the model to generalise from incomplete cues Grande et\\xa0al. ( 2019 ) .',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '4 Experimental Setup',\n",
       "    'paragraphs': 'The ability of an LLM to effectively answer a given question depends on the ability of the recall pipeline to accurately retrieve the relevant information.\\nIn this study, we used accuracy as a primary measure of knowledge graph performance. We compared BambooKG with three other knowledge graph methods: OpenIE, GraphRAG and KGGen Luo et\\xa0al. ( 2025 ); Etzioni et\\xa0al. ( 2015 ); Mo et\\xa0al. ( 2025 ) , as well as standard RAG serving as a baseline benchmark. As the other knowledge graph approaches are designed for embedding-based search algorithms, instead of using weighted edges to select top-k chunks, we used top-k embeddings as in ordinary RAG. The top-k for BambooKG, OpenIE and KGGen was set at 5-3 (5 first-degree neighbours and 3 second-degree neighbours), whilst for RAG the top-k was chosen to be 5. GraphRAG did not have an ability to choose the top-k chunks to be retrieved Edge et\\xa0al. ( 2024 ) .\\nWe chose two datasets to evaluate the performance of BambooKG: HotPotQA Yang et\\xa0al. ( 2018 ) for general knowledge recall and MuSiQue, which demands multi-hop knowledge retention and navigation and is considered to be one of the most challenging datasets of this kind Trivedi et\\xa0al. ( 2022 ) .\\nFor the purposes of cost limitation, we randomly selected 100 questions (both correct and distractor) from the HotPotQA dataset and 100 questions for each of the 2, 3 and 4 hops from the MuSiQue dataset.\\nTo evaluate accuracy, we sent the recalled subgraph to GPT-4o to generate an answer, and then used the same model as LLM-as-a-Judge Gu et\\xa0al. ( 2025 ) to evaluate whether the predicted answer matches the expected answer. It is important to note that since GPT-4o is a non-deterministic model, the results will vary slightly on each run Song et\\xa0al. ( 2024 ) .',\n",
       "    'subsections': []},\n",
       "   {'title': '5 Results',\n",
       "    'paragraphs': 'We used 100 randomly selected questions from HotPotQA and MuSiQue datasets to benchmark BambooKG. Additionally, in each dataset, we compared BambooKG against simple RAG, OpenIE, GraphRAG and KGGen. The full results are presented in Tables 1 and 2 .\\nUsing these benchmarks, we demonstrate that BambooKG outperforms other knowledge graph methods in two critical parameters: data recall and average retrieval time, in both simple questions (HotPotQA) and multi-hop questions (MuSiQue).\\nWe believe that there are several reasons for this performance. To start with minor issues, OpenIE often generates incoherent or nonsensical triplets (‚Äúif‚Äù as a valid node). GraphRAG generates a small number of nodes per article, resulting in information being missed. KGGen, in turn, performs well on simple questions (HotpotQA) but struggles with multi-hop questions due to poorly performing clusters.\\nCuriously, GraphRAG showed inferior performance in HotPotQA in spite of its relatively good stance as shown in other research Xu et\\xa0al. ( 2025 ) . We believe the reason for this is twofold: GraphRAG KG tends to miss answer node entities Han et\\xa0al. ( 2025 ) ; and that the corpus contained distractor documents. We have used both the correct documents (supporting facts) and distractor documents taken from the context field to generate the corpus for the 100 randomly selected questions. This most likely led to incorrect community generation, which hindered the performance of GraphRAG.\\nIn addition to the weighted edges, two seemingly counterintuitive factors contribute to the satisfactory performance of BambooKG: lack of triplets and the usage of arbitrary nodes. At the expense of the increased graph size and the loss of rigid structure, this leads to the decreased information loss whilst preserving epistemic connectivity between separate documents. Additionally, the difficulty of forming adequate single- or few-word embeddings when applying RAG to knowledge graph triplets means that embedding-based search approaches would naturally experience information loss, as well as increased extraction time Amur et\\xa0al. ( 2023 ); Edge et\\xa0al. ( 2024 ) .\\nAnother point of note is that BambooKG pipelines only use a single LLM call: one during the Memorisation Pipeline, to trigger the Tagger. The Recall Pipeline proceeds entirely without LLMs or embeddings, resulting in highly competitive extraction speeds.\\nOne downside of BambooKG is the increased context size when compared to other approaches. However, this is beyond the scope of this work, as the context window is entirely dependent on the LLM one uses and is wholly unrelated to the long-term memory approaches.',\n",
       "    'subsections': []},\n",
       "   {'title': '6 Future Work',\n",
       "    'paragraphs': 'The fundamental limitation of the BambooKG performance lies in the way that Tagger works. For the experiments run for this paper, the Tagger chooses arbitrary, generic tags, effectively summarising the text chunk into a set of discrete words. By focusing the Tagger‚Äôs attention on a specific domain (via fine-tuning or prompt engineering), we can achieve greater data retention and recall rates on specialised corpora. Another point of future research is the organic formation of communities and clusters (with or without LLM calls), which will become critical for graph navigation when the volume of information added into the knowledge graph becomes significantly large. Additionally, the selection and extraction of a subgraph during the recall stage demands further refinement to bring the context size down and accelerate the final, LLM-based decision-making.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Retrieval‚ÄëAugmented Generation (RAG) allows LLMs to access external knowledge, reducing hallucinations and ageing‚Äëdata issues. However, it treats retrieved chunks independently and struggles with multi‚Äëhop or relational reasoning, especially across documents Tang & Yang (2024). Knowledge Graphs (KG) enhance this by capturing the relationships between entities using triplets, enabling structured, multi‚Äëchunk reasoning; however, these tend to miss information that fails to conform to the triplet structure Peng (2024). We introduce BambooKG, a knowledge graph with frequency‚Äëbased weights on non-triplet edges which reflect link strength, drawing on the Hebbian principle of ‚Äúfire together, wire together‚Äù Hebb (1949). This decreases information loss and results in improved performance on single- and multi-hop reasoning, outperforming the existing solutions.'},\n",
       " '2510.25612v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'An LLM-based AI agent is a computer program that independently gathers information from its surroundings, analyzes it, makes informed decisions, and performs actions to accomplish defined goals John et\\xa0al. ( 2025 ) .\\nSuch AI agents are used in various domains and tasks, including productivity in the workplace √êula et\\xa0al. ( 2024 ) , and coding assistants Pinto et\\xa0al. ( 2024 ) .\\nTo tackle complex and multi-disciplinary tasks, several agents can be assembled into an Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, to work collaboratively towards a shared goal Yang et\\xa0al. ( 2024 ) .\\nThe market of such AI agents and AAWs is projected to experience an eightfold increase in adoption over the next six years 1 1 1 marketsandmarkets.com/Market-Reports/ai-agents-market-15761548.html .\\nThis growing interest, particularly in AAWs, raises the need for a deeper understanding of the AAW‚Äôs operation and the relations between its agents.\\nMoreover, due to the high autonomy of the AAW, it is important from both security and quality perspectives to understand the influence level of each agent on the AAW‚Äôs final output.\\nThis deeper understanding will be highly beneficial when applying additional quality and trust-supported downstream tasks on the AAW.\\nThese may include observability and monitoring Dong et\\xa0al. ( 2024 ) , as well as failure detection mechanisms Su et\\xa0al. ( 2024 ) .\\nAn example of such a solution is AI guardrails for harmful content - LLM-level safety mechanisms that enforce the LLM to comply with ethical guidelines Kumar et\\xa0al. ( 2023 ) .\\nApplying such guardrails on every LLM call performed by any agent within the AAW can introduce significant latency, potentially tripling the system‚Äôs inference time.\\nTheoretically, identifying the agents that have a greater influence on the AAW‚Äôs output will provide interpretability and understanding of the AAW.\\nSelectively applying guardrails to those agents will substantially reduce latency while still preventing the generation of harmful content.\\nThere are currently no methods for assessing each agent‚Äôs influence on the AAW‚Äôs final output.\\nAdopting solutions from related fields of study, such as graph theory Saxena and Iyengar ( 2020 ) , communication network security Mell ( 2021 ); Imran et\\xa0al. ( 2013 ) , and reinforcement learning Chen et\\xa0al. ( 2024 ) , to be used on AAW is not feasible since AAWs are dynamic by nature and those solutions perform a static analysis.\\nThese solutions:\\n1) would analyse only the AAW architecture and not the AAW internal operation (e.g., agent‚Äôs capabilities, final output, agents‚Äô outputs, etc.);\\n2) do not support a flexible AAW that changes the activated agents according to the input query;\\nand 3) are not timely feasible for use at AAW inference.\\nWe present CAIR - the first method for assessing the influence level of agents on the AAW‚Äôs output and determining which agents are the most influential.\\nAs illustrated in Figure 1 , CAIR has two phases - offline and online.\\nCAIR performs a deep task- and architecture-agnostic offline analysis of the AAW and leverages its insights to perform a fast and efficient online assessment, predicting the agents‚Äô expected influence rankings.\\nIn its offline analysis, CAIR leverages concepts from classical machine learning feature importance assessments.\\nIt begins by using a limited set of representative queries to trigger AAW activations.\\nThen it iterates over all activated agents and injects counterfactual agents‚Äô outputs.\\nFinally, it assesses the impact of the counterfactual change on the final AAW output.\\nAt inference, CAIR selects the agents‚Äô rankings based on the offline analysis results.\\nWe evaluate CAIR using AAW-Zoo - a dataset of our creation, containing 30 AAW use cases from three common architectures, comprising a total of 230 distinct functionalities.\\nAAW-Zoo was created by our AAW-Zoo-Generator - a dedicated LLM-chain that creates AAWs for an input use case description.\\nOur evaluation showed that CAIR :\\n1) substantially outperforms baselines adopted from other fields;\\n2) is correlated with the ground truth, both in offline and online settings;\\nand 3) produces consistent rankings for similar queries.\\nMoreover, we showed that using CAIR as part of a downstream task reduces the latency by \\xa027%, with a minimal performance effect.\\nThis research‚Äôs contributions are as follows:\\nThe first one-of-a-kind agent influence ranker for agentic AI workflows.\\nCAIR is task- and architecture-agnostic and can be used in both offline and online analysis, with negligible added latency to the workflow inference time.\\nThe first to enable downstream tasks designed for LLMs to be applied to multi-agent AAWs efficiently during inference.\\nA pioneering analysis of AAWs that provides interpretability and understanding of AAWs.\\nCAIR code 2 2 2 https://github.com/FujitsuResearch/CAIR.git , the AAW-Zoo dataset and the AAW-Zoo-Generator 3 3 3 https://github.com/FujitsuResearch/Auto_Agent_Creation.git used to create the dataset can be used by the research community to further investigate AAWs and their nature in a variety of quality and security aspects.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Works',\n",
       "    'paragraphs': 'The high autonomy of AAWs causes them to be unpredictable and uninterpretable Watson et\\xa0al. ( 2024 ) .\\nCurrently, there are no existing methods for interpreting the final output of an agentic workflow (AAW) concerning the individual agents involved, nor for assessing their respective influence on the outcome.\\nHowever, solutions can be adopted from related fields.\\nViewing the AAW as a connectivity graph by considering each agent as a node and each connection between agents as an edge enables the use of measures from the graph theory field.\\nIn the graph theory field, the importance of each node is computed by centrality measures Saxena and Iyengar ( 2020 ) .\\nThe mostly used measures are Betweenness centrality Saxena and Iyengar ( 2020 ) , which measures how often a node appears on the shortest paths between other nodes in a network, and Eigenvector centrality Saxena and Iyengar ( 2020 ) , which quantifies a node‚Äôs influence based on the centrality of its neighbors.\\nThe main limitation of using such measures is that the agents‚Äô scores will consider only the AAW structural properties and not internal functionality, e.g., the agent‚Äôs internal process and produced output.\\nAnother limitation is the inability to assess importance in flexible workflows where agent order is not predetermined.\\nAnother approach could be viewing the AAW as a network topology by considering each agent as a network node and each connection between agents as a communication line, and adopting solutions from the network engineering field.\\nIn this field, the importance of a node can be calculated using the LASCNN Algorithm Imran et\\xa0al. ( 2013 ) .\\nUnder this approach, a node is considered critical if its failure would partition the network into disconnected segments, disrupting communication among the remaining nodes.\\nThis approach inherits the limitations of graph theory measurements and introduces an additional one.\\nAn agent is considered important only if its failure prevents the AAW from producing any output, yet an agent can be highly important and fail, and the AAW will still produce an output.\\nAn alternative is to use human annotations when suitable methods or ground truth are unavailable Mell ( 2021 ) .\\nHowever, human annotation is costly, biased, and infeasible at inference time in the AAW, as it requires a human-in-the-loop.\\nAnother related field is reinforcement learning, where existing work focuses on multi-agent systems in the context of physical-space agents, rather than LLM-based agents.\\nExisting works add an RL agent trained to cause failures in other agents and measure the effect on the complete system at inference time Chen et\\xa0al. ( 2024 ) .\\nAlthough this effect relates to the flexibility of the multi-agent system, it has two major limitations: (1) it is highly dependent on the reward metric, making it unsuitable for AAW; and (2) it is not feasible to perform such extensive analysis during AAW inference.\\nThese emphasize that existing methods from related fields cannot be fully applied to AAWs, which are more complex and require a deeper analysis.',\n",
       "    'subsections': []},\n",
       "   {'title': '3 The Method',\n",
       "    'paragraphs': 'Given an AAW, an input query (prompt), and a set of representative queries, CAIR ranks each agent by its influence on the AAW‚Äôs final output.\\nCAIR operates in two phases (illustrated in Figure 1 ) - the offline and the online phases, where the offline phase results are the online phase‚Äôs initial starting point.\\nIn the offline phase (as illustrated in Figure 2 ), CAIR uses a set of queries that represent the different functionalities (i.e., representative queries) and analyzes the AAW exhibited behavior, resulting in a set of agent rankings for each representative query.\\nIn the online phase, CAIR receives a new input query and uses the offline phase results to deduce the relevant agents‚Äô rankings.\\nThe representative queries set is provided by the user and is expected to contain every possible functionality of the AAWs.\\nIn Appendix D , we provide a prompt that, when coupled with a system overview, can be used to extract a representative query set.\\nAn alternative is to group historical queries, cluster them, and take one from each group.\\nFurthermore, in the case of adding a functionality to the AAW, it can be added to the set, and the offline analysis would be performed only on it.\\nThe notation used is as follows:\\nLet W = { a i } i = 1 n W=\\\\{a_{i}\\\\}_{i=1}^{n} be an AAW containing n n LLM-based agents a i a_{i} .\\nLet q q be an input query and W f \\u200b ( q ) W_{f}(q) be the final output of W W given q q as input.\\nLet W a \\u200b ( q ) = { a j } a j ‚àà W J W_{a}(q)=\\\\{a_{j}\\\\}_{a_{j}\\\\in W}^{J} be the series of the activated agents (i.e., activation flow) for input query q q .\\nLet a i i \\u200b n k a_{i}^{in_{k}} denote the input received by a i a_{i} during the k k -th time it was activated in W a \\u200b ( q ) W_{a}(q) and let a i o \\u200b u \\u200b t k a_{i}^{out_{k}} denote the corresponding output produced by a i a_{i} during that activation.\\nLet r \\u200b q l ‚àà R \\u200b Q rq_{l}\\\\in RQ be a representative query in the set R \\u200b Q = { r \\u200b q l } l = 1 L RQ=\\\\{rq_{l}\\\\}_{l=1}^{L}\\nThe offline analysis of CAIR is inspired by methods for feature importance assessment in the classic ML domain.\\nFor a given input, these methods assess the influence of each feature value on the model‚Äôs output.\\nCAIR projects the AAW components into the feature importance assessment task.\\nThe AAW can be seen as the ML model, i.e., the final output of the AAW corresponds to the ML model output.\\nIn this analogy, the agents in the AAW represent the features, and their inputs and outputs correspond to the features‚Äô values.\\nSpecifically, the offline analysis of CAIR is inspired by the LIME feature importance technique Ribeiro et\\xa0al. ( 2016 ) .\\nLIME perturbs the input sample, measures the change of the final output of the model, and analyzes which features cause this change.\\nSimilarly, for every representative query, CAIR \"perturbs\" the internal AAW behavior (workflow variations recording), measures the change to the final AAW output (change measures calculation), and calculates the agent‚Äôs importance accordingly (final score calculation).\\nWe now describe each step of the offline phase as performed for every representative query.\\nFirst, the representative query r \\u200b q l rq_{l} is used to obtain the original activation flow W a \\u200b ( r \\u200b q l ) W_{a}(rq_{l}) , including a i i \\u200b n k a_{i}^{in_{k}} and a i o \\u200b u \\u200b t k a_{i}^{out_{k}} for every activated agent.\\nThen, CAIR systematically changes a i o \\u200b u \\u200b t k a_{i}^{out_{k}} of each agent by using an LLM (See Appendix D )\\nto be as far as possible from the original, yet still a valid output to the given input.\\nThe effect on the activation flow is recorded with the corresponding agent whose output was changed.\\nFor a single r \\u200b q l rq_{l} , the results of this step, { W a j \\u200b ( r \\u200b q l ) } j = 1 J \\\\{W_{a}^{j}(rq_{l})\\\\}_{j=1}^{J} , are the perturbed versions for each of the J J elements in the W a \\u200b ( r \\u200b q l ) W_{a}(rq_{l}) series.\\nIn this step, CAIR measures the effects of the systematic change on the rest of the activation flow.\\nThese effects can be seen as the change in the AAW‚Äôs final output W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) or in the agents‚Äô presence and order in W a j \\u200b ( r \\u200b q l ) W_{a}^{j}(rq_{l}) .\\nTo measure the change in W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) , CAIR converts the original final output W f \\u200b ( r \\u200b q l ) W_{f}(rq_{l}) and the perturbed output W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) to embedding vectors W f \\u200b ( r \\u200b q l ) v W_{f}(rq_{l})^{v} and W f l \\u200b ( r \\u200b q l ) v W_{f}^{l}(rq_{l})^{v} respectively, using a well-known projection technique (SBERT Reimers and Gurevych ( 2019 ) ) and measures their cosine distance.\\nThis is presented in Equation 1 :\\nwhere F \\u200b O \\u200b C j , l FOC_{j,l} denotes the change in the final output when modifying element a j a_{j} using r \\u200b q l rq_{l} as input.\\nThe F \\u200b O \\u200b C FOC measure reflects the perturbation effect on the AAW final output; however, it ignores the perturbation intensity and its effect on the computed score.\\nTo measure the perturbation intensity, a calculation similar to Equation 1 is performed, which measures the change in a j o \\u200b u \\u200b t k a_{j}^{out_{k}} :\\nwhere A \\u200b O \\u200b C j AOC_{j} denotes the change in the output of agent a j a_{j} , and a j o \\u200b u \\u200b t k \\u200b v a_{j}^{out_{k}}v and a j o \\u200b u \\u200b t k \\u200b v ^ \\\\widehat{a_{j}^{out_{k}}v} are the vector representations of the agents‚Äô original and perturbed outputs, respectively.\\nTo remove the perturbation effect from F \\u200b O \\u200b C FOC , A \\u200b O \\u200b C AOC is multiplied by an amplification factor ( A \\u200b F AF ) and subtracted from F \\u200b O \\u200b C FOC :\\nThe purpose of the amplification factor is to take into account the distance between a i o \\u200b u \\u200b t k ^ \\\\widehat{a_{i}^{out_{k}}} and W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) from the O \\u200b C OC score and eliminate its effect; more activations between a i o \\u200b u \\u200b t k ^ \\\\widehat{a_{i}^{out_{k}}} and W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) mean that the perturbation performed on a i o \\u200b u \\u200b t k a_{i}^{out_{k}} will affect a larger part of the workflow.\\nThe level of change performed on a i o \\u200b u \\u200b t k a_{i}^{out_{k}} is reflected by A \\u200b O \\u200b C AOC , but the effect on the other agents is not considered.\\nThe amplification factor A \\u200b F AF is set by the number of agents remaining to be activated divided by the total number of activated agents.\\nFor example, if | W a \\u200b ( r \\u200b q l ) | = 4 |W_{a}(rq_{l})|=4 and the output of the second activated agent is perturbed, then A \\u200b F = 3 4 AF=\\\\frac{3}{4} since three agents are still not activated (the current and the last two).\\nBy that, A \\u200b F AF normalizes A \\u200b O \\u200b C AOC to be proportional to the effect of the perturbation on the rest of the AAW.\\nThe perturbation to the agent output can also affect the activation flow itself.\\nDue to the mid-workflow perturbation, the agents in the AAW can change their decision on who will be the next designated agent.\\nSuch a major change means that the perturbation has a profound impact on the AAW operation.\\nCAIR measures this type of change by calculating the edit distance between the original and changed flow:\\nwhere W \\u200b C j , l WC_{j,l} denotes the workflow change score when modifying element a j a_{j} using r \\u200b q l rq_{l} as input.\\nBy that, any change to the activation flow (added/removed agents, change in order) is considered.\\nFor each r \\u200b q l rq_{l} , the influence score of each agent is calculated by a weighted sum of O \\u200b C OC and W \\u200b C WC ( Œ± \\u200b O \\u200b C + Œ≤ \\u200b W \\u200b C \\\\alpha OC+\\\\beta WC ). Œ± \\\\alpha and Œ≤ \\\\beta are set according to the AAW nature - when agents have more autonomy to change the activation sequence, Œ≤ \\\\beta should increase, and vice versa.\\nCAIR produces a single influence score for every agent.\\nIn activation flows where an agent is called more than once, CAIR takes the maximal score.\\nThis is because if an agent has a strong influence at any point in the activation flow, that influence should not be overlooked (i.e., taking the minimum) or reduced (i.e., taking the average).\\nAt the end of the offline phase, CAIR maintains a list of representative queries with the following information:\\n1) a vector representation of the representative query;\\nand 2) the influence scores and rankings of the agents according to their influence.\\nThis list is used for every new inference query to obtain the most relevant agents‚Äô rankings.\\nAt inference time, a new query q n \\u200b e \\u200b w q_{new} is given to the AAW.\\nBefore inputting it into the AAW, CAIR is applied - CAIR converts q n \\u200b e \\u200b w q_{new} into its vectorized representation q n \\u200b e \\u200b w v q_{new}^{v} and retrieves the most similar representative query from the list of representative queries.\\nThe agents‚Äô rankings for q n \\u200b e \\u200b w q_{new} are set as the rankings of the selected representative query.\\nThe added computations‚Äîembedding the input and computing cosine similarities with the representative queries‚Äîare negligible compared to a single LLM call, and especially so when compared to a full sequence of LLM-based agent activations.\\nBy that, CAIR provides an effective influence ranking prediction suited for inference time.',\n",
       "    'subsections': [{'title': '3.1 Offline Phase',\n",
       "      'paragraphs': 'The offline analysis of CAIR is inspired by methods for feature importance assessment in the classic ML domain.\\nFor a given input, these methods assess the influence of each feature value on the model‚Äôs output.\\nCAIR projects the AAW components into the feature importance assessment task.\\nThe AAW can be seen as the ML model, i.e., the final output of the AAW corresponds to the ML model output.\\nIn this analogy, the agents in the AAW represent the features, and their inputs and outputs correspond to the features‚Äô values.\\nSpecifically, the offline analysis of CAIR is inspired by the LIME feature importance technique Ribeiro et\\xa0al. ( 2016 ) .\\nLIME perturbs the input sample, measures the change of the final output of the model, and analyzes which features cause this change.\\nSimilarly, for every representative query, CAIR \"perturbs\" the internal AAW behavior (workflow variations recording), measures the change to the final AAW output (change measures calculation), and calculates the agent‚Äôs importance accordingly (final score calculation).\\nWe now describe each step of the offline phase as performed for every representative query.\\nFirst, the representative query r \\u200b q l rq_{l} is used to obtain the original activation flow W a \\u200b ( r \\u200b q l ) W_{a}(rq_{l}) , including a i i \\u200b n k a_{i}^{in_{k}} and a i o \\u200b u \\u200b t k a_{i}^{out_{k}} for every activated agent.\\nThen, CAIR systematically changes a i o \\u200b u \\u200b t k a_{i}^{out_{k}} of each agent by using an LLM (See Appendix D )\\nto be as far as possible from the original, yet still a valid output to the given input.\\nThe effect on the activation flow is recorded with the corresponding agent whose output was changed.\\nFor a single r \\u200b q l rq_{l} , the results of this step, { W a j \\u200b ( r \\u200b q l ) } j = 1 J \\\\{W_{a}^{j}(rq_{l})\\\\}_{j=1}^{J} , are the perturbed versions for each of the J J elements in the W a \\u200b ( r \\u200b q l ) W_{a}(rq_{l}) series.\\nIn this step, CAIR measures the effects of the systematic change on the rest of the activation flow.\\nThese effects can be seen as the change in the AAW‚Äôs final output W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) or in the agents‚Äô presence and order in W a j \\u200b ( r \\u200b q l ) W_{a}^{j}(rq_{l}) .\\nTo measure the change in W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) , CAIR converts the original final output W f \\u200b ( r \\u200b q l ) W_{f}(rq_{l}) and the perturbed output W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) to embedding vectors W f \\u200b ( r \\u200b q l ) v W_{f}(rq_{l})^{v} and W f l \\u200b ( r \\u200b q l ) v W_{f}^{l}(rq_{l})^{v} respectively, using a well-known projection technique (SBERT Reimers and Gurevych ( 2019 ) ) and measures their cosine distance.\\nThis is presented in Equation 1 :\\nwhere F \\u200b O \\u200b C j , l FOC_{j,l} denotes the change in the final output when modifying element a j a_{j} using r \\u200b q l rq_{l} as input.\\nThe F \\u200b O \\u200b C FOC measure reflects the perturbation effect on the AAW final output; however, it ignores the perturbation intensity and its effect on the computed score.\\nTo measure the perturbation intensity, a calculation similar to Equation 1 is performed, which measures the change in a j o \\u200b u \\u200b t k a_{j}^{out_{k}} :\\nwhere A \\u200b O \\u200b C j AOC_{j} denotes the change in the output of agent a j a_{j} , and a j o \\u200b u \\u200b t k \\u200b v a_{j}^{out_{k}}v and a j o \\u200b u \\u200b t k \\u200b v ^ \\\\widehat{a_{j}^{out_{k}}v} are the vector representations of the agents‚Äô original and perturbed outputs, respectively.\\nTo remove the perturbation effect from F \\u200b O \\u200b C FOC , A \\u200b O \\u200b C AOC is multiplied by an amplification factor ( A \\u200b F AF ) and subtracted from F \\u200b O \\u200b C FOC :\\nThe purpose of the amplification factor is to take into account the distance between a i o \\u200b u \\u200b t k ^ \\\\widehat{a_{i}^{out_{k}}} and W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) from the O \\u200b C OC score and eliminate its effect; more activations between a i o \\u200b u \\u200b t k ^ \\\\widehat{a_{i}^{out_{k}}} and W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) mean that the perturbation performed on a i o \\u200b u \\u200b t k a_{i}^{out_{k}} will affect a larger part of the workflow.\\nThe level of change performed on a i o \\u200b u \\u200b t k a_{i}^{out_{k}} is reflected by A \\u200b O \\u200b C AOC , but the effect on the other agents is not considered.\\nThe amplification factor A \\u200b F AF is set by the number of agents remaining to be activated divided by the total number of activated agents.\\nFor example, if | W a \\u200b ( r \\u200b q l ) | = 4 |W_{a}(rq_{l})|=4 and the output of the second activated agent is perturbed, then A \\u200b F = 3 4 AF=\\\\frac{3}{4} since three agents are still not activated (the current and the last two).\\nBy that, A \\u200b F AF normalizes A \\u200b O \\u200b C AOC to be proportional to the effect of the perturbation on the rest of the AAW.\\nThe perturbation to the agent output can also affect the activation flow itself.\\nDue to the mid-workflow perturbation, the agents in the AAW can change their decision on who will be the next designated agent.\\nSuch a major change means that the perturbation has a profound impact on the AAW operation.\\nCAIR measures this type of change by calculating the edit distance between the original and changed flow:\\nwhere W \\u200b C j , l WC_{j,l} denotes the workflow change score when modifying element a j a_{j} using r \\u200b q l rq_{l} as input.\\nBy that, any change to the activation flow (added/removed agents, change in order) is considered.\\nFor each r \\u200b q l rq_{l} , the influence score of each agent is calculated by a weighted sum of O \\u200b C OC and W \\u200b C WC ( Œ± \\u200b O \\u200b C + Œ≤ \\u200b W \\u200b C \\\\alpha OC+\\\\beta WC ). Œ± \\\\alpha and Œ≤ \\\\beta are set according to the AAW nature - when agents have more autonomy to change the activation sequence, Œ≤ \\\\beta should increase, and vice versa.\\nCAIR produces a single influence score for every agent.\\nIn activation flows where an agent is called more than once, CAIR takes the maximal score.\\nThis is because if an agent has a strong influence at any point in the activation flow, that influence should not be overlooked (i.e., taking the minimum) or reduced (i.e., taking the average).',\n",
       "      'subsections': [{'title': '3.1.1 Workflow Variations Recording',\n",
       "        'paragraphs': 'First, the representative query r \\u200b q l rq_{l} is used to obtain the original activation flow W a \\u200b ( r \\u200b q l ) W_{a}(rq_{l}) , including a i i \\u200b n k a_{i}^{in_{k}} and a i o \\u200b u \\u200b t k a_{i}^{out_{k}} for every activated agent.\\nThen, CAIR systematically changes a i o \\u200b u \\u200b t k a_{i}^{out_{k}} of each agent by using an LLM (See Appendix D )\\nto be as far as possible from the original, yet still a valid output to the given input.\\nThe effect on the activation flow is recorded with the corresponding agent whose output was changed.\\nFor a single r \\u200b q l rq_{l} , the results of this step, { W a j \\u200b ( r \\u200b q l ) } j = 1 J \\\\{W_{a}^{j}(rq_{l})\\\\}_{j=1}^{J} , are the perturbed versions for each of the J J elements in the W a \\u200b ( r \\u200b q l ) W_{a}(rq_{l}) series.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.1.2 Change Measures Calculation',\n",
       "        'paragraphs': 'In this step, CAIR measures the effects of the systematic change on the rest of the activation flow.\\nThese effects can be seen as the change in the AAW‚Äôs final output W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) or in the agents‚Äô presence and order in W a j \\u200b ( r \\u200b q l ) W_{a}^{j}(rq_{l}) .\\nTo measure the change in W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) , CAIR converts the original final output W f \\u200b ( r \\u200b q l ) W_{f}(rq_{l}) and the perturbed output W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) to embedding vectors W f \\u200b ( r \\u200b q l ) v W_{f}(rq_{l})^{v} and W f l \\u200b ( r \\u200b q l ) v W_{f}^{l}(rq_{l})^{v} respectively, using a well-known projection technique (SBERT Reimers and Gurevych ( 2019 ) ) and measures their cosine distance.\\nThis is presented in Equation 1 :\\nwhere F \\u200b O \\u200b C j , l FOC_{j,l} denotes the change in the final output when modifying element a j a_{j} using r \\u200b q l rq_{l} as input.\\nThe F \\u200b O \\u200b C FOC measure reflects the perturbation effect on the AAW final output; however, it ignores the perturbation intensity and its effect on the computed score.\\nTo measure the perturbation intensity, a calculation similar to Equation 1 is performed, which measures the change in a j o \\u200b u \\u200b t k a_{j}^{out_{k}} :\\nwhere A \\u200b O \\u200b C j AOC_{j} denotes the change in the output of agent a j a_{j} , and a j o \\u200b u \\u200b t k \\u200b v a_{j}^{out_{k}}v and a j o \\u200b u \\u200b t k \\u200b v ^ \\\\widehat{a_{j}^{out_{k}}v} are the vector representations of the agents‚Äô original and perturbed outputs, respectively.\\nTo remove the perturbation effect from F \\u200b O \\u200b C FOC , A \\u200b O \\u200b C AOC is multiplied by an amplification factor ( A \\u200b F AF ) and subtracted from F \\u200b O \\u200b C FOC :\\nThe purpose of the amplification factor is to take into account the distance between a i o \\u200b u \\u200b t k ^ \\\\widehat{a_{i}^{out_{k}}} and W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) from the O \\u200b C OC score and eliminate its effect; more activations between a i o \\u200b u \\u200b t k ^ \\\\widehat{a_{i}^{out_{k}}} and W f j \\u200b ( r \\u200b q l ) W_{f}^{j}(rq_{l}) mean that the perturbation performed on a i o \\u200b u \\u200b t k a_{i}^{out_{k}} will affect a larger part of the workflow.\\nThe level of change performed on a i o \\u200b u \\u200b t k a_{i}^{out_{k}} is reflected by A \\u200b O \\u200b C AOC , but the effect on the other agents is not considered.\\nThe amplification factor A \\u200b F AF is set by the number of agents remaining to be activated divided by the total number of activated agents.\\nFor example, if | W a \\u200b ( r \\u200b q l ) | = 4 |W_{a}(rq_{l})|=4 and the output of the second activated agent is perturbed, then A \\u200b F = 3 4 AF=\\\\frac{3}{4} since three agents are still not activated (the current and the last two).\\nBy that, A \\u200b F AF normalizes A \\u200b O \\u200b C AOC to be proportional to the effect of the perturbation on the rest of the AAW.\\nThe perturbation to the agent output can also affect the activation flow itself.\\nDue to the mid-workflow perturbation, the agents in the AAW can change their decision on who will be the next designated agent.\\nSuch a major change means that the perturbation has a profound impact on the AAW operation.\\nCAIR measures this type of change by calculating the edit distance between the original and changed flow:\\nwhere W \\u200b C j , l WC_{j,l} denotes the workflow change score when modifying element a j a_{j} using r \\u200b q l rq_{l} as input.\\nBy that, any change to the activation flow (added/removed agents, change in order) is considered.',\n",
       "        'subsections': []},\n",
       "       {'title': '3.1.3 Final Score Calculation',\n",
       "        'paragraphs': 'For each r \\u200b q l rq_{l} , the influence score of each agent is calculated by a weighted sum of O \\u200b C OC and W \\u200b C WC ( Œ± \\u200b O \\u200b C + Œ≤ \\u200b W \\u200b C \\\\alpha OC+\\\\beta WC ). Œ± \\\\alpha and Œ≤ \\\\beta are set according to the AAW nature - when agents have more autonomy to change the activation sequence, Œ≤ \\\\beta should increase, and vice versa.\\nCAIR produces a single influence score for every agent.\\nIn activation flows where an agent is called more than once, CAIR takes the maximal score.\\nThis is because if an agent has a strong influence at any point in the activation flow, that influence should not be overlooked (i.e., taking the minimum) or reduced (i.e., taking the average).',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.2 Online Phase',\n",
       "      'paragraphs': 'At the end of the offline phase, CAIR maintains a list of representative queries with the following information:\\n1) a vector representation of the representative query;\\nand 2) the influence scores and rankings of the agents according to their influence.\\nThis list is used for every new inference query to obtain the most relevant agents‚Äô rankings.\\nAt inference time, a new query q n \\u200b e \\u200b w q_{new} is given to the AAW.\\nBefore inputting it into the AAW, CAIR is applied - CAIR converts q n \\u200b e \\u200b w q_{new} into its vectorized representation q n \\u200b e \\u200b w v q_{new}^{v} and retrieves the most similar representative query from the list of representative queries.\\nThe agents‚Äô rankings for q n \\u200b e \\u200b w q_{new} are set as the rankings of the selected representative query.\\nThe added computations‚Äîembedding the input and computing cosine similarities with the representative queries‚Äîare negligible compared to a single LLM call, and especially so when compared to a full sequence of LLM-based agent activations.\\nBy that, CAIR provides an effective influence ranking prediction suited for inference time.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Evaluation',\n",
       "    'paragraphs': 'AAW Architecture Method TRS (%) P@3 (%) P@2 (%) P@1 (%) 1-SFD (%) Sequential Random 0.6 (0.3) 8.5 (2.3) 9.0 (1.5) 19.0 (1.5) 33.9 (0.8) BTW 18.0 (12.5) 68.0 (7.5) 42.0 (14.6) 29.0 (14.4) 39.7 (10.6) EV 26.0 (15.0) 76.0 (9.2) 57.0 (14.9) 48.0 (12.5) 46.7 (11.8) CAIR 24.0 (11.1) 76.0 (8.0) 84.0 (9.1) 35.0 (9.2) 41.1 (11.0) Orchestrator Random 0.2 (0.2) 5.3 (1.7) 6.8 (1.2) 16.8 (1.3) 34.8 (0.7) BTW 0.0 (0.0) 33.0 (16.7) 0.0 (0.0) 100.0 (0.0) 60.7 (4.3) EV 0.0 (0.0) 33.0 (16.7) 0.0 (0.0) 100.0 (0.0) 60.7 (4.3) CAIR 23.81 (19.1) 48.5 (17.0) 65.5 (15.5) 100.0 (0.0) 65.4 (10.6) Router Random 0.0019 (0.00098) 1.6 (0.4) 3.3 (0.6) 11.9 (1.2) 34.2 (0.3) BTW 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 50.3 (0.8) EV 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 18.3 (9.7) CAIR 40.0 (38.9) 63.3 (31.4) 93.3 (13.3) 93.3 (20.0) 79.7 (15.1) Overall Random 0.3 (0.2) 5.1 (1.5) 6.4 (1.1) 15.9 (1.3) 34.3 (0.6) BTW 6.0 (4.2) 33.6 (8.1) 14.0 (4.89) 43.0 (4.8) 50.2 (5.3) EV 8.7 (4.9) 36.3 (8.6) 19.0 (4.95) 49.3 (4.1) 41.9 (8.6) CAIR 29.27 (23.04) 62.6 (18.8) 80.95 (12.6) 76.1 (9.7) 62.1 (12.4)\\nExperimental settings can be found in Appendix B , and complexity analysis is given in Appendix C .\\nTo perform a high-scale evaluation, we used AAW-Generator and created AAW-Zoo, aiming to close the gap of a lack of open-source AAW datasets.\\nAAW-Zoo-Generator is an LLM chain for creating simple AAWs for evaluation, with 10 components handling request analysis, workflow design, and code generation.\\nWhen used to generate a new use case (i.e., an AAW for a specific purpose), AAW-Zoo-Generator takes a natural language description of the desired task and architecture and outputs all relevant components (see Appendix E ).\\nAAW-Zoo dataset contains 30 use cases in three architectures Mitchell et\\xa0al. ( 2025 ) :\\n1) 10 sequential use cases - where each agent receives its input from a specific agent and sends its output to a specific agent (i.e., a chain of agents);\\n2) 10 orchestrator use cases - all agents receive their input from and send their output to the orchestrator agent (i.e., the orchestrator selects which agent will be used at any point);\\nand 3) 10 router use cases - Based on the input query, the router selects one of several predefined sequential flows (branches) to activate‚Äîi.e., it determines which chain of agents will be activated, but does not influence the flow‚Äôs execution once selected.\\nSequential and orchestrator use cases have 10 functionalities; routers have 3 (one per branch).\\nAll use cases are validated to function appropriately for the input query‚Äîthat is, the activated agents and their outputs are relevant to the user‚Äôs request.\\nIn addition to the code of each AAW, each use case is accompanied by:\\n1) A representative queries list, containing one query from each functionality and its expected activation flow;\\n2) An additional set of 150 queries per functionality;\\n3) 10-30 toxic queries for each functionality.\\n4) additional metadata, such as the list of agents in the AAW and its builder function.\\nAll the queries were generated using an LLM (see Appendix E for more information on the dataset and the metadata creation).\\nCAIR is the first to tackle the challenge of influence-based agent ranking in AAWs.\\nThus, we applied methods from related fields to AAWs.\\nGraph theory measures (BTW and EV) - the AAW can be seen as a connectivity graph, where each agent is a node and each connection between agents is an edge.\\nThe Betweenness centrality (BTW) and Eigenvector (EV) Saxena and Iyengar ( 2020 ) measures were used as baselines.\\nClassical feature importance (CFI) - agents were represented as numerical features, allowing training of a classical ML model and analyzing its corresponding feature importance using SHapley Additive exPlanations (SHAP) Lundberg and Lee ( 2017 ) .\\nSHAP, a common feature importance calculator, can not be applied directly to the AAW, so a proxy model was created.\\nThe AAW was activated with 150 samples per functionality, recording each agent‚Äôs inputs and outputs. These recordings formed the samples, with the AAW‚Äôs final output as the training label.\\nNumerical features were then extracted, representing each agent as the difference between its input and output.\\nThe added information was represented by the penultimate layer vector of a Semantic Textual Similarity model, which predicts similarity between two texts.\\nAll agent representations per AAW activation were concatenated into one sample vector.\\nThe final output of the AAW activation was embedded as well using SBERT and used as a label.\\nFor each functionality, a Support Vector Machine (SVM) regressor was trained using the samples and labels.\\nSHAP was used to extract the feature importance of each feature in each SVM.\\nTo represent agents rather than individual features, we averaged feature importance values per agent call and used the maximum if called multiple times.\\nThe learned model for each functionality maps agent-level semantic contributions to the final result. This supervised mapping captures a statistical signal of which agents consistently alter final outputs in predictable ways.\\nSince the task of ranking agents in an AAW by their influence on the final output is unexplored, there are no available rankings to be considered as ground truth.\\nTherefore, we use the CFI method as the closest proxy for ground truth.\\nThis choice can be justified by three factors:\\n(1) the task of extracting feature importance scores in classical ML is well studied;\\n(2) CFI incorporates the input query;\\nand (3) CFI accounts for agent behavior.\\nHowever, while suitable as a reference, CFI is not practical, as it requires extensive data‚Äîat least 150 example queries per functionality with full agent activations and training a model for each functionality.\\nIn contrast, CAIR needs only a single query for each functionality.\\nTo evaluate CAIR ‚Äôs rankings and the graph theory measure‚Äôs quality, we used the following metrics: Total ranking success (TRS) . The percentage of queries for which all the agents‚Äô rankings were identical to the ground truth. Precision@3/@2/@1 (P@3, P@2, P@1) . The percentage of queries in which the group of the top three/two/one agents contains the same agents as the ground truth. The P@1, P@2, and P@3 metrics capture critical ranking errors: overestimating an agent‚Äôs influence may lead to unnecessary guardrails and increased latency, while underestimating a truly influential agent risks reduced safety and degraded output quality. 1 - Normalized Spearman‚Äôs Footrule Distance (1-SFD) . One minus the average absolute difference between predicted and true ranks across all agents, measuring how close the predicted ranking is to the ground truth, i.e., SFD is the distance and 1-SFD is the similarity (formulation is in Appendix H ). The P@1, P@2, and P@3 metrics capture critical ranking errors: overestimating an agent‚Äôs influence may lead to unnecessary guardrails and increased latency, while underestimating a truly influential agent risks reduced safety and degraded output quality.\\nTo evaluate the effectiveness of CAIR rankings when incorporated in the downstream task of toxicity guardrails, the following metrics were used: 1) Latency improvement (POI) , percentage of reduction in inference time compared to applying guardrails on every LLM call;\\n2) Effectiveness Change (EC) , drop in non-toxic output rate relative to the baseline, where all agents have guardrails.\\n\\n\\nArchitecture POI(%) ‚Üë \\\\uparrow EC(%) ‚Üë \\\\uparrow CAIR CFI CAIR CFI Sequential 25.49 28.24 1.13 -11.75 Orchestrator 17.92 14.72 -13.20 -17.71 Router 39.75 36.52 -2.22 -3.89 Overall 27.72 26.49 -4.76 -11.12',\n",
       "    'subsections': [{'title': '4.1 AAW-Zoo and AAW-Zoo-Generator',\n",
       "      'paragraphs': 'To perform a high-scale evaluation, we used AAW-Generator and created AAW-Zoo, aiming to close the gap of a lack of open-source AAW datasets.\\nAAW-Zoo-Generator is an LLM chain for creating simple AAWs for evaluation, with 10 components handling request analysis, workflow design, and code generation.\\nWhen used to generate a new use case (i.e., an AAW for a specific purpose), AAW-Zoo-Generator takes a natural language description of the desired task and architecture and outputs all relevant components (see Appendix E ).\\nAAW-Zoo dataset contains 30 use cases in three architectures Mitchell et\\xa0al. ( 2025 ) :\\n1) 10 sequential use cases - where each agent receives its input from a specific agent and sends its output to a specific agent (i.e., a chain of agents);\\n2) 10 orchestrator use cases - all agents receive their input from and send their output to the orchestrator agent (i.e., the orchestrator selects which agent will be used at any point);\\nand 3) 10 router use cases - Based on the input query, the router selects one of several predefined sequential flows (branches) to activate‚Äîi.e., it determines which chain of agents will be activated, but does not influence the flow‚Äôs execution once selected.\\nSequential and orchestrator use cases have 10 functionalities; routers have 3 (one per branch).\\nAll use cases are validated to function appropriately for the input query‚Äîthat is, the activated agents and their outputs are relevant to the user‚Äôs request.\\nIn addition to the code of each AAW, each use case is accompanied by:\\n1) A representative queries list, containing one query from each functionality and its expected activation flow;\\n2) An additional set of 150 queries per functionality;\\n3) 10-30 toxic queries for each functionality.\\n4) additional metadata, such as the list of agents in the AAW and its builder function.\\nAll the queries were generated using an LLM (see Appendix E for more information on the dataset and the metadata creation).',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Baseline methods',\n",
       "      'paragraphs': 'CAIR is the first to tackle the challenge of influence-based agent ranking in AAWs.\\nThus, we applied methods from related fields to AAWs.\\nGraph theory measures (BTW and EV) - the AAW can be seen as a connectivity graph, where each agent is a node and each connection between agents is an edge.\\nThe Betweenness centrality (BTW) and Eigenvector (EV) Saxena and Iyengar ( 2020 ) measures were used as baselines.\\nClassical feature importance (CFI) - agents were represented as numerical features, allowing training of a classical ML model and analyzing its corresponding feature importance using SHapley Additive exPlanations (SHAP) Lundberg and Lee ( 2017 ) .\\nSHAP, a common feature importance calculator, can not be applied directly to the AAW, so a proxy model was created.\\nThe AAW was activated with 150 samples per functionality, recording each agent‚Äôs inputs and outputs. These recordings formed the samples, with the AAW‚Äôs final output as the training label.\\nNumerical features were then extracted, representing each agent as the difference between its input and output.\\nThe added information was represented by the penultimate layer vector of a Semantic Textual Similarity model, which predicts similarity between two texts.\\nAll agent representations per AAW activation were concatenated into one sample vector.\\nThe final output of the AAW activation was embedded as well using SBERT and used as a label.\\nFor each functionality, a Support Vector Machine (SVM) regressor was trained using the samples and labels.\\nSHAP was used to extract the feature importance of each feature in each SVM.\\nTo represent agents rather than individual features, we averaged feature importance values per agent call and used the maximum if called multiple times.\\nThe learned model for each functionality maps agent-level semantic contributions to the final result. This supervised mapping captures a statistical signal of which agents consistently alter final outputs in predictable ways.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Evaluation metrics',\n",
       "      'paragraphs': 'Since the task of ranking agents in an AAW by their influence on the final output is unexplored, there are no available rankings to be considered as ground truth.\\nTherefore, we use the CFI method as the closest proxy for ground truth.\\nThis choice can be justified by three factors:\\n(1) the task of extracting feature importance scores in classical ML is well studied;\\n(2) CFI incorporates the input query;\\nand (3) CFI accounts for agent behavior.\\nHowever, while suitable as a reference, CFI is not practical, as it requires extensive data‚Äîat least 150 example queries per functionality with full agent activations and training a model for each functionality.\\nIn contrast, CAIR needs only a single query for each functionality.\\nTo evaluate CAIR ‚Äôs rankings and the graph theory measure‚Äôs quality, we used the following metrics: Total ranking success (TRS) . The percentage of queries for which all the agents‚Äô rankings were identical to the ground truth. Precision@3/@2/@1 (P@3, P@2, P@1) . The percentage of queries in which the group of the top three/two/one agents contains the same agents as the ground truth. The P@1, P@2, and P@3 metrics capture critical ranking errors: overestimating an agent‚Äôs influence may lead to unnecessary guardrails and increased latency, while underestimating a truly influential agent risks reduced safety and degraded output quality. 1 - Normalized Spearman‚Äôs Footrule Distance (1-SFD) . One minus the average absolute difference between predicted and true ranks across all agents, measuring how close the predicted ranking is to the ground truth, i.e., SFD is the distance and 1-SFD is the similarity (formulation is in Appendix H ). The P@1, P@2, and P@3 metrics capture critical ranking errors: overestimating an agent‚Äôs influence may lead to unnecessary guardrails and increased latency, while underestimating a truly influential agent risks reduced safety and degraded output quality.\\nTo evaluate the effectiveness of CAIR rankings when incorporated in the downstream task of toxicity guardrails, the following metrics were used: 1) Latency improvement (POI) , percentage of reduction in inference time compared to applying guardrails on every LLM call;\\n2) Effectiveness Change (EC) , drop in non-toxic output rate relative to the baseline, where all agents have guardrails.\\n\\n\\nArchitecture POI(%) ‚Üë \\\\uparrow EC(%) ‚Üë \\\\uparrow CAIR CFI CAIR CFI Sequential 25.49 28.24 1.13 -11.75 Orchestrator 17.92 14.72 -13.20 -17.71 Router 39.75 36.52 -2.22 -3.89 Overall 27.72 26.49 -4.76 -11.12',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Results and Discussion',\n",
       "    'paragraphs': 'Table 1 presents a rankings analysis performed on all use cases in the AAW-Zoo dataset grouped by the AAW architecture and an overall view.\\nEach metric was calculated for the random choice case (see Appendix I ), betweenness (BTW) and Eigenvector (EV) measures, and CAIR using the CFI method as ground truth.\\nIt can be seen that CAIR results outperform BTW and EV in all metrics for the orchestrator and router AAWs and in the overall measure.\\nThis can be explained by the fact that BTW and EV are architecture-based measures, and both orchestrator and router AAWs have relatively high autonomy, resulting in a non-fixed architecture during activation.\\nIn the sequential AAWs, which have less architecture-related autonomy when activated, EV has higher TRS, P@1, and 1-SFD, yet CAIR remains competitive.\\nThis can be explained by the phenomenon that the architecture has a massive impact on the agent‚Äôs influence on the final output in sequential architectures, i.e., agents that are activated late in the workflow will have a higher influence on the final output.\\nThis phenomenon was exhibited by the rankings of all methods, including CFI, which serves as the ground truth.\\nAlthough CAIR is mostly aligned with the CFI rankings, we further investigated functionalities where the agents were ranked differently by CAIR and CFI and performed human verification.\\nHuman verifiers chose the ranking set that best matched perceived agent importance.\\nIn most cases, CAIR rankings were more aligned with human perception (See Appendix J ).\\n\\nAt inference time, CAIR pairs the input query to a representative query based on their proximity (see Section 3.2 ).\\nFigure 3 shows an example of CAIR online phase for a single AAW sequential use case with minimal settings - only one representative query for each functionality.\\nTen runtime queries (triangle) are shown with their paired representative queries (circle).\\nAll queries in the plot are represented by their SBERT embedding vectors projected to 2D using T-SNE Van\\xa0der Maaten and Hinton ( 2008 ) .\\nOne input query of each functionality was randomly chosen and represented by its functionality number.\\nWhen examining the cosine similarity score for each input query, it can be seen that queries were correctly paired accordingly to their functionalities.\\nThis indicates that CAIR can be successfully used in online settings.\\nAs noted, CAIR can be used to bridge the gap in downstream tasks of applying LLM-level solutions to the workflow-level.\\nIn this section, we demonstrate this ability using toxicity guardrails Kumar et\\xa0al. ( 2023 ) as the downstream task.\\nIt is essential, in various use cases, that an LLM does not produce toxic outputs, as this can cause financial 4 4 4 cbc.ca/news/canada/british-columbia/air-canada-chatbot-lawsuit-1.7116416 and public relations 5 5 5 bbc.co.uk/news/technology-68025677 harms.\\nFor that, toxicity guardrails are applied as an additional layer of protection at every LLM call.\\nCurrently, there are no established best practices for applying toxicity guardrails at the agent or workflow level.\\nAs a result, guardrails are applied to every LLM call, leading to significant additional latency during inference.\\nLeveraging CAIR ‚Äôs rankings (or CFI rankings), guardrails can be selectively enforced on the most important agents, reducing added latency without compromising guardrails‚Äô effectiveness.\\nThe guardrails suite contains output guardrails, each evaluates a different type of toxicity.\\nIn addition, when the LLM output is found to be toxic, the guardrails suite applies three rounds of output correction.\\nFor more guardrails suite information, see Appendix G .\\nFigure 4 and Table 2 show results for applying guardrails to only the top-ranked half of agents versus all agents, across nine use cases spanning three architectures.\\nFigure 4 shows the added latency due to applying guardrails (in percentage).\\nIt can be seen that applying guardrails only on critical agents (regardless of the used method) reduces the inference time substantially.\\nTable 2 reports the percentage improvement in latency and the change in effectiveness when applying guardrails using CAIR and CFI.\\nCAIR achieves an average latency reduction of 27.72%, with only a 4.76% drop in effectiveness.\\nIn comparison, CFI yields a similar latency gain but results in a significantly larger drop in effectiveness (11.12%), increasing the risk of generating toxic outputs.\\nThe importance of each component in CAIR ‚Äôs offline phase was evaluated by ablation studies.\\nFigure 5 (a) shows the 1-SFD of CAIR rankings when using different variations of CAIR components.\\nIt can be seen that the rankings with the highest compatibility to CFI and with the lowest variability are the result of using all components combined (Orig. OC).\\nIn addition, the sensitivity of CAIR to different Œ± \\\\alpha and Œ≤ \\\\beta values was evaluated.\\nFigure 5 (b) shows the 1-SFD of CAIR rankings when using different values of Œ± \\\\alpha and Œ≤ \\\\beta .\\nIt can be seen that deviating from the values selected in the paper ( Œ± = 0.6 \\\\alpha=0.6 and Œ≤ = 0.4 \\\\beta=0.4 ) still yields similarly good rankings; however, when Œ± \\\\alpha and Œ≤ \\\\beta are pushed toward the extremes of their range, the 1-SFD score declines.\\nIn addition, it can be seen that the rankings at any setting are stable with low standard deviation.\\nFig. 5 (c) shows the average 1-SFD across varying numbers of representative queries per functionality, showing CAIR ‚Äôs ranking quality stays consistent across query set sizes.\\n\\nIn addition, we evaluated the effectiveness of using SBERT for measuring string differences.\\nAlthough this ability was already shown Rauf et\\xa0al. ( 2023 ) , we conducted a small qualitative experiment shown in Figure 6 .\\nThe experiment results show the effectiveness of using SBERT for measuring string differences in structured data.\\nWe conducted an additional evaluation of CAIR using a publicly available use case from a LangGraph tutorial 6 6 6 langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/ .\\nThis hierarchical AAW setup includes three supervisor agents and five worker agents, demonstrating production-ready complexity. The agents are grouped into a \"research team\"‚Äîcomprising a ‚Äúsearch‚Äù agent (uses a search API), a ‚Äúweb scraper‚Äù agent (scrapes web pages via URLs), and a research orchestrator‚Äîand a \"document writing\" team with a ‚Äúdoc writer‚Äù agent (drafts documents using filesystem tools), ‚Äúnote taker‚Äù agent (saves short notes as files), ‚Äúchart generator‚Äù agent (creates visualizations via Python REPL), and a writing orchestrator. The top-level orchestrator receives the user request and routes it to the appropriate team orchestrator.\\nWe performed the evaluation on three representative queries and showed that, opposed to other methods, CAIR is aligned with the expected rankings (full experiment details are in Appendix K ).',\n",
       "    'subsections': [{'title': '5.1 CAIR vs. Baselines',\n",
       "      'paragraphs': 'Table 1 presents a rankings analysis performed on all use cases in the AAW-Zoo dataset grouped by the AAW architecture and an overall view.\\nEach metric was calculated for the random choice case (see Appendix I ), betweenness (BTW) and Eigenvector (EV) measures, and CAIR using the CFI method as ground truth.\\nIt can be seen that CAIR results outperform BTW and EV in all metrics for the orchestrator and router AAWs and in the overall measure.\\nThis can be explained by the fact that BTW and EV are architecture-based measures, and both orchestrator and router AAWs have relatively high autonomy, resulting in a non-fixed architecture during activation.\\nIn the sequential AAWs, which have less architecture-related autonomy when activated, EV has higher TRS, P@1, and 1-SFD, yet CAIR remains competitive.\\nThis can be explained by the phenomenon that the architecture has a massive impact on the agent‚Äôs influence on the final output in sequential architectures, i.e., agents that are activated late in the workflow will have a higher influence on the final output.\\nThis phenomenon was exhibited by the rankings of all methods, including CFI, which serves as the ground truth.\\nAlthough CAIR is mostly aligned with the CFI rankings, we further investigated functionalities where the agents were ranked differently by CAIR and CFI and performed human verification.\\nHuman verifiers chose the ranking set that best matched perceived agent importance.\\nIn most cases, CAIR rankings were more aligned with human perception (See Appendix J ).',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 CAIR in online settings',\n",
       "      'paragraphs': '\\nAt inference time, CAIR pairs the input query to a representative query based on their proximity (see Section 3.2 ).\\nFigure 3 shows an example of CAIR online phase for a single AAW sequential use case with minimal settings - only one representative query for each functionality.\\nTen runtime queries (triangle) are shown with their paired representative queries (circle).\\nAll queries in the plot are represented by their SBERT embedding vectors projected to 2D using T-SNE Van\\xa0der Maaten and Hinton ( 2008 ) .\\nOne input query of each functionality was randomly chosen and represented by its functionality number.\\nWhen examining the cosine similarity score for each input query, it can be seen that queries were correctly paired accordingly to their functionalities.\\nThis indicates that CAIR can be successfully used in online settings.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 Downstream task - Toxicity guardrails',\n",
       "      'paragraphs': 'As noted, CAIR can be used to bridge the gap in downstream tasks of applying LLM-level solutions to the workflow-level.\\nIn this section, we demonstrate this ability using toxicity guardrails Kumar et\\xa0al. ( 2023 ) as the downstream task.\\nIt is essential, in various use cases, that an LLM does not produce toxic outputs, as this can cause financial 4 4 4 cbc.ca/news/canada/british-columbia/air-canada-chatbot-lawsuit-1.7116416 and public relations 5 5 5 bbc.co.uk/news/technology-68025677 harms.\\nFor that, toxicity guardrails are applied as an additional layer of protection at every LLM call.\\nCurrently, there are no established best practices for applying toxicity guardrails at the agent or workflow level.\\nAs a result, guardrails are applied to every LLM call, leading to significant additional latency during inference.\\nLeveraging CAIR ‚Äôs rankings (or CFI rankings), guardrails can be selectively enforced on the most important agents, reducing added latency without compromising guardrails‚Äô effectiveness.\\nThe guardrails suite contains output guardrails, each evaluates a different type of toxicity.\\nIn addition, when the LLM output is found to be toxic, the guardrails suite applies three rounds of output correction.\\nFor more guardrails suite information, see Appendix G .\\nFigure 4 and Table 2 show results for applying guardrails to only the top-ranked half of agents versus all agents, across nine use cases spanning three architectures.\\nFigure 4 shows the added latency due to applying guardrails (in percentage).\\nIt can be seen that applying guardrails only on critical agents (regardless of the used method) reduces the inference time substantially.\\nTable 2 reports the percentage improvement in latency and the change in effectiveness when applying guardrails using CAIR and CFI.\\nCAIR achieves an average latency reduction of 27.72%, with only a 4.76% drop in effectiveness.\\nIn comparison, CFI yields a similar latency gain but results in a significantly larger drop in effectiveness (11.12%), increasing the risk of generating toxic outputs.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.4 Ablation and sensitivity analysis',\n",
       "      'paragraphs': 'The importance of each component in CAIR ‚Äôs offline phase was evaluated by ablation studies.\\nFigure 5 (a) shows the 1-SFD of CAIR rankings when using different variations of CAIR components.\\nIt can be seen that the rankings with the highest compatibility to CFI and with the lowest variability are the result of using all components combined (Orig. OC).\\nIn addition, the sensitivity of CAIR to different Œ± \\\\alpha and Œ≤ \\\\beta values was evaluated.\\nFigure 5 (b) shows the 1-SFD of CAIR rankings when using different values of Œ± \\\\alpha and Œ≤ \\\\beta .\\nIt can be seen that deviating from the values selected in the paper ( Œ± = 0.6 \\\\alpha=0.6 and Œ≤ = 0.4 \\\\beta=0.4 ) still yields similarly good rankings; however, when Œ± \\\\alpha and Œ≤ \\\\beta are pushed toward the extremes of their range, the 1-SFD score declines.\\nIn addition, it can be seen that the rankings at any setting are stable with low standard deviation.\\nFig. 5 (c) shows the average 1-SFD across varying numbers of representative queries per functionality, showing CAIR ‚Äôs ranking quality stays consistent across query set sizes.\\n\\nIn addition, we evaluated the effectiveness of using SBERT for measuring string differences.\\nAlthough this ability was already shown Rauf et\\xa0al. ( 2023 ) , we conducted a small qualitative experiment shown in Figure 6 .\\nThe experiment results show the effectiveness of using SBERT for measuring string differences in structured data.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.5 Production-ready use case',\n",
       "      'paragraphs': 'We conducted an additional evaluation of CAIR using a publicly available use case from a LangGraph tutorial 6 6 6 langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/ .\\nThis hierarchical AAW setup includes three supervisor agents and five worker agents, demonstrating production-ready complexity. The agents are grouped into a \"research team\"‚Äîcomprising a ‚Äúsearch‚Äù agent (uses a search API), a ‚Äúweb scraper‚Äù agent (scrapes web pages via URLs), and a research orchestrator‚Äîand a \"document writing\" team with a ‚Äúdoc writer‚Äù agent (drafts documents using filesystem tools), ‚Äúnote taker‚Äù agent (saves short notes as files), ‚Äúchart generator‚Äù agent (creates visualizations via Python REPL), and a writing orchestrator. The top-level orchestrator receives the user request and routes it to the appropriate team orchestrator.\\nWe performed the evaluation on three representative queries and showed that, opposed to other methods, CAIR is aligned with the expected rankings (full experiment details are in Appendix K ).',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Conclusions and Future Work',\n",
       "    'paragraphs': 'In this paper, we presented Counterfactual-based Agent Influence Ranker (CAIR ) - the first method for assessing the influence level of each agent on the AAW‚Äôs output and determining which agents are the most influential.\\nIn our experiments, conducted on 30 different AAWs with 230 unique tasks, we showed that CAIR :\\n1) produces high-quality agent rankings aligned with the ground truth;\\n2) is the only method applicable at inference time with negligible added latency;\\nand 3) can be used to adjust LLM-level downstream tasks (such as toxicity guardrails) to be used in AAWs with minimal added latency while maintaining their effectiveness.\\nFuture work may include CAIR evaluation on hybrid AAWs architectures (such as AAWs with several orchestrators and user profiles), AAWs with a higher level of agency (i.e., the agents are more autonomous), and with lower access to the AAW agents‚Äô output (i.e., only to the input and final output).\\nAdditional future work may include extending CAIR to perform downstream tasks by design, i.e., adding components to CAIR to perform the downstream task.\\nFor example, performing risk assessment in AAWs by adding CAIR components that relate to agent failures.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Limitations',\n",
       "    'paragraphs': 'The CAIR method has a few dependencies that may pose limitations.\\nCAIR depends on the user providing a set of representative queries, one from each functionality.\\nIn the case of a poor-quality set, the performance of CAIR can be affected.\\nTo mitigate this risk, one can use the prompt provided in Appendix D , which can be used to generate representative queries based on an AAW overview.\\nA variation of this prompt was successfully used in the presented experiments to generate the representative queries of our AAW-Zoo dataset.\\nIn addition, one can provide a larger set of representative queries, reducing the probability of a functionality being ignored.\\nAnother dependency is that CAIR relies on access to the output of each agent in the AAW.\\nThis prevents using CAIR as a third-party analysis that was provided only with black box access to the AAW (i.e., having access only to query the AAW and receive its final output).\\nHowever, this is a low-risk limitation, as it is a reasonable assumption that one asking to assess the agents‚Äô influence will have at least access to the agents‚Äô outputs.\\nMoreover, CAIR ‚Äôs offline process uses two parameters that the user has to set - Œ± \\\\alpha and Œ≤ \\\\beta , which opens CAIR to the risk of misconfiguration.\\nOur experiments demonstrate that CAIR produces stable rankings across a broad range of Œ± \\\\alpha and Œ≤ \\\\beta values, allowing for flexibility in parameter selection.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Overview',\n",
       "    'paragraphs': 'This appendix provides additional details to supplement the main paper and is organized as follows:\\nLLM Assistance Statement : A short statement about the use of LLMs in our research - Section A .\\nExperimental Settings : This section describes the hardware and software infrastructure used for the experiments in the paper - Section B .\\nComplexity Analysis : This section analyzes the complexity of CAIR ‚Äôs offline phase to estimate its latency - Section C .\\nPrompts used in CAIR : This section gives examples of the prompts used in CAIR to perturb agent outputs and generate representative queries - Section D .\\nAAW-Zoo-Generator : This section details the design and implementation of the AAW-Zoo-Generator which was used to produce our dataset AAW-Zoo - Section E .\\nAAW-Zoo : This section specifies the different architecture types included in the AAW-Zoo dataset and provides a description of each use case - Section F .\\nGuardrails Suite : This section describes the implementation of the toxic guardrails suite - Section G .\\nMetrics Formulation : This section defines the metrics used in Table 1 - Section H .\\nRandom Setting Formulation : This section explains the values presented under the \"Random\" setting in Table 1 - Section I .\\nHuman Verification : This section outlines the methodology used for human verification of the rankings produced by CAIR and the baseline CFI method - Section J .\\nProduction-ready full results : This section presents the full results analysis of the production-ready use case - Section K .\\nEach section is designed to provide clarity and reproducibility for researchers looking to replicate or build upon our work.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix A LLM Assistance Statement',\n",
       "    'paragraphs': 'We used LLMs to assist with both code development and sentence-level editing throughout the research and paper preparation process.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Experimental settings',\n",
       "    'paragraphs': 'All experiments were conducted in an Azure environment on a virtual machine with Ubuntu 24.04 x64 OS, 6 vCPUs and 112 GiB RAM.\\nThe Python environment contains langgraph V0.2.69, langfuse V2.59.3, Networkx V3.4.2, and scikit-learn V1.6.1.\\nAll AAWs, CAIR and the guardrails use gpt-4o, gpt-o1, and gpt-4o as the fundamental LLMs, respectively, taken from Azure deployments.\\nThe SBERT model used in all experiments is sentence-transformers/all-MiniLM-L6-v2 7 7 7 huggingface.co/sentence-transformers/all-MiniLM-L6-v2 .\\nIn all experiments, we used stsb-roberta-base 8 8 8 huggingface.co/cross-encoder/stsb-roberta-base as the Semantic Textual Similarity predictor and SHAP KernelExplainer as the SHAP model.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Complexity Analysis',\n",
       "    'paragraphs': 'CAIR ‚Äôs offline analysis begins by executing the use case on a given representative query without any interference. Let J J denote the number of agent activations during this unperturbed execution.\\nCAIR then perturbs the output of each of these J J activated agents individually. For each perturbation, the remainder of the use case is re-executed, resulting in a modified sequence of activations. While the exact number of activations may vary from the original execution (sometimes fewer, sometimes more), we observe that, on average, J ‚àí j J-j activations are performed for the perturbation of the j j -th agent, where j ‚àà { 1 , ‚Ä¶ , J } j\\\\in\\\\{1,\\\\dots,J\\\\} .\\nThe total number of agent activations across all executions is:\\nIn addition, there are J J additional LLM calls made by CAIR to generate the perturbed outputs of each activated agent, before re-executing the workflow.\\nAssuming that each LLM perturbation call is not more expensive than an agent activation, we can upper bound the total number of activations (including LLM calls) as:\\nThis represents the overall computational cost of the offline phase in terms of agent activations, computed per representative query.\\nTo provide context, we use the longest observed CAIR analysis as an upper bound - specifically, functionality 5 from the adult_story_gen_orch use case, which involved J = 13 J=13 agent activations and required 1465.81 seconds to complete the full offline analysis.\\nFirst, we estimate the average time per activation using:\\nUsing this estimate, we approximate the total offline runtime for a scenario involving 15 representative queries and an average of 70 agent activations per query as:\\nThe estimated total time is approximately 600 , 358 600{,}358 seconds, which is equivalent to about 166.76 166.76 hours or 6.95 6.95 days‚Äîapproximately one week.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D CAIR Used Prompts',\n",
       "    'paragraphs': 'The following prompts were used in the offline phase to perform the perturbation of each agent‚Äôs output to a counterfactual one. A prompt was created for each AAW architecture.\\nIn addition, we provide a prompt that generates representative queries based on an AAW overview. The overview may be in plain text or formatted as a JSON file, and typically includes the agents involved and a high-level description of the workflow. Additional context can be provided but is not strictly required.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix E AAW-Zoo-Generator',\n",
       "    'paragraphs': 'Motivation. Prompting an LLM to generate a complete AAW is feasible. While the output may differ from the exact desired workflow and can include some code issues, LLMs generally possess the necessary knowledge. Our experiments show that by identifying mistakes and iterating on the results, the quality improves significantly with each step.\\nHigh Level Idea. Break the full task into several smaller steps, each performed through a dedicated LLM call. By simplifying each request into a well-defined, closed-form prompt, mistakes and errors are dramatically reduced. This structured, step-by-step approach naturally supports a chain-of-thought reasoning process, allowing each component to build upon the outputs of prior steps with increased clarity and context. An additional important aspect is passing each response through a \"supervisor\" component, which evaluates the quality of the response concerning the user‚Äôs request and the previously completed steps. If the supervisor deems the result inadequate, it sends the output back to the responsible component for revision‚Äîencouraging iterative improvement across the workflow.\\nImplementation Details. AAW-Zoo-Generator is implemented using the LangGraph framework. Each LLM call is encapsulated as a node, which accesses relevant information and routes the output via the supervisor to the next node in the flow. All LLM calls use the gpt-o1 model, with no additional tools or external databases involved.\\nWe divide the full generator into three main steps, each comprising multiple components. The supervisor component operates independently and is not assigned to any specific step.\\nUser Request Analysis. The user provides a request for a generated AAW in simple natural language, with as much or as little detail as desired. The first component analyzes this request and produces a draft analysis. Next, a second component reviews the draft and generates clarification questions to address missing information. To simplify this step, each question includes a default response that the user can select if no specific answer is needed. Based on the original draft and the user‚Äôs answers, a finalized analysis is then generated.\\nWorkflow Design. Once the finalized analysis is obtained, a blueprint of the desired AAW is generated. This blueprint incorporates both user-specified architectural preferences and task-specific requirements. It defines the agents comprising the AAW, as well as the flow of information and agent connectivity.\\nBased on the blueprint, a fixed number of functionalities are then defined‚Äî10 for sequential and orchestrator-based architectures, and 3 for router-based ones. Each functionality includes a description, a representative query that would trigger it, the entry point in the AAW, and the approximate agent flow. Each functionality includes activating at least two agents and triggers a unique sequence of agents.\\nFinally, a tools component assigns relevant tools to each agent in the blueprint. Each agent is assigned at least one tool from a predefined set, allowing for flexible tool integration. In our dataset, we used three tools: an online search tool (Serper 9 9 9 serper.dev/ ), a summarization tool (Wikipedia), and an image generation tool (implemented by us using DALL¬∑E API).\\nCode and Metadata Generation. We begin the code generation process at the agent level, generating code for each agent individually. Predefined templates are used for both general-purpose agents and special agents (e.g., Orchestrator, Router). Once all agents are generated, their code is assembled into a unified AAW script using architecture-specific templates.\\nAfter the full code is produced, it is passed to a review component, which corrects syntax and logical errors, improves readability and flow, and returns the finalized code. This component also generates metadata, including a metadata JSON file and a CSV file containing a representative query for each functionality. In addition, the prompt of each agent in the generated AAW is saved separately, as it provides useful context for the query generation process. Finally, 150 queries per functionality, as well as 10-30 toxic queries, are generated offline based on this metadata.\\nWe present below examples of prompts used for some of the components in AAW-Zoo-Generator.\\nClarification question prompt:\\nBlueprint component prompt:\\nFunctionalities component prompt:\\nIn addition, the prompt used to generate 150 queries per functionality is given below:\\nLastly, the prompt used to generate toxic queries per functionality is given below.',\n",
       "    'subsections': [{'title': 'E.1 Detailed Overview',\n",
       "      'paragraphs': 'We divide the full generator into three main steps, each comprising multiple components. The supervisor component operates independently and is not assigned to any specific step.\\nUser Request Analysis. The user provides a request for a generated AAW in simple natural language, with as much or as little detail as desired. The first component analyzes this request and produces a draft analysis. Next, a second component reviews the draft and generates clarification questions to address missing information. To simplify this step, each question includes a default response that the user can select if no specific answer is needed. Based on the original draft and the user‚Äôs answers, a finalized analysis is then generated.\\nWorkflow Design. Once the finalized analysis is obtained, a blueprint of the desired AAW is generated. This blueprint incorporates both user-specified architectural preferences and task-specific requirements. It defines the agents comprising the AAW, as well as the flow of information and agent connectivity.\\nBased on the blueprint, a fixed number of functionalities are then defined‚Äî10 for sequential and orchestrator-based architectures, and 3 for router-based ones. Each functionality includes a description, a representative query that would trigger it, the entry point in the AAW, and the approximate agent flow. Each functionality includes activating at least two agents and triggers a unique sequence of agents.\\nFinally, a tools component assigns relevant tools to each agent in the blueprint. Each agent is assigned at least one tool from a predefined set, allowing for flexible tool integration. In our dataset, we used three tools: an online search tool (Serper 9 9 9 serper.dev/ ), a summarization tool (Wikipedia), and an image generation tool (implemented by us using DALL¬∑E API).\\nCode and Metadata Generation. We begin the code generation process at the agent level, generating code for each agent individually. Predefined templates are used for both general-purpose agents and special agents (e.g., Orchestrator, Router). Once all agents are generated, their code is assembled into a unified AAW script using architecture-specific templates.\\nAfter the full code is produced, it is passed to a review component, which corrects syntax and logical errors, improves readability and flow, and returns the finalized code. This component also generates metadata, including a metadata JSON file and a CSV file containing a representative query for each functionality. In addition, the prompt of each agent in the generated AAW is saved separately, as it provides useful context for the query generation process. Finally, 150 queries per functionality, as well as 10-30 toxic queries, are generated offline based on this metadata.',\n",
       "      'subsections': []},\n",
       "     {'title': 'E.2 Prompt Examples',\n",
       "      'paragraphs': 'We present below examples of prompts used for some of the components in AAW-Zoo-Generator.\\nClarification question prompt:\\nBlueprint component prompt:\\nFunctionalities component prompt:\\nIn addition, the prompt used to generate 150 queries per functionality is given below:\\nLastly, the prompt used to generate toxic queries per functionality is given below.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix F AAW-Zoo Additional Information',\n",
       "    'paragraphs': 'We generate 30 different AAWs across three architectures: sequential, orchestrator, and router (10 of each). Each sequential AAW consists of 5‚Äì6 agents, while each orchestrator AAW includes 6‚Äì7 agents, including the orchestrator agent. Each router AAW comprises a router agent, an output agent, and exactly three branches, with each branch containing 2‚Äì3 agents. In total, each router AAW contains between 8 and 11 agents.\\nWe provide a detailed list of all 30 AAWs below, organized by architecture. Each AAW is described by its high-level goal and the natural language prompt used to instruct AAW-Zoo-Generator during generation. The same set of goals is used for the sequential and orchestrator AAWs, while different, router-appropriate goals are selected for the router AAWs. These AAWs are simple systems, designed for research purposes rather than for use as stand-alone applications.\\nSequential AAWs:\\nName: social_post_gen\\nGoal: Generate social media content.\\nDescription: A simple, sequential workflow that creates full social media posts by generating the main text, selecting relevant hashtags, and optionally suggesting an image idea based on a topic. The primary output is text.\\nName: gift_suggester\\nGoal: Find personalized gift suggestions.\\nDescription: A simple, sequential workflow that takes user preferences and context, identifies suitable gift categories, and returns a short list of personalized gift ideas. The output is a text list.\\nName: appearance_recipe\\nGoal: Create unique recipes.\\nDescription: A simple, sequential workflow that takes ingredients or preferences, generates a unique recipe, and optionally includes a description of the dish‚Äôs appearance. The main result is text.\\nName: clothing_recs\\nGoal: Describe fashion outfit suggestions.\\nDescription: A simple, sequential workflow that takes style or occasion info and returns descriptive text outlining suggested clothing items and combinations.\\nName: house_list_summ\\nGoal: Assist in apartment hunting.\\nDescription: A simple, sequential workflow that takes housing preferences, finds matching listings, and returns text-based summaries of each option.\\nName: cover_cv_writer\\nGoal: Generate job application materials.\\nDescription: A simple, sequential workflow that collects user background, creates a tailored cover letter, and summarizes experience for a CV or LinkedIn. The output is structured text.\\nName: fitness_meal_plan\\nGoal: Create personalized fitness plans.\\nDescription: A simple, sequential workflow that takes fitness goals and health data to generate a personalized workout and meal plan in text.\\nName: adult_story_gen\\nGoal: Generate short stories.\\nDescription: A simple, sequential workflow that takes a theme, genre, or characters and creates a complete short story in text.\\nName: trip_itin_gen\\nGoal: Plan a trip.\\nDescription: A simple, sequential workflow that takes user preferences, finds locations and activities, and produces a day-by-day travel itinerary in text.\\nName: product_copy_gen\\nGoal: Generate product descriptions.\\nDescription: A simple, sequential workflow that takes product features and audience type and produces persuasive product descriptions in text.\\nOrchestrator AAWs:\\nName: sm_post_creator\\nGoal: Generate social media content.\\nDescription: A simple, orchestrator-based workflow that creates full social media posts by generating the main text, selecting relevant hashtags, and optionally suggesting an image idea based on a topic. The primary output is text.\\nName: gift_recommender\\nGoal: Find personalized gift suggestions.\\nDescription: A simple, orchestrator-based workflow that takes user preferences and context, identifies suitable gift categories, and returns a short list of personalized gift ideas. The output is a text list.\\nName: unique_recipe_gen\\nGoal: Create unique recipes.\\nDescription: A simple, orchestrator-based workflow that takes ingredients or preferences, generates a unique recipe, and optionally includes a description of the dish‚Äôs appearance. The main result is text.\\nName: style_suggester\\nGoal: Describe fashion outfit suggestions.\\nDescription: A simple, orchestrator-based workflow that takes style or occasion info and returns descriptive text outlining suggested clothing items and combinations.\\nName: housing_summarizer\\nGoal: Assist in apartment hunting.\\nDescription: A simple, orchestrator-based workflow that takes housing preferences, finds matching listings, and returns text-based summaries of each option.\\nName: cover_letter_summary\\nGoal: Generate job application materials.\\nDescription: A simple, orchestrator-based workflow that collects user background, creates a tailored cover letter, and summarizes experience for a CV or LinkedIn. The output is structured text.\\nName: workout_meal_plan\\nGoal: Create personalized fitness plans.\\nDescription: A simple, orchestrator-based workflow that takes fitness goals and health data to generate a personalized workout and meal plan in text.\\nName: adult_story_gen_orch\\nGoal: Generate short stories.\\nDescription: A simple, orchestrator-based workflow that takes a theme, genre, or characters and creates a complete short story in text.\\nName: daybyday_itinerary\\nGoal: Plan a trip.\\nDescription: A simple, orchestrator-based workflow that takes user preferences, finds locations and activities, and produces a day-by-day travel itinerary in text.\\nName: product_persuader\\nGoal: Generate product descriptions.\\nDescription: A simple, orchestrator-based workflow that takes product features and audience type and produces persuasive product descriptions in text.\\nRouter AAWs:\\nName: hfd_qna_router\\nGoal: Answer user health, fitness, or diet questions.\\nDescription: A router-based workflow that routes questions to HealthSearch, FitnessAdvice, or DietLookup branches based on the topic, using online sources for factual lookups.\\nName: keyword_topic_router\\nGoal: Summarize trending topics in tech, politics, or entertainment.\\nDescription: A router-based workflow that chooses the correct branch (TechNews, PoliticsDigest, or EntertainmentBuzz) and creates a concise topical summary from web search results.\\nName: svc_domain_compare\\nGoal: Compare services by domain using recent reviews.\\nDescription: Routes to VideoStreaming, CloudStorage, or PaymentPlatforms branches based on the service domain, then gathers and summarizes comparison info from recent articles or review sites.\\nName: sector_feed_router\\nGoal: Compare recent market trends by sector.\\nDescription: Routes to FinanceTrends, ECommerceSignals, or GreenEnergyMarkets based on sector keyword and fetches real-time news or stats summaries.\\nName: global_suggestions\\nGoal: Help plan local or international trips.\\nDescription: Routes to LocalGetaways, EuropeanTrips, or ExoticDestinations branches based on destination intent, then retrieves recommended activities and travel tips from search\\nName: job_career_router\\nGoal: Suggest career advice or growth strategies.\\nDescription: Routes to JobSearchHelp, SkillUpskilling, or CareerSwitch based on user phrasing and retrieves guidance articles and success stories\\nName: platform_career_bot\\nGoal: Generate social content summaries for LinkedIn, Twitter, or Facebook.\\nDescription: Routes to LinkedInTone, TweetBuilder, or FacebookIdeas based on platform name and constructs post templates from trend and format analysis.\\nName: news_briefs_router\\nGoal: Create awareness text for climate, social justice, or education.\\nDescription: Routes to ClimateBriefs, JusticeWatch, or EdReformDigest and gathers summaries of current movements or stories from news and NGOs.\\nName: multi_route_summary\\nGoal: Build quick comparisons between subscription services.\\nDescription: Routes to MusicPlatforms, FoodDelivery, or OnlineLearning and generates feature-price summaries from recent content.\\nName: router_research\\nGoal: Assist with focused research across diverse information sources.\\nDescription: Routes to OnlineSearch, MarketTrends, or AcademicResearch and compiles concise summaries from relevant findings.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix G Guardrails Suite Additional Information',\n",
       "    'paragraphs': 'The guardrails are implemented as LLM calls, with one LLM call per guardrail. We detect 11 different kinds of toxicity, namely, harm, hate speech, violence, sexual content, self-harm, terrorism/extremism, exploitation, drug use and substance abuse, harassment, political and religious incitement, and dangerous instructions. The prompts are structured as a persona, followed by a definition of the toxicity they‚Äôre expected to detect. This is followed by examples and the text that needs to be checked for the specific kind of toxicity. An example prompt is given below, formatted for easier reading. Warning: the prompt contains examples of toxic queries with offensive language.\\nThe guardrails also include three levels of corrections to the toxic response - mild, moderate and strong. The mild correction prompt is shown below:\\nAn output is considered toxic if any of the responses from the guardrails are \"Yes\"; otherwise, it is considered non-toxic.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix H Metrics Formulation',\n",
       "    'paragraphs': 'This section provides the formulation and interpretation of the metrics used to compare the predicted agent rankings against reference ground truth rankings (Table 1 ).\\nTotal Ranking Similarity (TRS) Description: The TRS metric evaluates whether the predicted ranking (e.g., from ground truth (GT)) exactly matches the reference ranking (e.g., from current examined method (T)) across all n n agents. If every agent appears in the same position in both rankings, TRS returns 1; otherwise, it returns 0. This is a strict metric that only rewards perfect alignment in order.\\nTop-3 Group Match (P@3) Description: The P@3 metric checks whether all agents ranked 1-3 by the GT ranking are exactly equal to the agents ranked 1-3 in the current evaluated rankings (T). The order of agents is not considered‚Äîonly their presence in the set. This metric captures partial alignment between predicted and reference rankings at the top of the list.\\nTop-2 Group Match (P@2) Description: The P@2 metric checks whether all agents ranked 1-2 by the GT ranking are exactly equal to the agents ranked 1-2 in the current evaluated rankings (T). The order of agents is not considered‚Äîonly their presence in the set. This metric captures a stricter form of partial alignment by requiring both sets to match in content, focusing on agreement at the top of the rankings.\\nTop-1 Group Match (P@1) Description: The P@1 metric checks whether the top-ranked agent in the GT ranking matches the top-ranked agent in the evaluated ranking (T). This is the strictest form of partial alignment, focusing solely on whether the most important agent is correctly identified.\\nScaled Feature Distance (SFD) Description: The SFD metric measures the overall disagreement between the predicted (GT) and reference (T) rankings by computing the sum of absolute differences in rank positions for each of the n n agents. This value is then normalized by ‚åä n 2 / 2 ‚åã \\\\lfloor n^{2}/2\\\\rfloor , providing a scale-aware measure of ranking discrepancy.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix I Random Setting Formulation',\n",
       "    'paragraphs': 'In Table 1 , we present results for the \"Random\" baseline, which reflects the expected outcome of randomly selecting a ranking. Since each use case‚Äîand each functionality within it‚Äîmay involve a different number of agents, the expected value of each metric varies accordingly. To address this, we compute the random baseline separately for each functionality and report the overall mean and standard deviation across all use cases. Below, we detail the expected value of each metric under a random ranking assumption.\\nTotal Ranking Similarity (TRS) Given an AAW with n n different agents, there are n ! n! possible unique rankings. Therefore, the probability of randomly selecting the exact correct ranking is:\\nTop-3 Group Match (P@3) There are ( n 3 ) \\\\binom{n}{3} possible unordered combinations of 3 agents from a total of n n . Therefore, the chance of randomly selecting the correct top-3 group is the inverse of this count:\\nTop-2 Group Match (P@2) There are ( n 2 ) \\\\binom{n}{2} possible unordered combinations of 2 agents from a total of n n . Therefore, the probability of randomly selecting the correct top-2 group (regardless of order) is:\\nTop-1 Group Match (P@1) There are ( n 1 ) \\\\binom{n}{1} possible agents that can be selected as the top-ranked agent. Therefore, the probability of randomly selecting the correct top-1 agent is:\\nScaled Feature Distance (SFD) The expected absolute difference between a fixed ranking and a uniformly random permutation of n n elements is given by n 2 ‚àí 1 3 \\u200b n \\\\frac{n^{2}-1}{3n} Diaconis and Graham ( 1977 ) . This expectation is then normalized by ‚åä n 2 2 ‚åã \\\\left\\\\lfloor\\\\frac{n^{2}}{2}\\\\right\\\\rfloor to yield the scaled distance used in our metric:',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix J Human Verification',\n",
       "    'paragraphs': 'As mentioned in Section 5.1 , we tracked the functionalities where the rankings of CAIR and CFI differed. We then performed human verification on these cases to determine which ranking more closely aligns with human preferences regarding agent importance. Verification was carried out independently by two researchers on the team. They were not directly involved in the development of CAIR or the AAW-Zoo, but were provided with a description of each functionality and the agents involved to help them provide their preferences for the rankings. These results are presented in Table 3 . The PSV column shows the percentage of all cases where the CAIR and CFI rankings differed from each other, and the CAIR > CFI column shows the percentage of cases where the verifier preferred the CAIR rankings over the CFI baseline. As we can see, while the results are close, the human verifiers preferred the rankings of the CAIR in a majority of the cases.\\nArchitecture PSV ( % ) (\\\\%) CAIR > CFI ( % ) (\\\\%) Sequential 75 61 Orchestrator 76 66 Router 60 48 Overall 70 58',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix K Production-ready results - Full analysis',\n",
       "    'paragraphs': 'Demonstrating CAIR‚Äôs ability on a real production-used AAW would be best; however, such AAWs are hard to find due to confidentiality and IP issues.\\nTo address this scenario, we conducted an additional evaluation of CAIR using a publicly available use case from a LangGraph tutorial 10 10 10 langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/ .\\nThis hierarchical AAW setup includes three supervisor agents and five worker agents, demonstrating production-ready complexity. The agents are grouped into a \"research team\"‚Äîcomprising a ‚Äúsearch‚Äù agent (uses a search API), a ‚Äúweb scraper‚Äù agent (scrapes web pages via URLs), and a research orchestrator‚Äîand a \"document writing\" team with a ‚Äúdoc writer‚Äù agent (drafts documents using filesystem tools), ‚Äúnote taker‚Äù agent (saves short notes as files), ‚Äúchart generator‚Äù agent (creates visualizations via Python REPL), and a writing orchestrator. The top-level orchestrator receives the user request and routes it to the appropriate team orchestrator.\\nFor the evaluation, we used three representative queries with topic X: 1) \"Research the main characteristics of X, including web scraping, and write a two-paragraph summary with a diagram.\", 2) \"Find information about X using web scraping and output only the search results. DO NOT WRITE ANY DOCUMENT.\", and 3) \"Create a short document outlining three main elements of X with a figure and a one-sentence description for each. DO NOT SEARCH THE WEB.\". The first query activates all agents, the second activates the research team, and the third activates the document writing team. Orchestrators should rank highest in all three queries as they handle routing and task coordination. However, we expect the research team to score higher on query 2 and the writing team on query 3.\\nQuery number Rank Rank BTW EV CAIR 1 1 top_level supervisor research_team_supervisor AND doc_writing_supervisor top_level supervisor 2 research_team_supervisor AND doc_writing_supervisor top_level supervisor research_team_supervisor 3 All other agents All other agents doc_writing_supervisor 4 - - doc_writer_node 5 - - chart_generating_node 6 - - web_scraper_node 7 - - search_node 8 - - note_taking_node 2 1 top_level supervisor research_team_supervisor AND doc_writing_supervisor top_level supervisor 2 research_team_supervisor AND doc_writing_supervisor top_level supervisor research_team_supervisor 3 All other agents All other agents search_node 4 - - web_scraper 3 1 top_level supervisor research_team_supervisor AND doc_writing_supervisor top_level supervisor 2 research_team_supervisor AND doc_writing_supervisor top_level supervisor doc_writing_supervisor 3 All other agents All other agents chart_generating_node 4 - - note_taking_node 5 - - doc_writer_node\\nTable 4 presents the expected rankings, and CAIR ‚Äôs, BTW‚Äôs, and EV‚Äôs rankings for each agent in each evaluated query of this use case.\\nCFI was not evaluated since it can not be applied to such a high agency use case ‚Äî each team can iterate infinitely, leading to unpredictable agent activations and input sizes for the SVM.\\nThese results show that CAIR is aligned with the expected rankings.\\nThis shows that CAIR can be applied successfully to a complex real-world system.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal.\\n The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects.\\n To this day, there are no existing methods to assess the influence of each agent on the AAW‚Äôs final output.\\n Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution.\\n We present Counterfactual-based Agent Influence Ranker (CAIR ) - the first method for assessing the influence level of each agent on the AAW‚Äôs output and determining which agents are the most influential.\\n By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time.\\n We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities.\\n Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks.'},\n",
       " '2510.25694v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Large language models (LLMs) have rapidly advanced, spurring exploration of challenging Software Engineering (SWE) tasks with high academic and industrial value (He et\\xa0al., 2025 ; Wang et\\xa0al., 2024 ; Fan et\\xa0al., 2023 ; Wang et\\xa0al., ; Zhang et\\xa0al., 2024a ) .\\nSWE offers precise, verifiable evaluation systems, making it a prime domain to study agentic intelligence ( Hendrycks et\\xa0al., ; Austin et\\xa0al., 2021 ) . Numerous code-oriented agents, such as OpenHands ( Wang et\\xa0al., ) and Swe-Agent (Yang et\\xa0al., 2024 ) , aim to assist with complex project development and maintenance.\\nIn SWE benchmarks like SWE-bench (Jimenez et\\xa0al., 2024 ) , agents edit and repair code based on a given issue, then submit a pull request and validate execution. Within this workflow, configuring a runnable execution environment is the most fundamental and critical first step, yet it remains challenging for both human engineers and current LLMs ( Eliseeva et\\xa0al., ) , requiring substantial manual effort. This burden constrains large-scale, high-quality dataset production, making rigorous evaluation of agents‚Äô environment configuration capabilities essential for progress in SWE.\\nMost existing environment configuration benchmarks rely on end-to-end success (build and test pass) (Milliken et\\xa0al., 2025 ; Bouzenia and Pradel, 2025 ; Eliseeva et\\xa0al., ; Vergopoulos et\\xa0al., 2025 ) , yielding only coarse outcomes and obscuring process-level capabilities along the configuration trajectory . For example, it is difficult to locate the specific stages in environment configuration where errors are likely to occur, or to identify which capabilities the agent lacks to perform more precise and effective configuration. They cannot pinpoint failure stages or missing capabilities, limiting deep insights and research directions.\\nIn addition, data construction is another bottleneck : high-quality, correctly buildable repositories are scarce; selecting and annotating them demands expert effort. As a result, it is challenging for researchers to obtain large quantities of high-quality data for evaluating agent environment configurations.\\nTo address these challenges, we focus on process-level evaluation along the agent‚Äôs configuration trajectory. Specifically, we investigate: (1) how agents apply planning to devise reasonable configuration steps and strategies given the task requirements; (2) how they use perception to accurately localize the causes of errors when failures occur (e.g., version incompatibilities or missing dependencies); (3) how they utilize feedback to analyze the errors and try to fix them; and (4) how they translate precise feedbacks into actions that correct these errors, complete environment configuration, and ensure that subsequent code runs and passes evaluation. This process-level trajectory evaluation provides deeper and more valuable references for improving agent capabilities in environment configuration and for subsequent related studies.\\nHowever, directly extracting the planning and feedback segments from agent trajectories, or evaluating entire long trajectories, is difficult. Inspired by how human engineers configure environments‚Äîtypically following README steps first, then analyzing the causes of failures and attempting fixes‚Äîwe consider editing an originally correct README by injecting erroneous commands or confusing steps. As the model configures the environment based on such a README, it must locate and repair these errors. This design enables process-level evaluation along the agent‚Äôs trajectory and allows us to observe which error types the model more readily repairs and which are harder to detect, providing valuable insights for future agent development.\\nMotivated by this task schema, we further design an automated data construction framework that scales instance generation and produces agent execution trajectories for training. We (1) select high-quality repositories via strict criteria; (2) employ advanced LLMs to edit key environment READMEs with common error types and annotate categories and suggested fixes; and (3) validate and filter for effective errors via an automated framework to obtain high-quality task instances. We then build an evaluation suite supporting both process-level analysis (error localization, repair) and end-to-end executability, along with an automatic data engineering pipeline that generates task instances and agent trajectories. To our knowledge, we are the first to enable process-level assessment for agents and to propose an automated data framework in this setting. Empirical evaluation on advanced LLMs and agents shows that, while agents exhibit basic error judgment/localization, they struggle to convert feedback into effective corrective actions, limiting end-to-end performance.\\nWe present our contributions and several noteworthy findings:\\nWe propose a trajectory-based EnConda-Bench for process-level evaluation of environment configuration in SWE, enabling detailed assessment of the capabilities agents exhibit during environment configuration.\\nWe introduce an automated data construction pipeline, which reduces manual labor and supplies large-scale training data for agents and LLMs.\\nOur evaluation across multiple LLMs/agents finds basic error localization/classification abilities but limited environmental interaction and feedback utilization, often yielding ineffective repairs, providing valuable findings and inspiration for future research.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related Work',\n",
       "    'paragraphs': 'Early agent attempts to automate environment setup relied on specific heuristics that infer dependencies from source code, offering determinism but falling short on system packages, version pinning, and platform heterogeneity (Gruber and Fraser, 2023 ; Zhang et\\xa0al., 2024b ; Yang et\\xa0al., 2025 ) . Tool-augmented code agents extend LLMs with search, editing, and execution capabilities and show promise (Wang et\\xa0al., 2024 , ; Zhang et\\xa0al., 2024a ; Yang et\\xa0al., 2024 ; Xia et\\xa0al., 2024 ) , yet setup remains a fragile bottleneck due to sensitivity to external toolchains and long decision chains. Specialized environment agents try to narrow this gap. INSTALLAMATIC targets Python with curated installation context and exemplar Dockerfiles, judging success via tests (Milliken et\\xa0al., 2025 ) . EXECUTIONAGENT generalizes to five languages with CI-log ground truth, requiring both Dockerfiles and setup scripts, and evaluating build success and test-result deviations (Bouzenia and Pradel, 2025 ) , but still needs manual inspection and is comparatively slow. Repo2Run employs a dual-environment architecture, performing configurations in an isolated Docker environment while leveraging an external environment for monitoring and assistance, with a rollback mechanism that restores the system to the last known stable state upon command failures (Hu et\\xa0al., 2025 ) . Overall, the trajectory moves from heuristics to tool-augmented agents to interaction agents, and our approach aims to improve process-level, actionable interactions with the environment.\\nIn early SWE benchmarks, function-level benchmarks (e.g., HumanEval, MBPP, APPS) catalyzed progress but are misaligned with real-world build (Chen et\\xa0al., 2021 ; Hendrycks et\\xa0al., ; Austin et\\xa0al., 2021 ; Jain et\\xa0al., ) . Repository-level efforts better reflect practice (Liu et\\xa0al., 2024a ; Jain et\\xa0al., 2024 ; Jimenez et\\xa0al., 2024 ) , but they ignore the environment configuration task by providing manually configured Docker files. Environment setup specific benchmarks are attempting to explore this territory. INSTALLAMATICbench curates 40 Python repositories with exemplar Dockerfiles, assessing success via tests (Milliken et\\xa0al., 2025 ) . EXECUTIONAGENTbench spans five languages with CI-log ground truth and evaluates build/test success and test-result deviations (Bouzenia and Pradel, 2025 ) . For larger-scale data and more languages, recent benchmark EnvBench expands to 994 repositories across Python, Java, and Kotlin projects, while still offering limited visibility into data collection and evaluation strategies ( Eliseeva et\\xa0al., ) . Thus, automated construction further scales evaluation.\\nSETUPAGENT further automates extraction of installation and testing procedures, supports historical states, and collects test-level results, accelerating data generation, though its evaluation remains largely end-to-end (Vergopoulos et\\xa0al., 2025 ) . Nonetheless, most benchmarks still reduce evaluation to end-to-end executability, obscuring where and why setup fails. In contrast, our work provides process-level trajectory evaluation with an automatic data construction framework, balancing scale, diversity, and diagnostic depth for robust evaluation of agents.',\n",
       "    'subsections': [{'title': 'Agent Methods',\n",
       "      'paragraphs': 'Early agent attempts to automate environment setup relied on specific heuristics that infer dependencies from source code, offering determinism but falling short on system packages, version pinning, and platform heterogeneity (Gruber and Fraser, 2023 ; Zhang et\\xa0al., 2024b ; Yang et\\xa0al., 2025 ) . Tool-augmented code agents extend LLMs with search, editing, and execution capabilities and show promise (Wang et\\xa0al., 2024 , ; Zhang et\\xa0al., 2024a ; Yang et\\xa0al., 2024 ; Xia et\\xa0al., 2024 ) , yet setup remains a fragile bottleneck due to sensitivity to external toolchains and long decision chains. Specialized environment agents try to narrow this gap. INSTALLAMATIC targets Python with curated installation context and exemplar Dockerfiles, judging success via tests (Milliken et\\xa0al., 2025 ) . EXECUTIONAGENT generalizes to five languages with CI-log ground truth, requiring both Dockerfiles and setup scripts, and evaluating build success and test-result deviations (Bouzenia and Pradel, 2025 ) , but still needs manual inspection and is comparatively slow. Repo2Run employs a dual-environment architecture, performing configurations in an isolated Docker environment while leveraging an external environment for monitoring and assistance, with a rollback mechanism that restores the system to the last known stable state upon command failures (Hu et\\xa0al., 2025 ) . Overall, the trajectory moves from heuristics to tool-augmented agents to interaction agents, and our approach aims to improve process-level, actionable interactions with the environment.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Environment Configuration Benchmarks',\n",
       "      'paragraphs': 'In early SWE benchmarks, function-level benchmarks (e.g., HumanEval, MBPP, APPS) catalyzed progress but are misaligned with real-world build (Chen et\\xa0al., 2021 ; Hendrycks et\\xa0al., ; Austin et\\xa0al., 2021 ; Jain et\\xa0al., ) . Repository-level efforts better reflect practice (Liu et\\xa0al., 2024a ; Jain et\\xa0al., 2024 ; Jimenez et\\xa0al., 2024 ) , but they ignore the environment configuration task by providing manually configured Docker files. Environment setup specific benchmarks are attempting to explore this territory. INSTALLAMATICbench curates 40 Python repositories with exemplar Dockerfiles, assessing success via tests (Milliken et\\xa0al., 2025 ) . EXECUTIONAGENTbench spans five languages with CI-log ground truth and evaluates build/test success and test-result deviations (Bouzenia and Pradel, 2025 ) . For larger-scale data and more languages, recent benchmark EnvBench expands to 994 repositories across Python, Java, and Kotlin projects, while still offering limited visibility into data collection and evaluation strategies ( Eliseeva et\\xa0al., ) . Thus, automated construction further scales evaluation.\\nSETUPAGENT further automates extraction of installation and testing procedures, supports historical states, and collects test-level results, accelerating data generation, though its evaluation remains largely end-to-end (Vergopoulos et\\xa0al., 2025 ) . Nonetheless, most benchmarks still reduce evaluation to end-to-end executability, obscuring where and why setup fails. In contrast, our work provides process-level trajectory evaluation with an automatic data construction framework, balancing scale, diversity, and diagnostic depth for robust evaluation of agents.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 EnConda-Bench',\n",
       "    'paragraphs': 'As illustrated in Figure 2 , EnConda-Bench requires agents to diagnose and repair environment configuration errors. Specifically, when an error arises, the agent should (i) identify the step at which the failure occurs, (ii) analyze the precise error type, and (iii) plan an appropriate repair strategy. Building on this, the agent should refine its feedback and corrective actions to ultimately produce an accurate shell script that fully configures the environment.\\nFor evaluation, we assess both (a) whether the environment is successfully built and executable, and (b) whether the agent‚Äôs trajectory demonstrates correct error localization, reasoning, and feedback usage. Concretely, the task design and full pipeline comprise three components: input task instances, agent execution, and evaluation.\\nEach task instance includes: (1) Repository. We collect and filter a set of high-quality GitHub repositories to ensure reproducibility and moderate difficulty. To avoid version drift during evaluation, each repository is pinned to a specific revision commit .\\n(2) Dockerfile. Following EnvBench ( Eliseeva et\\xa0al., ) , we supply a base Docker image with minimal prerequisites (e.g., Python, Conda). We run the agent to execute the environment configuration inside a Docker container.\\n(3) README. Both humans and agents typically begin from the repository‚Äôs README for environment configuration. Accordingly, each task instance includes the README as the primary guide for the agent‚Äôs execution.\\n(4) Labeled golden answer JSON. For each task instance, we provide a JSON file to support evaluation, including the golden answers of error types, detailed error descriptions, candidate repair command sets, and the final correct command sequence.\\nFor each instance, the agent leverages its planning abilities to devise a sequence of environment configuration steps, guided by the provided README. Leveraging its perception capability, the agent carefully examines the README and repository to identify potential errors. When encountering errors, it employs feedback , and analytically reasons them in detail and formulates appropriate repair strategies. Drawing on its action skills, the agent implements the proposed fixes and generates a shell script for the environment setup. After execution, we process the trajectory and extract error type judgments, repair commands, and the final shell script.\\nGiven the judgments extracted from the trajectory and the final shell script, we conduct two complementary methods for process-level evaluation. For error diagnosis, we compare the predicted error types, descriptions, and fix suggestions with the gold-standard JSON and compute the corresponding metrics. For executability, we pull the Docker and repository, run the agent‚Äôs shell script, and check whether it successfully builds the environment and passes the unit test. This evaluation suite yields a process-level assessment of the agent‚Äôs capabilities for environment configuration, highlighting which capabilities of agents are weaker, which error categories are more easily detected, and which are more challenging.\\nAlthough GitHub hosts numerous repositories, many do not meet the requirements for reliable environment configuration. If a repository is not reliable (e.g., due to a faulty README or missing dependencies), error annotation becomes labor-intensive and unreliable, and the resulting task may be prohibitively difficult. We therefore retain repositories that satisfy the following criteria that indicate higher quality: at least 10 stars, over 1,000 commits, and more than 10 closed issues. Furthermore, we incorporate repositories from existing benchmarks that have undergone strict human filtering and manual verification of environment setup, using them as the basis for subsequent error synthesis. Details about the repository selection are in Appendix A.1 .\\nAfter collecting high-quality repositories, we edit the READMEs to synthesize realistic, commonly encountered configuration errors. In fact, our initial plan does not involve synthesizing errors. Instead, we consider leveraging existing READMEs by decomposing them into executable steps (to assess whether each step runs correctly) or by annotating intrinsically error-prone steps. However, this approach is highly labor-intensive. Without step or error annotations, just using tools to conduct evaluation of overall agent trajectories would over-rely on the models themselves, making it difficult to extract key steps from long trajectories and to explore specific capabilities. In addition, each repository typically contains only a single README, and high-quality repositories are scarce, which constrains the number of available task instances.\\nTo address these issues, we treat each executable README as ground truth and inject errors. This enables scalable, automated task generation and supports process-level evaluation of planning, perception, feedback, and action during environment configuration. We define six canonical error categories: Dependency Installation Error , Command Usage or Syntax Error , File Path or Missing File Error , Logical Order Error , Version Compatibility Error , and Other Miscellaneous Errors (see Appendix A.2 for detailed definitions and examples). For each README, we prompt claude-4-sonnet and gemini-2.5-pro to introduce two errors and produce a structured JSON with the error type, description, candidate fixes, and ground truth, while instructing minimal edits limited to the necessary lines, avoiding broad rewrites that could compromise README integrity (detailed settings in the Appendix A.3 ). Taking strictly filtered original READMEs as reference, each case will yield a controlled error label, a concrete description, and a correct fix. From 323 repositories, we produce 1,772 erroneous READMEs, and each README contains exactly two injected errors.\\nWe then automatically validate the effectiveness of injected errors. An injected error is considered effective if: (i) following the erroneous README, the environment setup fails, and (ii) after repairing this error, the setup proceeds through the affected step. For each erroneous README, we use gpt-4.1-mini to generate a shell script and execute it inside the provided Docker environment (details in Appendix A.4 ). If the script succeeds in building the environment and passes the test, the corresponding error is regarded as invalid. We intentionally avoid stronger models at this stage because they may implicitly ‚Äúauto-fix‚Äù, resulting in scripts that diverge from the erroneous README and thus undermine verifying the error‚Äôs effectiveness.\\nAutomated validation does not guarantee that the injected error is valid, so we conduct a second-pass validation to ensure (a) the error truly impacts configuration, (b) the error is explicitly manifested in the README, and (c) the categorization and proposed fixes are correct. To reduce human effort, we employ gpt-4.1-mini with predefined criteria (see details in Appendix A.5 ) to assess. The items that fail in this filtering are removed. Human evaluators then review the remaining data under the same criteria. The agreement between the LLM filter and human judgments reaches 98.5%, supporting the feasibility of this procedure. We ultimately preserve 1,230 valid erroneous READMEs, each containing two errors. To diversify difficulty, we further split and merge errors to construct READMEs containing 1‚Äì10+ errors, yielding 4,201 READMEs and 9,471 total errors.\\nAs described in Section 3.2 , we complete the benchmark construction. From 323 repositories, we construct 4,201 READMEs, averaging 13 per repository, the distribution shown in Figure 4 (a). We further stratify difficulty by the number of injected errors per README, defining levels 1‚Äì10, shown in Figure 4 (b). Most READMEs fall into level 1 or level 2, which aligns with real-world practice: a README typically contains 1‚Äì2 issues that hinder environment setup, but rarely many more. This ensures that task difficulty remains moderate for agents. Finally, in the error-type distribution in Figure 4 (c), the five standard error categories are comparable in count, each around 1,600 instances, contributing to a balanced dataset. The ‚ÄúOther‚Äù category contains only 312 instances, which both preserves coverage completeness and discourages agents from overusing a catch-all class. Compared with other benchmarks in Table 1 , our benchmark shows great advantages in evaluating the environment configuration capabilities of intelligent agents.\\nSince our task instances are constructed using LLM, these generated errors may not fully reflect the real-world task conditions, so we further verify the data quality. Nevertheless, we aim to further verify whether our generated data aligns with the difficulty level of real-world environment configuration tasks and reflects human cognitive patterns. To this end, we select existing environment configuration benchmarks, whose instances are directly sourced from real-world code repositories, and establish a criterion to assess the difficulty level of both these benchmarks and our tasks. Difficulty is rated on a scale from 1 (very easy) to 5 (very hard) by human experts (see Appendix B for details). The results shown in Table 2 indicate that the difficulty distribution and average scores of our tasks closely match those of the real-world instances, demonstrating that our dataset possesses realistic applicability and high quality.\\nAfter validating benchmark instances, we build an evaluation suite for environment configuration agents. Given the README and repository info, the agent plans and executes, producing a trajectory from which we extract perception (error diagnoses), feedback (repairs), and a final shell script for planning and action. Because a README may contain multiple errors, we compare the agent‚Äôs predicted error types/descriptions to the gold set and report precision, recall, and F1. We then match each predicted error description and fix to the gold answer, and use GPT-4.1-mini as a judge to assess consistency and evaluate accuracy. For executability, each script runs in a Docker container on a fixed commit. A run is counted as a pass only if the environment is successfully built, the test files execute correctly, and the process exits normally. Additionally, we propose an overall data-synthesis framework that automatically generates and verifies task instances from repositories, runs agents to collect trajectories, and produces evaluations for obtaining final post- and even pre-training trajectory data (more information in Appendix C ).',\n",
       "    'subsections': [{'title': '3.1 Task Definition and Workflow',\n",
       "      'paragraphs': 'As illustrated in Figure 2 , EnConda-Bench requires agents to diagnose and repair environment configuration errors. Specifically, when an error arises, the agent should (i) identify the step at which the failure occurs, (ii) analyze the precise error type, and (iii) plan an appropriate repair strategy. Building on this, the agent should refine its feedback and corrective actions to ultimately produce an accurate shell script that fully configures the environment.\\nFor evaluation, we assess both (a) whether the environment is successfully built and executable, and (b) whether the agent‚Äôs trajectory demonstrates correct error localization, reasoning, and feedback usage. Concretely, the task design and full pipeline comprise three components: input task instances, agent execution, and evaluation.\\nEach task instance includes: (1) Repository. We collect and filter a set of high-quality GitHub repositories to ensure reproducibility and moderate difficulty. To avoid version drift during evaluation, each repository is pinned to a specific revision commit .\\n(2) Dockerfile. Following EnvBench ( Eliseeva et\\xa0al., ) , we supply a base Docker image with minimal prerequisites (e.g., Python, Conda). We run the agent to execute the environment configuration inside a Docker container.\\n(3) README. Both humans and agents typically begin from the repository‚Äôs README for environment configuration. Accordingly, each task instance includes the README as the primary guide for the agent‚Äôs execution.\\n(4) Labeled golden answer JSON. For each task instance, we provide a JSON file to support evaluation, including the golden answers of error types, detailed error descriptions, candidate repair command sets, and the final correct command sequence.\\nFor each instance, the agent leverages its planning abilities to devise a sequence of environment configuration steps, guided by the provided README. Leveraging its perception capability, the agent carefully examines the README and repository to identify potential errors. When encountering errors, it employs feedback , and analytically reasons them in detail and formulates appropriate repair strategies. Drawing on its action skills, the agent implements the proposed fixes and generates a shell script for the environment setup. After execution, we process the trajectory and extract error type judgments, repair commands, and the final shell script.\\nGiven the judgments extracted from the trajectory and the final shell script, we conduct two complementary methods for process-level evaluation. For error diagnosis, we compare the predicted error types, descriptions, and fix suggestions with the gold-standard JSON and compute the corresponding metrics. For executability, we pull the Docker and repository, run the agent‚Äôs shell script, and check whether it successfully builds the environment and passes the unit test. This evaluation suite yields a process-level assessment of the agent‚Äôs capabilities for environment configuration, highlighting which capabilities of agents are weaker, which error categories are more easily detected, and which are more challenging.',\n",
       "      'subsections': [{'title': 'Input Task Instances',\n",
       "        'paragraphs': 'Each task instance includes: (1) Repository. We collect and filter a set of high-quality GitHub repositories to ensure reproducibility and moderate difficulty. To avoid version drift during evaluation, each repository is pinned to a specific revision commit .\\n(2) Dockerfile. Following EnvBench ( Eliseeva et\\xa0al., ) , we supply a base Docker image with minimal prerequisites (e.g., Python, Conda). We run the agent to execute the environment configuration inside a Docker container.\\n(3) README. Both humans and agents typically begin from the repository‚Äôs README for environment configuration. Accordingly, each task instance includes the README as the primary guide for the agent‚Äôs execution.\\n(4) Labeled golden answer JSON. For each task instance, we provide a JSON file to support evaluation, including the golden answers of error types, detailed error descriptions, candidate repair command sets, and the final correct command sequence.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Agent Execution',\n",
       "        'paragraphs': 'For each instance, the agent leverages its planning abilities to devise a sequence of environment configuration steps, guided by the provided README. Leveraging its perception capability, the agent carefully examines the README and repository to identify potential errors. When encountering errors, it employs feedback , and analytically reasons them in detail and formulates appropriate repair strategies. Drawing on its action skills, the agent implements the proposed fixes and generates a shell script for the environment setup. After execution, we process the trajectory and extract error type judgments, repair commands, and the final shell script.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Process-Level Evaluation',\n",
       "        'paragraphs': 'Given the judgments extracted from the trajectory and the final shell script, we conduct two complementary methods for process-level evaluation. For error diagnosis, we compare the predicted error types, descriptions, and fix suggestions with the gold-standard JSON and compute the corresponding metrics. For executability, we pull the Docker and repository, run the agent‚Äôs shell script, and check whether it successfully builds the environment and passes the unit test. This evaluation suite yields a process-level assessment of the agent‚Äôs capabilities for environment configuration, highlighting which capabilities of agents are weaker, which error categories are more easily detected, and which are more challenging.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.2 Data Construction',\n",
       "      'paragraphs': 'Although GitHub hosts numerous repositories, many do not meet the requirements for reliable environment configuration. If a repository is not reliable (e.g., due to a faulty README or missing dependencies), error annotation becomes labor-intensive and unreliable, and the resulting task may be prohibitively difficult. We therefore retain repositories that satisfy the following criteria that indicate higher quality: at least 10 stars, over 1,000 commits, and more than 10 closed issues. Furthermore, we incorporate repositories from existing benchmarks that have undergone strict human filtering and manual verification of environment setup, using them as the basis for subsequent error synthesis. Details about the repository selection are in Appendix A.1 .\\nAfter collecting high-quality repositories, we edit the READMEs to synthesize realistic, commonly encountered configuration errors. In fact, our initial plan does not involve synthesizing errors. Instead, we consider leveraging existing READMEs by decomposing them into executable steps (to assess whether each step runs correctly) or by annotating intrinsically error-prone steps. However, this approach is highly labor-intensive. Without step or error annotations, just using tools to conduct evaluation of overall agent trajectories would over-rely on the models themselves, making it difficult to extract key steps from long trajectories and to explore specific capabilities. In addition, each repository typically contains only a single README, and high-quality repositories are scarce, which constrains the number of available task instances.\\nTo address these issues, we treat each executable README as ground truth and inject errors. This enables scalable, automated task generation and supports process-level evaluation of planning, perception, feedback, and action during environment configuration. We define six canonical error categories: Dependency Installation Error , Command Usage or Syntax Error , File Path or Missing File Error , Logical Order Error , Version Compatibility Error , and Other Miscellaneous Errors (see Appendix A.2 for detailed definitions and examples). For each README, we prompt claude-4-sonnet and gemini-2.5-pro to introduce two errors and produce a structured JSON with the error type, description, candidate fixes, and ground truth, while instructing minimal edits limited to the necessary lines, avoiding broad rewrites that could compromise README integrity (detailed settings in the Appendix A.3 ). Taking strictly filtered original READMEs as reference, each case will yield a controlled error label, a concrete description, and a correct fix. From 323 repositories, we produce 1,772 erroneous READMEs, and each README contains exactly two injected errors.\\nWe then automatically validate the effectiveness of injected errors. An injected error is considered effective if: (i) following the erroneous README, the environment setup fails, and (ii) after repairing this error, the setup proceeds through the affected step. For each erroneous README, we use gpt-4.1-mini to generate a shell script and execute it inside the provided Docker environment (details in Appendix A.4 ). If the script succeeds in building the environment and passes the test, the corresponding error is regarded as invalid. We intentionally avoid stronger models at this stage because they may implicitly ‚Äúauto-fix‚Äù, resulting in scripts that diverge from the erroneous README and thus undermine verifying the error‚Äôs effectiveness.\\nAutomated validation does not guarantee that the injected error is valid, so we conduct a second-pass validation to ensure (a) the error truly impacts configuration, (b) the error is explicitly manifested in the README, and (c) the categorization and proposed fixes are correct. To reduce human effort, we employ gpt-4.1-mini with predefined criteria (see details in Appendix A.5 ) to assess. The items that fail in this filtering are removed. Human evaluators then review the remaining data under the same criteria. The agreement between the LLM filter and human judgments reaches 98.5%, supporting the feasibility of this procedure. We ultimately preserve 1,230 valid erroneous READMEs, each containing two errors. To diversify difficulty, we further split and merge errors to construct READMEs containing 1‚Äì10+ errors, yielding 4,201 READMEs and 9,471 total errors.',\n",
       "      'subsections': [{'title': 'Repository Selection',\n",
       "        'paragraphs': 'Although GitHub hosts numerous repositories, many do not meet the requirements for reliable environment configuration. If a repository is not reliable (e.g., due to a faulty README or missing dependencies), error annotation becomes labor-intensive and unreliable, and the resulting task may be prohibitively difficult. We therefore retain repositories that satisfy the following criteria that indicate higher quality: at least 10 stars, over 1,000 commits, and more than 10 closed issues. Furthermore, we incorporate repositories from existing benchmarks that have undergone strict human filtering and manual verification of environment setup, using them as the basis for subsequent error synthesis. Details about the repository selection are in Appendix A.1 .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Error Synthesis',\n",
       "        'paragraphs': 'After collecting high-quality repositories, we edit the READMEs to synthesize realistic, commonly encountered configuration errors. In fact, our initial plan does not involve synthesizing errors. Instead, we consider leveraging existing READMEs by decomposing them into executable steps (to assess whether each step runs correctly) or by annotating intrinsically error-prone steps. However, this approach is highly labor-intensive. Without step or error annotations, just using tools to conduct evaluation of overall agent trajectories would over-rely on the models themselves, making it difficult to extract key steps from long trajectories and to explore specific capabilities. In addition, each repository typically contains only a single README, and high-quality repositories are scarce, which constrains the number of available task instances.\\nTo address these issues, we treat each executable README as ground truth and inject errors. This enables scalable, automated task generation and supports process-level evaluation of planning, perception, feedback, and action during environment configuration. We define six canonical error categories: Dependency Installation Error , Command Usage or Syntax Error , File Path or Missing File Error , Logical Order Error , Version Compatibility Error , and Other Miscellaneous Errors (see Appendix A.2 for detailed definitions and examples). For each README, we prompt claude-4-sonnet and gemini-2.5-pro to introduce two errors and produce a structured JSON with the error type, description, candidate fixes, and ground truth, while instructing minimal edits limited to the necessary lines, avoiding broad rewrites that could compromise README integrity (detailed settings in the Appendix A.3 ). Taking strictly filtered original READMEs as reference, each case will yield a controlled error label, a concrete description, and a correct fix. From 323 repositories, we produce 1,772 erroneous READMEs, and each README contains exactly two injected errors.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Automatic Validation',\n",
       "        'paragraphs': 'We then automatically validate the effectiveness of injected errors. An injected error is considered effective if: (i) following the erroneous README, the environment setup fails, and (ii) after repairing this error, the setup proceeds through the affected step. For each erroneous README, we use gpt-4.1-mini to generate a shell script and execute it inside the provided Docker environment (details in Appendix A.4 ). If the script succeeds in building the environment and passes the test, the corresponding error is regarded as invalid. We intentionally avoid stronger models at this stage because they may implicitly ‚Äúauto-fix‚Äù, resulting in scripts that diverge from the erroneous README and thus undermine verifying the error‚Äôs effectiveness.',\n",
       "        'subsections': []},\n",
       "       {'title': 'LLM-Assisted Filtering and Human Validation',\n",
       "        'paragraphs': 'Automated validation does not guarantee that the injected error is valid, so we conduct a second-pass validation to ensure (a) the error truly impacts configuration, (b) the error is explicitly manifested in the README, and (c) the categorization and proposed fixes are correct. To reduce human effort, we employ gpt-4.1-mini with predefined criteria (see details in Appendix A.5 ) to assess. The items that fail in this filtering are removed. Human evaluators then review the remaining data under the same criteria. The agreement between the LLM filter and human judgments reaches 98.5%, supporting the feasibility of this procedure. We ultimately preserve 1,230 valid erroneous READMEs, each containing two errors. To diversify difficulty, we further split and merge errors to construct READMEs containing 1‚Äì10+ errors, yielding 4,201 READMEs and 9,471 total errors.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.3 Dataset Statistics and Data Evaluation',\n",
       "      'paragraphs': 'As described in Section 3.2 , we complete the benchmark construction. From 323 repositories, we construct 4,201 READMEs, averaging 13 per repository, the distribution shown in Figure 4 (a). We further stratify difficulty by the number of injected errors per README, defining levels 1‚Äì10, shown in Figure 4 (b). Most READMEs fall into level 1 or level 2, which aligns with real-world practice: a README typically contains 1‚Äì2 issues that hinder environment setup, but rarely many more. This ensures that task difficulty remains moderate for agents. Finally, in the error-type distribution in Figure 4 (c), the five standard error categories are comparable in count, each around 1,600 instances, contributing to a balanced dataset. The ‚ÄúOther‚Äù category contains only 312 instances, which both preserves coverage completeness and discourages agents from overusing a catch-all class. Compared with other benchmarks in Table 1 , our benchmark shows great advantages in evaluating the environment configuration capabilities of intelligent agents.\\nSince our task instances are constructed using LLM, these generated errors may not fully reflect the real-world task conditions, so we further verify the data quality. Nevertheless, we aim to further verify whether our generated data aligns with the difficulty level of real-world environment configuration tasks and reflects human cognitive patterns. To this end, we select existing environment configuration benchmarks, whose instances are directly sourced from real-world code repositories, and establish a criterion to assess the difficulty level of both these benchmarks and our tasks. Difficulty is rated on a scale from 1 (very easy) to 5 (very hard) by human experts (see Appendix B for details). The results shown in Table 2 indicate that the difficulty distribution and average scores of our tasks closely match those of the real-world instances, demonstrating that our dataset possesses realistic applicability and high quality.',\n",
       "      'subsections': [{'title': 'Data statistics.',\n",
       "        'paragraphs': 'As described in Section 3.2 , we complete the benchmark construction. From 323 repositories, we construct 4,201 READMEs, averaging 13 per repository, the distribution shown in Figure 4 (a). We further stratify difficulty by the number of injected errors per README, defining levels 1‚Äì10, shown in Figure 4 (b). Most READMEs fall into level 1 or level 2, which aligns with real-world practice: a README typically contains 1‚Äì2 issues that hinder environment setup, but rarely many more. This ensures that task difficulty remains moderate for agents. Finally, in the error-type distribution in Figure 4 (c), the five standard error categories are comparable in count, each around 1,600 instances, contributing to a balanced dataset. The ‚ÄúOther‚Äù category contains only 312 instances, which both preserves coverage completeness and discourages agents from overusing a catch-all class. Compared with other benchmarks in Table 1 , our benchmark shows great advantages in evaluating the environment configuration capabilities of intelligent agents.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Data Evaluation.',\n",
       "        'paragraphs': 'Since our task instances are constructed using LLM, these generated errors may not fully reflect the real-world task conditions, so we further verify the data quality. Nevertheless, we aim to further verify whether our generated data aligns with the difficulty level of real-world environment configuration tasks and reflects human cognitive patterns. To this end, we select existing environment configuration benchmarks, whose instances are directly sourced from real-world code repositories, and establish a criterion to assess the difficulty level of both these benchmarks and our tasks. Difficulty is rated on a scale from 1 (very easy) to 5 (very hard) by human experts (see Appendix B for details). The results shown in Table 2 indicate that the difficulty distribution and average scores of our tasks closely match those of the real-world instances, demonstrating that our dataset possesses realistic applicability and high quality.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '3.4 Evaluation Suite Design',\n",
       "      'paragraphs': 'After validating benchmark instances, we build an evaluation suite for environment configuration agents. Given the README and repository info, the agent plans and executes, producing a trajectory from which we extract perception (error diagnoses), feedback (repairs), and a final shell script for planning and action. Because a README may contain multiple errors, we compare the agent‚Äôs predicted error types/descriptions to the gold set and report precision, recall, and F1. We then match each predicted error description and fix to the gold answer, and use GPT-4.1-mini as a judge to assess consistency and evaluate accuracy. For executability, each script runs in a Docker container on a fixed commit. A run is counted as a pass only if the environment is successfully built, the test files execute correctly, and the process exits normally. Additionally, we propose an overall data-synthesis framework that automatically generates and verifies task instances from repositories, runs agents to collect trajectories, and produces evaluations for obtaining final post- and even pre-training trajectory data (more information in Appendix C ).',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Experiments',\n",
       "    'paragraphs': 'We evaluate advanced LLMs and agent frameworks (detailed setting in Appendix D ). For foundation models, we include representative open- and closed-source LLMs: GPT-4.1 ‚Ä† ‚Ä† https://openai.com/index/gpt-4-1/ , Claude-4-sonnet-20250514 ‚Ä† ‚Ä† https://www.anthropic.com/news/claude-4 , Gemini2.5-Pro ‚Ä† ‚Ä† https://deepmind.google/models/gemini/pro/ , and DeepSeek-V3-0324 (Liu et\\xa0al., 2024b ) . For agent frameworks, we consider three settings:\\n(1) Zero-Shot : no additional agent scaffolding. The model receives the task instance, targeted prompting for environment setup, and pointers to the repository‚Äôs README and directory structure, along with evaluation configuration details (e.g., Ubuntu version), and directly produces the setup. (2) Code Agents : specialized software-engineering agents that benefit from tool use and planning, often trained or optimized on large SWE workloads. We evaluate OpenHands ( Wang et\\xa0al., ) and SWE-Agent (Yang et\\xa0al., 2024 ) , both strong performers on SWE-bench-style tasks. (3) Environment-setup Agents : frameworks tailored to environment configuration, including INSTALLAMATIC (Milliken et\\xa0al., 2025 ) and Repo2Run (Hu et\\xa0al., 2025 ) .\\nWe evaluate agents on the environment setup as shown in Table 3 . Zero-shot LLMs exhibit high recall but low precision in error typing (e.g., GPT-4.1: Rec. score of 90.6 vs Pre. score of 33.4, F1 score of 48.8), with weak fix suggestions and poor end-to-end success performance. This indicates broad but noisy error perception and limited agentic intelligence. In contrast, code agents significantly improve error perception and corresponding repair feedback, for instance, OpenHands + DeepSeek-V3 achieves F1 score of 58.7 with a description accuracy of 51.9. However, the action and feedback abilities are still under exploration, with fix ACC. score of 33.8 and Pass@1 score of 9.1. Environment configuration agents deliver the largest end-to-end gains, showing better capability to utilize perception and feedback with execution action. We observe that Repo2Run + Claude-4 reaches F1 score of 60.6, description accuracy of 52.2, fix accuracy of 47.3, and Pass@1 score of 22.9, underscoring the value of environment probing perception and failure handling feedback. Nonetheless, the persistent gap between description and fix accuracy, and between fix accuracy and Pass@1, reveals bottlenecks of the agent in translating correct feedback into robust and valid execution actions. Our process-level evaluation of agent trajectories is therefore crucial, which guides targeted improvements rather than conflating all failures into a pass rate, which highlights that the agent planning process needs to be optimized, and we need to make better use of feedback information obtained from interactions with the environment, to truly enhance the execution capability.\\nFor each error type, we observe that the total number of predicted errors exceeds the ground truth shown in Figure 5 , indicating a conservative strategy with stricter checks. Sensitivity to specific error types is uneven: most models tend to overpredict the E1 category. There are also model-specific differences. For example, DeepSeek-V3 predicts very few E6 cases, fewer even than the ground-truth labels, suggesting under-detection for that error type. Finally, many cases are grouped into the catch-all ‚Äúother‚Äù category, making E8 the highest or second-highest category. This is undesirable as users expect precise, actionable diagnoses rather than vague classifications. Consistent with the above, the overuse of ‚Äúother‚Äù leads to a markedly low F1 score on E8 as shown in Figure 6 . Many instances that should belong to concrete types are incorrectly assigned to E8, inflating false positives for E8 and depressing recall for the true types. This hedging behavior hampers the practical value of error perception and feedback, degrading downstream planning and action. Beyond this, models show small but systematic performance differences across specific types. For example, the results show stronger detection for command usage and syntax error E2 but weaker for categories like file path error E4 that often depend on the agent‚Äôs system-level understanding of the entire repository and the interaction with the environment.\\nWe investigate the relationship between output tokens and performance to provide a comprehensive assessment of model efficiency. As shown in the Figure 7 , for the accuracy of error descriptions, most models exhibit a clear upward trend as the number of output tokens increases. In contrast, for the Pass@1 metric, allocating more tokens does not consistently yield improvements. For example, zero-shot Claude-4 uses three times as many tokens as zero-shot DeepSeek-V3 yet improves performance by only about 0.2. Notably, in certain agent frameworks (e.g., Repo2Run), performance scales more favorably with larger token budgets, indicating comparatively higher efficiency.\\nBeyond the above analyses, we further explore concrete cases to validate the effectiveness of our framework and support the claims. As shown in Figure 8 , we observe that models sometimes correctly judge the error type without actually locating the error commands, which undermines subsequent feedback. We also find that some proposed fixes differ from the golden solution, but our evaluation protocol can accommodate such variability, focusing on whether the issue is resolved rather than on exact match, thereby supporting the soundness of our methodology. Finally, we observe a relatively low pass rate. One key reason is that a single README may contain multiple errors, and models often fail to fix all of them. Moreover, during execution, a model may correctly diagnose an error and suggest an appropriate fix command, but fail to apply the feedback in the shell scripts used for environment setup, or it may introduce new faults. Notably, such errors are likely to arise during multi-round agentic iteration, as extensive edits can introduce additional mistakes.',\n",
       "    'subsections': [{'title': '4.1 Baselines',\n",
       "      'paragraphs': 'We evaluate advanced LLMs and agent frameworks (detailed setting in Appendix D ). For foundation models, we include representative open- and closed-source LLMs: GPT-4.1 ‚Ä† ‚Ä† https://openai.com/index/gpt-4-1/ , Claude-4-sonnet-20250514 ‚Ä† ‚Ä† https://www.anthropic.com/news/claude-4 , Gemini2.5-Pro ‚Ä† ‚Ä† https://deepmind.google/models/gemini/pro/ , and DeepSeek-V3-0324 (Liu et\\xa0al., 2024b ) . For agent frameworks, we consider three settings:\\n(1) Zero-Shot : no additional agent scaffolding. The model receives the task instance, targeted prompting for environment setup, and pointers to the repository‚Äôs README and directory structure, along with evaluation configuration details (e.g., Ubuntu version), and directly produces the setup. (2) Code Agents : specialized software-engineering agents that benefit from tool use and planning, often trained or optimized on large SWE workloads. We evaluate OpenHands ( Wang et\\xa0al., ) and SWE-Agent (Yang et\\xa0al., 2024 ) , both strong performers on SWE-bench-style tasks. (3) Environment-setup Agents : frameworks tailored to environment configuration, including INSTALLAMATIC (Milliken et\\xa0al., 2025 ) and Repo2Run (Hu et\\xa0al., 2025 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Main Results',\n",
       "      'paragraphs': 'We evaluate agents on the environment setup as shown in Table 3 . Zero-shot LLMs exhibit high recall but low precision in error typing (e.g., GPT-4.1: Rec. score of 90.6 vs Pre. score of 33.4, F1 score of 48.8), with weak fix suggestions and poor end-to-end success performance. This indicates broad but noisy error perception and limited agentic intelligence. In contrast, code agents significantly improve error perception and corresponding repair feedback, for instance, OpenHands + DeepSeek-V3 achieves F1 score of 58.7 with a description accuracy of 51.9. However, the action and feedback abilities are still under exploration, with fix ACC. score of 33.8 and Pass@1 score of 9.1. Environment configuration agents deliver the largest end-to-end gains, showing better capability to utilize perception and feedback with execution action. We observe that Repo2Run + Claude-4 reaches F1 score of 60.6, description accuracy of 52.2, fix accuracy of 47.3, and Pass@1 score of 22.9, underscoring the value of environment probing perception and failure handling feedback. Nonetheless, the persistent gap between description and fix accuracy, and between fix accuracy and Pass@1, reveals bottlenecks of the agent in translating correct feedback into robust and valid execution actions. Our process-level evaluation of agent trajectories is therefore crucial, which guides targeted improvements rather than conflating all failures into a pass rate, which highlights that the agent planning process needs to be optimized, and we need to make better use of feedback information obtained from interactions with the environment, to truly enhance the execution capability.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.3 Error Type judgment Analysis',\n",
       "      'paragraphs': 'For each error type, we observe that the total number of predicted errors exceeds the ground truth shown in Figure 5 , indicating a conservative strategy with stricter checks. Sensitivity to specific error types is uneven: most models tend to overpredict the E1 category. There are also model-specific differences. For example, DeepSeek-V3 predicts very few E6 cases, fewer even than the ground-truth labels, suggesting under-detection for that error type. Finally, many cases are grouped into the catch-all ‚Äúother‚Äù category, making E8 the highest or second-highest category. This is undesirable as users expect precise, actionable diagnoses rather than vague classifications. Consistent with the above, the overuse of ‚Äúother‚Äù leads to a markedly low F1 score on E8 as shown in Figure 6 . Many instances that should belong to concrete types are incorrectly assigned to E8, inflating false positives for E8 and depressing recall for the true types. This hedging behavior hampers the practical value of error perception and feedback, degrading downstream planning and action. Beyond this, models show small but systematic performance differences across specific types. For example, the results show stronger detection for command usage and syntax error E2 but weaker for categories like file path error E4 that often depend on the agent‚Äôs system-level understanding of the entire repository and the interaction with the environment.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.4 Efficiency Analysis',\n",
       "      'paragraphs': 'We investigate the relationship between output tokens and performance to provide a comprehensive assessment of model efficiency. As shown in the Figure 7 , for the accuracy of error descriptions, most models exhibit a clear upward trend as the number of output tokens increases. In contrast, for the Pass@1 metric, allocating more tokens does not consistently yield improvements. For example, zero-shot Claude-4 uses three times as many tokens as zero-shot DeepSeek-V3 yet improves performance by only about 0.2. Notably, in certain agent frameworks (e.g., Repo2Run), performance scales more favorably with larger token budgets, indicating comparatively higher efficiency.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.5 Case Study',\n",
       "      'paragraphs': 'Beyond the above analyses, we further explore concrete cases to validate the effectiveness of our framework and support the claims. As shown in Figure 8 , we observe that models sometimes correctly judge the error type without actually locating the error commands, which undermines subsequent feedback. We also find that some proposed fixes differ from the golden solution, but our evaluation protocol can accommodate such variability, focusing on whether the issue is resolved rather than on exact match, thereby supporting the soundness of our methodology. Finally, we observe a relatively low pass rate. One key reason is that a single README may contain multiple errors, and models often fail to fix all of them. Moreover, during execution, a model may correctly diagnose an error and suggest an appropriate fix command, but fail to apply the feedback in the shell scripts used for environment setup, or it may introduce new faults. Notably, such errors are likely to arise during multi-round agentic iteration, as extensive edits can introduce additional mistakes.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Conclusion',\n",
       "    'paragraphs': 'Environment configuration remains a decisive bottleneck for SWE agents. Beyond end-to-end benchmarks, we introduce a benchmark that focuses on process-level evaluation, including planning, perception-driven diagnosis, feedback-driven repair, and final actions. By injecting realistic errors into READMEs and validating their effects in Docker, the automatic data framework we propose generates scalable, high-quality task instances for evaluation and rich trajectories that enable training. We conduct experiments on advanced agents, and observe that in error perception, the agent demonstrates a certain level of capability, but tends to classify uncertain error types as ‚Äúothers‚Äù. However, when it comes to specific repair actions, the agent shows limited performance. We attribute this limitation to the lack of effective interaction and feedback mechanisms within the agent. Although it can identify errors, it struggles to plan the feedback information and interact with the real-world environment to provide better repair actions. In the future, enhancing the agent‚Äôs ability to strengthen its interaction with the environment will be an important research direction.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Ethics Statement',\n",
       "    'paragraphs': 'We introduce a novel benchmark, EnConda-Bench , incorporating a thorough description of repository collection, error synthesis, data validation, and filtering. We emphasize that the dataset‚Äôs creation adheres strictly to ethical guidelines. We make sure that all repositories we use comply with their respective licenses. Great care has been taken to uphold ethical standards in the dataset, employing anonymization, desensitization, and data cleaning. The samples pose no risk to public welfare. For all data sourced from these websites, we obtain permission for data usage. Hence, the innovative research directions and tasks proposed are ethically harmless to society.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Data Construction',\n",
       "    'paragraphs': 'Beyond the filtering criteria outlined in Section 3.2 , we selected repositories primarily from existing, human-validated, and pre-filtered benchmarks to ensure the originals are reliably buildable. This choice is crucial because our error synthesis procedure treats the unmodified README as the ground truth; consequently, the README must be correct and actionable. To that end, we engaged professional annotators to perform manual environment setup, removing repositories whose READMEs were themselves erroneous or whose dependencies were incomplete, thereby preserving high-quality baselines. We further verified that all selected repositories carry licenses permitting non-restrictive research use: most adopt permissive licenses (e.g., BSD, MIT, Apache), while the remainder fall under copyleft licenses (e.g., GPL), which are compatible with our intended research scenarios. We also reviewed repositories under custom licenses and confirmed their suitability for the uses contemplated in this study. A promising direction for future work is to broaden repository coverage to include codebases written in a wider range of programming languages, extending the evaluation to environment configuration tasks across more language ecosystems.\\nGuided by failure modes frequently encountered when configuring execution environments from software repositories, we define a six-type taxonomy intended to cover the vast majority of practical issues while maintaining clear, operational boundaries between categories. The scheme is designed to support consequent error generation, and the set is comprehensive for routine evaluation and error synthesis.\\nE1 Typical symptoms include package-not-found errors (e.g., 404 on indexes or channels), missing system libraries (e.g., OpenSSL, GCC toolchains), or failing installers (apt/conda/pip).\\nE2 Often manifests as immediate termination with usage messages or exit code 2, invalid or deprecated flags, shell quoting/escaping errors, or invoking commands from the wrong working directory.\\nE4 Common signals are ‚ÄúNo such file or directory,‚Äù misspelled filenames, incorrect relative paths, or reliance on artifacts not checked into version control.\\nE6 Symptoms include installing into an inactive environment, failing to source activation scripts before use, or attempting builds before installing toolchains.\\nE7 Presents as solver conflicts, runtime ImportErrors due to ABI/GLIBC/CUDA mismatches, or subtle behavior differences across Python or library minor versions.\\nE8 Catch-all for issues outside the above scopes, such as incomplete or ambiguous instructions, inconsistent naming, or extraneous formatting that obscures required steps; curators should use this sparingly and prefer specific categories when feasible.\\nWe use Claude-4-sonnet and Gemini 2.5-pro together to generate errors. We input the original README file, the desired types of errors to insert, the number of errors per README, the desired number of output README files, and a list of error type definitions, and then have them output the following:\\nErroneous README (Markdown or RST file)\\nList (JSON format):\\nReadme id\\nError type\\nError description (natural language)\\nCandidate fix suggestions (operational tips)\\nground truth of fix answer (golden answer)\\nHere is the instruction for error generation:\\nAfter generating the erroneous README file, as in the formal evaluation process, we used GPT-4.1-mini to generate a shell script based on the given README and the directory structure of the repository. This script was then run within Docker to verify whether the environment could be successfully built. We instructed the model to strictly follow the instructions in the README when generating the script, without making any modifications or corrections, to ensure accurate evaluation results. The specific prompt used is as follows:\\nWe further annotate each error using GPT-4.1-mini to ensure that the final generated errors are valid. We input the error‚Äôs README, a JSON file containing the error annotation, the error definition, and the basic Dockerfile design for our environment configuration. We ask GPT-4.1-mini to check the following: (1) Is the error type classification accurate? If not, suggest a corrected type. (2) Is this error described in the README? (3) Is this error valid? We consider an error valid only if it truly prevents a step in the environment configuration from succeeding, and its fix allows the configuration to execute correctly. (4) Is the standard solution for this error correct? After this screening, we discard all READMEs corresponding to invalid errors. The specific prompt is as follows:',\n",
       "    'subsections': [{'title': 'A.1 Repository Selection',\n",
       "      'paragraphs': 'Beyond the filtering criteria outlined in Section 3.2 , we selected repositories primarily from existing, human-validated, and pre-filtered benchmarks to ensure the originals are reliably buildable. This choice is crucial because our error synthesis procedure treats the unmodified README as the ground truth; consequently, the README must be correct and actionable. To that end, we engaged professional annotators to perform manual environment setup, removing repositories whose READMEs were themselves erroneous or whose dependencies were incomplete, thereby preserving high-quality baselines. We further verified that all selected repositories carry licenses permitting non-restrictive research use: most adopt permissive licenses (e.g., BSD, MIT, Apache), while the remainder fall under copyleft licenses (e.g., GPL), which are compatible with our intended research scenarios. We also reviewed repositories under custom licenses and confirmed their suitability for the uses contemplated in this study. A promising direction for future work is to broaden repository coverage to include codebases written in a wider range of programming languages, extending the evaluation to environment configuration tasks across more language ecosystems.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Error Type Definition',\n",
       "      'paragraphs': 'Guided by failure modes frequently encountered when configuring execution environments from software repositories, we define a six-type taxonomy intended to cover the vast majority of practical issues while maintaining clear, operational boundaries between categories. The scheme is designed to support consequent error generation, and the set is comprehensive for routine evaluation and error synthesis.\\nE1 Typical symptoms include package-not-found errors (e.g., 404 on indexes or channels), missing system libraries (e.g., OpenSSL, GCC toolchains), or failing installers (apt/conda/pip).\\nE2 Often manifests as immediate termination with usage messages or exit code 2, invalid or deprecated flags, shell quoting/escaping errors, or invoking commands from the wrong working directory.\\nE4 Common signals are ‚ÄúNo such file or directory,‚Äù misspelled filenames, incorrect relative paths, or reliance on artifacts not checked into version control.\\nE6 Symptoms include installing into an inactive environment, failing to source activation scripts before use, or attempting builds before installing toolchains.\\nE7 Presents as solver conflicts, runtime ImportErrors due to ABI/GLIBC/CUDA mismatches, or subtle behavior differences across Python or library minor versions.\\nE8 Catch-all for issues outside the above scopes, such as incomplete or ambiguous instructions, inconsistent naming, or extraneous formatting that obscures required steps; curators should use this sparingly and prefer specific categories when feasible.',\n",
       "      'subsections': [{'title': 'Taxonomy of Common Environment-Configuration Errors',\n",
       "        'paragraphs': 'Guided by failure modes frequently encountered when configuring execution environments from software repositories, we define a six-type taxonomy intended to cover the vast majority of practical issues while maintaining clear, operational boundaries between categories. The scheme is designed to support consequent error generation, and the set is comprehensive for routine evaluation and error synthesis.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Examples',\n",
       "        'paragraphs': 'E1 Typical symptoms include package-not-found errors (e.g., 404 on indexes or channels), missing system libraries (e.g., OpenSSL, GCC toolchains), or failing installers (apt/conda/pip).\\nE2 Often manifests as immediate termination with usage messages or exit code 2, invalid or deprecated flags, shell quoting/escaping errors, or invoking commands from the wrong working directory.\\nE4 Common signals are ‚ÄúNo such file or directory,‚Äù misspelled filenames, incorrect relative paths, or reliance on artifacts not checked into version control.\\nE6 Symptoms include installing into an inactive environment, failing to source activation scripts before use, or attempting builds before installing toolchains.\\nE7 Presents as solver conflicts, runtime ImportErrors due to ABI/GLIBC/CUDA mismatches, or subtle behavior differences across Python or library minor versions.\\nE8 Catch-all for issues outside the above scopes, such as incomplete or ambiguous instructions, inconsistent naming, or extraneous formatting that obscures required steps; curators should use this sparingly and prefer specific categories when feasible.',\n",
       "        'subsections': []}]},\n",
       "     {'title': 'A.3 Error Generation',\n",
       "      'paragraphs': 'We use Claude-4-sonnet and Gemini 2.5-pro together to generate errors. We input the original README file, the desired types of errors to insert, the number of errors per README, the desired number of output README files, and a list of error type definitions, and then have them output the following:\\nErroneous README (Markdown or RST file)\\nList (JSON format):\\nReadme id\\nError type\\nError description (natural language)\\nCandidate fix suggestions (operational tips)\\nground truth of fix answer (golden answer)\\nHere is the instruction for error generation:',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.4 Automatic Validation',\n",
       "      'paragraphs': 'After generating the erroneous README file, as in the formal evaluation process, we used GPT-4.1-mini to generate a shell script based on the given README and the directory structure of the repository. This script was then run within Docker to verify whether the environment could be successfully built. We instructed the model to strictly follow the instructions in the README when generating the script, without making any modifications or corrections, to ensure accurate evaluation results. The specific prompt used is as follows:',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.5 LLM as judge to Validate',\n",
       "      'paragraphs': 'We further annotate each error using GPT-4.1-mini to ensure that the final generated errors are valid. We input the error‚Äôs README, a JSON file containing the error annotation, the error definition, and the basic Dockerfile design for our environment configuration. We ask GPT-4.1-mini to check the following: (1) Is the error type classification accurate? If not, suggest a corrected type. (2) Is this error described in the README? (3) Is this error valid? We consider an error valid only if it truly prevents a step in the environment configuration from succeeding, and its fix allows the configuration to execute correctly. (4) Is the standard solution for this error correct? After this screening, we discard all READMEs corresponding to invalid errors. The specific prompt is as follows:',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Difficulty Rate',\n",
       "    'paragraphs': 'Because our task instances incorporate error injection into existing README files using LLM, we wanted to verify whether these generated task instances reflect the challenges encountered in real-world environment configuration, thus possessing characteristics similar to real-world environment setup tasks. Therefore, we selected several benchmarks that collect task instances directly from real-world code repositories, including INSTALLAMATIC Bench, ExecutionAgent Bench, EnvBench, and SetupBench, and compared their difficulty scores with our benchmark. For EnvBench and our benchmark, we sampled 100 task instances each; for the other benchmarks, we scored all available instances. By analyzing the distribution of difficulty scores, we can assess whether the methods used in our benchmark align with those used in real-world task instances. Specifically, we used a 1-5 scale, where 1 is very easy, 2 is easy, 3 is moderate, 4 is difficult, and 5 is very difficult. We invited professional annotators to score the selected task instances, considering factors such as the clarity and completeness of the instructions in the README, whether the commands execute directly, whether additional files or pages need to be consulted, and the number of dependencies that need to be considered.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix C Trajectory Training Data Genration Framework',\n",
       "    'paragraphs': 'We have automated the process of generating environment configuration task instances and designed a comprehensive evaluation suite that allows agents to execute these task instances, capture their execution trajectories, and perform evaluations. Therefore, we can build a complete synthetic data framework based on this to generate synthetic trajectory data representing both successful and failed agent executions of these environment configuration tasks. This can efficiently produce large amounts of trajectory data for model fine-tuning or large-scale pre-training, provided that a sufficient quantity of high-quality original repository data is available. The specific data generation process is illustrated in the Figure 9 .',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix D Experiment Settings',\n",
       "    'paragraphs': 'When the agent is tasked with configuring the environment, we provide instructions including the repository directory information, README information, and the basic environment requirements. We also outline a feasible workflow for the agent to follow, ensuring that the entire environment configuration adheres to the specified standards. We require the agent to explicitly identify any errors during execution, and to perform unit tests after completing the environment configuration to verify its success. The specific prompt is as follows:',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix E The Use of Large Language Models (LLMs)',\n",
       "    'paragraphs': 'In accordance with the official policy on the use of large language models, we used LLMs solely as general-purpose assistive tools for grammar checking and minor wording refinement during manuscript preparation. All LLM-suggested edits were manually reviewed and selectively accepted by the authors. Our usage complies with the official requirements, and we disclose it here.',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets.\\n Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail.\\n We introduce the Environment Configuration Diagnosis Benchmark, EnConda-Bench\\xa0, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration.\\n Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation.\\n EnConda-Bench\\xa0combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates.\\n Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance.\\n To our knowledge, EnConda-Bench\\xa0is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.'},\n",
       " '2510.25679v1': {'content': [{'title': 'Introduction',\n",
       "    'paragraphs': 'The rapidly increasing number of Unmanned Aerial Vehicles (UAVs) in urban environments is due to the number of tasks they can perform, from surveillance and traffic monitoring to package delivery and their ability to reach locations that can be challenging for other aerial vehicles, such as helicopters. This also presents problems related to acoustic pollution and the risk of accidents. It is crucial to develop an efficient strategy for the autonomous navigation of UAVs in complex environments, not only to perform the aforementioned tasks but also to reduce their environmental impact related to acoustic pollution. Path planning becomes essential in order to reduce navigation time and minimize energy consumption. Navigation in urban environments is extremely challenging due to the complexity of the environment itself, with the presence of buildings and complex three-dimensional wind velocity distributions. The velocity field is characterized by the presence of turbulent wakes, vortex shedding, turbulent velocity fluctuations, gusts, recirculation zones and complex interaction phenomena due to the presence of obstacles [ 7 , 4 ] . Traditional path planning employs deterministic algorithms and heuristic methods. Popular algorithms are potential-field methods [ 17 ] , grid-based algorithms [ 26 , 30 , 6 ] , and sampling-based methods such as Rapidly-exploring Random Trees (RRT) [ 21 , 45 , 23 , 43 ] , as well as Probabilistic Roadmaps (PRM) [ 20 , 2 ] . Although these algorithms have been shown to be successful in environments which do not exhibit uncertainties, they yield poor performance in dynamic environments. Simultaneously Localization And Mapping (SLAM) algorithms include obstacle detection and avoidance by including a mapping of the environment [ 10 , 22 , 28 , 9 ] , but in large-scale environments they exhibit degraded efficiency since building a map of the whole environment is practically unfeasible. Deep-Learning (DL)-based methods\\nhave recently received significant attention. DL can significantly improve obstacle avoidance and path planning for UAVs by using neural networks to process and interpret sensory data, such as images from cameras, or LiDAR signals [ 34 , 24 , 29 ] or ground-based stations [ 42 ] .\\nThese methods allow UAVs to detect and avoid obstacles more\\nefficiently by recognizing patterns and predicting potential collisions in real time. Deep-Learning methods are extremely\\nefficient for environments featuring small variations, as they are based on labels and sensitive to environmental\\nchanges. They are suitable for closed geometries but less reliable for urban environments, where the conditions typically change very rapidly. This brings the necessity to develop reinforcement-learning (RL) methods to understand and automate decision-making processes, in which the agent learns by trial-and-error [ 33 ] . Deep Reinforcement Learning (DRL) has been successfully applied to UAV navigation tasks in complex environments. Bouhamed et al. [ 5 ] used a Deep Deterministic Policy Gradient (DDPG) algorithm to train quadrocopters to navigate in a three-dimensional environment avoiding collisions with static and moving obstacles. Wang et al. [ 40 ] implemented a Faster Regions with Convolutional Neural Network Feature (Faster R-CNN) algorithm for obstacle detection, improved upon a traditional Deep-Q Network (DQN) approach. Collision avoidance capability was significant in unknown or dynamically changing environments. Sheng et al. [ 32 ] used a Twin Delayed Deep Deterministic Policy Gradient (TD3) to train a UAV to navigate a highly dynamic environment with multiple obstacles achieving high success rates and efficient path planning. Wang et al. [ 41 ] proposed a Distributed Privileged Reinforcement Learning (DPRL) framework to address partial observability in navigation tasks, outperforming conventional vision-based methods. AlMahamid and Grolinger [ 1 ] introduced Agile DQN, an adaptive deep recurrent attention reinforcement learning method that enhances UAV obstacle avoidance by processing only the most relevant visual regions, achieving faster training and higher performance in simulated 3D environments. Raj and Kos [ 27 ] applied a DQN-based approach for mobile robot navigation in unknown 2D environments, showing effective obstacle avoidance and improved navigation performance. Zhao et al. [ 46 ] proposed an elastic adaptive DRL strategy to stabilize training in autonomous navigation tasks, demonstrating superior collision avoidance in multi-agent traffic scenarios. Berg et al. [ 3 ] combined deep reinforcement learning with nonlinear model predictive control (NMPC) for autonomous surface vessels, improving digital twin synchronization and obstacle avoidance through adaptive control policies. Recently, DRL has been applied to autonomous navigation tasks where the environment is given by high-fidelity simulations. Gunnarson et al. [ 13 ] applied a V-Racer algorithm with a Remember and Forget Experience Replay to discover time-efficient navigation policies to steer a fixed-speed swimmer through unsteady two-dimensional flow fields. Jiao et al. [ 19 ] showed that gradient sensing was critical for a navigation task in unsteady wake scenarios, applying DRL in a 2D unsteady flow field. The geocentric agent (with access to global direction) could reach the goal by sensing local flow velocity alone. However, the egocentric agent consistently failed with only local velocity sensors. An egocentric navigator needed the extra information on how the flow changed around it to break symmetries and reliably navigate the wake. Tonti et al. [ 36 ] used a Proximal Policy Optimization (PPO) algorithm [ 31 ] in combination with Long Short-Term Memory (LSTM) cells [ 15 ] for UAV navigation in a two-dimensional urban-like turbulent flow field obtained from a high-fidelity simulation. This study included random start and target locations, as well as random start snapshots, showing the adaptability of the agent in complex flow fields. The present study is an application to a three-dimensional urban environment, represented by a high-fidelity simulation, comparing three different architectures: PPO with LSTM cells, PPO with Gated Transformer XL (GTrXL) [ 25 ] , and PPO with GTrXL with an auxiliary task for flow prediction, in a multi-objective reinforcement learning framework, so that the UAV can further optimize its trajectory exploiting the prediction of the flow field of the next snapshot. Proximal policy optimization combined with GTrXL has recently been used for navigation tasks. Huang et al. [ 16 ] applied PPO+GTrXL in an indoor environment with obstacles, comparing the results with a Soft Actor-Critic (SAC) algorithm [ 14 ] combined with a GTrXL. Federici et al. [ 8 ] applied PPO+GTrXL to a meta-reinforcement learning task applied to spacecraft navigation, showing an improvement in performance with respect to vanilla PPO. To the authors‚Äô knowledge, this work is not only the first to bring PPO+GTrXL into a 3D turbulent flow field with obstacles obtained by a high-fidelity simulation, but also the first to integrate a Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) encoder, extracting spatial and temporal flow context, into GTrXL, and to train an auxiliary flow prediction head alongside policy and value objectives. This unified, multi-objective architecture, combining PPO+GTrXL and CNN+GRU embeddings, and transformer-based global attention for both control and real-time flow forecasting, represents a novel contribution to trajectory-optimization tasks.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Results',\n",
       "    'paragraphs': 'In this section, the results of the trained policies are shown and compared.\\nAll episode returns shown below are normalized to a common scale:\\nwhere R R is the total reward as defined in Section Methods, min \\u2061 ( R all ) \\\\min(R_{\\\\text{all}}) and max \\u2061 ( R all ) \\\\max(R_{\\\\text{all}}) are the\\nlowest and highest raw rewards observed over all policies, respectively. This places\\nPPO+LSTM, PPO+GTrXL and the proposed Flow-aware GTrXL on the\\nsame [ ‚àí 1 , 1 ] [-1,1] range, facilitating a direct comparison of learning speed and\\nasymptotic performance. Throughout this work, we adopt the following terminology to avoid ambiguity. An environment step refers to a single interaction between the agent and the environment, consisting of selecting an action, receiving a reward, and observing the next state. An episode is a full sequence of steps in the environment from an initial state to a terminal condition. A training iteration corresponds to one optimization cycle, during which the model parameters are updated using gradients computed from batches of collected data. An update step refers specifically to a single gradient descent update within a training iteration, when using several mini-batches per iteration. Performance metrics are plotted against training iterations. The methods compared in this section are described in Methods.\\nFigure 1(a) shows that the LSTM baseline (blue) leaves the\\nexploration plateau after 300 training iterations and\\nsaturates at R norm = 0.94 ¬± 0.5 R_{\\\\text{norm}}\\\\!=\\\\!0.94\\\\pm 0.5 .\\nReplacing the recurrent core with attention\\nraises the plateau to 0.98 ¬± 0.2 0.98\\\\pm 0.2 .\\nAdding the auxiliary flow‚Äìprediction head introduces a short\\nburn-in, yet overtakes the vanilla transformer by iteration\\xa0600 and reaches\\nthe top of the scale, 1.0 ¬± 0.1 1.0\\\\pm 0.1 . The curves are the result of a moving average over 200 episodes.\\nFigure 1(b) shows that the probability of success during training increases from LSTM to vanilla GTrXL and flow-aware GTrXL, reahcing peaks of 1.0 for the flow-aware method.\\nThe corresponding crash rates in Figure 1(c) drop from PPO+LSTM to Flow-aware GTrXL, confirming that attention reduces the incidence of crashes and\\nconditioning on flow history drops it further by allowing the\\nagent to anticipate gust-driven drift, reaching sometimes 0 collisions. During inference, where the algorithms are tested on a unseen environment, we can see that the flow-aware method still reaches a higher performance with respect to the PPO+LSTM and PPO+GTrXL methods.\\nAlthough Zermelo‚Äôs algorithm computes theoretically optimal trajectories by minimizing a cost function that accounts for control, obstacle avoidance, and final target accuracy, its performance in the unsteady 3D urban environment is significantly inferior to learned policies. Specifically, Zermelo achieves a success rate of only 61.3%, compared to 97.6% with flow-aware PPO+GTrXL, 95.7% with PPO+GTrXL and 86.7% with PPO+LSTM. This gap is primarily attributed to the open-loop nature of the Zermelo solution: the trajectory is optimized once using a single, randomly selected snapshot of the velocity field and a randomized initial UAV position, without any feedback or re-planning during execution. As a result, the computed trajectory is inherently brittle to perturbations and cannot adapt to the evolving flow field, which is especially detrimental in highly dynamic and partially observable environments.\\nIn contrast, DRL policies are trained in closed-loop settings and are explicitly optimized to generalize across a wide distribution of initial conditions and flow realizations. Architectures like GTrXL and LSTM enable these policies to encode temporal dependencies and respond adaptively to local variations in the flow. Furthermore, DRL agents can implicitly learn obstacle avoidance strategies and exploit transient flow structures through repeated interaction with the environment, capabilities that static optimization-based methods like Zermelo lack. These results demonstrate the critical importance of reactivity and temporal awareness for robust navigation in complex, time-varying environments, highlighting a fundamental limitation of classical optimal control when deployed in real-time.\\nFigure 2 shows two examples of trajectories given by the flow-aware PPO+GTrXL. The trajectories are shown in the 3D domain, and a 2D slice of the streamwise velocity field is shown to help the visualization. The environment is represented by a three dimensional high-fidelity simulation of an urban flow. The domain coordinates are x ‚àà [ ‚àí 2.0 , 5.0 ] x\\\\in[-2.0,5.0] , y ‚àà [ 0 , 3.0 ] y\\\\in[0,3.0] , z ‚àà [ ‚àí 1.0 , 1.0 ] z\\\\in[-1.0,1.0] . The obstacle coordinates are x o \\u200b 1 x_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , y o \\u200b 1 y_{o1} ‚àà [ 0.0 , 1.0 ] \\\\in[0.0,1.0] , z o \\u200b 1 z_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , x o \\u200b 2 x_{o2} ‚àà [ 1.25 , 1.75 ] \\\\in[1.25,1.75] , y o \\u200b 2 y_{o2} ‚àà [ 0.0 , 0.5 ] \\\\in[0.0,0.5] , z o \\u200b 2 z_{o2} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] . All distances are scaled by the obstacle height h h . More details can be found in Section Methods.',\n",
       "    'subsections': [{'title': 'Learning speed and asymptotic return',\n",
       "      'paragraphs': 'Figure 1(a) shows that the LSTM baseline (blue) leaves the\\nexploration plateau after 300 training iterations and\\nsaturates at R norm = 0.94 ¬± 0.5 R_{\\\\text{norm}}\\\\!=\\\\!0.94\\\\pm 0.5 .\\nReplacing the recurrent core with attention\\nraises the plateau to 0.98 ¬± 0.2 0.98\\\\pm 0.2 .\\nAdding the auxiliary flow‚Äìprediction head introduces a short\\nburn-in, yet overtakes the vanilla transformer by iteration\\xa0600 and reaches\\nthe top of the scale, 1.0 ¬± 0.1 1.0\\\\pm 0.1 . The curves are the result of a moving average over 200 episodes.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Task-level safety',\n",
       "      'paragraphs': 'Figure 1(b) shows that the probability of success during training increases from LSTM to vanilla GTrXL and flow-aware GTrXL, reahcing peaks of 1.0 for the flow-aware method.\\nThe corresponding crash rates in Figure 1(c) drop from PPO+LSTM to Flow-aware GTrXL, confirming that attention reduces the incidence of crashes and\\nconditioning on flow history drops it further by allowing the\\nagent to anticipate gust-driven drift, reaching sometimes 0 collisions. During inference, where the algorithms are tested on a unseen environment, we can see that the flow-aware method still reaches a higher performance with respect to the PPO+LSTM and PPO+GTrXL methods.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Classical baseline: Zermelo‚Äôs optimal navigation',\n",
       "      'paragraphs': 'Although Zermelo‚Äôs algorithm computes theoretically optimal trajectories by minimizing a cost function that accounts for control, obstacle avoidance, and final target accuracy, its performance in the unsteady 3D urban environment is significantly inferior to learned policies. Specifically, Zermelo achieves a success rate of only 61.3%, compared to 97.6% with flow-aware PPO+GTrXL, 95.7% with PPO+GTrXL and 86.7% with PPO+LSTM. This gap is primarily attributed to the open-loop nature of the Zermelo solution: the trajectory is optimized once using a single, randomly selected snapshot of the velocity field and a randomized initial UAV position, without any feedback or re-planning during execution. As a result, the computed trajectory is inherently brittle to perturbations and cannot adapt to the evolving flow field, which is especially detrimental in highly dynamic and partially observable environments.\\nIn contrast, DRL policies are trained in closed-loop settings and are explicitly optimized to generalize across a wide distribution of initial conditions and flow realizations. Architectures like GTrXL and LSTM enable these policies to encode temporal dependencies and respond adaptively to local variations in the flow. Furthermore, DRL agents can implicitly learn obstacle avoidance strategies and exploit transient flow structures through repeated interaction with the environment, capabilities that static optimization-based methods like Zermelo lack. These results demonstrate the critical importance of reactivity and temporal awareness for robust navigation in complex, time-varying environments, highlighting a fundamental limitation of classical optimal control when deployed in real-time.\\nFigure 2 shows two examples of trajectories given by the flow-aware PPO+GTrXL. The trajectories are shown in the 3D domain, and a 2D slice of the streamwise velocity field is shown to help the visualization. The environment is represented by a three dimensional high-fidelity simulation of an urban flow. The domain coordinates are x ‚àà [ ‚àí 2.0 , 5.0 ] x\\\\in[-2.0,5.0] , y ‚àà [ 0 , 3.0 ] y\\\\in[0,3.0] , z ‚àà [ ‚àí 1.0 , 1.0 ] z\\\\in[-1.0,1.0] . The obstacle coordinates are x o \\u200b 1 x_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , y o \\u200b 1 y_{o1} ‚àà [ 0.0 , 1.0 ] \\\\in[0.0,1.0] , z o \\u200b 1 z_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , x o \\u200b 2 x_{o2} ‚àà [ 1.25 , 1.75 ] \\\\in[1.25,1.75] , y o \\u200b 2 y_{o2} ‚àà [ 0.0 , 0.5 ] \\\\in[0.0,0.5] , z o \\u200b 2 z_{o2} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] . All distances are scaled by the obstacle height h h . More details can be found in Section Methods.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Discussion',\n",
       "    'paragraphs': 'Attention alone can address long‚Äêhorizon credit assignment, but at the cost of\\na very broad memory pool in which irrelevant observations compete with\\ncritical ones. Conditioning the transformer on a trained flow embedding achieves two complementary effects.\\nFirst, it injects dense stepwise gradients, stabilizing the policy update and\\nreducing reward variance. Second, it equips the agent with an implicit\\npredictive model of the local velocity field, enabling anticipatory actions\\nrather than purely reactive control.\\nThese mechanisms are supported by the sharp reduction in the collision rate and\\nthe smoother control spectrum observed during the zero-shot evaluation. The Zermelo‚Äôs benchmark (Table 1) highlights the\\npitfalls of ignoring flow and of open-loop execution: only 61.3% of success when replayed in the turbulent flow field. In contrast, the flow-aware GTrXL\\nsuccess rate reaches 97.6% of the same tasks, without any pre-planning\\nlatency, and produces shorter trajectories. Even the LSTM baseline\\noutperforms the Zermelo‚Äôs benchmark in reaching the target,\\nunderlining the inadequacy of closed-loop planners in highly disturbed\\ndomains.\\nBecause all returns are normalized to a common [ ‚àí 1 , 1 ] [-1,1] range, the curves are directly comparable. While all models exhibit similar initial performance, the Transformer-based architectures (GTrXL and Flow-aware GTrXL) show significantly faster convergence and higher asymptotic reward compared to the LSTM baseline. This is attributed to the architectural differences: LSTMs compress the entire temporal history into a fixed-size hidden state, which limits their ability to capture long-range dependencies, especially in partially observable and dynamically evolving environments. In contrast, GTrXL employs gated self-attention mechanisms with explicit memory access, enabling better retention and exploitation of temporally distant observations, which results in improved policy learning efficiency and stability.\\nThe Flow-aware GTrXL further extends this advantage by incorporating a multi-objective learning framework that includes a contrastive loss aimed at encoding the underlying flow dynamics. This auxiliary task enforces the development of latent representations that are sensitive to changes in the local flow field, thereby enhancing the agent‚Äôs ability to react to unsteady flow patterns during navigation. As seen in Figure 1 , the Flow-aware GTrXL reaches higher sample efficiency and generalization. These results confirm that coupling policy learning with auxiliary flow representation objectives can significantly improve robustness and adaptivity in complex, time-varying environments. All experiments were conducted in a high‚Äêfidelity numerical database; real‚Äêworld\\naerodynamic gusts, sensor noise, and limited onboard computation may\\ndegrade performance. Furthermore, the auxiliary head uses ground-truth flow\\nsnapshots during training, which is not a possibility during flight. Future work will\\nreplace this with a self-supervised predictor driven by onboard pressure or computer vision tools. Finally, we considered a single UAV; the presence of multiple UAVs may\\nexhibit interactions that challenge the present architecture. Our results suggest that\\njointly learning to predict and control is\\ncritical for safe navigation in turbulent 3D environments. Transformer\\npolicies augmented with physically meaningful auxiliary tasks not only match,\\nbut decisively outperform, state-of-the-art sampling planners in robustness and efficiency. We anticipate that similar multi-objective\\ndesigns will become standard for autonomous vehicles operating in complex,\\npartially predictable environments such as urban flow fields or crowded air corridors.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Methods',\n",
       "    'paragraphs': 'The environment is a three-dimensional high-fidelity simulation of an urban turbulent flow field, with two obstacles representing the buildings. The simulation was run using the spectral-element code Nek5000. All the details can be found in Zampino et al. [ 44 ] . The domain coordinates are x ‚àà [ ‚àí 2.0 , 5.0 ] x\\\\in[-2.0,5.0] , y ‚àà [ 0 , 3.0 ] y\\\\in[0,3.0] , z ‚àà [ ‚àí 1.0 , 1.0 ] z\\\\in[-1.0,1.0] . The coordinates of the obstacles are x o \\u200b 1 x_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , y o \\u200b 1 y_{o1} ‚àà [ 0.0 , 1.0 ] \\\\in[0.0,1.0] , z o \\u200b 1 z_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , x o \\u200b 2 x_{o2} ‚àà [ 1.25 , 1.75 ] \\\\in[1.25,1.75] , y o \\u200b 2 y_{o2} ‚àà [ 0.0 , 0.5 ] \\\\in[0.0,0.5] , z o \\u200b 2 z_{o2} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] . All distances are scaled by the height of the upstream obstacle h h . The dataset used for the DRL algorithms is a set of 300 snapshots separated by 0.08750 time units, with the time span of the dataset being 26.25 time units. Time is normalized with h h and the freestream velocity U ‚àû U_{\\\\infty} . Due to the dimensions of the simulation results, an efficient pre-processing strategy had to be adopted. In order to avoid Input/Output (I/O) overhead during the execution of the algorithm described in subsection Algorithms, the flow field was split into small blocks for each snapshot. The original mesh of the numerical simulation was interpolated on a structured grid of 250 √ó \\\\times 250 √ó \\\\times 250 points. Each snapshot of the simulation has blocks of dimensions 10 √ó \\\\times 10 √ó \\\\times 10 mesh cells in x , y x,y and z z grid points. The blocks overlap to ensure that each cell of the domain is covered during the navigation of the UAV, so that the flow field can be continuously mapped. The core of the implementation is the lightweight, scalable framework combined with disk-efficient storage of the decomposed data and a local tricubic interpolation in space and time using KD-tree indexing.\\nAlgorithm 1 presents a lightweight, on-the-fly extraction of the velocity field in a block-decomposed simulation, performed using tricubic interpolation in space and cubic interpolation in time. Beginning with a mesh file that defines the global domain, the algorithm automatically scans a directory of per-block output files, each named by its time step and spatial indices, to build a time-indexed metadata map. For each stored timestep, it computes the physical spatial bounds of every block, determines the center of each block, and inserts these centers into a KD-tree to enable logarithmic-time nearest-neighbor searches.\\nAt query time for a given position ( x , y , z ) (x,y,z) and time t t , the algorithm first identifies the bracketing time interval [ t i , t i + 1 ] [t_{i},t_{i+1}] containing t t . The it performs spatial interpolation at four control times { t i ‚àí 1 , t i , t i + 1 , t i + 2 } \\\\{t_{i-1},t_{i},t_{i+1},t_{i+2}\\\\} to construct a cubic temporal interpolation stencil, with appropriate boundary clamping when fewer than four timesteps are available. For each control time, only the k k nearest blocks to the query position are considered, loaded on-demand using an in-memory cache to minimize disk I/O overhead.\\nWithin each relevant block, a parallelized tricubic interpolation routine (implemented via a pybind11 C++ module) computes the velocity components using Catmull-Rom splines over a 4√ó4√ó4 neighborhood. When multiple blocks contain the query position, their spatial estimates are combined via inverse-distance weighting to ensure smooth transitions across block boundaries. If no block strictly contains the query point, the algorithm falls back to using the nearest block regardless of its spatial bounds.\\nFinally, the four spatially-interpolated velocity vectors are blended temporally using Catmull-Rom cubic splines with the interpolation factor Œ± = ( t ‚àí t i ) / ( t i + 1 ‚àí t i ) \\\\alpha=(t-t_{i})/(t_{i+1}-t_{i}) to produce the final velocity vector ùêÆ \\u200b ( x , y , z , t ) \\\\mathbf{u}(x,y,z,t) . The caching strategy ensures that once blocks around ( x , y , z ) (x,y,z) are loaded for one control time, they remain available for neighboring times, significantly improving performance in realistic workflows. By never storing the full four-dimensional dataset in memory, this approach scales efficiently to very large simulations while delivering fast and accurate reconstruction in both space and time.\\nDuring the UAV navigation, the underlying flow field components have to be retrieved to inform the agent with the velocity vector components in real-time. The procedure is explained in Algorithm 2 . First, the position of the UAV is clamped to the valid range of coordinates corresponding to the simulation domain. Then, the query position is quantized with the current time to form a cache key, checking if this specific time-position combination has already been computed. If the cache key is missing, the algorithm proceeds with full tricubic spatial and cubic temporal interpolation.\\nFor temporal interpolation, the algorithm identifies the bracketing time interval and constructs a four-point Catmull-Rom stencil using control times { t i ‚àí 1 , t i , t i + 1 , t i + 2 } \\\\{t_{i-1},t_{i},t_{i+1},t_{i+2}\\\\} , with appropriate boundary clamping when at the start or end of the time series. At each of these four control times, spatial interpolation is performed using the procedure described in Algorithm 1 , which employs tricubic interpolation within individual blocks and inverse-distance weighting to blend results from multiple overlapping blocks.\\nThe four spatially-interpolated velocity vectors are then combined using Catmull-Rom cubic splines in time with the interpolation factor Œ± = ( t ‚àí t i ) / ( t i + 1 ‚àí t i ) \\\\alpha=(t-t_{i})/(t_{i+1}-t_{i}) . The final result is stored in the cache as the last valid velocity, and the velocity vector is returned. This caching strategy ensures that subsequent queries at nearby positions and times experience minimal computational overhead while maintaining high accuracy in the velocity field extraction.\\nThe UAV is modeled as a mass point with six degrees of freedom in translation and two in orientation, in particular yaw œà \\\\psi and pitch œë \\\\vartheta . The state vector is defined as\\nand evolves under the combined influence of thrust V V , the turn rates expressed as Œî \\u200b œà \\\\Delta\\\\psi and Œî \\u200b œë \\\\Delta\\\\vartheta and the underlying flow field velocity components ùêÆ ùêüùê•ùê®ùê∞ = ( u f , v f , w f ) \\\\mathbf{u_{flow}}=(u_{f},v_{f},w_{f}) . Note that u g , v g , w g u_{g},v_{g},w_{g} are the u , v , w u,v,w components of the velocity vector calculated as a sum of the UAV and flow field velocity components in each direction, respectively. Furthermore, x , y , z x,y,z are the coordinates of the position of the UAV at the current timestep. Since the UAV is represented as a mass point, the roll angle is not included in the system of equations, which is then described as:\\nŒî \\u200b œà \\\\Delta\\\\psi and Œî \\u200b œë \\\\Delta\\\\vartheta are the variations of the yaw and pitch angles in the Œî \\u200b t \\\\Delta t time interval.\\nThe state of the UAV at time t + 1 t+1 is given by a classical fourth‚Äêorder Runge‚ÄìKutta integrator (RK4). In this work, Œî \\u200b t \\\\Delta t corresponds to 0.08750 time units and is divided into 40 RK4 substeps. Because the flow field is only updated every Œî \\u200b t \\\\Delta t , a single RK4 step would assume that the flow is spatially uniform along the UAV trajectory, leading to significant errors whenever the vehicle traverses regions of strong velocity gradients. By integrating the dynamics at 40 intermediate positions, we faithfully capture these spatial variations. Although RK4 is formally unconditionally stable for smooth ordinary differential equations (ODEs), coupling to a block‚Äêdecomposed interpolation grid imposes a pseudo‚ÄêCourant‚ÄìFriedrichs‚ÄìLewy (pseudo-CFL) constraint: the UAV must not step more than a small fraction of a block or execute a large angular deflection in one step, otherwise the KD‚Äêtree lookups may skip blocks and cause artifacts. Using multiple RK4 substeps per timestep in the environment ensures a smooth evolution of the state of the UAV. This approach maintains continuity in position and orientation even when the agent applies maximum thrust and angular rate commands. Finally, having\\nblock‚Äêtree structures and recently accessed block data remaining in memory, the additional integration calls per interval incur negligible extra I/O. Consequently, the sub‚Äêstepping strategy delivers substantial gains in accuracy and robustness at minimal computational cost. These considerations justify the use of RK4 sub-steps per state update, yielding an effective balance between numerical stability and performance in large‚Äêscale block‚Äêdecomposed simulations.\\nDeep reinforcement learning has been widely used in recent years for control and optimization in fluid mechanics [ 12 , 39 , 11 ] .\\nThe environment is built using Gymnasium [ 37 ] , an open-source library for reinforcement learning, focusing on the interface for simulation environments.\\nObservation and action spaces are continuous. The observation space is defined as:\\nwhere œà , œë \\\\psi,\\\\vartheta are the yaw and pitch angles of the UAV, œà target , Œ∏ target \\\\psi_{\\\\textrm{target}},\\\\theta_{\\\\textrm{{target}}} the relative yaw and pitch angles of the UAV with respect to the target, d target d_{\\\\textrm{{target}}} the Euclidean distance between the UAV and the target, x , y , z x,y,z the coordinates of the UAV in the domain, and Œ∏ i , œï j \\\\theta_{i},\\\\phi_{j} the elevation and azimuth angles associated with the obstacle detection sensors, with i ‚àà [ 0 , 8 ] i\\\\in[0,8] and j ‚àà [ 0 , 4 ] j\\\\in[0,4] , spanning the angles between ‚àí œÄ -\\\\pi and œÄ \\\\pi in their respective directions. Figures 3(a) and 3(b) graphically show the definition of the environment.\\nThe action space is designed to include thrust V ‚àà [ ‚àí 2.0 , 2.0 ] V\\\\in[-2.0,2.0] and yaw and pitch changes Œî \\u200b œà ‚àà [ ‚àí œÄ / 4 , œÄ / 4 ] \\\\Delta\\\\psi\\\\in[-\\\\pi/4,\\\\pi/4] and Œî \\u200b œë ‚àà [ ‚àí œÄ / 4 , œÄ / 4 ] \\\\Delta\\\\vartheta\\\\in[-\\\\pi/4,\\\\pi/4] , respectively. Obstacles are detected by providing the agent with a set of directions, since UAVs are in relation to the surroundings typically by images from cameras,\\nradar signals, or range finders. In this work, obstacle detection is achieved by implementing a\\nray-tracing technique [ 35 ] . First of all, the UAV has to check for free space in its perspective. The input is the position\\nof the UAV and the output is a boolean variable which indicates whether the path is free from obstacles or not. Then, if\\nthe obstacle is present, the intersection with the traced rays is computed. First, the direction of the ray is calculated,\\nbased on the ray origin and final point, as well as the coordinates of the obstacles. Then, it is verified whether parallel\\ndirections to the obstacles are present. If the detected directions are not parallel to the obstacles, the intersection point\\nbetween the ray and the obstacle is calculated and the distance to the intersection is returned. Figure 3(c) sketches the process.\\nThe starting and target points are randomly chosen before the first and after the second obstacle in the domain, as well as the initial orientation of the UAV. The starting snapshot of the algorithm is also randomly chosen among the 300 available. This setup is chosen so that the initial conditions of the flow field themselves exhibit uncertainties. The agent can take a maximum of 100 steps in the domain. The UAV state vector is defined in Equation ( 1 ) and it is inferred from the observations, which are given as input to the neural network and described in Equation\\n( 3 ). The problem described here can be then considered as Partially Observable Markov Decision Process (POMDP). A POMDP is characterized by the fact that the agent cannot directly observe the state s t s_{t} , but receives a set of observations o t o_{t} with a distribution p \\u200b ( o t | s t ) p(o_{t}|s_{t}) . The sequence of observations does not satisfy the Markov property, since p \\u200b ( o t + 1 | a t , o t , a t ‚àí 1 , o t ‚àí 1 , ‚Ä¶ , o 0 ) ‚â† p \\u200b ( o t + 1 | o t , a t ) p(o_{t+1}|a_{t},o_{t},a_{t-1},o_{t-1},...,o_{0})\\\\neq p(o_{t+1}|o_{t},a_{t}) . Consequently, the agent has to infer the current state s t s_{t} based on the history of trajectories.\\nDRL is a process that encourages learning by trial and error and this process is triggered by a reward which is given to the agent when it takes the right actions to complete the assigned task. The structure of the reward is crucial because this guides the agent towards a more effective learning. This component of the algorithm has to be carefully designed and tuned for a specific task. The reward structure has been extended from the 2D version of the problem in Tonti et al. [ 36 ] . The reward structure is designed to guide the UAV towards the target, while minimizing collisions with obstacles, reducing energy consumption and preventing leaving the designated operational bounds. The reward function is given by several components, each one addressing a different aspect of the UAV‚Äôs performance. The final reward at each time step t t is the sum of:\\nTransition reward r trans r_{\\\\mathrm{trans}} : proportional to the reduction in distance to the target,\\nwhere œÉ ‚àà ‚Ñù \\\\sigma\\\\in\\\\mathbb{R} is a scaling constant and ùê± \\\\mathbf{x} is the position vector.\\nObstacle penalty r obs r_{\\\\mathrm{obs}} : an exponential penalty based on the minimum distance d min d_{\\\\min} to any obstacle,\\nwith Œæ , Œ≤ ‚àà ‚Ñù \\\\xi,\\\\beta\\\\in\\\\mathbb{R} , and d min = min \\u2061 { d 1 , ‚Ä¶ , d n } . d_{\\\\min}=\\\\min\\\\{d_{1},\\\\dots,d_{n}\\\\}.\\nFree‚Äêspace bonus r free r_{\\\\mathrm{free}} :\\nwhere R free ‚àà ‚Ñù R_{\\\\mathrm{free}}\\\\in\\\\mathbb{R} is a constant.\\nBest‚Äêdirection bonus r best r_{\\\\mathrm{best}} : when the forward direction is blocked, a small reward in proportion to the chosen changes in angles,\\nwhere Œî \\u200b œï , Œî \\u200b œë \\\\Delta\\\\phi,\\\\Delta\\\\vartheta are the yaw and pitch offsets of the best free direction.\\nStep penalty r step r_{\\\\mathrm{step}} : a constant negative reward per time step,\\nProximity‚Äìvelocity penalty r prox r_{\\\\mathrm{prox}} : discourages high speed when very close to the target,\\nEnergy penalty r energy r_{\\\\mathrm{energy}} : proportional to the norm of the propulsion velocity,\\nCombining all the components, the reward is defined as follows for each time step t t :\\nAt the end of an episode of m m ‚àà ‚Ñï \\\\in\\\\mathbb{N} steps, we add:\\nTarget reached bonus : if ‚à• ùê± m ‚àí ùê± target ‚à• ‚â§ R target \\\\lVert\\\\mathbf{x}_{m}-\\\\mathbf{x}_{\\\\mathrm{target}}\\\\rVert\\\\leq R_{\\\\mathrm{target}} , then a positive constant is added to the reward and the episode ends. R target R_{\\\\mathrm{target}} is the radius of the target sphere.\\nCollision penalty : if a collision is detected, a negative constant is added and the episode ends.\\nOut‚Äêof‚Äêbounds penalty : if the UAV exits the domain, [ x min , x max ] √ó [ y min , y max ] √ó [ z min , z max ] [x_{\\\\min},x_{\\\\max}]\\\\times[y_{\\\\min},y_{\\\\max}]\\\\times[z_{\\\\min},z_{\\\\max}] , a negative constant is added, and the episode ends.\\nNear‚Äêtarget bonus : if | ‚à• ùê± m ‚àí ùê± target ‚à• ‚àí R target | < 0.5 \\\\bigl|\\\\lVert\\\\mathbf{x}_{m}-\\\\mathbf{x}_{\\\\mathrm{target}}\\\\rVert-R_{\\\\mathrm{target}}\\\\bigr|<0.5 , then a small positive constant is added.\\nThree different algorithms are compared. The first is the Proximal Policy Optimization (PPO) [ 31 ] with Long Short-Term Memory (LSTM) cells [ 15 ] . The second algorithm tested is a PPO with a Gated Transformer eXtra Large (GTrXL) unit [ 25 ] . The third is a PPO + GTrXL but with a dedicated auxiliary task to integrate a flow prediction head to improve UAV navigation in the environment. The task is then not only trajectory optimization, but a multi-objective DRL where the flow field is also predicted. The training is made on 200 snapshots of the dataset, while 100 snapshots are used for inference to test the learned policies in an unknown environment. In three-dimensional environments, the fixed-size hidden state of the LSTM and the inherently sequential update scheme limit its capacity to maintain and selectively recall the information necessary for effective policy learning. Although LSTM cells can mitigate short-term dependencies via gated memory updates, their recurrence still enforces a strict temporal bottleneck. Each new observation must propagate through all intermediate time steps before influencing the current decision, which can cause important distant events to be ‚Äúforgotten‚Äù or diluted in the hidden state over long horizons. Moreover, LSTMs lack an explicit mechanism for relating non-adjacent states, making it difficult to capture the complex spatial relationships that arise in 3D domains. On the other hand, the GTrXL architecture embeds multihead self-attention layers with gated residual connections, allowing the agent to attend directly to any past observation regardless of its temporal distance. This non-sequential attentional access not only alleviates vanishing-gradient issues but also provides a flexible memory buffer whose capacity grows with trajectory length, enabling more robust encoding of 3D structure and long-range dependencies. Figure 4(b) shows the structure of a GTrXL block. The input sequence Y œÑ Y_{\\\\tau} enters the block and first undergoes input embedding ( E œÑ E_{\\\\tau} ), producing I œÑ I_{\\\\tau} , which is the embedded input. The core processing consists of two sequential components with gated connections. First, the multi-head attention layer computes queries ( Q œÑ Q_{\\\\tau} ), keys ( K œÑ K_{\\\\tau} ), and values ( V œÑ V_{\\\\tau} ) from I ~ œÑ \\\\tilde{I}_{\\\\tau} , applies attention with masking to hide future data ( A œÑ A_{\\\\tau} ) with relative positional encoding ( R œÑ R_{\\\\tau} ), followed by a GRU gate ( G œÑ G_{\\\\tau} ) that controls information integration and layer normalization. Second, a position-wise feed-forward network ( F œÑ F_{\\\\tau} ) is applied, followed by another GRU gate and layer normalization. Both components use skip connections (curved arrows) that enable direct gradient flow. The block maintains recurrent memory states M œÑ M_{\\\\tau} and M œÑ + 1 M_{\\\\tau+1} that persist across time steps, along with a hidden state H œÑ H_{\\\\tau} for capturing long-term dependencies. The output consists of the processed sequence and updated memory state M œÑ + 1 M_{\\\\tau+1} . The key variables are: Y œÑ Y_{\\\\tau} (input sequence), E œÑ E_{\\\\tau} (input embedding), I ~ œÑ \\\\tilde{I}_{\\\\tau} (embedded input), H œÑ H_{\\\\tau} (recurrent hidden state), M œÑ M_{\\\\tau} (input memory), M œÑ + 1 M_{\\\\tau+1} (output memory), Q œÑ Q_{\\\\tau} / K œÑ K_{\\\\tau} / V œÑ V_{\\\\tau} (query/key/value matrices), A œÑ A_{\\\\tau} (attention weights), R œÑ R_{\\\\tau} (relative positional encoding), F œÑ F_{\\\\tau} (feed-forward transformation), G œÑ G_{\\\\tau} (GRU gates). This architecture combines transformer attention mechanisms with RNN gating properties, enabling both effective sequence modeling and stable training dynamics through controlled information flow. Figure 4(a) shows the PPO+GTrXL architecture, where the MLP encoder is here the PPO algorithm. The observations y k ‚àí n , ‚Ä¶ , y k {y_{k-n},...,y_{k}} are the inputs to the encoder, which here is a PPO. The encoded observations are then fed to the GTrXL block, which receives a memory state M k ‚àí 1 M_{k-1} . The block outputs the hidden state H k H_{k} and a memory state M k M_{k} . H k H_{k} is passed to a linear activation layer, which outputs logits a k a_{k} and values V ^ k \\\\hat{V}_{k} . M k M_{k} is fed back as input of the GTrXL block.\\nThe third algorithm used in this work is a modification of the PPO+GTrXL. This custom model implements a PPO agent in which observations and turbulent flow history are first separately encoded, then jointly processed by a stack of GTrXL blocks, and finally decoded into three objectives: action logits, value estimates, and next‚Äêflow‚Äêsnapshot predictions. A dedicated encoder ingests a sequence of past flow vectors with a predefined sequence length, applies multiscale 1D convolutions (kernel sizes 3 and 5) to capture short‚Äê and medium‚Äêterm temporal features, fuses them via a linear layer, and feeds the result through a GRU to model longer‚Äêterm dynamics, normalizing the output to produce per‚Äêtimestep flow embeddings. In parallel, the agent‚Äôs proprioceptive characteristics are projected through a PPO in the same embedding space. A small CNN processes a local flow patch into an embedding, which is concatenated along with the flow‚Äêhistory and observation embeddings. All tokens are then augmented with learnable relative positional encodings and passed through GTrXL blocks, each combining multi‚Äêhead self‚Äêattention with gated residual connections and a feedforward GRU to produce contextually enriched representations. Finally, the most recent observation token is routed to separate policy and value heads for PPO‚Äôs on-policy updates, while the most recent flow token is sent to a flow-prediction head trained with a supervised loss. By jointly optimizing control and flow forecasting within this multi‚Äêobjective architecture, the model leverages transformer-based global attention over CNN+GRU‚Äìderived embeddings to navigate and anticipate complex 3D turbulent dynamics in real time. Algorithm 3 gives the pseudo-code, and Figure 4(c) shows a sketch of the algorithm.\\nThe total loss is the sum of the basic vanilla PPO loss and a supervised auxiliary loss on the flow prediction. The auxiliary loss is based on contrastive learning. We propose a contrastive learning framework to improve flow-field representations by learning to distinguish between relevant and irrelevant flow patterns for navigation. Rather than relying solely on reconstruction accuracy, our approach is based on the principle that flows beneficial for navigation should have similar representations, while flows from different contexts should be distinguishable in the learned embedding space.\\nThe contrastive flow loss operates on predicted and target flow fields ùêü ^ , ùêü ‚àà ‚Ñù d \\\\hat{\\\\mathbf{f}},\\\\mathbf{f}\\\\in\\\\mathbb{R}^{d} by first encoding them through a shared projection network œï : ‚Ñù d ‚Üí ‚Ñù h \\\\phi:\\\\mathbb{R}^{d}\\\\rightarrow\\\\mathbb{R}^{h} comprising two fully-connected layers with ReLU activation and layer normalization. The resulting embeddings are L2-normalized to unit vectors:\\nWe define the positive similarity between predicted and target flows as their scaled dot product:\\nwhere œÑ > 0 \\\\tau>0 is a temperature hyperparameter. When negative samples { ùêü j ‚àí } j = 1 K \\\\{\\\\mathbf{f}_{j}^{-}\\\\}_{j=1}^{K} are available (sampled from different temporal steps or spatial locations), we compute negative similarities:\\nThe contrastive loss follows the Information Noise Contrastive Estimation (InfoNCE) objective [ 38 ] , which maximizes mutual information between positive pairs while treating negative samples as noise to contrast against:\\nThis formulation encourages the model to assign high similarity to the predicted-target pair while maintaining low similarity to negative samples. When negative samples are unavailable, we use the simplified objective ‚Ñí contrastive = ‚àí sim + \\\\mathcal{L}_{\\\\text{contrastive}}=-\\\\text{sim}_{+} , which directly maximizes the similarity between predictions and targets. This contrastive formulation encourages the model to learn flow representations that capture meaningful structure, improving the agent‚Äôs ability to reason about flow dynamics for trajectory planning.\\nFinally, the results of the described approaches are compared with a classical optimization algorithm. This work considers the Zermelo‚Äôs navigation algorithm as applied in [ 18 ] ran on the 3D urban environment described in Section Environment of the DRL algorithm. The Zermelo‚Äôs optimal navigation problem minimizes a cost functional:\\nsubject to system dynamics, control bounds | u | |u| ‚â§ \\\\leq u m \\u200b a \\u200b x u_{max} , collision avoidance constraints, and domain boundaries.\\nThe term T f T_{f} represents the final time (mission duration), and its minimization promotes time-efficient flight.\\nThe control effort is penalized by the quadratic term\\nwhere œá \\\\mathbf{\\\\chi} is the control input vector (yaw rate, pitch rate, thrust), and R R is a positive definite weighting matrix. This term ensures smooth and energy-efficient maneuvers by discouraging large control inputs.\\nTo avoid collisions, the cost function includes an obstacle avoidance penalty\\nwhere d i \\u200b ( ùê± ) d_{i}(\\\\mathbf{x}) is the distance from the current position ùê± \\\\mathbf{x} to the i i -th obstacle, and Œ± i , Œ≤ i > 0 \\\\alpha_{i},\\\\beta_{i}>0 are tuning parameters that control the strength and sharpness of the penalty.\\nA terminal cost is imposed through\\nwhich penalizes deviations from the desired target position ùê± target \\\\mathbf{x}_{\\\\text{target}} at the final time T f T_{f} , with Œ∫ > 0 \\\\kappa>0 controlling the importance of accurately reaching the target.\\nAll cost components involving control and obstacle penalties are accumulated over the trajectory via the integral\\nwhich ensures that efficiency and safety are maintained throughout the entire mission duration.\\nThe trajectory is parameterized using B-spline basis functions with control points C j C_{j} , transforming the infinite-dimensional optimal control problem into a finite-dimensional nonlinear programming problem solved by sequential quadratic programming. Collision constraints are enforced through penalty methods using the environment‚Äôs geometric obstacle detection, while the flow field influence is incorporated through trilinear interpolation of the discretized velocity data at each trajectory evaluation point.\\nThis comparison is made to enable direct benchmarking between the classical optimization algorithm and DRL-learned policies.',\n",
       "    'subsections': [{'title': 'Numerical database and data pre-processing',\n",
       "      'paragraphs': 'The environment is a three-dimensional high-fidelity simulation of an urban turbulent flow field, with two obstacles representing the buildings. The simulation was run using the spectral-element code Nek5000. All the details can be found in Zampino et al. [ 44 ] . The domain coordinates are x ‚àà [ ‚àí 2.0 , 5.0 ] x\\\\in[-2.0,5.0] , y ‚àà [ 0 , 3.0 ] y\\\\in[0,3.0] , z ‚àà [ ‚àí 1.0 , 1.0 ] z\\\\in[-1.0,1.0] . The coordinates of the obstacles are x o \\u200b 1 x_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , y o \\u200b 1 y_{o1} ‚àà [ 0.0 , 1.0 ] \\\\in[0.0,1.0] , z o \\u200b 1 z_{o1} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] , x o \\u200b 2 x_{o2} ‚àà [ 1.25 , 1.75 ] \\\\in[1.25,1.75] , y o \\u200b 2 y_{o2} ‚àà [ 0.0 , 0.5 ] \\\\in[0.0,0.5] , z o \\u200b 2 z_{o2} ‚àà [ ‚àí 0.25 , 0.25 ] \\\\in[-0.25,0.25] . All distances are scaled by the height of the upstream obstacle h h . The dataset used for the DRL algorithms is a set of 300 snapshots separated by 0.08750 time units, with the time span of the dataset being 26.25 time units. Time is normalized with h h and the freestream velocity U ‚àû U_{\\\\infty} . Due to the dimensions of the simulation results, an efficient pre-processing strategy had to be adopted. In order to avoid Input/Output (I/O) overhead during the execution of the algorithm described in subsection Algorithms, the flow field was split into small blocks for each snapshot. The original mesh of the numerical simulation was interpolated on a structured grid of 250 √ó \\\\times 250 √ó \\\\times 250 points. Each snapshot of the simulation has blocks of dimensions 10 √ó \\\\times 10 √ó \\\\times 10 mesh cells in x , y x,y and z z grid points. The blocks overlap to ensure that each cell of the domain is covered during the navigation of the UAV, so that the flow field can be continuously mapped. The core of the implementation is the lightweight, scalable framework combined with disk-efficient storage of the decomposed data and a local tricubic interpolation in space and time using KD-tree indexing.\\nAlgorithm 1 presents a lightweight, on-the-fly extraction of the velocity field in a block-decomposed simulation, performed using tricubic interpolation in space and cubic interpolation in time. Beginning with a mesh file that defines the global domain, the algorithm automatically scans a directory of per-block output files, each named by its time step and spatial indices, to build a time-indexed metadata map. For each stored timestep, it computes the physical spatial bounds of every block, determines the center of each block, and inserts these centers into a KD-tree to enable logarithmic-time nearest-neighbor searches.\\nAt query time for a given position ( x , y , z ) (x,y,z) and time t t , the algorithm first identifies the bracketing time interval [ t i , t i + 1 ] [t_{i},t_{i+1}] containing t t . The it performs spatial interpolation at four control times { t i ‚àí 1 , t i , t i + 1 , t i + 2 } \\\\{t_{i-1},t_{i},t_{i+1},t_{i+2}\\\\} to construct a cubic temporal interpolation stencil, with appropriate boundary clamping when fewer than four timesteps are available. For each control time, only the k k nearest blocks to the query position are considered, loaded on-demand using an in-memory cache to minimize disk I/O overhead.\\nWithin each relevant block, a parallelized tricubic interpolation routine (implemented via a pybind11 C++ module) computes the velocity components using Catmull-Rom splines over a 4√ó4√ó4 neighborhood. When multiple blocks contain the query position, their spatial estimates are combined via inverse-distance weighting to ensure smooth transitions across block boundaries. If no block strictly contains the query point, the algorithm falls back to using the nearest block regardless of its spatial bounds.\\nFinally, the four spatially-interpolated velocity vectors are blended temporally using Catmull-Rom cubic splines with the interpolation factor Œ± = ( t ‚àí t i ) / ( t i + 1 ‚àí t i ) \\\\alpha=(t-t_{i})/(t_{i+1}-t_{i}) to produce the final velocity vector ùêÆ \\u200b ( x , y , z , t ) \\\\mathbf{u}(x,y,z,t) . The caching strategy ensures that once blocks around ( x , y , z ) (x,y,z) are loaded for one control time, they remain available for neighboring times, significantly improving performance in realistic workflows. By never storing the full four-dimensional dataset in memory, this approach scales efficiently to very large simulations while delivering fast and accurate reconstruction in both space and time.\\nDuring the UAV navigation, the underlying flow field components have to be retrieved to inform the agent with the velocity vector components in real-time. The procedure is explained in Algorithm 2 . First, the position of the UAV is clamped to the valid range of coordinates corresponding to the simulation domain. Then, the query position is quantized with the current time to form a cache key, checking if this specific time-position combination has already been computed. If the cache key is missing, the algorithm proceeds with full tricubic spatial and cubic temporal interpolation.\\nFor temporal interpolation, the algorithm identifies the bracketing time interval and constructs a four-point Catmull-Rom stencil using control times { t i ‚àí 1 , t i , t i + 1 , t i + 2 } \\\\{t_{i-1},t_{i},t_{i+1},t_{i+2}\\\\} , with appropriate boundary clamping when at the start or end of the time series. At each of these four control times, spatial interpolation is performed using the procedure described in Algorithm 1 , which employs tricubic interpolation within individual blocks and inverse-distance weighting to blend results from multiple overlapping blocks.\\nThe four spatially-interpolated velocity vectors are then combined using Catmull-Rom cubic splines in time with the interpolation factor Œ± = ( t ‚àí t i ) / ( t i + 1 ‚àí t i ) \\\\alpha=(t-t_{i})/(t_{i+1}-t_{i}) . The final result is stored in the cache as the last valid velocity, and the velocity vector is returned. This caching strategy ensures that subsequent queries at nearby positions and times experience minimal computational overhead while maintaining high accuracy in the velocity field extraction.',\n",
       "      'subsections': []},\n",
       "     {'title': 'UAV dynamics',\n",
       "      'paragraphs': 'The UAV is modeled as a mass point with six degrees of freedom in translation and two in orientation, in particular yaw œà \\\\psi and pitch œë \\\\vartheta . The state vector is defined as\\nand evolves under the combined influence of thrust V V , the turn rates expressed as Œî \\u200b œà \\\\Delta\\\\psi and Œî \\u200b œë \\\\Delta\\\\vartheta and the underlying flow field velocity components ùêÆ ùêüùê•ùê®ùê∞ = ( u f , v f , w f ) \\\\mathbf{u_{flow}}=(u_{f},v_{f},w_{f}) . Note that u g , v g , w g u_{g},v_{g},w_{g} are the u , v , w u,v,w components of the velocity vector calculated as a sum of the UAV and flow field velocity components in each direction, respectively. Furthermore, x , y , z x,y,z are the coordinates of the position of the UAV at the current timestep. Since the UAV is represented as a mass point, the roll angle is not included in the system of equations, which is then described as:\\nŒî \\u200b œà \\\\Delta\\\\psi and Œî \\u200b œë \\\\Delta\\\\vartheta are the variations of the yaw and pitch angles in the Œî \\u200b t \\\\Delta t time interval.\\nThe state of the UAV at time t + 1 t+1 is given by a classical fourth‚Äêorder Runge‚ÄìKutta integrator (RK4). In this work, Œî \\u200b t \\\\Delta t corresponds to 0.08750 time units and is divided into 40 RK4 substeps. Because the flow field is only updated every Œî \\u200b t \\\\Delta t , a single RK4 step would assume that the flow is spatially uniform along the UAV trajectory, leading to significant errors whenever the vehicle traverses regions of strong velocity gradients. By integrating the dynamics at 40 intermediate positions, we faithfully capture these spatial variations. Although RK4 is formally unconditionally stable for smooth ordinary differential equations (ODEs), coupling to a block‚Äêdecomposed interpolation grid imposes a pseudo‚ÄêCourant‚ÄìFriedrichs‚ÄìLewy (pseudo-CFL) constraint: the UAV must not step more than a small fraction of a block or execute a large angular deflection in one step, otherwise the KD‚Äêtree lookups may skip blocks and cause artifacts. Using multiple RK4 substeps per timestep in the environment ensures a smooth evolution of the state of the UAV. This approach maintains continuity in position and orientation even when the agent applies maximum thrust and angular rate commands. Finally, having\\nblock‚Äêtree structures and recently accessed block data remaining in memory, the additional integration calls per interval incur negligible extra I/O. Consequently, the sub‚Äêstepping strategy delivers substantial gains in accuracy and robustness at minimal computational cost. These considerations justify the use of RK4 sub-steps per state update, yielding an effective balance between numerical stability and performance in large‚Äêscale block‚Äêdecomposed simulations.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Environment of the DRL algorithm',\n",
       "      'paragraphs': 'Deep reinforcement learning has been widely used in recent years for control and optimization in fluid mechanics [ 12 , 39 , 11 ] .\\nThe environment is built using Gymnasium [ 37 ] , an open-source library for reinforcement learning, focusing on the interface for simulation environments.\\nObservation and action spaces are continuous. The observation space is defined as:\\nwhere œà , œë \\\\psi,\\\\vartheta are the yaw and pitch angles of the UAV, œà target , Œ∏ target \\\\psi_{\\\\textrm{target}},\\\\theta_{\\\\textrm{{target}}} the relative yaw and pitch angles of the UAV with respect to the target, d target d_{\\\\textrm{{target}}} the Euclidean distance between the UAV and the target, x , y , z x,y,z the coordinates of the UAV in the domain, and Œ∏ i , œï j \\\\theta_{i},\\\\phi_{j} the elevation and azimuth angles associated with the obstacle detection sensors, with i ‚àà [ 0 , 8 ] i\\\\in[0,8] and j ‚àà [ 0 , 4 ] j\\\\in[0,4] , spanning the angles between ‚àí œÄ -\\\\pi and œÄ \\\\pi in their respective directions. Figures 3(a) and 3(b) graphically show the definition of the environment.\\nThe action space is designed to include thrust V ‚àà [ ‚àí 2.0 , 2.0 ] V\\\\in[-2.0,2.0] and yaw and pitch changes Œî \\u200b œà ‚àà [ ‚àí œÄ / 4 , œÄ / 4 ] \\\\Delta\\\\psi\\\\in[-\\\\pi/4,\\\\pi/4] and Œî \\u200b œë ‚àà [ ‚àí œÄ / 4 , œÄ / 4 ] \\\\Delta\\\\vartheta\\\\in[-\\\\pi/4,\\\\pi/4] , respectively. Obstacles are detected by providing the agent with a set of directions, since UAVs are in relation to the surroundings typically by images from cameras,\\nradar signals, or range finders. In this work, obstacle detection is achieved by implementing a\\nray-tracing technique [ 35 ] . First of all, the UAV has to check for free space in its perspective. The input is the position\\nof the UAV and the output is a boolean variable which indicates whether the path is free from obstacles or not. Then, if\\nthe obstacle is present, the intersection with the traced rays is computed. First, the direction of the ray is calculated,\\nbased on the ray origin and final point, as well as the coordinates of the obstacles. Then, it is verified whether parallel\\ndirections to the obstacles are present. If the detected directions are not parallel to the obstacles, the intersection point\\nbetween the ray and the obstacle is calculated and the distance to the intersection is returned. Figure 3(c) sketches the process.\\nThe starting and target points are randomly chosen before the first and after the second obstacle in the domain, as well as the initial orientation of the UAV. The starting snapshot of the algorithm is also randomly chosen among the 300 available. This setup is chosen so that the initial conditions of the flow field themselves exhibit uncertainties. The agent can take a maximum of 100 steps in the domain. The UAV state vector is defined in Equation ( 1 ) and it is inferred from the observations, which are given as input to the neural network and described in Equation\\n( 3 ). The problem described here can be then considered as Partially Observable Markov Decision Process (POMDP). A POMDP is characterized by the fact that the agent cannot directly observe the state s t s_{t} , but receives a set of observations o t o_{t} with a distribution p \\u200b ( o t | s t ) p(o_{t}|s_{t}) . The sequence of observations does not satisfy the Markov property, since p \\u200b ( o t + 1 | a t , o t , a t ‚àí 1 , o t ‚àí 1 , ‚Ä¶ , o 0 ) ‚â† p \\u200b ( o t + 1 | o t , a t ) p(o_{t+1}|a_{t},o_{t},a_{t-1},o_{t-1},...,o_{0})\\\\neq p(o_{t+1}|o_{t},a_{t}) . Consequently, the agent has to infer the current state s t s_{t} based on the history of trajectories.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Reward function',\n",
       "      'paragraphs': 'DRL is a process that encourages learning by trial and error and this process is triggered by a reward which is given to the agent when it takes the right actions to complete the assigned task. The structure of the reward is crucial because this guides the agent towards a more effective learning. This component of the algorithm has to be carefully designed and tuned for a specific task. The reward structure has been extended from the 2D version of the problem in Tonti et al. [ 36 ] . The reward structure is designed to guide the UAV towards the target, while minimizing collisions with obstacles, reducing energy consumption and preventing leaving the designated operational bounds. The reward function is given by several components, each one addressing a different aspect of the UAV‚Äôs performance. The final reward at each time step t t is the sum of:\\nTransition reward r trans r_{\\\\mathrm{trans}} : proportional to the reduction in distance to the target,\\nwhere œÉ ‚àà ‚Ñù \\\\sigma\\\\in\\\\mathbb{R} is a scaling constant and ùê± \\\\mathbf{x} is the position vector.\\nObstacle penalty r obs r_{\\\\mathrm{obs}} : an exponential penalty based on the minimum distance d min d_{\\\\min} to any obstacle,\\nwith Œæ , Œ≤ ‚àà ‚Ñù \\\\xi,\\\\beta\\\\in\\\\mathbb{R} , and d min = min \\u2061 { d 1 , ‚Ä¶ , d n } . d_{\\\\min}=\\\\min\\\\{d_{1},\\\\dots,d_{n}\\\\}.\\nFree‚Äêspace bonus r free r_{\\\\mathrm{free}} :\\nwhere R free ‚àà ‚Ñù R_{\\\\mathrm{free}}\\\\in\\\\mathbb{R} is a constant.\\nBest‚Äêdirection bonus r best r_{\\\\mathrm{best}} : when the forward direction is blocked, a small reward in proportion to the chosen changes in angles,\\nwhere Œî \\u200b œï , Œî \\u200b œë \\\\Delta\\\\phi,\\\\Delta\\\\vartheta are the yaw and pitch offsets of the best free direction.\\nStep penalty r step r_{\\\\mathrm{step}} : a constant negative reward per time step,\\nProximity‚Äìvelocity penalty r prox r_{\\\\mathrm{prox}} : discourages high speed when very close to the target,\\nEnergy penalty r energy r_{\\\\mathrm{energy}} : proportional to the norm of the propulsion velocity,\\nCombining all the components, the reward is defined as follows for each time step t t :\\nAt the end of an episode of m m ‚àà ‚Ñï \\\\in\\\\mathbb{N} steps, we add:\\nTarget reached bonus : if ‚à• ùê± m ‚àí ùê± target ‚à• ‚â§ R target \\\\lVert\\\\mathbf{x}_{m}-\\\\mathbf{x}_{\\\\mathrm{target}}\\\\rVert\\\\leq R_{\\\\mathrm{target}} , then a positive constant is added to the reward and the episode ends. R target R_{\\\\mathrm{target}} is the radius of the target sphere.\\nCollision penalty : if a collision is detected, a negative constant is added and the episode ends.\\nOut‚Äêof‚Äêbounds penalty : if the UAV exits the domain, [ x min , x max ] √ó [ y min , y max ] √ó [ z min , z max ] [x_{\\\\min},x_{\\\\max}]\\\\times[y_{\\\\min},y_{\\\\max}]\\\\times[z_{\\\\min},z_{\\\\max}] , a negative constant is added, and the episode ends.\\nNear‚Äêtarget bonus : if | ‚à• ùê± m ‚àí ùê± target ‚à• ‚àí R target | < 0.5 \\\\bigl|\\\\lVert\\\\mathbf{x}_{m}-\\\\mathbf{x}_{\\\\mathrm{target}}\\\\rVert-R_{\\\\mathrm{target}}\\\\bigr|<0.5 , then a small positive constant is added.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Algorithms',\n",
       "      'paragraphs': 'Three different algorithms are compared. The first is the Proximal Policy Optimization (PPO) [ 31 ] with Long Short-Term Memory (LSTM) cells [ 15 ] . The second algorithm tested is a PPO with a Gated Transformer eXtra Large (GTrXL) unit [ 25 ] . The third is a PPO + GTrXL but with a dedicated auxiliary task to integrate a flow prediction head to improve UAV navigation in the environment. The task is then not only trajectory optimization, but a multi-objective DRL where the flow field is also predicted. The training is made on 200 snapshots of the dataset, while 100 snapshots are used for inference to test the learned policies in an unknown environment. In three-dimensional environments, the fixed-size hidden state of the LSTM and the inherently sequential update scheme limit its capacity to maintain and selectively recall the information necessary for effective policy learning. Although LSTM cells can mitigate short-term dependencies via gated memory updates, their recurrence still enforces a strict temporal bottleneck. Each new observation must propagate through all intermediate time steps before influencing the current decision, which can cause important distant events to be ‚Äúforgotten‚Äù or diluted in the hidden state over long horizons. Moreover, LSTMs lack an explicit mechanism for relating non-adjacent states, making it difficult to capture the complex spatial relationships that arise in 3D domains. On the other hand, the GTrXL architecture embeds multihead self-attention layers with gated residual connections, allowing the agent to attend directly to any past observation regardless of its temporal distance. This non-sequential attentional access not only alleviates vanishing-gradient issues but also provides a flexible memory buffer whose capacity grows with trajectory length, enabling more robust encoding of 3D structure and long-range dependencies. Figure 4(b) shows the structure of a GTrXL block. The input sequence Y œÑ Y_{\\\\tau} enters the block and first undergoes input embedding ( E œÑ E_{\\\\tau} ), producing I œÑ I_{\\\\tau} , which is the embedded input. The core processing consists of two sequential components with gated connections. First, the multi-head attention layer computes queries ( Q œÑ Q_{\\\\tau} ), keys ( K œÑ K_{\\\\tau} ), and values ( V œÑ V_{\\\\tau} ) from I ~ œÑ \\\\tilde{I}_{\\\\tau} , applies attention with masking to hide future data ( A œÑ A_{\\\\tau} ) with relative positional encoding ( R œÑ R_{\\\\tau} ), followed by a GRU gate ( G œÑ G_{\\\\tau} ) that controls information integration and layer normalization. Second, a position-wise feed-forward network ( F œÑ F_{\\\\tau} ) is applied, followed by another GRU gate and layer normalization. Both components use skip connections (curved arrows) that enable direct gradient flow. The block maintains recurrent memory states M œÑ M_{\\\\tau} and M œÑ + 1 M_{\\\\tau+1} that persist across time steps, along with a hidden state H œÑ H_{\\\\tau} for capturing long-term dependencies. The output consists of the processed sequence and updated memory state M œÑ + 1 M_{\\\\tau+1} . The key variables are: Y œÑ Y_{\\\\tau} (input sequence), E œÑ E_{\\\\tau} (input embedding), I ~ œÑ \\\\tilde{I}_{\\\\tau} (embedded input), H œÑ H_{\\\\tau} (recurrent hidden state), M œÑ M_{\\\\tau} (input memory), M œÑ + 1 M_{\\\\tau+1} (output memory), Q œÑ Q_{\\\\tau} / K œÑ K_{\\\\tau} / V œÑ V_{\\\\tau} (query/key/value matrices), A œÑ A_{\\\\tau} (attention weights), R œÑ R_{\\\\tau} (relative positional encoding), F œÑ F_{\\\\tau} (feed-forward transformation), G œÑ G_{\\\\tau} (GRU gates). This architecture combines transformer attention mechanisms with RNN gating properties, enabling both effective sequence modeling and stable training dynamics through controlled information flow. Figure 4(a) shows the PPO+GTrXL architecture, where the MLP encoder is here the PPO algorithm. The observations y k ‚àí n , ‚Ä¶ , y k {y_{k-n},...,y_{k}} are the inputs to the encoder, which here is a PPO. The encoded observations are then fed to the GTrXL block, which receives a memory state M k ‚àí 1 M_{k-1} . The block outputs the hidden state H k H_{k} and a memory state M k M_{k} . H k H_{k} is passed to a linear activation layer, which outputs logits a k a_{k} and values V ^ k \\\\hat{V}_{k} . M k M_{k} is fed back as input of the GTrXL block.\\nThe third algorithm used in this work is a modification of the PPO+GTrXL. This custom model implements a PPO agent in which observations and turbulent flow history are first separately encoded, then jointly processed by a stack of GTrXL blocks, and finally decoded into three objectives: action logits, value estimates, and next‚Äêflow‚Äêsnapshot predictions. A dedicated encoder ingests a sequence of past flow vectors with a predefined sequence length, applies multiscale 1D convolutions (kernel sizes 3 and 5) to capture short‚Äê and medium‚Äêterm temporal features, fuses them via a linear layer, and feeds the result through a GRU to model longer‚Äêterm dynamics, normalizing the output to produce per‚Äêtimestep flow embeddings. In parallel, the agent‚Äôs proprioceptive characteristics are projected through a PPO in the same embedding space. A small CNN processes a local flow patch into an embedding, which is concatenated along with the flow‚Äêhistory and observation embeddings. All tokens are then augmented with learnable relative positional encodings and passed through GTrXL blocks, each combining multi‚Äêhead self‚Äêattention with gated residual connections and a feedforward GRU to produce contextually enriched representations. Finally, the most recent observation token is routed to separate policy and value heads for PPO‚Äôs on-policy updates, while the most recent flow token is sent to a flow-prediction head trained with a supervised loss. By jointly optimizing control and flow forecasting within this multi‚Äêobjective architecture, the model leverages transformer-based global attention over CNN+GRU‚Äìderived embeddings to navigate and anticipate complex 3D turbulent dynamics in real time. Algorithm 3 gives the pseudo-code, and Figure 4(c) shows a sketch of the algorithm.\\nThe total loss is the sum of the basic vanilla PPO loss and a supervised auxiliary loss on the flow prediction. The auxiliary loss is based on contrastive learning. We propose a contrastive learning framework to improve flow-field representations by learning to distinguish between relevant and irrelevant flow patterns for navigation. Rather than relying solely on reconstruction accuracy, our approach is based on the principle that flows beneficial for navigation should have similar representations, while flows from different contexts should be distinguishable in the learned embedding space.\\nThe contrastive flow loss operates on predicted and target flow fields ùêü ^ , ùêü ‚àà ‚Ñù d \\\\hat{\\\\mathbf{f}},\\\\mathbf{f}\\\\in\\\\mathbb{R}^{d} by first encoding them through a shared projection network œï : ‚Ñù d ‚Üí ‚Ñù h \\\\phi:\\\\mathbb{R}^{d}\\\\rightarrow\\\\mathbb{R}^{h} comprising two fully-connected layers with ReLU activation and layer normalization. The resulting embeddings are L2-normalized to unit vectors:\\nWe define the positive similarity between predicted and target flows as their scaled dot product:\\nwhere œÑ > 0 \\\\tau>0 is a temperature hyperparameter. When negative samples { ùêü j ‚àí } j = 1 K \\\\{\\\\mathbf{f}_{j}^{-}\\\\}_{j=1}^{K} are available (sampled from different temporal steps or spatial locations), we compute negative similarities:\\nThe contrastive loss follows the Information Noise Contrastive Estimation (InfoNCE) objective [ 38 ] , which maximizes mutual information between positive pairs while treating negative samples as noise to contrast against:\\nThis formulation encourages the model to assign high similarity to the predicted-target pair while maintaining low similarity to negative samples. When negative samples are unavailable, we use the simplified objective ‚Ñí contrastive = ‚àí sim + \\\\mathcal{L}_{\\\\text{contrastive}}=-\\\\text{sim}_{+} , which directly maximizes the similarity between predictions and targets. This contrastive formulation encourages the model to learn flow representations that capture meaningful structure, improving the agent‚Äôs ability to reason about flow dynamics for trajectory planning.\\nFinally, the results of the described approaches are compared with a classical optimization algorithm. This work considers the Zermelo‚Äôs navigation algorithm as applied in [ 18 ] ran on the 3D urban environment described in Section Environment of the DRL algorithm. The Zermelo‚Äôs optimal navigation problem minimizes a cost functional:\\nsubject to system dynamics, control bounds | u | |u| ‚â§ \\\\leq u m \\u200b a \\u200b x u_{max} , collision avoidance constraints, and domain boundaries.\\nThe term T f T_{f} represents the final time (mission duration), and its minimization promotes time-efficient flight.\\nThe control effort is penalized by the quadratic term\\nwhere œá \\\\mathbf{\\\\chi} is the control input vector (yaw rate, pitch rate, thrust), and R R is a positive definite weighting matrix. This term ensures smooth and energy-efficient maneuvers by discouraging large control inputs.\\nTo avoid collisions, the cost function includes an obstacle avoidance penalty\\nwhere d i \\u200b ( ùê± ) d_{i}(\\\\mathbf{x}) is the distance from the current position ùê± \\\\mathbf{x} to the i i -th obstacle, and Œ± i , Œ≤ i > 0 \\\\alpha_{i},\\\\beta_{i}>0 are tuning parameters that control the strength and sharpness of the penalty.\\nA terminal cost is imposed through\\nwhich penalizes deviations from the desired target position ùê± target \\\\mathbf{x}_{\\\\text{target}} at the final time T f T_{f} , with Œ∫ > 0 \\\\kappa>0 controlling the importance of accurately reaching the target.\\nAll cost components involving control and obstacle penalties are accumulated over the trajectory via the integral\\nwhich ensures that efficiency and safety are maintained throughout the entire mission duration.\\nThe trajectory is parameterized using B-spline basis functions with control points C j C_{j} , transforming the infinite-dimensional optimal control problem into a finite-dimensional nonlinear programming problem solved by sequential quadratic programming. Collision constraints are enforced through penalty methods using the environment‚Äôs geometric obstacle detection, while the flow field influence is incorporated through trilinear interpolation of the discretized velocity data at each trajectory evaluation point.\\nThis comparison is made to enable direct benchmarking between the classical optimization algorithm and DRL-learned policies.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Acknowledgments',\n",
       "    'paragraphs': 'Federica Tonti and Ricardo Vinuesa acknowledge funding from the European Union‚Äôs HORIZON\\nResearch and Innovation Program, project REFMAP, under Grant Agreement number 101096698. The computations were carried out at the supercomputer Dardel at PDC, KTH, and the computer time was provided by the\\nNational Academic Infrastructure for Supercomputing in Sweden (NAISS).',\n",
       "    'subsections': []},\n",
       "   {'title': 'Data Availability Statement',\n",
       "    'paragraphs': 'All the codes and data used in this work will be made available open access when the article is published here: https://github.com/KTH-FlowAI',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for delivery and surveillance purposes. In this work, we develop an optimal navigation strategy based on Deep Reinforcement Learning. The environment is represented by a three-dimensional high-fidelity simulation of an urban flow, characterized by turbulence and recirculation zones. The algorithm presented here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated Transformer eXtra Large (GTrXL) architecture, giving the agent richer information about the turbulent flow field in which it navigates. The results are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO combined with Long Short Term Memory (LSTM) cells and a traditional navigation algorithm. The obtained results show a significant increase in the success rate (SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the classical Zermelo‚Äôs navigation algorithm, paving the way to a completely reimagined UAV landscape in complex urban environments.'},\n",
       " '2510.25616v1': {'content': [{'title': '1. Introduction',\n",
       "    'paragraphs': 'Vision‚ÄìLanguage Models (VLMs) have demonstrated remarkable success due to their ability to integrate large-scale multimodal datasets, thereby acquiring semantic grounding and generalizable visual-language (VL) representations (NVIDIA et al., 2025a ; Bai et al., 2025 ; Wang et al., 2025b ; Beyer et al., 2024 ; Driess et al., 2023 ; Awadalla et al., 2023 ) . When exposed to novel visual or linguistic contexts, such models exhibit robust cross-modal understanding and compositional perception ‚Äì properties that underpin their strong zero and few-shot generalization beyond the training distribution. These advancements have naturally inspired the extension of VLMs toward embodied domains.\\nVision‚ÄìLanguage‚ÄìAction (VLA) models represent a prominent direction in this research trajectory. They adapt pretrained VLMs to action prediction tasks in robotic settings, with the goal of leveraging the semantic priors and cognition abilities inherited from large-scale vision‚Äìlanguage pretraining. The underlying hypothesis is that, if appropriately adapted, VLA models can transfer the visual‚Äìsemantic representations of their initial VLM to the action domain, enabling generalization to previously unseen scenes, instructions, and scenarios. However, in practice, adapting VLMs to the action modality often introduces new challenges. Several recent studies (Pugacheva et al., 2025 ; Driess et al., 2025 ; Liu et al., 2023 ; Mees et al., 2022 ; Chen et al., 2023 ) have shown that current VLA models struggle to maintain generalization in visually and linguistically complex tasks, raising questions about whether strong VL capabilities of VLMs truly transfer to embodied settings. This issue becomes the most evident during task-specific fine-tuning, where limited data diversity and datasets frequently lead to overfitting (Zang et al., 2024 ; Staroverov et al., 2023 ; Ding et al., 2024 ; Pugacheva et al., 2025 ; Cherepanov et al., 2025b ) .\\nDuring large-scale robotic pretraining, recent works have attempted to mitigate this degradation by preserving multimodal understanding capabilities. Prior strategies include incorporating auxiliary reasoning objectives (Chen et al., 2025 ) , applying multimodal co-training on web-scale data (Yang et al., 2025 ) , or freezing pretrained visual‚Äìlanguage backbones to preserve VL representations and improve instruction following (NVIDIA et al., 2025b ; Driess et al., 2025 ) . While these approaches help retain vision‚Äìlanguage knowledge and improve generalization, they often depend on heavy supervision, high computational cost, or constrained model architecture. Yet, despite these advances at the pretraining stage, there remain no effective methods to address representation degradation during task-specific supervised fine-tuning (SFT) ‚Äì the critical phase where VLA models must adapt to certain robotic domains without losing their semantic grounding and VL abilities.\\nIn this work, we adopt a realistic VLA deployment setting: starting from a pretrained VLA and adapting it with limited data for supervised fine-tuning in a chosen embodiment and domain. Under these constraints, we conduct a systematic investigation into the degradation of VL representations and multimodal understanding abilities in VLA models and ask a central question: Can we design a simple yet effective method to recover the inherited VL representations during fine-tuning on robotic actions?\\nTo answer this question, we first examined the attention maps and feature activations of the VLA model in comparison to VLM‚Äôs across matched image-instruction pairs from the robotics domain.\\nOur analysis of attention maps revealed that: while the pretrained VLM accurately focuses on task-relevant objects, the fine-tuned VLA models often produce diffuse or misplaced activations, failing to attend to key entities under out-of-distribution (OOD) conditions ( Figure\\xa04 ).\\nNext, we conducted a t-SNE (van der Maaten and Hinton, 2008a ) analysis of intermediate representations across VLM‚Äôs and VLA‚Äôs layers, which exposed a clear representation collapse (Barbero et al., 2024 ; Arefin et al., 2024 ) in VLA models ‚Äì indicating that standard action fine-tuning compresses diverse internal features into a narrow representation space, reducing representational diversity and generalization capacity. Next, we propose VL-Think task suite ( section\\xa04 ) to assess transfer of VL knowledge from VLMs to VLA models, benchmark several strong VLMs and compare OpenVLA‚Äì7B (Kim et al., 2024 ) to its pretrained base (PrismaticVLM (Karamcheti et al., 2024 ) ). We observe systematic, domain-specific forgetting after action fine-tuning, indicating that VLAs lose VL knowledge about domains absent from the robotics fine-tuning data.\\nTo address this representational degradation, we introduce a lightweight Visual Representation Alignment method inspired by the Platonic Representation Hypothesis (Huh et al., 2024 ) .\\nThis hypothesis suggests that large vision and language models tend to converge toward a shared latent representation space that encodes general visual and semantic representations across generalist models. Our method explicitly constrains the visual representations of a VLA to remain aligned with a generalist vision model throughout fine-tuning. By maintaining this link, the VLA preserves semantic consistency while adapting its action policy to new tasks. The method adds negligible computational overhead and integrates seamlessly with SFT ( Figure\\xa02 ). Extensive experiments on different variations of Simpler (Li et al., 2024 ) benchmark demonstrates that this alignment consistently improves out-of-distribution generalization ‚Äì yielding up to a 10% relative gain over naive SFT ( LABEL:tab:ood_performance ).\\nOur key contributions are as follows:\\nWe systematically demonstrate that naive VLA fine-tuning induces representation collapse and attention sink relative to their initial VLM.\\nWe introduce VL-Think, a diagnostic task suite for assessing transfer of VL knowledge from VLMs across VLA models and show that VLA action fine-tuning lead to domain-specific forgetting.\\nWe propose a simple and efficient visual alignment method that anchors the VLA‚Äôs vision representations to strong visual teacher features, preserving multimodal understanding and improving OOD generalization without added complexity ( Figure\\xa02 ).\\nTaken together, our findings provide new insights into the trade-off between action fine-tuning and representation degradation in VLA models. They underscore the importance of maintaining visual-language alignment during fine-tuning and provide a practical recipe for building VLAs that do not ‚Äúblind‚Äù the pretrained perceptual knowledge they rely upon.',\n",
       "    'subsections': []},\n",
       "   {'title': '2. Related Works',\n",
       "    'paragraphs': 'VLA models aim to unify perception, reasoning, and control through large-scale multimodal learning. Early approaches such as RT-1 (Brohan et al., 2023b ) and RT-2 (Brohan et al., 2023a ) demonstrated that scaling VL pretraining to robot data enables generalization across diverse manipulation tasks. Subsequent works ‚Äì including OpenVLA (Kim et al., 2024 ) , Octo (Team et al., 2024 ) , MolmoAct (Lee et al., 2025 ) , OneTwoVLA (Lin et al., 2025b ) , and œÄ 0 \\\\pi_{0} (Black et al., 2024 ) ‚Äì explored large scale robotic pretraining, compact diffusion-based policies, modular reasoning architectures, token-based decision sequencing, and continuous flow-matching policies. Across these models, the shared goal is to couple semantic grounding with low-level motor control in a unified policy, while maintaining efficiency and generalization in real-world settings. A central challenge remains the preservation and retention of VL understanding capabilities during robot fine-tuning.\\nRecent studies reveal a consistent pattern: as models scale in parameters, data, and tasks, their representations increasingly align across architectures and modalities. The Platonic Representation Hypothesis (Huh et al., 2024 ) frames this as convergence to a shared statistical model of reality, independently trained vision and language encoders show semantically compatible spaces, and large language-free visual models reach CLIP-level performance while naturally aligning with text (Maniparambil et al., 2024 ; Fan et al., 2025 , 2025 ) .\\nRecent representation learning methods reinforce this trend: REPA (Yu et al., 2025 ) aligns diffusion hidden states to strong image encoders (faster training, better ImageNet quality), OLA-VLM (Jain et al., 2025 ) distills multi-teacher targets into intermediate LLM layers via predictive embedding losses, 3DRS (Huang et al., 2025 ) injects 3D-aware supervision with multi-view correspondence, and Geometry Forcing (Wu et al., 2025 ) aligns video-diffusion features with a 3D backbone via angular/scale objectives for temporally consistent generations.',\n",
       "    'subsections': [{'title': '2.1. Vision-Language-Action models',\n",
       "      'paragraphs': 'VLA models aim to unify perception, reasoning, and control through large-scale multimodal learning. Early approaches such as RT-1 (Brohan et al., 2023b ) and RT-2 (Brohan et al., 2023a ) demonstrated that scaling VL pretraining to robot data enables generalization across diverse manipulation tasks. Subsequent works ‚Äì including OpenVLA (Kim et al., 2024 ) , Octo (Team et al., 2024 ) , MolmoAct (Lee et al., 2025 ) , OneTwoVLA (Lin et al., 2025b ) , and œÄ 0 \\\\pi_{0} (Black et al., 2024 ) ‚Äì explored large scale robotic pretraining, compact diffusion-based policies, modular reasoning architectures, token-based decision sequencing, and continuous flow-matching policies. Across these models, the shared goal is to couple semantic grounding with low-level motor control in a unified policy, while maintaining efficiency and generalization in real-world settings. A central challenge remains the preservation and retention of VL understanding capabilities during robot fine-tuning.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2. Representation alignment',\n",
       "      'paragraphs': 'Recent studies reveal a consistent pattern: as models scale in parameters, data, and tasks, their representations increasingly align across architectures and modalities. The Platonic Representation Hypothesis (Huh et al., 2024 ) frames this as convergence to a shared statistical model of reality, independently trained vision and language encoders show semantically compatible spaces, and large language-free visual models reach CLIP-level performance while naturally aligning with text (Maniparambil et al., 2024 ; Fan et al., 2025 , 2025 ) .\\nRecent representation learning methods reinforce this trend: REPA (Yu et al., 2025 ) aligns diffusion hidden states to strong image encoders (faster training, better ImageNet quality), OLA-VLM (Jain et al., 2025 ) distills multi-teacher targets into intermediate LLM layers via predictive embedding losses, 3DRS (Huang et al., 2025 ) injects 3D-aware supervision with multi-view correspondence, and Geometry Forcing (Wu et al., 2025 ) aligns video-diffusion features with a 3D backbone via angular/scale objectives for temporally consistent generations.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3. Preliminaries',\n",
       "    'paragraphs': 'VLA architecture. Let the input multimodal token sequence to the VLM backbone be\\nwhere x 1 : k x_{1:k} correspond to visual tokens\\nand x k + 1 : n x_{k+1:n} correspond to textual instruction tokens.\\nThese tokens are obtained from two encoders:\\nwhere E image E_{\\\\mathrm{image}} and E text E_{\\\\mathrm{text}} denote the image and text encoders into the common\\nembedding space of dimension d e d_{e} of the VLA model,\\nand I I and ‚Ñì \\\\ell are the input image and textual instruction, respectively. The combined sequence x 1 : n x_{1:n} is processed by a multimodal Transformer backbone B Œ∏ : ‚Ñù n √ó d e ‚Üí ‚Ñù n √ó d e B_{\\\\theta}:\\\\mathbb{R}^{n\\\\times d_{e}}\\\\to\\\\mathbb{R}^{n\\\\times d_{e}} with L L stacked layers.\\nDenote the hidden states after layer i i by h 1 : n i ‚àà ‚Ñù n √ó d e h^{i}_{1:n}\\\\in\\\\mathbb{R}^{n\\\\times d_{e}} . Each layer updates the hidden states using standard self-attention with h 1 : n 0 = x 1 : n h^{0}_{1:n}=x_{1:n} .:\\nAutoregressive objective. Let y 1 : m y_{1:m} denote the target output tokens (from the same vocabulary as text tokens).\\nAt the decoding step t t , the model conditions on the concatenation of the input and the previously\\ngenerated tokens:\\nThe Transformer then defines the autoregressive distribution\\nwhere W o W_{o} is the output projection to the token vocabulary, the causal mask in B Œ∏ B_{\\\\theta} ensures that h n + t ‚àí 1 L h^{L}_{n+t-1} depends only on x 1 : n x_{1:n} and y 1 : t ‚àí 1 y_{1:t-1} . Training uses the standard next-token loss:\\nwith mask M M selecting target positions (we consider the usual causal language-modeling setup).',\n",
       "    'subsections': []},\n",
       "   {'title': '4. VL-Think Task Suite',\n",
       "    'paragraphs': 'Current evaluations of VLA models (Liu et al., 2023 ; Mees et al., 2022 ; Cherepanov et al., 2025a ) primarily emphasize task execution under distribution shifts ‚Äì such as changes in objects, scenes, recall-based demands or textures but provide little insight into whether the VL capabilities and knowledge inherited from the pretrained VLM are preserved after action fine-tuning. To address this gap, we introduce the VL-Think Task Suite , a diagnostic suite designed to evaluate the transfer of VL capabilities from VLMs to VLAs independently of their low-level control performance. The suite focuses on testing whether a model continues to understand visual symbols, compositional cues, and categorical distinctions that are commonly evaluated in VLM datasets but underrepresented in robotics domain ‚Äì rather than whether it can successfully execute grasp or placement actions. We intentionally minimize control complexity to ensure that any observed performance degradation reflects a loss of VL understanding, rather than action execution.\\nTo quantify the gap in VL capabilities, we perform evaluations across both VLA and VLM models.\\nVLA evaluation. The agent observes RGB frames and language instructions. The success rate is recorded if a well-known object is placed on the correct target board. Since motion complexity is fixed, this directly measures the model‚Äôs capacity to ground language in visual categories rather than its manipulation skills.\\nVLM evaluation. To assess reasoning in robotics setup without actions, the same scenes are presented as static initial images with the probe: ‚ÄúDo you see the ¬°board_name¬ø?‚Äù. Answer ‚Äòyes‚Äô or ‚Äòno‚Äô. If yes, specify where: ‚Äòleft‚Äô, ‚Äòcenter‚Äô, or ‚Äòright‚Äô‚Äù . A response is counted as successful only if both the predicted board and its target location match the ground truth, yielding a success rate that serves as an action-free measure of semantic grounding.\\nTo reduce the embodiment and setup-specific adaptation bottlenecks, VL-Think Task Suite is based on the realistic Simpler (Li et al., 2024 ) benchmark with WidowX-250S arm pick-and-place task. Each episode spawns a single source well-known object (carrot) positioned to yield 100% grasp reliability and multiple planar ‚Äúboards‚Äù textured with abstract categories (e.g., icons, shapes, numerals). A language instruction specifies a single target concept (shape, color, icon class, direction, or parity). The agent succeeds if it places the carrot on the board that matches the instructed concept. By keeping the objects and action complexity fixed, the evaluation isolates VL skills while bounding execution complexity.\\nThe VL-Think suite consists of eight board-selection tasks that probe different aspects of knowledge (see Figure 3 ). In each task, the agent must place the object on the board that matches the instructed concept: Shape ‚Äì the board whose graphic is the named geometric shape; e.g., ‚ÄúPut the object on the star.‚Äù), Color ‚Äì the board whose shape has the named color; e.g., ‚ÄúPut the object on the blue shape‚Äù, Traffic ‚Äì the board depicting one of 24 common traffic signs; e.g., ‚Äúthe yield sign‚Äù, Laundry care ‚Äì the board depicting one of 17 standard laundry symbols, e.g., ‚ÄúDo not bleach‚Äù, Weather ‚Äì the board depicting one of 9 common weather icons; e.g., ‚Äúsunny‚Äù, ‚Äúcloudy‚Äù, Directional arrow ‚Äì the board whose arrow points in the named direction: ‚Äúup‚Äù, ‚Äúdown‚Äù, ‚Äúleft‚Äù, ‚Äúright‚Äù, Public information ‚Äì the board depicting one of 14 public-information signs; e.g., ‚Äúno dogs allowed‚Äù, and Numeral parity ‚Äì the board whose printed numeral matches the requested parity (‚Äúodd‚Äù or ‚Äúeven‚Äù); e.g., ‚ÄúPut the object on the odd number‚Äù.',\n",
       "    'subsections': [{'title': '4.1. Evaluation protocol',\n",
       "      'paragraphs': 'To quantify the gap in VL capabilities, we perform evaluations across both VLA and VLM models.\\nVLA evaluation. The agent observes RGB frames and language instructions. The success rate is recorded if a well-known object is placed on the correct target board. Since motion complexity is fixed, this directly measures the model‚Äôs capacity to ground language in visual categories rather than its manipulation skills.\\nVLM evaluation. To assess reasoning in robotics setup without actions, the same scenes are presented as static initial images with the probe: ‚ÄúDo you see the ¬°board_name¬ø?‚Äù. Answer ‚Äòyes‚Äô or ‚Äòno‚Äô. If yes, specify where: ‚Äòleft‚Äô, ‚Äòcenter‚Äô, or ‚Äòright‚Äô‚Äù . A response is counted as successful only if both the predicted board and its target location match the ground truth, yielding a success rate that serves as an action-free measure of semantic grounding.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2. VL-Think description',\n",
       "      'paragraphs': 'To reduce the embodiment and setup-specific adaptation bottlenecks, VL-Think Task Suite is based on the realistic Simpler (Li et al., 2024 ) benchmark with WidowX-250S arm pick-and-place task. Each episode spawns a single source well-known object (carrot) positioned to yield 100% grasp reliability and multiple planar ‚Äúboards‚Äù textured with abstract categories (e.g., icons, shapes, numerals). A language instruction specifies a single target concept (shape, color, icon class, direction, or parity). The agent succeeds if it places the carrot on the board that matches the instructed concept. By keeping the objects and action complexity fixed, the evaluation isolates VL skills while bounding execution complexity.\\nThe VL-Think suite consists of eight board-selection tasks that probe different aspects of knowledge (see Figure 3 ). In each task, the agent must place the object on the board that matches the instructed concept: Shape ‚Äì the board whose graphic is the named geometric shape; e.g., ‚ÄúPut the object on the star.‚Äù), Color ‚Äì the board whose shape has the named color; e.g., ‚ÄúPut the object on the blue shape‚Äù, Traffic ‚Äì the board depicting one of 24 common traffic signs; e.g., ‚Äúthe yield sign‚Äù, Laundry care ‚Äì the board depicting one of 17 standard laundry symbols, e.g., ‚ÄúDo not bleach‚Äù, Weather ‚Äì the board depicting one of 9 common weather icons; e.g., ‚Äúsunny‚Äù, ‚Äúcloudy‚Äù, Directional arrow ‚Äì the board whose arrow points in the named direction: ‚Äúup‚Äù, ‚Äúdown‚Äù, ‚Äúleft‚Äù, ‚Äúright‚Äù, Public information ‚Äì the board depicting one of 14 public-information signs; e.g., ‚Äúno dogs allowed‚Äù, and Numeral parity ‚Äì the board whose printed numeral matches the requested parity (‚Äúodd‚Äù or ‚Äúeven‚Äù); e.g., ‚ÄúPut the object on the odd number‚Äù.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5. VL representations analysis',\n",
       "    'paragraphs': 'In this section, we ask: what happens to VL representations and knowledge in VLA models after action fine-tuning? Does knowledge transfer from VLMs actually occur, and is strong semantic grounding retained?\\nTo examine how strongly VL representations degrade in VLA models, we conduct complementary analyses. First, we use t-SNE (van der Maaten and Hinton, 2008b ) visualization to assess whether the model preserves a structured and separable latent space for instruction-related tokens. Second, we analyze attention maps to evaluate how accurately the model focuses on objects referenced in the input instruction. Finally, using the VL-Think suite, we assess the transferability of VLM VL skills to VLA policies. Together, these methods provide intuitive and interpretable diagnostics of VL representation degradation and domain forgetting ‚Äì revealing whether the model maintains focused visual grounding, coherent latent organization and erodes domain-specific knowledge after action fine-tuning.\\nTo further investigate how fine-tuning affects the VL grounding capabilities of VLA models, we examine their attention maps, which reveal how effectively the model focuses on the object referenced in a textual instruction. This analysis provides a direct probe into how well the model maintains connection between visual and language features. For each model, we visualize the attention maps for visual patch embeddings from the middle layers. Following prior studies (Zhang et al., 2025 ) , we observe ( Figure\\xa04 ) that the strongest and most semantically meaningful attention patterns typically emerge in the middle transformer layers (layers 14‚Äì24), where vision‚Äìlanguage fusion is the most active.\\nAmong the evaluated models, Qwen2.5-VL exhibits clear and relevant object-aligned attention, indicating that its attention is precisely localized on the queried object with minimal spatial noise. In contrast, OpenVLA displays substantial degradation in attention quality: the maps become diffuse, noisy, and weakly correlated with the target object indicating attention sink (Kang et al., 2025 ; Lin et al., 2025a ) . Instead of concentrating on relevant image regions, the OpenVLA‚Äôs attention maps frequently leak into irrelevant background regions or concentrate on distractor objects (for more results see subsection\\xa0A.2 ). By contrast, our proposed Visual Representation Alignment approach remedies this issue: OpenVLA (Align) trained with it produces crisp, object-centric attention maps (see subsection\\xa0A.2 for details).\\nTo analyze how action fine-tuning affects the internal VL representations of VLA models, we conducted a t-SNE representation probe comparing Qwen2.5-VL (Bai et al., 2025 ) , PrismaticVLM (Karamcheti et al., 2024 ) , and OpenVLA (Kim et al., 2024 ) . This experiment provides a qualitative view of how the semantic structure in the latent space evolves through the action training process. We use the COCO dataset (Lin et al., 2014 ) and select samples from three common household object classes: cup, bottle, and knife. For each image, the model receives a textual query of the form ‚ÄúDo you see ¬°object_name¬ø?‚Äù . Then we extract the embedding corresponding to the token ¬°object_name¬ø from transformer layers and then project these embeddings into two dimensions using the t-SNE algorithm. Each point in the visualization is color-coded by its object class, allowing us to observe how distinct or entangled the category clusters become.\\nFigure\\xa05 illustrates this comparison for the middle layers, revealing how the latent space is organized across the different model‚Äôs layers. In the PrismaticVLM and Qwen2.5-VL, embeddings for the three categories form well-separated clusters reflecting a coherent and semantically organized latent space typical of large-scale VLMs. In contrast, OpenVLA exhibits blurred and overlapping clusters, indicating that fine-tuning for robot control disrupts the structured organization of its inherited representations. This loss of separability corresponds to a phenomenon akin to representation collapse (Barbero et al., 2024 ; Arefin et al., 2024 ) , where previously distinct VL representations converge into less discriminative subspaces.\\nUsing the VL-Think task suite ( section\\xa04 ), we evaluate VL capabilities across several state-of-the-art VLMs: InternVL3.5 (Wang et al., 2025b ) , Ovis2.5 (Lu et al., 2025 ) , Qwen2.5-VL (Bai et al., 2025 ) and focus on OpenVLA‚Äì7B (Kim et al., 2024 ) versus its pretrained base PrismaticVLM (Karamcheti et al., 2024 ) , which we use as an approximate upper bound. This comparison probes how much VL knowledge and semantic grounding skills persist after action fine-tuning.\\nTwo clear trends emerge. First, strong VLMs achieve high success rate across all domains, reflecting robust semantic grounding. Second, action fine-tuning induces systematic, domain-specific forgetting in VLA models: relative to its pretrained counterpart, OpenVLA‚Äì7B exhibits substantial drops in nearly all domains, with the largest declines in symbolic and abstract categories (traffic, arrows, public information, weather). We hypothesize that VLA models lose knowledge about domains that are absent in robotics fine-tuning datasets. The single domain where transfer persists is Color : the success rate remains at the level of the initial VLM, likely because color cues are directly useful for control and are implicitly present in robotics datasets.',\n",
       "    'subsections': [{'title': '5.1. Attention sink',\n",
       "      'paragraphs': 'To further investigate how fine-tuning affects the VL grounding capabilities of VLA models, we examine their attention maps, which reveal how effectively the model focuses on the object referenced in a textual instruction. This analysis provides a direct probe into how well the model maintains connection between visual and language features. For each model, we visualize the attention maps for visual patch embeddings from the middle layers. Following prior studies (Zhang et al., 2025 ) , we observe ( Figure\\xa04 ) that the strongest and most semantically meaningful attention patterns typically emerge in the middle transformer layers (layers 14‚Äì24), where vision‚Äìlanguage fusion is the most active.\\nAmong the evaluated models, Qwen2.5-VL exhibits clear and relevant object-aligned attention, indicating that its attention is precisely localized on the queried object with minimal spatial noise. In contrast, OpenVLA displays substantial degradation in attention quality: the maps become diffuse, noisy, and weakly correlated with the target object indicating attention sink (Kang et al., 2025 ; Lin et al., 2025a ) . Instead of concentrating on relevant image regions, the OpenVLA‚Äôs attention maps frequently leak into irrelevant background regions or concentrate on distractor objects (for more results see subsection\\xa0A.2 ). By contrast, our proposed Visual Representation Alignment approach remedies this issue: OpenVLA (Align) trained with it produces crisp, object-centric attention maps (see subsection\\xa0A.2 for details).',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2. Representations collapse',\n",
       "      'paragraphs': 'To analyze how action fine-tuning affects the internal VL representations of VLA models, we conducted a t-SNE representation probe comparing Qwen2.5-VL (Bai et al., 2025 ) , PrismaticVLM (Karamcheti et al., 2024 ) , and OpenVLA (Kim et al., 2024 ) . This experiment provides a qualitative view of how the semantic structure in the latent space evolves through the action training process. We use the COCO dataset (Lin et al., 2014 ) and select samples from three common household object classes: cup, bottle, and knife. For each image, the model receives a textual query of the form ‚ÄúDo you see ¬°object_name¬ø?‚Äù . Then we extract the embedding corresponding to the token ¬°object_name¬ø from transformer layers and then project these embeddings into two dimensions using the t-SNE algorithm. Each point in the visualization is color-coded by its object class, allowing us to observe how distinct or entangled the category clusters become.\\nFigure\\xa05 illustrates this comparison for the middle layers, revealing how the latent space is organized across the different model‚Äôs layers. In the PrismaticVLM and Qwen2.5-VL, embeddings for the three categories form well-separated clusters reflecting a coherent and semantically organized latent space typical of large-scale VLMs. In contrast, OpenVLA exhibits blurred and overlapping clusters, indicating that fine-tuning for robot control disrupts the structured organization of its inherited representations. This loss of separability corresponds to a phenomenon akin to representation collapse (Barbero et al., 2024 ; Arefin et al., 2024 ) , where previously distinct VL representations converge into less discriminative subspaces.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3. Domain forgetting in VLA models',\n",
       "      'paragraphs': 'Using the VL-Think task suite ( section\\xa04 ), we evaluate VL capabilities across several state-of-the-art VLMs: InternVL3.5 (Wang et al., 2025b ) , Ovis2.5 (Lu et al., 2025 ) , Qwen2.5-VL (Bai et al., 2025 ) and focus on OpenVLA‚Äì7B (Kim et al., 2024 ) versus its pretrained base PrismaticVLM (Karamcheti et al., 2024 ) , which we use as an approximate upper bound. This comparison probes how much VL knowledge and semantic grounding skills persist after action fine-tuning.\\nTwo clear trends emerge. First, strong VLMs achieve high success rate across all domains, reflecting robust semantic grounding. Second, action fine-tuning induces systematic, domain-specific forgetting in VLA models: relative to its pretrained counterpart, OpenVLA‚Äì7B exhibits substantial drops in nearly all domains, with the largest declines in symbolic and abstract categories (traffic, arrows, public information, weather). We hypothesize that VLA models lose knowledge about domains that are absent in robotics fine-tuning datasets. The single domain where transfer persists is Color : the success rate remains at the level of the initial VLM, likely because color cues are directly useful for control and are implicitly present in robotics datasets.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6. Method',\n",
       "    'paragraphs': 'Following the Platonic Representation Hypothesis (Huh et al., 2024 ) ,\\nwe assume that high-performing vision, language, and multimodal models\\ntend to converge toward a shared latent representation space\\nthat captures general semantic and perceptual structure across different modalities.\\nEach modality provides a distinct but compatible view of this shared space,\\nencoding complementary aspects of the same underlying VL regularities.\\nFrom this perspective, a VLA model can be regarded as a policy that grounds its decision-making in a subset of these multimodal representations.\\nHowever, during task-specific fine-tuning,\\nthe policy‚Äôs internal features may drift away from this generalized representation space,\\ncausing it to lose connection to broad, transferable semantics. To mitigate this effect, we introduce a Visual Representation Alignment objective\\nthat anchors the VLA‚Äôs visual representations to a stable external reference\\nencoding consistent, general-purpose visual semantics ( Figure\\xa01 ).\\nWe propose a lightweight visual alignment method that recover\\ngeneralized and semantically consistent visual representations inside\\na VLA model by regularizing its internal embeddings to remain close to those of a frozen, pretrained vision teacher.\\nIn the Platonic interpretation, the teacher encoder provides a more stable\\nand semantically precise projection of the generalized representation space, while the VLA‚Äôs own representations form a task-adapted approximation of this space.\\nBy minimizing their discrepancy, the model is guided back toward a\\ncommon semantic structure.\\nLet E img ‚ãÜ E^{\\\\star}_{\\\\mathrm{img}} denote the frozen teacher encoder that produces patch-level features\\nwhere each patch embedding z m ‚àí 1 : m z_{m-1:m} captures localized visual semantics within the teacher‚Äôs high-level feature space.\\nWithin the VLA model, we select an internal layer i ‚ãÜ i^{\\\\star} that carries\\nsemantically rich visual information and extract the corresponding vision tokens h 1 : k i ‚ãÜ ‚àà ‚Ñù k √ó d e h^{i^{\\\\star}}_{1:k}\\\\in\\\\mathbb{R}^{k\\\\times d_{e}} .\\nSince the dimensionalities differ, we propose a projector P œÜ : ‚Ñù d e ‚Üí ‚Ñù d t P_{\\\\varphi}:\\\\mathbb{R}^{d_{e}}\\\\to\\\\mathbb{R}^{d_{t}} and define\\nWe then compute a patch-wise similarity between the student‚Äôs projected embeddings\\nand the teacher‚Äôs features:\\nThis objective encourages the hidden representations from the VLA‚Äôs latent feature space\\nto remain aligned with the teacher‚Äôs generalized visual representations,\\nhelping preserve perceptual consistency across tasks and environments.\\nThe total loss integrates the standard autoregressive action objective\\nwith the alignment term:\\nHere, ‚Ñí VLA \\\\mathcal{L}_{\\\\mathrm{VLA}} supervises policy learning within the current environment,\\nwhile ‚Ñí align \\\\mathcal{L}_{\\\\mathrm{align}} acts as a regularizer that limits representational drift\\naway from generalized visual features.\\nGradients propagate through the VLA‚Äôs visual encoder E img E_{\\\\mathrm{img}} ,\\ntext encoder E text E_{\\\\mathrm{text}} , and transformer backbone B Œ∏ B_{\\\\theta} ,\\nwhile the teacher encoder E img ‚ãÜ E^{\\\\star}_{\\\\mathrm{img}} remains frozen,\\nserving as a fixed reference to stable perceptual structure. From the Platonic viewpoint, our method maintains an semantic prior\\nto shared, generalized VL knowledge.\\nAction fine-tuning alone narrows the model‚Äôs perceptual space toward the\\nstatistics of a specific dataset or embodiment, causing the internal\\nfeatures to drift away from broad generalized representations.\\nThe alignment loss restores this balance by enforcing consistency between\\nthe student‚Äôs intermediate features and those of a strong, pre-trained vision model that encodes more general visual‚Äìsemantic relationships.',\n",
       "    'subsections': [{'title': '6.1. Visual representation alignment',\n",
       "      'paragraphs': 'We propose a lightweight visual alignment method that recover\\ngeneralized and semantically consistent visual representations inside\\na VLA model by regularizing its internal embeddings to remain close to those of a frozen, pretrained vision teacher.\\nIn the Platonic interpretation, the teacher encoder provides a more stable\\nand semantically precise projection of the generalized representation space, while the VLA‚Äôs own representations form a task-adapted approximation of this space.\\nBy minimizing their discrepancy, the model is guided back toward a\\ncommon semantic structure.\\nLet E img ‚ãÜ E^{\\\\star}_{\\\\mathrm{img}} denote the frozen teacher encoder that produces patch-level features\\nwhere each patch embedding z m ‚àí 1 : m z_{m-1:m} captures localized visual semantics within the teacher‚Äôs high-level feature space.\\nWithin the VLA model, we select an internal layer i ‚ãÜ i^{\\\\star} that carries\\nsemantically rich visual information and extract the corresponding vision tokens h 1 : k i ‚ãÜ ‚àà ‚Ñù k √ó d e h^{i^{\\\\star}}_{1:k}\\\\in\\\\mathbb{R}^{k\\\\times d_{e}} .\\nSince the dimensionalities differ, we propose a projector P œÜ : ‚Ñù d e ‚Üí ‚Ñù d t P_{\\\\varphi}:\\\\mathbb{R}^{d_{e}}\\\\to\\\\mathbb{R}^{d_{t}} and define\\nWe then compute a patch-wise similarity between the student‚Äôs projected embeddings\\nand the teacher‚Äôs features:\\nThis objective encourages the hidden representations from the VLA‚Äôs latent feature space\\nto remain aligned with the teacher‚Äôs generalized visual representations,\\nhelping preserve perceptual consistency across tasks and environments.',\n",
       "      'subsections': []},\n",
       "     {'title': '6.2. Objective',\n",
       "      'paragraphs': 'The total loss integrates the standard autoregressive action objective\\nwith the alignment term:\\nHere, ‚Ñí VLA \\\\mathcal{L}_{\\\\mathrm{VLA}} supervises policy learning within the current environment,\\nwhile ‚Ñí align \\\\mathcal{L}_{\\\\mathrm{align}} acts as a regularizer that limits representational drift\\naway from generalized visual features.\\nGradients propagate through the VLA‚Äôs visual encoder E img E_{\\\\mathrm{img}} ,\\ntext encoder E text E_{\\\\mathrm{text}} , and transformer backbone B Œ∏ B_{\\\\theta} ,\\nwhile the teacher encoder E img ‚ãÜ E^{\\\\star}_{\\\\mathrm{img}} remains frozen,\\nserving as a fixed reference to stable perceptual structure. From the Platonic viewpoint, our method maintains an semantic prior\\nto shared, generalized VL knowledge.\\nAction fine-tuning alone narrows the model‚Äôs perceptual space toward the\\nstatistics of a specific dataset or embodiment, causing the internal\\nfeatures to drift away from broad generalized representations.\\nThe alignment loss restores this balance by enforcing consistency between\\nthe student‚Äôs intermediate features and those of a strong, pre-trained vision model that encodes more general visual‚Äìsemantic relationships.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7. Experiments',\n",
       "    'paragraphs': 'We evaluate our approach in several robotics environment based on the Simpler (Li et al., 2024 ; Tao et al., 2025 ) using proposed VL-Think task suite ( section\\xa04 ) and adopted benchmark introduced in (Liu et al., 2025 ) , designed to assess VLA generalization across three axes: Vision, Semantics, and Execution:\\nVision variations alter foreground and background via dynamic textures and image-level noise, testing robustness to weak and strong visual perturbations.\\nSemantics introduces unseen objects and receptacles, paraphrased instructions, and multi-object or distractor scenarios that challenge compositional reasoning.\\nExecution changes low-level control conditions through randomized initial poses and mid-episode object repositioning, probing action-level robustness.\\nOOD evaluation holds out at least one variation factor per axis, including 9 novel objects, 16 unseen receptacles, 5 new scene textures, and 16 distractor backgrounds. Additionally, we perform linear probing on ImageNet-100 (Tian et al., 2020 ) to quantify the quality of VLA‚Äôs representations learned using different methods.\\nEach model variant is evaluated over 128 randomized seeds, we report success as mean ¬± \\\\pm standard deviation (SD). In section\\xa08 we use the paired Wilcoxon signed-rank test (Wilcoxon, 1945 ) with one-sided alternative, and report p-values. All models are trained for the same number of epochs with identical hyperparameters to ensure fair comparison.\\nFor supervised fine-tuning, we collect 1400 expert demonstration trajectories using the MPLib motion planner (Guo et al., 2025 ) . Training randomization spans 16 tables, 16 objects (yielding on average ‚àº \\\\sim 5 episodes per training variation), and multiple pose perturbations. During all fine-tuning runs, LoRA adapters (Hu et al., 2021 ) are applied to all linear layers of the VLA.\\nUsing a widely adopted open-source OpenVLA model, we compare our proposed alignment method against several fine-tuning baselines.\\nDefault ‚Äì Standard supervised fine-tuning (SFT) using cross-entropy loss on demonstration data, serving as the primary baseline.\\nFreeze ‚Äì SFT with the VLA‚Äôs visual encoder weights frozen during training; this setup tests the hypothesis that frozen representations might help with generalization.\\nAlign (ours) ‚Äì SFT combined with our auxiliary visual representation alignment loss, described in subsection\\xa06.1 , which explicitly anchors the VLA‚Äôs vision encoder to a pretrained generalist vision teacher.\\nModel Accuracy (%) C-RADIOv3 87.31 \\\\boldsymbol{87.31} OpenVLA Align 82.13 ¬Ø \\\\underline{82.13} OpenVLA Pretrained 79.88 79.88 OpenVLA SFT 77.48 77.48\\nResults in LABEL:tab:ood_performance shows that our visual alignment method yields consistent improvements across all evaluation axes: Semantic, Vision, and Execution. This result underscores the effectiveness of visual representation alignment in enhancing robustness to visual shifts, text instruction variations, texture changes, and background perturbations that frequently occur in real-world scenarios. The improvement indicates that aligning internal visual-language embeddings not only stabilizes perception but also reinforces the semantic grounding. Conversely, the Freeze baseline completely fails across all categories (as also observed in (Wang et al., 2025a ) ), yielding near-zero performance. This confirms that simply freezing the pretrained visual encoder does not preserve useful representations during adaptation. Without joint optimization, the frozen features become mismatched with the evolving action components, leading to severe degradation of both perception and control.\\nOverall, these results validate that visual alignment serves as an effective regularizer against representation degradation, allowing the model to recover general-purpose visual semantics while adapting to new robotic environments.\\nTo further evaluate the representational quality learned by our model, we conduct a linear probing analysis on the ImageNet-100 dataset (Tian et al., 2020 ) . Specifically, we extract patch embeddings from the final layer of the C-RADIOv3 (Heinrich et al., 2025 ) teacher and from the intermediate visual layers of different OpenVLA variants. Following standard practice in representation learning (Yu et al., 2025 ; Huh et al., 2024 ) , we freeze the visual encoders and train a single linear classifier on top of their frozen features to measure the separability of semantic categories. This setup directly quantifies how linearly decodable the visual features remain after action fine-tuning.\\nThe results summarized in Table\\xa03 reveal several consistent trends. As expected, the C-RADIOv3 teacher achieves the highest probing accuracy, reflecting its strong pretrained representations. Among the VLA variants, the OpenVLA fine-tuned with our proposed Visual Representation Alignment method outperforms both the pretrained checkpoint and the model fine-tuned with naive SFT. This improvement indicates that our alignment strategy effectively enhances the VLA‚Äôs representations during action fine-tuning. In contrast, naive SFT substantially reduces probing accuracy relative to the pretrained model, confirming that standard fine-tuning harms representational quality. Our aligned model not only mitigates this degradation but surpasses the pretrained baseline, indicating that the alignment loss strengthens semantic consistency and leads to more transferable visual features.\\nFollowing the experiments in subsection\\xa05.3 , we evaluate of OpenVLA fine-tuned with our proposed visual representation alignment method (OpenVLA-7B Align), under identical data, budget, and evaluation settings. Results in Table\\xa02 show that SFT-Align partially mitigates domain forgetting observed under default SFT. In particular, performance on Color and Shape domains consistently improves even surpassing the PrismaticVLM upper bound, but leaving other domains mostly unchanged.\\nThese outcomes highlight both the promise and limits of the proposed representation alignment under constrained settings. We hypothesize that the modest size and diversity of the SFT dataset and the limited expressivity of LoRA updates are insufficient to restore less frequent VL concepts that are underrepresented in robotics data. We hypothesize that expanding data breadth and relaxing parameter-efficiency constraints will unlock broader gains beyond commonly represented domains. Verifying this hypothesis is an important direction for future work.',\n",
       "    'subsections': [{'title': '7.1. Evaluation setup',\n",
       "      'paragraphs': 'We evaluate our approach in several robotics environment based on the Simpler (Li et al., 2024 ; Tao et al., 2025 ) using proposed VL-Think task suite ( section\\xa04 ) and adopted benchmark introduced in (Liu et al., 2025 ) , designed to assess VLA generalization across three axes: Vision, Semantics, and Execution:\\nVision variations alter foreground and background via dynamic textures and image-level noise, testing robustness to weak and strong visual perturbations.\\nSemantics introduces unseen objects and receptacles, paraphrased instructions, and multi-object or distractor scenarios that challenge compositional reasoning.\\nExecution changes low-level control conditions through randomized initial poses and mid-episode object repositioning, probing action-level robustness.\\nOOD evaluation holds out at least one variation factor per axis, including 9 novel objects, 16 unseen receptacles, 5 new scene textures, and 16 distractor backgrounds. Additionally, we perform linear probing on ImageNet-100 (Tian et al., 2020 ) to quantify the quality of VLA‚Äôs representations learned using different methods.\\nEach model variant is evaluated over 128 randomized seeds, we report success as mean ¬± \\\\pm standard deviation (SD). In section\\xa08 we use the paired Wilcoxon signed-rank test (Wilcoxon, 1945 ) with one-sided alternative, and report p-values. All models are trained for the same number of epochs with identical hyperparameters to ensure fair comparison.',\n",
       "      'subsections': []},\n",
       "     {'title': '7.2. Training setup',\n",
       "      'paragraphs': 'For supervised fine-tuning, we collect 1400 expert demonstration trajectories using the MPLib motion planner (Guo et al., 2025 ) . Training randomization spans 16 tables, 16 objects (yielding on average ‚àº \\\\sim 5 episodes per training variation), and multiple pose perturbations. During all fine-tuning runs, LoRA adapters (Hu et al., 2021 ) are applied to all linear layers of the VLA.',\n",
       "      'subsections': []},\n",
       "     {'title': '7.3. Baselines',\n",
       "      'paragraphs': 'Using a widely adopted open-source OpenVLA model, we compare our proposed alignment method against several fine-tuning baselines.\\nDefault ‚Äì Standard supervised fine-tuning (SFT) using cross-entropy loss on demonstration data, serving as the primary baseline.\\nFreeze ‚Äì SFT with the VLA‚Äôs visual encoder weights frozen during training; this setup tests the hypothesis that frozen representations might help with generalization.\\nAlign (ours) ‚Äì SFT combined with our auxiliary visual representation alignment loss, described in subsection\\xa06.1 , which explicitly anchors the VLA‚Äôs vision encoder to a pretrained generalist vision teacher.\\nModel Accuracy (%) C-RADIOv3 87.31 \\\\boldsymbol{87.31} OpenVLA Align 82.13 ¬Ø \\\\underline{82.13} OpenVLA Pretrained 79.88 79.88 OpenVLA SFT 77.48 77.48',\n",
       "      'subsections': []},\n",
       "     {'title': '7.4. Results: OOD Evaluation',\n",
       "      'paragraphs': 'Results in LABEL:tab:ood_performance shows that our visual alignment method yields consistent improvements across all evaluation axes: Semantic, Vision, and Execution. This result underscores the effectiveness of visual representation alignment in enhancing robustness to visual shifts, text instruction variations, texture changes, and background perturbations that frequently occur in real-world scenarios. The improvement indicates that aligning internal visual-language embeddings not only stabilizes perception but also reinforces the semantic grounding. Conversely, the Freeze baseline completely fails across all categories (as also observed in (Wang et al., 2025a ) ), yielding near-zero performance. This confirms that simply freezing the pretrained visual encoder does not preserve useful representations during adaptation. Without joint optimization, the frozen features become mismatched with the evolving action components, leading to severe degradation of both perception and control.\\nOverall, these results validate that visual alignment serves as an effective regularizer against representation degradation, allowing the model to recover general-purpose visual semantics while adapting to new robotic environments.',\n",
       "      'subsections': []},\n",
       "     {'title': '7.5. Results: Linear probing',\n",
       "      'paragraphs': 'To further evaluate the representational quality learned by our model, we conduct a linear probing analysis on the ImageNet-100 dataset (Tian et al., 2020 ) . Specifically, we extract patch embeddings from the final layer of the C-RADIOv3 (Heinrich et al., 2025 ) teacher and from the intermediate visual layers of different OpenVLA variants. Following standard practice in representation learning (Yu et al., 2025 ; Huh et al., 2024 ) , we freeze the visual encoders and train a single linear classifier on top of their frozen features to measure the separability of semantic categories. This setup directly quantifies how linearly decodable the visual features remain after action fine-tuning.\\nThe results summarized in Table\\xa03 reveal several consistent trends. As expected, the C-RADIOv3 teacher achieves the highest probing accuracy, reflecting its strong pretrained representations. Among the VLA variants, the OpenVLA fine-tuned with our proposed Visual Representation Alignment method outperforms both the pretrained checkpoint and the model fine-tuned with naive SFT. This improvement indicates that our alignment strategy effectively enhances the VLA‚Äôs representations during action fine-tuning. In contrast, naive SFT substantially reduces probing accuracy relative to the pretrained model, confirming that standard fine-tuning harms representational quality. Our aligned model not only mitigates this degradation but surpasses the pretrained baseline, indicating that the alignment loss strengthens semantic consistency and leads to more transferable visual features.',\n",
       "      'subsections': []},\n",
       "     {'title': '7.6. Results: VL-Think',\n",
       "      'paragraphs': 'Following the experiments in subsection\\xa05.3 , we evaluate of OpenVLA fine-tuned with our proposed visual representation alignment method (OpenVLA-7B Align), under identical data, budget, and evaluation settings. Results in Table\\xa02 show that SFT-Align partially mitigates domain forgetting observed under default SFT. In particular, performance on Color and Shape domains consistently improves even surpassing the PrismaticVLM upper bound, but leaving other domains mostly unchanged.\\nThese outcomes highlight both the promise and limits of the proposed representation alignment under constrained settings. We hypothesize that the modest size and diversity of the SFT dataset and the limited expressivity of LoRA updates are insufficient to restore less frequent VL concepts that are underrepresented in robotics data. We hypothesize that expanding data breadth and relaxing parameter-efficiency constraints will unlock broader gains beyond commonly represented domains. Verifying this hypothesis is an important direction for future work.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '8. Ablations',\n",
       "    'paragraphs': 'In this section, we conduct a systematic ablation study to analyze how different design choices affect the performance of our visual alignment method. We examine the impact of the teacher model used for alignment, the alignment strategy and target layers, the projector type and the loss functions. Together, these experiments provide insights into which components are most critical for effective alignment of visual representations.\\nA key question in our approach concerns the choice of the teacher model that provides reference representations for alignment.\\nFrom the Platonic perspective, each vision foundation encoder captures a different projection of broadly generalizable visual knowledge,\\nand alignment to a stronger teacher helps preserve these high-level, transferable abstractions within the VLA during fine-tuning.\\nWe therefore examine whether foundation models trained on large-scale, diverse, and multi-view data yield better alignment and stronger transfer.\\nTo test this, we evaluate several state-of-the-art vision encoders, including DINOv2 (Oquab et al., 2024 ) , SigLIP (Zhai et al., 2023 ) , C-RADIOv3 (Heinrich et al., 2025 ) , and Theia (Shang et al., 2024 ) . As shown in LABEL:tab:teacher_backbone , C-RADIOv3 achieves the best overall results,\\nindicating that stronger and more capable vision models those trained on large-scale, semantically rich, and multimodal data\\noffer more stable and generalizable visual features for alignment. Such teachers serve as stronger Platonic anchors, guiding the VLA to align with transferable and semantically consistent representations that improve robustness across tasks and domains.\\nWe next evaluate different alignment paradigms to determine which level of the VLA model benefits most from visual representation alignment. Two principal strategies are tested:\\nBackbone2Enc ‚Äì Aligning the representations of the VLA‚Äôs transformer backbone to the final-layer features of the teacher‚Äôs visual encoder.\\nEnc2Enc ‚Äì Aligning the features of the VLA‚Äôs own visual encoder directly to the teacher model‚Äôs final embeddings.\\nOur experiments reveal ( LABEL:tab:align_method ) that Backbone2Enc consistently yields stronger results. This indicates that the primary representational degradation occurs not in the early encoder layers but in the middle-to-late fusion layers, where VL integration and task-specific adaptation are most active. Regularizing these deeper representations appears crucial for maintaining visual‚Äìsemantic consistency while allowing the lower layers to adapt freely to domain-specific low-level cues.\\nTo evaluate how different projection mappings affect representation alignment, we compare several projector variants that map the VLA‚Äôs hidden states ‚Ñù d e \\\\mathbb{R}^{d_{\\\\text{e}}} to the teacher‚Äôs embedding space ‚Ñù d t \\\\mathbb{R}^{d_{t}} . All projectors share identical input‚Äìoutput dimensions but differ in their internal transformation P œÜ : ‚Ñù d e ‚Üí ‚Ñù d t P_{\\\\varphi}:\\\\mathbb{R}^{d_{\\\\text{e}}}\\\\rightarrow\\\\mathbb{R}^{d_{t}} .\\nWe examine multiple projection strategies, including linear, cosine-similarity‚Äìbased, orthogonal, spectral-normalized, FiLM-conditioned, Whitening‚Äìaffine, and MLP-based mappings. Our experiments show that the frozen MLP projector yields the most reliable, robust alignment across all evaluation dimensions. We hypothesize that freezing the projector is critical in our setup: when trainable, the model minimizes alignment loss primarily through projector adaptation rather than meaningful changes in the VLA‚Äôs internal representations. In this case, the projector quickly learns to output embeddings that merely approximate the teacher‚Äôs space, effectively bypassing representational correction. We attribute this to two factors: the relatively small amount of alignment data and the substantial dimensionality gap between the vision teacher and the VLA backbone embeddings ( d t = 768 d_{t}=768 , d e = 4096 d_{e}=4096 ). Freezing the projector constrains this shortcut, forcing the alignment objective to act directly on the student‚Äôs hidden representations, yielding more semantically grounded and transferable feature alignment.\\nWe further investigate which layers within the VLA transformer‚Äôs backbone should be aligned to achieve the most effective representation recovery. Prior literature on VLM interpretability (Zhang et al., 2025 ) and our own analyses ( Figure\\xa04 ) suggest that middle layers are primarily responsible for VL fusion and semantic grounding, whereas early layers encode low-level features and later layers specialize in action prediction. Accordingly, we perform experiments aligning different types of layers: Early, Middle, Late. The results ( LABEL:tab:alig_layers ) confirm that the middle layers play a central role in semantic grounding and and aligning them yields the most substantial improvements across generalization axes.\\nFinally, we assess the impact of the alignment loss and its weighting coefficient. We test several variants, including cosine similarity (Cossim), L2, and contrastive NT-Xent (Chen et al., 2020 ) losses, across alignment coefficients Œª \\\\lambda = {0.2, 0.5, 1.0, 3.0}. The results demonstrate ( LABEL:tab:agg_loss_func ) that Cossim loss achieves the most stable and consistent improvements, particularly when the auxiliary weight is set to Œª \\\\lambda = 0.2. This setting effectively constrains representation drift without overpowering the task objective.',\n",
       "    'subsections': [{'title': '8.1. Visual teacher models',\n",
       "      'paragraphs': 'A key question in our approach concerns the choice of the teacher model that provides reference representations for alignment.\\nFrom the Platonic perspective, each vision foundation encoder captures a different projection of broadly generalizable visual knowledge,\\nand alignment to a stronger teacher helps preserve these high-level, transferable abstractions within the VLA during fine-tuning.\\nWe therefore examine whether foundation models trained on large-scale, diverse, and multi-view data yield better alignment and stronger transfer.\\nTo test this, we evaluate several state-of-the-art vision encoders, including DINOv2 (Oquab et al., 2024 ) , SigLIP (Zhai et al., 2023 ) , C-RADIOv3 (Heinrich et al., 2025 ) , and Theia (Shang et al., 2024 ) . As shown in LABEL:tab:teacher_backbone , C-RADIOv3 achieves the best overall results,\\nindicating that stronger and more capable vision models those trained on large-scale, semantically rich, and multimodal data\\noffer more stable and generalizable visual features for alignment. Such teachers serve as stronger Platonic anchors, guiding the VLA to align with transferable and semantically consistent representations that improve robustness across tasks and domains.',\n",
       "      'subsections': []},\n",
       "     {'title': '8.2. Alignment method',\n",
       "      'paragraphs': 'We next evaluate different alignment paradigms to determine which level of the VLA model benefits most from visual representation alignment. Two principal strategies are tested:\\nBackbone2Enc ‚Äì Aligning the representations of the VLA‚Äôs transformer backbone to the final-layer features of the teacher‚Äôs visual encoder.\\nEnc2Enc ‚Äì Aligning the features of the VLA‚Äôs own visual encoder directly to the teacher model‚Äôs final embeddings.\\nOur experiments reveal ( LABEL:tab:align_method ) that Backbone2Enc consistently yields stronger results. This indicates that the primary representational degradation occurs not in the early encoder layers but in the middle-to-late fusion layers, where VL integration and task-specific adaptation are most active. Regularizing these deeper representations appears crucial for maintaining visual‚Äìsemantic consistency while allowing the lower layers to adapt freely to domain-specific low-level cues.',\n",
       "      'subsections': []},\n",
       "     {'title': '8.3. Projector type',\n",
       "      'paragraphs': 'To evaluate how different projection mappings affect representation alignment, we compare several projector variants that map the VLA‚Äôs hidden states ‚Ñù d e \\\\mathbb{R}^{d_{\\\\text{e}}} to the teacher‚Äôs embedding space ‚Ñù d t \\\\mathbb{R}^{d_{t}} . All projectors share identical input‚Äìoutput dimensions but differ in their internal transformation P œÜ : ‚Ñù d e ‚Üí ‚Ñù d t P_{\\\\varphi}:\\\\mathbb{R}^{d_{\\\\text{e}}}\\\\rightarrow\\\\mathbb{R}^{d_{t}} .\\nWe examine multiple projection strategies, including linear, cosine-similarity‚Äìbased, orthogonal, spectral-normalized, FiLM-conditioned, Whitening‚Äìaffine, and MLP-based mappings. Our experiments show that the frozen MLP projector yields the most reliable, robust alignment across all evaluation dimensions. We hypothesize that freezing the projector is critical in our setup: when trainable, the model minimizes alignment loss primarily through projector adaptation rather than meaningful changes in the VLA‚Äôs internal representations. In this case, the projector quickly learns to output embeddings that merely approximate the teacher‚Äôs space, effectively bypassing representational correction. We attribute this to two factors: the relatively small amount of alignment data and the substantial dimensionality gap between the vision teacher and the VLA backbone embeddings ( d t = 768 d_{t}=768 , d e = 4096 d_{e}=4096 ). Freezing the projector constrains this shortcut, forcing the alignment objective to act directly on the student‚Äôs hidden representations, yielding more semantically grounded and transferable feature alignment.',\n",
       "      'subsections': []},\n",
       "     {'title': '8.4. Alignment layers',\n",
       "      'paragraphs': 'We further investigate which layers within the VLA transformer‚Äôs backbone should be aligned to achieve the most effective representation recovery. Prior literature on VLM interpretability (Zhang et al., 2025 ) and our own analyses ( Figure\\xa04 ) suggest that middle layers are primarily responsible for VL fusion and semantic grounding, whereas early layers encode low-level features and later layers specialize in action prediction. Accordingly, we perform experiments aligning different types of layers: Early, Middle, Late. The results ( LABEL:tab:alig_layers ) confirm that the middle layers play a central role in semantic grounding and and aligning them yields the most substantial improvements across generalization axes.',\n",
       "      'subsections': []},\n",
       "     {'title': '8.5. Loss functions and alignment coefficient',\n",
       "      'paragraphs': 'Finally, we assess the impact of the alignment loss and its weighting coefficient. We test several variants, including cosine similarity (Cossim), L2, and contrastive NT-Xent (Chen et al., 2020 ) losses, across alignment coefficients Œª \\\\lambda = {0.2, 0.5, 1.0, 3.0}. The results demonstrate ( LABEL:tab:agg_loss_func ) that Cossim loss achieves the most stable and consistent improvements, particularly when the auxiliary weight is set to Œª \\\\lambda = 0.2. This setting effectively constrains representation drift without overpowering the task objective.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '9. Conclusion',\n",
       "    'paragraphs': 'In this work, we examined how fine-tuning VLA models on robotic tasks leads to degradation of VL understanding and representation quality. To analyze this effect, we introduced the VL-Think diagnostic suite and interpretability probes, including attention map analyses and linear probing, which reveal how VL skills degrade during action fine-tuning. To address this issue, we proposed a lightweight Visual Alignment method that anchors the VLA to its pretrained visual teacher, consistently improving OOD generalization across diverse domains including novel objects, unseen scene compositions, texture and lighting variations, and instruction paraphrases. Due to compute constraints, our study focused on fine-tuning rather than full-scale pretraining. We hope this study guides future efforts toward scalable robotic pretraining and systematic evaluation of how VLAs inherit and retain VL knowledge from VLMs.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Appendix',\n",
       "    'paragraphs': 'This appendix provides additional technical details, extended results, and supplementary materials that support and complement the main findings presented in the paper. We include comprehensive ablations on alignment strategies, projector architectures, teacher models, and layer selection and alignment coefficient. These materials aim to enhance the transparency, reproducibility, and interpretability of our proposed method.\\nLABEL:tab:openvla_train_config_alig presents the training parameters used during the visual alignment fine-tuning. All other training parameters remained unchanged across both experiments and model variants.\\nTo further validate the qualitative effect of our alignment objective, we visualize attention maps for Qwen2.5-VL, OpenVLA SFT, and OpenVLA Align (ours) across middle layers of the internal transformer backbone. These layers correspond to the region of strongest vision‚Äìlanguage fusion, where attention patterns most directly reflect the model‚Äôs visual grounding quality.\\nAs shown in Figure\\xa06 , the default OpenVLA SFT exhibits diffuse and spatially inconsistent attention, often extending beyond the queried object. In contrast, our OpenVLA Align model restores sharp, localized focus on task-relevant regions. This confirms that the proposed visual alignment effectively mitigates attention sink introduced by naive fine-tuning and preserves coherent object-centered attention.\\nIn subsection\\xa05.2 we show t-SNE (van der Maaten and Hinton, 2008a ) of internal representations for the VLM models and OpenVLA. To keep comparisons strict, we use an out-of-the-box t-SNE implementation with no tuning: perplexity = 30 =30 , max iterations = 1000 =1000 , fixed random seed = 42 =42 (all other parameters at library defaults). These plots are illustrative only (t-SNE distorts global geometry); quantitative conclusions come from linear probing in subsection\\xa07.5 .\\nFor reproducibility and fair comparison, we evaluate representational quality with a frozen-feature linear probe under a single, fixed configuration (see LABEL:tab:linear_probe_imagenet100_config ). Concretely, we extract patch embeddings (mean-pooled to a single vector) from the final C-RADIOv3 teacher and from intermediate visual layers of each OpenVLA variant. A single linear classifier is trained on top of these frozen features. All hyperparameters are held constant across models and layers, and the same random seed and data split are used for every run. Due to computational constraints, we operate on a reduced ImageNet-100. We report top-1 accuracy on the evaluation split, without per-any tuning, so any differences reflect only the underlying representations rather than linear probe tuning changes.\\nThis section provides the complete ablation results that underlie the analyses in section\\xa08 , Vision teachers ( LABEL:tab:app_abb_vision_teacher ),\\nAlignment projectors ( LABEL:tab:app_abb_vision_projector ),\\nAlignment layers ( LABEL:tab:openvla_train_config_alig ), Alignment coefficients ( LABEL:tab:app_coef ). In the ablation studies presented in section\\xa08 , we test the hypothesis that a given model variant (denoted B) outperforms the baseline variant (A) in terms of success rate. For each pairwise comparison, we fix all other parameters altering only the component under investigation (e.g., alignment objective, layer depth, projection type). This ensures that any observed performance difference can be attributed solely to the ablated design choice.\\nTo assess statistical significance, we use the paired Wilcoxon signed-rank test (Wilcoxon, 1945 ) , a non-parametric test suited for comparing two matched samples that do not necessarily follow a normal distribution. The unit of analysis is the per-seed success rate over matched trials, evaluated independently for each environment type (Semantic, Vision, Execution). The test uses a one-sided alternative hypothesis ( H 1 H_{1} : B ¬ø A), corresponding to our directional research question and report the exact p-value. All comparisons are conducted over 128 shared random seeds, ensuring that each seed‚Äìenvironment pair is identical across the methods being compared. This careful experimental control allows us to draw meaningful conclusions about the contribution of each individual design choice.\\nBelow, we provide detailed formulations of the various projectors used in subsection\\xa08.3 of our experiments.\\nA normalized linear projection that preserves angular similarity:\\nA fixed linear transform with orthonormal columns:\\nA fixed randomized mapping that implicitly approximates a kernel feature space:\\nCombines feature whitening and affine normalization:\\nA constrained linear mapping enforcing bounded operator norm:',\n",
       "    'subsections': [{'title': 'A.1. Training Hyperparameters',\n",
       "      'paragraphs': 'LABEL:tab:openvla_train_config_alig presents the training parameters used during the visual alignment fine-tuning. All other training parameters remained unchanged across both experiments and model variants.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2. Attention maps visualization',\n",
       "      'paragraphs': 'To further validate the qualitative effect of our alignment objective, we visualize attention maps for Qwen2.5-VL, OpenVLA SFT, and OpenVLA Align (ours) across middle layers of the internal transformer backbone. These layers correspond to the region of strongest vision‚Äìlanguage fusion, where attention patterns most directly reflect the model‚Äôs visual grounding quality.\\nAs shown in Figure\\xa06 , the default OpenVLA SFT exhibits diffuse and spatially inconsistent attention, often extending beyond the queried object. In contrast, our OpenVLA Align model restores sharp, localized focus on task-relevant regions. This confirms that the proposed visual alignment effectively mitigates attention sink introduced by naive fine-tuning and preserves coherent object-centered attention.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.3. t-SNE visualization',\n",
       "      'paragraphs': 'In subsection\\xa05.2 we show t-SNE (van der Maaten and Hinton, 2008a ) of internal representations for the VLM models and OpenVLA. To keep comparisons strict, we use an out-of-the-box t-SNE implementation with no tuning: perplexity = 30 =30 , max iterations = 1000 =1000 , fixed random seed = 42 =42 (all other parameters at library defaults). These plots are illustrative only (t-SNE distorts global geometry); quantitative conclusions come from linear probing in subsection\\xa07.5 .',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.4. Linear probing',\n",
       "      'paragraphs': 'For reproducibility and fair comparison, we evaluate representational quality with a frozen-feature linear probe under a single, fixed configuration (see LABEL:tab:linear_probe_imagenet100_config ). Concretely, we extract patch embeddings (mean-pooled to a single vector) from the final C-RADIOv3 teacher and from intermediate visual layers of each OpenVLA variant. A single linear classifier is trained on top of these frozen features. All hyperparameters are held constant across models and layers, and the same random seed and data split are used for every run. Due to computational constraints, we operate on a reduced ImageNet-100. We report top-1 accuracy on the evaluation split, without per-any tuning, so any differences reflect only the underlying representations rather than linear probe tuning changes.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.5. Ablations',\n",
       "      'paragraphs': 'This section provides the complete ablation results that underlie the analyses in section\\xa08 , Vision teachers ( LABEL:tab:app_abb_vision_teacher ),\\nAlignment projectors ( LABEL:tab:app_abb_vision_projector ),\\nAlignment layers ( LABEL:tab:openvla_train_config_alig ), Alignment coefficients ( LABEL:tab:app_coef ). In the ablation studies presented in section\\xa08 , we test the hypothesis that a given model variant (denoted B) outperforms the baseline variant (A) in terms of success rate. For each pairwise comparison, we fix all other parameters altering only the component under investigation (e.g., alignment objective, layer depth, projection type). This ensures that any observed performance difference can be attributed solely to the ablated design choice.\\nTo assess statistical significance, we use the paired Wilcoxon signed-rank test (Wilcoxon, 1945 ) , a non-parametric test suited for comparing two matched samples that do not necessarily follow a normal distribution. The unit of analysis is the per-seed success rate over matched trials, evaluated independently for each environment type (Semantic, Vision, Execution). The test uses a one-sided alternative hypothesis ( H 1 H_{1} : B ¬ø A), corresponding to our directional research question and report the exact p-value. All comparisons are conducted over 128 shared random seeds, ensuring that each seed‚Äìenvironment pair is identical across the methods being compared. This careful experimental control allows us to draw meaningful conclusions about the contribution of each individual design choice.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.6. Different projection approaches',\n",
       "      'paragraphs': 'Below, we provide detailed formulations of the various projectors used in subsection\\xa08.3 of our experiments.\\nA normalized linear projection that preserves angular similarity:\\nA fixed linear transform with orthonormal columns:\\nA fixed randomized mapping that implicitly approximates a kernel feature space:\\nCombines feature whitening and affine normalization:\\nA constrained linear mapping enforcing bounded operator norm:',\n",
       "      'subsections': [{'title': 'Cosine Projector.',\n",
       "        'paragraphs': 'A normalized linear projection that preserves angular similarity:',\n",
       "        'subsections': []},\n",
       "       {'title': 'Orthogonal Projector.',\n",
       "        'paragraphs': 'A fixed linear transform with orthonormal columns:',\n",
       "        'subsections': []},\n",
       "       {'title': 'Random Fourier Feature (RFF) Projector.',\n",
       "        'paragraphs': 'A fixed randomized mapping that implicitly approximates a kernel feature space:',\n",
       "        'subsections': []},\n",
       "       {'title': 'Whitening‚ÄìAffine Projector.',\n",
       "        'paragraphs': 'Combines feature whitening and affine normalization:',\n",
       "        'subsections': []},\n",
       "       {'title': 'Spectral‚ÄìNorm Projector.',\n",
       "        'paragraphs': 'A constrained linear mapping enforcing bounded operator norm:',\n",
       "        'subsections': []}]}]}],\n",
       "  'abstract': 'The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA‚Äôs hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: blind-vla-paper.github.io'},\n",
       " '2510.25732v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'If machine learning mirrors human cognition to learn, can machine unlearning similarly reflect how knowledge is forgotten? Lake et\\xa0al. ( 2017 ) Unlearning has recently emerged as a crucial capability for large language models (LLMs), especially as these systems increasingly memorize personally identifiable information, propagate outdated facts, or retain knowledge that developers may wish to remove Carlini et\\xa0al. ( 2023 ); Tirumala et\\xa0al. ( 2022 ); Xuan and Li ( 2025 ); Lukas et\\xa0al. ( 2023 ); Karamolegkou et\\xa0al. ( 2023 ); Chang et\\xa0al. ( 2023 ) . However, removing information may leave traces, activate related associations, or cause unexpected side effects like hallucinations Xu et\\xa0al. ( 2023a ); Maini et\\xa0al. ( 2024 ) .\\nThis challenge stems from the entangled nature of knowledge representations in LLMs Liu et\\xa0al. ( 2025 ); Zhang et\\xa0al. ( 2025 ) , much like the interconnected networks described in cognitive theories of human memory. Hebbian theory shows that \"neurons that fire together, wire together\" Hebb ( 1949 ) ; co-activated concepts form strengthened associations that resist targeted erasure. Understanding knowledge entanglement is therefore critical because it reveals how information is represented, stored, and retrieved within LLMs. Just as human memories are embedded in dense neural networks where concepts mutually reinforce each other through repeated co-activation, LLM knowledge may similarly distribute across overlapping parameter spaces, making surgical removal impossible without disrupting adjacent representations. These organizational patterns determine whether information can be truly erased, with implications for privacy protection, harm prevention, and regulatory compliance with data protection laws such as the GDPR‚Äôs Right to Be Forgotten Voigt and Von\\xa0dem Bussche ( 2017 ) . Toward this goal of evaluating unlearned model behavior, we investigate whether knowledge entanglement metrics can predict unlearning robustness under persuasive framing attacks. Our framework tests the hypothesis that densely interconnected concepts resist unlearning because closely linked, frequently co-activated Hebb ( 1949 ); Anderson ( 1983 ) associations create multiple retrieval pathways that rhetorical strategies can exploit.\\nRecent work has shown that even state-of-the-art LLMs struggle with factual knowledge, particularly for less popular (tail) entities, with GPT-4 achieving only a 31% accuracy on comprehensive factual QA benchmarks Sun et\\xa0al. ( 2023 ) . This factuality gap becomes even more critical in the unlearning context: if models do not successfully retain knowledge even during normal training, how can we verify that targeted unlearning has successfully removed specific information? Our work extends this line of inquiry by investigating not only whether knowledge persists after unlearning, but how its entanglement structure and retrieval mechanisms determine what remains accessible under different prompt framings.\\nEvaluating unlearning robustness requires understanding how knowledge structure and prompt delivery interact. Figure 1 illustrates our framework‚Äôs core hypothesis: unlearned LLM behavior depends on both the semantic entanglement of target knowledge and the rhetorical framing used to elicit it. Drawing inspiration from the Obliviate charm in Harry Potter , a spell that removes specific memories but often leaves traces depending on the caster‚Äôs skill, and from the selective memory erasure depicted in Eternal Sunshine of the Spotless Mind , we investigate whether LLMs show similar vulnerabilities when forgotten knowledge is probed through different rhetorical strategies. Given LLMs‚Äô structural resemblance to human cognitive processing through attention mechanisms Zheng et\\xa0al. ( 2024 ) and their demonstrated alignment with human behavioral patterns Binz and Schulz ( 2023 ) , are LLMs more susceptible to emotional and authority appeals (mirroring human psychological manipulation) or logical reasoning (reflecting their computational nature)?\\nAlthough recent work has investigated unlearning robustness in LLMs through adversarial optimization Carlini et\\xa0al. ( 2023 ); Xuan and Li ( 2025 ); To and Le ( 2025 ) and jailbreaking techniques Zeng et\\xa0al. ( 2024 ); Chao et\\xa0al. ( 2025 ); Xu et\\xa0al. ( 2023b ) , these approaches focus primarily on what information is requested but neglect two critical dimensions: (1) the structural entanglement of knowledge and (2) the communicative delivery of prompts. Therefore, we propose the Stimulus-Knowledge Entanglement-Behavior Framework ( SKeB ), which synthesizes spreading activation theories from cognitive science (ACT-R and Hebbian) with communication principles to comprehensively evaluate how knowledge entanglement and persuasive framing interact to bypass unlearning. Our contributions are summarized as follows :\\nWe introduce SKeB , a theory-grounded framework investigating unlearning robustness through the interaction of semantic entanglement (what can be activated) and persuasive framing (what will be activated).\\nWe develop nine graph-based entanglement metrics and show that distance-weighted influence ( ‚Ñ≥ 9 \\\\mathcal{M}_{9} ) strongly predicts factual recall ( r = 0.77 r=0.77 ), with authority framing producing 9.3√ó higher entanglement activation.\\nWe reveal persuasive framing effectiveness negatively correlates with model size ( r = ‚àí 0.89 r=-0.89 ): smaller models show 128% factual recall increases versus 15% for larger models.\\nOur framework enables a predictive model explaining 78% of variance in unlearning robustness, allowing us to filter queries susceptible for knowledge leakage in unlearned LLMs.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Motivation',\n",
       "    'paragraphs': 'Anderson‚Äôs ACT-R theory Anderson ( 1983 ) models information as cognitive units whose activation strength depends on usage, with retrieval occurring through spreading activation across semantic networks. Under this theory, forgetting does not necessarily mean erasure ; it can result from decreased activation strength or disconnection from related concepts. We draw a parallel to unlearning in LLMs: adversarial prompts can reactivate adjacent knowledge units, showing that information is suppressed rather than erased from the model‚Äôs latent space Eldan and Russinovich ( 2023 ); Xu et\\xa0al. ( 2025 ) . Hebb‚Äôs principle that \"neurons that fire together, wire together\" reinforces this view, as frequently co-activated representations form stronger associative links Hebb ( 1949 ) . In LLMs, overlapping representations or latent pathways are more likely to co-activate, forming what we refer to as knowledge entanglement , where concepts are interconnected such that \"forgotten\" information can still be indirectly \"reactivated\" or recalled . Just as human memories persist through associative connections when direct recall fails, LLM knowledge remains accessible through indirect pathways . This entanglement structure determines what knowledge can be activated and represents the inherent retrievability of supposedly unlearned information.\\nThis activation-based view resonates with Watson‚Äôs stimulus-response view of behavior Watson ( 1913 ) . Prompts function as external stimuli; model outputs represent observable behaviors. When prompt framing varies, models may exhibit different responses despite unchanged underlying knowledge. The stimulus determines what knowledge will be activated, converting retrievability of latent knowledge into actual model behavior. This emphasizes that unlearning cannot be studied solely as an internal, entangled representation. It requires examining the dynamic stimulus-behavior interaction .\\nStimulus , or the way a piece of information is requested, fundamentally shapes whether it surfaces. This idea can be understood through communication theory‚Äôs three-part structure: the message (what is requested), the receiver (model‚Äôs internal state), and the delivery (rhetorical framing). Classic frameworks like Shannon and Weaver‚Äôs sender-message-receiver model and persuasion theory Cialdini ( 2006 ); Petty and Cacioppo ( 1986 ) demonstrate that identical content yields dramatically different responses depending on delivery. In human communication, authority endorsement, emotional appeals, and logical reasoning activate distinct cognitive pathways such as in how humans comply with authority figures even when requests conflict with prior knowledge Cialdini ( 1993 ) .\\nWe hypothesize that LLMs also exhibit analogous stimulus-behavior sensitivities. However, no work has systematically investigated how different delivery methods , such as persuasive prompts that avoid directly mentioning target information, elicit residual knowledge from unlearned models. This gap is critical: real-world unlearning failures may occur not through filterable direct queries but through indirect persuasive framing.\\nBased on our analysis in Sec. 2.1 and 2.2 , we hypothesize that knowledge retrieval in LLMs operates through two interacting mechanisms: (1) semantic entanglement of concepts in the knowledge space, determining what knowledge can be activated, and (2) communicative framing of prompts, determining what knowledge will be activated. We combine these into a unified framework, which we term Stimulus-Knowledge Entanglement-Behavior Framework ( SKeB ) .\\nIntuitively, SKeB proposes that unlearning evaluation requires evaluating not only content-based probing Eldan and Russinovich ( 2023 ) , but also how delivery strategies exploit entangled model knowledge. This framework formalizes the relationship that different stimuli (persuasive techniques) interact with knowledge entanglement structures to produce observable changes in model outputs. The framework moves beyond binary questions such as \"can the model recall X X ?\" toward more nuanced thoughts: \"under what communicative conditions does X X resurface, and what does this reveal about unlearning completeness and prompt effectiveness?\" For instance, highly entangled concepts, those with many strong connections in the knowledge base (Harry Potter and Voldemort), resist unlearning because suppressing direct access leaves numerous indirect pathways intact. When activated through persuasive framing such as authority, emotion, logic, supposedly unlearned knowledge resurfaces. SKeB models this as Stimulus √ó \\\\times Knowledge Entanglement ‚Üí \\\\rightarrow Behavior , where the interaction between prompt framing and structural entanglement determines the degree of information leakage.',\n",
       "    'subsections': [{'title': '2.1 ACT-R, Hebbian Theory, and Knowledge Entanglement in LLMs',\n",
       "      'paragraphs': 'Anderson‚Äôs ACT-R theory Anderson ( 1983 ) models information as cognitive units whose activation strength depends on usage, with retrieval occurring through spreading activation across semantic networks. Under this theory, forgetting does not necessarily mean erasure ; it can result from decreased activation strength or disconnection from related concepts. We draw a parallel to unlearning in LLMs: adversarial prompts can reactivate adjacent knowledge units, showing that information is suppressed rather than erased from the model‚Äôs latent space Eldan and Russinovich ( 2023 ); Xu et\\xa0al. ( 2025 ) . Hebb‚Äôs principle that \"neurons that fire together, wire together\" reinforces this view, as frequently co-activated representations form stronger associative links Hebb ( 1949 ) . In LLMs, overlapping representations or latent pathways are more likely to co-activate, forming what we refer to as knowledge entanglement , where concepts are interconnected such that \"forgotten\" information can still be indirectly \"reactivated\" or recalled . Just as human memories persist through associative connections when direct recall fails, LLM knowledge remains accessible through indirect pathways . This entanglement structure determines what knowledge can be activated and represents the inherent retrievability of supposedly unlearned information.\\nThis activation-based view resonates with Watson‚Äôs stimulus-response view of behavior Watson ( 1913 ) . Prompts function as external stimuli; model outputs represent observable behaviors. When prompt framing varies, models may exhibit different responses despite unchanged underlying knowledge. The stimulus determines what knowledge will be activated, converting retrievability of latent knowledge into actual model behavior. This emphasizes that unlearning cannot be studied solely as an internal, entangled representation. It requires examining the dynamic stimulus-behavior interaction .',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Communication Theory and Rhetorical Framing',\n",
       "      'paragraphs': 'Stimulus , or the way a piece of information is requested, fundamentally shapes whether it surfaces. This idea can be understood through communication theory‚Äôs three-part structure: the message (what is requested), the receiver (model‚Äôs internal state), and the delivery (rhetorical framing). Classic frameworks like Shannon and Weaver‚Äôs sender-message-receiver model and persuasion theory Cialdini ( 2006 ); Petty and Cacioppo ( 1986 ) demonstrate that identical content yields dramatically different responses depending on delivery. In human communication, authority endorsement, emotional appeals, and logical reasoning activate distinct cognitive pathways such as in how humans comply with authority figures even when requests conflict with prior knowledge Cialdini ( 1993 ) .\\nWe hypothesize that LLMs also exhibit analogous stimulus-behavior sensitivities. However, no work has systematically investigated how different delivery methods , such as persuasive prompts that avoid directly mentioning target information, elicit residual knowledge from unlearned models. This gap is critical: real-world unlearning failures may occur not through filterable direct queries but through indirect persuasive framing.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.3 Stimulus-Knowledge Entanglement-Behavior Framework (SKeB)',\n",
       "      'paragraphs': 'Based on our analysis in Sec. 2.1 and 2.2 , we hypothesize that knowledge retrieval in LLMs operates through two interacting mechanisms: (1) semantic entanglement of concepts in the knowledge space, determining what knowledge can be activated, and (2) communicative framing of prompts, determining what knowledge will be activated. We combine these into a unified framework, which we term Stimulus-Knowledge Entanglement-Behavior Framework ( SKeB ) .\\nIntuitively, SKeB proposes that unlearning evaluation requires evaluating not only content-based probing Eldan and Russinovich ( 2023 ) , but also how delivery strategies exploit entangled model knowledge. This framework formalizes the relationship that different stimuli (persuasive techniques) interact with knowledge entanglement structures to produce observable changes in model outputs. The framework moves beyond binary questions such as \"can the model recall X X ?\" toward more nuanced thoughts: \"under what communicative conditions does X X resurface, and what does this reveal about unlearning completeness and prompt effectiveness?\" For instance, highly entangled concepts, those with many strong connections in the knowledge base (Harry Potter and Voldemort), resist unlearning because suppressing direct access leaves numerous indirect pathways intact. When activated through persuasive framing such as authority, emotion, logic, supposedly unlearned knowledge resurfaces. SKeB models this as Stimulus √ó \\\\times Knowledge Entanglement ‚Üí \\\\rightarrow Behavior , where the interaction between prompt framing and structural entanglement determines the degree of information leakage.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Problem Formulation',\n",
       "    'paragraphs': 'We formalize the SKeB framework through three components that interact to produce model behavior: Stimulus ‚Üí \\\\rightarrow Knowledge Entanglement ‚Üí \\\\rightarrow Behavior . The stimulus (prompt framing) activates regions of the domain graph; the entanglement structure (semantic connectivity) determines how activation spreads; and the resulting behavior (model output) reflects the extent to which knowledge pathways were successfully accessed.\\nWe begin with a pre-trained language model parameterized by Œ∏ \\\\theta : f Œ∏ : ùí≥ ‚Üí ùí¥ f_{\\\\theta}:\\\\mathcal{X}\\\\to\\\\mathcal{Y} , where ùí≥ \\\\mathcal{X} represents the input space (prompts) and ùí¥ \\\\mathcal{Y} represents the output space (generated texts). This model has been trained on a corpus ùíü = ùíü general ‚à™ ùíü ùíØ \\\\mathcal{D}{=}\\\\mathcal{D}_{\\\\text{general}}{\\\\cup}\\\\mathcal{D}_{\\\\mathcal{T}} , where ùíü general \\\\mathcal{D}_{\\\\text{general}} contains general knowledge and ùíü ùíØ \\\\mathcal{D}_{\\\\mathcal{T}} contains a specific target domain ùíØ \\\\mathcal{T} that we want to unlearn.\\nThe unlearning process then aims to produce a modified model f Œ∏ ‚àó ‚àó : ùí≥ ‚Üí ùí¥ f^{*}_{\\\\theta^{*}}:\\\\mathcal{X}\\\\to\\\\mathcal{Y} , where the parameters Œ∏ ‚àó \\\\theta^{*} are adjusted such that the model‚Äôs behavior on queries related to ùíØ \\\\mathcal{T} is suppressed, while maintaining performance on ùíü general \\\\mathcal{D}_{\\\\text{general}} . Formally, unlearning aims to achieve:\\nwhere ùí≥ ùíØ \\\\mathcal{X}_{\\\\mathcal{T}} represents prompts that directly query knowledge about domain ùíØ \\\\mathcal{T} .\\nHowever, unlearning evaluation typically only tests direct queries in ùí≥ ùíØ \\\\mathcal{X}_{\\\\mathcal{T}} . We introduce rhetorical framing via persuasion or persuasive prompt transformations P i : ùí≥ ‚Üí ùí≥ P_{i}:\\\\mathcal{X}\\\\to\\\\mathcal{X} , where each transformation P i P_{i} applies a distinct rhetorical strategy while preserving the underlying content. Given a base prompt x ‚àà ùí≥ ùíØ x\\\\in\\\\mathcal{X}_{\\\\mathcal{T}} , we define four different persuasive prompting strategies P emo P_{\\\\text{emo}} , P logic P_{\\\\text{logic}} , and P auth P_{\\\\text{auth}} together with identity transformation P orig P_{\\\\text{orig}} (Table 1 ). The key idea is that all transformations target the same underlying knowledge: content \\u200b ( P i \\u200b ( x ) ) = content \\u200b ( x ) \\\\text{content}(P_{i}(x))=\\\\text{content}(x) , but they differ in delivery mechanism: delivery \\u200b ( P i \\u200b ( x ) ) ‚â† delivery \\u200b ( P j \\u200b ( x ) ) \\\\text{delivery}(P_{i}(x))\\\\neq\\\\text{delivery}(P_{j}(x)) for i ‚â† j i\\\\neq j .\\nTo model the structural entanglement of knowledge in domain ùíØ \\\\mathcal{T} , we formulate a domain graph G = ( V , E , d ) G{=}(V,E,d) as a proxy. For each prompt x x , we denote the set of entities N x ‚äÜ V N_{x}{\\\\subseteq}V mentioned in the prompt and compute the induced subgraph G x = ( N x , E x , d x ) G_{x}{=}(N_{x},E_{x},d_{x}) , where E x E_{x} contains all edges connecting entities in N x N_{x} . The weight function d d : E ‚Üí ‚Ñù + E\\\\rightarrow\\\\mathbb{R}^{+} assigns importance to each edge based on co-occurrence frequency and semantic proximity in the original corpus, capturing how strongly concepts are associated. This weighting is critical for calculating entanglement metrics, as it reflects the strength of spreading activation between connected nodes.\\nWe then define a family of entanglement metrics { ‚Ñ≥ 1 , ‚Ä¶ , ‚Ñ≥ 9 } \\\\{\\\\mathcal{M}_{1},\\\\ldots,\\\\mathcal{M}_{9}\\\\} that quantify different aspects of knowledge entanglement (detailed in Table 2 and Appendix A.4 ). Each metric ‚Ñ≥ k : G x ‚Üí ‚Ñù \\\\mathcal{M}_{k}:G_{x}\\\\to\\\\mathbb{R} maps a prompt‚Äôs induced subgraph to a scalar entanglement score. These metrics capture intuitions from the aforementioned spreading activation theories: higher entanglement scores indicate that the prompt activates more densely connected regions of the domain graph, creating multiple pathways for information retrieval and therefore a higher chance of factual knowledge recall.\\nWe evaluate model outputs along three mutually exclusive dimensions: Factual Knowledge Recall, Non-Factual Content, and Hallucination. Factual Knowledge Recall measures the proportion of generated content that correctly reproduces information from the target domain ùíØ \\\\mathcal{T} . This is the degree to which supposedly unlearned knowledge remains retrievable. Non-Factual Content is plausible but incorrect information related to domain ùíØ \\\\mathcal{T} that does not appear in the original corpus. Finally, Hallucination is fabricated content, unrelated to ùíØ \\\\mathcal{T} , or general incoherence, indicating generation errors or activation of semantically distant concepts. For instance, for each response y = f Œ∏ ‚àó ‚àó \\u200b ( x ) y{=}f^{*}_{\\\\theta^{*}}(x) , we compute factuality scores s fact \\u200b ( y ) ‚àà [ 0 , 1 ] s_{\\\\text{fact}}(y){\\\\in}[0,1] using an ensemble of judge models (detailed in A.5 ), where higher scores indicate greater retention of supposedly unlearned knowledge.\\nOur study utilizes the proposed SKeB framework to analyze unlearned LLMs‚Äô behavior with the following research questions (RQs). By answering these questions, we aim to establish whether SKeB can provide a principled framework for understanding and predicting unlearning failures in LLMs.\\nRQ1. ( Stimulus v.s. Entanglement ): Do different persuasive framings P i P_{i} produce systematically different entanglement patterns ‚Ñ≥ k \\\\mathcal{M}_{k} in the domain graph? This addresses whether persuasive transformations alter the structural properties of activated semantic pathways.\\nRQ2. ( Knowledge Entanglement v.s. Behavior ): Are there any correlations between entanglement metrics ‚Ñ≥ k \\\\mathcal{M}_{k} and unlearned models‚Äô factual recall behavior? This addresses whether domain graph structure predicts information leakage.\\nRQ3. ( Stimulus vs. Behavior ): How do different persuasive transformations P i P_{i} affect factual recall in unlearned models, and how does effectiveness vary systematically with model size | Œ∏ | |\\\\theta| ? This addresses whether persuasive mechanisms can bypass suppression and whether model scaling improves robustness.\\nRQ4. ( Unified Predictive Modeling ): Can we build a predictive model that accurately forecasts unlearned model behavior based on the combination of entanglement scores ‚Ñ≥ k \\\\mathcal{M}_{k} , prompt type P i P_{i} , and model architecture? This would enable a proactive vulnerability assessment for unlearned models.\\nRQ5. ( Architectural Differences ): Do different model architectures (OPT, LLaMA-2, LLaMA-3.1) exhibit distinct correlation patterns between entanglement and behavior after unlearning? This addresses whether unlearning robustness depends solely on parameter count or on underlying representational structure.',\n",
       "    'subsections': [{'title': '3.1 Stimulus - Rhetorical Framing',\n",
       "      'paragraphs': 'However, unlearning evaluation typically only tests direct queries in ùí≥ ùíØ \\\\mathcal{X}_{\\\\mathcal{T}} . We introduce rhetorical framing via persuasion or persuasive prompt transformations P i : ùí≥ ‚Üí ùí≥ P_{i}:\\\\mathcal{X}\\\\to\\\\mathcal{X} , where each transformation P i P_{i} applies a distinct rhetorical strategy while preserving the underlying content. Given a base prompt x ‚àà ùí≥ ùíØ x\\\\in\\\\mathcal{X}_{\\\\mathcal{T}} , we define four different persuasive prompting strategies P emo P_{\\\\text{emo}} , P logic P_{\\\\text{logic}} , and P auth P_{\\\\text{auth}} together with identity transformation P orig P_{\\\\text{orig}} (Table 1 ). The key idea is that all transformations target the same underlying knowledge: content \\u200b ( P i \\u200b ( x ) ) = content \\u200b ( x ) \\\\text{content}(P_{i}(x))=\\\\text{content}(x) , but they differ in delivery mechanism: delivery \\u200b ( P i \\u200b ( x ) ) ‚â† delivery \\u200b ( P j \\u200b ( x ) ) \\\\text{delivery}(P_{i}(x))\\\\neq\\\\text{delivery}(P_{j}(x)) for i ‚â† j i\\\\neq j .',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Knowledge Entanglement - Graph Construction and Metrics',\n",
       "      'paragraphs': 'To model the structural entanglement of knowledge in domain ùíØ \\\\mathcal{T} , we formulate a domain graph G = ( V , E , d ) G{=}(V,E,d) as a proxy. For each prompt x x , we denote the set of entities N x ‚äÜ V N_{x}{\\\\subseteq}V mentioned in the prompt and compute the induced subgraph G x = ( N x , E x , d x ) G_{x}{=}(N_{x},E_{x},d_{x}) , where E x E_{x} contains all edges connecting entities in N x N_{x} . The weight function d d : E ‚Üí ‚Ñù + E\\\\rightarrow\\\\mathbb{R}^{+} assigns importance to each edge based on co-occurrence frequency and semantic proximity in the original corpus, capturing how strongly concepts are associated. This weighting is critical for calculating entanglement metrics, as it reflects the strength of spreading activation between connected nodes.\\nWe then define a family of entanglement metrics { ‚Ñ≥ 1 , ‚Ä¶ , ‚Ñ≥ 9 } \\\\{\\\\mathcal{M}_{1},\\\\ldots,\\\\mathcal{M}_{9}\\\\} that quantify different aspects of knowledge entanglement (detailed in Table 2 and Appendix A.4 ). Each metric ‚Ñ≥ k : G x ‚Üí ‚Ñù \\\\mathcal{M}_{k}:G_{x}\\\\to\\\\mathbb{R} maps a prompt‚Äôs induced subgraph to a scalar entanglement score. These metrics capture intuitions from the aforementioned spreading activation theories: higher entanglement scores indicate that the prompt activates more densely connected regions of the domain graph, creating multiple pathways for information retrieval and therefore a higher chance of factual knowledge recall.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.3 Behavior - Response Evaluation Metrics',\n",
       "      'paragraphs': 'We evaluate model outputs along three mutually exclusive dimensions: Factual Knowledge Recall, Non-Factual Content, and Hallucination. Factual Knowledge Recall measures the proportion of generated content that correctly reproduces information from the target domain ùíØ \\\\mathcal{T} . This is the degree to which supposedly unlearned knowledge remains retrievable. Non-Factual Content is plausible but incorrect information related to domain ùíØ \\\\mathcal{T} that does not appear in the original corpus. Finally, Hallucination is fabricated content, unrelated to ùíØ \\\\mathcal{T} , or general incoherence, indicating generation errors or activation of semantically distant concepts. For instance, for each response y = f Œ∏ ‚àó ‚àó \\u200b ( x ) y{=}f^{*}_{\\\\theta^{*}}(x) , we compute factuality scores s fact \\u200b ( y ) ‚àà [ 0 , 1 ] s_{\\\\text{fact}}(y){\\\\in}[0,1] using an ensemble of judge models (detailed in A.5 ), where higher scores indicate greater retention of supposedly unlearned knowledge.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.4 Research Questions',\n",
       "      'paragraphs': 'Our study utilizes the proposed SKeB framework to analyze unlearned LLMs‚Äô behavior with the following research questions (RQs). By answering these questions, we aim to establish whether SKeB can provide a principled framework for understanding and predicting unlearning failures in LLMs.\\nRQ1. ( Stimulus v.s. Entanglement ): Do different persuasive framings P i P_{i} produce systematically different entanglement patterns ‚Ñ≥ k \\\\mathcal{M}_{k} in the domain graph? This addresses whether persuasive transformations alter the structural properties of activated semantic pathways.\\nRQ2. ( Knowledge Entanglement v.s. Behavior ): Are there any correlations between entanglement metrics ‚Ñ≥ k \\\\mathcal{M}_{k} and unlearned models‚Äô factual recall behavior? This addresses whether domain graph structure predicts information leakage.\\nRQ3. ( Stimulus vs. Behavior ): How do different persuasive transformations P i P_{i} affect factual recall in unlearned models, and how does effectiveness vary systematically with model size | Œ∏ | |\\\\theta| ? This addresses whether persuasive mechanisms can bypass suppression and whether model scaling improves robustness.\\nRQ4. ( Unified Predictive Modeling ): Can we build a predictive model that accurately forecasts unlearned model behavior based on the combination of entanglement scores ‚Ñ≥ k \\\\mathcal{M}_{k} , prompt type P i P_{i} , and model architecture? This would enable a proactive vulnerability assessment for unlearned models.\\nRQ5. ( Architectural Differences ): Do different model architectures (OPT, LLaMA-2, LLaMA-3.1) exhibit distinct correlation patterns between entanglement and behavior after unlearning? This addresses whether unlearning robustness depends solely on parameter count or on underlying representational structure.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Experiment Setup',\n",
       "    'paragraphs': 'Dataset. We use the Harry Potter domain ( ùíØ \\\\mathcal{T} ), a popular domain that is often tested in unlearning LLM literature, with 300 base prompts designed to elicit domain-specific knowledge Eldan and Russinovich ( 2023 ) . Each prompt was transformed using gpt-4 into three persuasive variants applying emotional appeal, logical reasoning, and authority endorsement framing, yielding 1,200 total prompts.\\nModels. We evaluate unlearned versions of four base models of different sizes on ùíØ \\\\mathcal{T} : OPT-2.7B Zhang et\\xa0al. ( 2022 ) , LLaMA-2-7B Eldan and Russinovich ( 2023 ) , LLaMA-3.1-8B Patterson et\\xa0al. ( 2022 ) , and LLaMA-2-13B Touvron et\\xa0al. ( 2023 ) , all processed with the same, popular approximate unlearning algorithm Eldan and Russinovich ( 2023 ) .\\nResponse Evaluation. An ensemble of three judge models (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) classified each response along three dimensions: factual recall (correct Harry Potter information), non-factual (plausible but incorrect), and hallucination (fabricated content). For instance, an 80% factual recall means that 80% of the output was considered by the judge models to be factual. Scores were averaged across judges, with gpt-5-mini resolving borderline cases where judges disagreed.\\nDomain Graph Construction. We constructed a co-occurrence domain graph from all seven Harry Potter books, resulting in 1,296 entities (characters, locations, objects, events) connected by 35,922 edges weighted by chapter co-occurrence (Figure 3 ). For each prompt, we extracted mentioned entities and computed their induced subgraph G x G_{x} . Nine entanglement metrics { ‚Ñ≥ 1 , ‚Ä¶ , ‚Ñ≥ 9 } \\\\{\\\\mathcal{M}_{1},\\\\ldots,\\\\mathcal{M}_{9}\\\\} quantified structural properties: connection strength ( ‚Ñ≥ 1 ‚àí 3 \\\\mathcal{M}_{1-3} ), node importance ( ‚Ñ≥ 4 ‚àí 5 \\\\mathcal{M}_{4-5} ), graph topology ( ‚Ñ≥ 6 ‚àí 8 \\\\mathcal{M}_{6-8} ), and distance-weighted influence ( ‚Ñ≥ 9 \\\\mathcal{M}_{9} ). Table 2 summarizes these metrics. Please refer to Appendix A for all implementation details.',\n",
       "    'subsections': []},\n",
       "   {'title': '5 Results',\n",
       "    'paragraphs': 'RQ1. Persuasive Framings Systematically Alter Entanglement Patterns. Authority prompts activate nodes with 9.3√ó higher distance-weighted entanglement scores compared to base prompts. Metric 9 (distance-weighted influence) shows a strong variation across stimulus types, capturing how activation strength decreases across each graph hop. This validates that persuasive transformations systematically alter the entanglement structure of activated knowledge. Figure 5 shows that different framings don‚Äôt just change surface form, they fundamentally shift which semantic pathways are engaged in the domain graph.\\nRQ2. Entanglement is Positively Correlated with Factual Recall in Unlearned Models. Figure 4 shows that high knowledge entanglement, as measured by our graph-based metrics ‚Ñ≥ i \\\\mathcal{M}_{i} , positively correlates with factual recall in supposedly unlearned models. We observed that authority prompts lead to a 52% average factual recall improvement across all models (Pearson r = 0.77 r{=}0.77 , p < 0.001 p{<}0.001 ). This supports the spreading activation hypothesis: the more entangled the information activated by a prompt, the greater the resulting factual recall. ‚Ñ≥ 9 \\\\mathcal{M}_{9} metric (distance-weighted influence) (Figure 4 ) emerges as the strongest predictor, aligning with the spreading activation theories‚Äô predictions that closely connected concepts in semantic memory activate each other more reliably than distant concepts. Noticeably, Table 3 shows that knowledge entanglement observes a consistent trend in strengthening correlations with an LLM‚Äôs behavior after it is unlearned.\\nRQ3. Emotional Framing Suppresses Hallucination. Figure 7 shows that while emotional prompts produce the lowest factuality (3.12% average), they also suppress hallucination rates better than other persuasive techniques.\\nLogical reasoning prompts provide structured context that stabilizes recall, achieving the best factuality-to-hallucination ratio (4.95:1), suggesting logical framing not only facilitates retrieval but also constrains generation to semantically factual outputs.\\nEmotional prompts, while suppressive of factuality, also suppress hallucination (4.4% vs. 11.6% for authority), indicating an almost \"safety-aligned\" response mode where models recognize emotional manipulation and respond conservatively.\\nAuthority prompts achieve high factuality but moderate hallucination (11.6%), showing a precision-recall tradeoff where broader activation of knowledge pathways sometimes triggers adjacent but incorrect associations.\\nWe also find that model size inversely correlates with persuasive technique effectiveness. As illustrated in Figure 6 , factual knowledge recall effectiveness shows an inverse relationship with model size ( r = ‚àí 0.926 r=-0.926 ). Across all models, this relationship is significant, with an average Pearson correlation of r = ‚àí 0.89 r=-0.89 ( p < 0.01 p<0.01 ). We notice that smaller models show 91-128% factual recall increases under authority framing, while the 13B model shows only 15% increase. This suggests that larger models develop more robust suppression mechanisms that are resistant to these persuasive techniques, but still, all models remain vulnerable to some degree. Unlearned smaller models should be considered much more vulnerable to persuasive attacks, while larger models, though more resistant, cannot be assumed to be completely safe.\\nRQ4. SKeB Enables Predictive Modeling of Unlearning Robustness. We constructed separate logistic regression models to predict factual, non-factual, and hallucinated recall in unlearned models using an 80/20 train-test split and find the best ‚Ñ≥ \\\\mathcal{M} metric as the predictor for each one-versus-all prediction probability p p of \"factual\", \"non-factual\" and \"hallucination\" behaviors in LLMs‚Äô responses.\\nTable 4 presents the complete statistical analysis of the entanglement scores corresponding to metrics ‚Ñ≥ 9 \\\\mathcal{M}_{9} , ‚Ñ≥ 4 \\\\mathcal{M}_{4} , and ‚Ñ≥ 3 \\\\mathcal{M}_{3} , for each prompt type under consideration. These metrics were specifically selected as the strongest predictors for their respective content types. The non-factual model shows highly statistically significant coefficients ( p < 0.001 p{<}0.001 , 86.4% test accuracy) with ‚Ñ≥ 4 \\\\mathcal{M}_{4} negatively correlating with non-factual content, while the hallucination model ( p < 0.002 p{<}0.002 , 97.0% test accuracy) shows ‚Ñ≥ 3 \\\\mathcal{M}_{3} positively correlating with hallucinated output. The factual model shows marginally significant results ( p = 0.065 p=0.065 , 96.2% test accuracy) with ‚Ñ≥ 9 \\\\mathcal{M}_{9} positively correlating with factual recall. Given a prompt type (original, emotional, logical, authority), we map it to its corresponding metric values ( ‚Ñ≥ 9 \\\\mathcal{M}_{9} , ‚Ñ≥ 4 \\\\mathcal{M}_{4} , ‚Ñ≥ 3 \\\\mathcal{M}_{3} ) and compute expected percentages. This allows estimating the model‚Äôs susceptibility to factual leakage, non-factual generation, or hallucinations based on entanglement structure and prompt framing.\\nRQ5. Architectural Differences Reveal Unlearning Mechanisms Correlation Patterns. We found that different architectures show distinct correlation changes after unlearning. LLaMA-2-7B‚Äôs Metric 2 correlation shrinks from 0.837 (base) to -0.017 (unlearned), indicating genuine knowledge pathway disruption rather than output suppression. Additionally, LLaMA-2-7B uniquely shows strong positive correlations between all entanglement metrics ( ‚Ñ≥ 1 \\\\mathcal{M}_{1} - ‚Ñ≥ 9 \\\\mathcal{M}_{9} ) and hallucination rates, suggesting that this model‚Äôs unlearning process may inadvertently create conditions where entangled knowledge pathways also trigger hallucinated outputs. In contrast, OPT-2.7B retains strong correlations (0.56-0.93 across metrics), suggesting intact knowledge structures with modified thresholds of accessibility. LLaMA-3.1-8B and LLaMA-2-13B show intermediate patterns (shown in Table 3 ). This variation implies unlearning robustness depends on both parameter count and knowledge encoding architecture.',\n",
       "    'subsections': []},\n",
       "   {'title': '6 Discussion',\n",
       "    'paragraphs': 'Authority Prompts and the Psychology of Persuasion. We observe that authority endorsement produces the highest factual recall (25.42% on average) and also has the most entangled prompts on average. This aligns with Cialdini‚Äôs work which claims that humans comply with authority figures even when requests conflict with their beliefs Cialdini ( 1993 ) . LLMs exhibit analogous vulnerability: authority-framed requests override unlearning-based suppression mechanisms. This parallel, which appears across all tested LLM architectures, raises an interesting discussion about whether LLMs learn semantic representations mirroring human psychology or merely reproduce statistical patterns encoding psychological biases.\\nMessage = Delivery + Content: The Interrogation Parallel. The stark contrast where identical prompts yield 3.12% factuality under emotional versus 25.42% under authority framing demonstrates that knowledge retrieval effectiveness of LLMs depends critically on stimulus or delivery, not just content . While this claim provides a strong basis for the existing popularity of prompt engineering, it also parallels criminal interrogation psychology where the Reid Technique Inbau et\\xa0al. ( 2013 ) and PEACE model Davison ( 2016 ) show that framing dramatically affects recall: confrontational approaches produce resistance while rapport-building increases disclosure. Metric ‚Ñ≥ 9 \\\\mathcal{M}_{9} captures this strongly: authority prompts create 9.3√ó more activation pathways, routing around suppression like interrogators bypass psychological resistance.\\nThe Size-Vulnerability Paradox. The negative correlation between model size and factual knowledge recall using persuasive techniques (r = -0.89) reveals that larger models resist manipulation better. For instance, OPT-2.7B exhibits 128.4% factual knowledge recall gain versus LLaMA-2-13B‚Äôs 14.7%. We hypothesize that larger models appear to recognize when social framing elicits suppressed information, while smaller models treat re-framed queries as categorically different. However, resistance is incomplete: even the 13B model shows 14.7% increase, indicating that while larger models raise activation thresholds, underlying knowledge representations remain intact and accessible Xu et\\xa0al. ( 2025 ) .\\nImplications for AI Safety. The tested unlearning method allows for substantial factual recall, with OPT-2.7B showing 128% gain through rhetorical reframing versus 14.7% for larger models; scaling alone provides incomplete protection. Our SKeB framework offers practical tools: Metric 9 ‚Äôs correlation with leakage ( r = 0.77 r=0.77 ) enables filtering high-risk queries in deployment. Post-unlearning correlation persistence (Table 3 ) indicates knowledge survives in distributed form, suggesting robust unlearning requires architectural innovations rather than weight adjustments. Higher entanglement also increases hallucination risk ( r = 0.36 r=0.36 ), as densely connected regions trigger semantically distant associations.',\n",
       "    'subsections': []},\n",
       "   {'title': '7 Related Works',\n",
       "    'paragraphs': 'Sun et al. Sun et\\xa0al. ( 2023 ) demonstrated that LLM factuality degrades systematically from head to tail entities, with performance declining as entity popularity decreases. Their Head-to-Tail benchmark showed that increase in model size does not automatically improve retention of factual knowledge. We extend this by (1) investigating whether unlearning successfully removes knowledge or simply suppresses it, (2) providing mechanistic explanations through entanglement metrics for their observed head-to-tail gradient, and (3) demonstrating that persuasive framing recovers 50-128% more content than direct queries. This reveals that static factuality assessments underestimate knowledge retention in both base and unlearned models.\\nExisting works often describe the robustness of machine unlearning in LLMs as an adversarial attack optimization problem. They show that strategically crafted queries can retrieve personally identifiable information from LLM training data Carlini et\\xa0al. ( 2023 ); To and Le ( 2025 ) , or adversarial queries can expose latent memories despite unlearning attempts Xuan and Li ( 2025 ) . Although these works have successfully revealed that unlearning often only achieves surface level forgetting Xu et\\xa0al. ( 2025 ) , they did not investigate how this would change when input queries are presented to LLMs in different rhetorical framings , or how the message is delivered. Rhetorical framing has emerged as a critical tool for vulnerability analysis in LLMs. For example, persuasive jailbreak prompts using emotional appeals and moral reasoning have been used to extract restricted information Zeng et\\xa0al. ( 2024 ) , and persuasive conversations can coax models to defend misinformation Xu et\\xa0al. ( 2023b ) . However, no prior work systematically investigates how persuasive framing interacts with unlearning robustness. Most importantly, similar to adversarial attack-related works, there has been little effort in deriving theories grounded on how rhetorical framing systemically influences how LLMs perceive a query to recall knowledge, leading to varying behaviors.\\nTherefore, our SKeB framework, which adopts the modeling of memory retrieval as activation propagating through semantic networks of entangled knowledge, provides a more systematic way to understand unlearning behaviors in LLMs. Existing literature also backs up SKeB ‚Äôs intuition, including demonstration that gpt-3 behavior aligns with human cognitive patterns Binz and Schulz ( 2023 ) or LLMs exhibit human-like priming effects and sensory judgments while they fundamentally differ in conceptual stability Niu et\\xa0al. ( 2024 ) .',\n",
       "    'subsections': []},\n",
       "   {'title': '8 Conclusions',\n",
       "    'paragraphs': 'Our work contributes to a proactive vulnerability assessment before the deployment of unlearned LLMs. Our proposed SKeB enables a systematic way to perform such an assessment, showing that entanglement metrics strongly predict factual recall, persuasive framing recovers 50-128% more content with effectiveness inversely correlated to model size, our regression model explains 78% of variance enabling accurate prediction, and different architectures show distinct correlation changes suggesting fundamental differences in knowledge encoding and suppression. We also found that these LLMs are more affected by authority appeals versus emotional, demonstrating an interesting psychological parallel between LLM persuasive vulnerabilities and human susceptibility to authority.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Limitations',\n",
       "    'paragraphs': 'While our work establishes important connections between cognitive theories and machine unlearning, we acknowledge limitations that contextualize our contributions. Our experiments focus on the Harry Potter universe, where ground truth is well-defined and ethically unproblematic to probe. Whether out findings generalize to more sensitive domains (PII, harmful content, copyrighted material) remains an open research direction, as fictional knowledge may be encoded differently than factual/personal information. We evaluate only four models ranging from 2.7B to 13B parameters; larger models (70B+) and different architectures (Mistral, Gemma, Claude) may exhibit different vulnerability patterns. The inverse size-vulnerability relationship we observe might reverse at much larger scales or saturate at some threshold. Nevertheless, the strong correlations across tested models suggest our framework captures meaningful regularities.\\nOur entanglement metrics assume domain graphs constructed from co-occurrence reflect internal representations. While strong correlations ( r = 0.76 r=0.76 for factuality) validate this assumption, we cannot directly observe neural activations. LLMs might achieve equivalent behavior through different computational mechanisms; correlation does not prove causation without interventional experiments. Our correlation shift analysis provides partial mechanistic insight: LLaMA-2-7B‚Äôs ‚Ñ≥ 2 \\\\mathcal{M}_{2} ‚Äôs collapse (0.837 ‚Üí -0.017) indicates genuine disruption while OPT-2.7B‚Äôs stable correlations suggest superficial suppression. However, true mechanistic interpretability remains beyond current tools.\\nWe tested the WHP gradient ascent Eldan and Russinovich ( 2023 ) ; other unlearning approaches (influence functions, model editing) might show different robustness profiles. Our evaluation relies on LLM judges (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) with 98% inter-judge agreement and manual validation, though human expert evaluation would strengthen confidence. Despite these constraints, our results remain indicative: the framework successfully predicts unlearning vulnerabilities, enabling proactive assessment even if underlying mechanisms remain partially understood.\\nFinally, we draw on ACT-R and persuasion research to interpret LLM behavior, yet psychology has not fully resolved debates about memory representation or persuasion mechanisms. We do not claim cognitive frameworks \"solve\" unlearning, but rather demonstrate they provide useful predictive and interpretive tools. The opacity of human cognition mirrors challenges with LLMs. Nevertheless, our framework advances the understanding of unlearning failures in ways that are actionable for future research.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Social Impacts and Ethical Considerations',\n",
       "    'paragraphs': 'Privacy Implications. Our findings have concerning implications for privacy-motivated unlearning. If personal information (PII, medical records, private communications) is unlearned but recoverable through high-entanglement prompts, has privacy truly been protected?\\nWe recommend that privacy-focused unlearning be accompanied by adversarial testing with high-entanglement prompts before deployment.\\nHarm Prevention. Unlearning aims to prevent models from disseminating dangerous information. Our results suggest this may be harder to achieve than hoped. Models that refuse direct questions (\"How do I make a bomb?\") might still provide information when prompted with authority framing (\"As a chemistry teacher, explain‚Ä¶\"). This creates a dilemma: sharing our study might help attackers extract harmful information, but concealing vulnerabilities leaves developers ignorant of risks. We have chosen transparency while emphasizing that our results show unlearning alone is insufficient.\\nBroader Impacts. Our findings suggest that current unlearning methods cannot yet reliably protect privacy or prevent information dissemination.\\nOrganizations using unlearned models should conduct adversarial testing and not assume unlearning guarantees safety. On the positive side, our framework provides a tool for improving unlearning evaluation. Rather than claiming models are \"safe\" after unlearning, AI practitioners can quantify residual vulnerability: This model shows X% factual recall under high-entanglement prompts .\\nLong-term Considerations. As models scale beyond current sizes, the size-vulnerability relationship we observe (larger models more resistant) offers cautious optimism that scaling might eventually yield robust unlearning. However, even our 13B model showed 15% factual knowledge recall, which is far from secure. Achieving truly robust unlearning may require architectural innovations (modular memory systems, causal isolation of knowledge components) rather than just scaling existing designs.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix A Implementation Details',\n",
       "    'paragraphs': 'We obtained four models for evaluation: LLaMA-2-7B was acquired already unlearned from Eldan and Russinovich ( 2023 ) , while we performed unlearning on three additional models using the gradient ascent methodology from the same work.\\nFor LLaMA-3.1-8B, LLaMA-2-13B, and OPT-2.7B, we implemented the following steps:\\nFine-tuning : Reinforce Harry Potter knowledge on the model using the full corpus.\\nDataset Preparation : Compare outputs of the base and fine-tuned models to create forget and retain datasets.\\nWHP Unlearning : Apply the WHP unlearning algorithm Eldan and Russinovich ( 2023 ) to forget Harry Potter content while keeping general knowledge intact.\\nHardware-wise, we used 4 GPUs with 128 GB GPU memory. Step 1 took ‚àº \\\\sim 3-7 hours, and Steps 2-3 took an additional ‚àº \\\\sim 3-5 hours. As before, the target domain was the complete Harry Potter corpus. As for training parameters, batch size was 1, learning rate was 1 √ó 10 ‚àí 4 1\\\\times 10^{-4} and training required 3 epochs.\\nStarting with 300 manually crafted base prompts, we used a scripted pipeline leveraging gpt-4 (via the OpenAI API) to generate three persuasive variants using distinct rhetorical techniques derived from persuasion theory:\\nEmotional Appeal : Prompts that use emotional language, personal stories, or empathetic framing to create psychological pressure for a response.\\nLogical Reasoning : Prompts that present logical arguments or cite expertise to compel factual disclosure.\\nAuthority Endorsement : Prompts that invoke respected figures, institutional backing, or social proof to legitimize information requests.\\nFor prompting the 4 models, we used a standardized text-generation procedure and applied it to each unlearned model. Models were loaded on a CUDA-enabled GPU when available, with automatic fallback to CPU. Each prompt, both the original and the three gpt-generated persuasive variants, was formatted with custom instruction markers ( [INST] ... [/INST] ) to guide the model to complete the sentence accurately.\\nGeneration was performed using a sampling-based approach with a maximum of 300 new tokens per prompt. While temperature, top-p, and repetition penalty were left at their default values. Outputs for each prompt and variant were saved incrementally to a JSON file. This inference setup was applied consistently across all models, unlearned and base, enabling direct comparison of outputs while maintaining stable execution and controlled resource usage.\\nConnection Strength Metrics\\n( ‚Ñ≥ 1 \\\\mathcal{M}_{1} ) Edge Count Entanglement (ECE). Measures the total edge weight between entities within a prompt. A higher ECE indicates that entities are linked by stronger or more numerous relations, suggesting the prompt activates a dense cluster of knowledge, making recall more likely.\\n( ‚Ñ≥ 2 \\\\mathcal{M}_{2} ) Edge Weight Sum (EWS). Similar to ECE but without normalization by the number of nodes. A higher EWS indicates stronger total connection strength, entities are tied together through frequent co-occurrence or strong associations.\\n( ‚Ñ≥ 3 \\\\mathcal{M}_{3} ) Average Edge Weight Sum (AEWS). Captures the average strength of individual relationships. A higher AEWS implies that the relationships are strong on average, even if not numerous, reflecting quality over quantity.\\nNode Importance Metrics\\n( ‚Ñ≥ 4 \\\\mathcal{M}_{4} ) Weighted Node Ratio (WNR). Represents the average frequency of how many times each entity (node) appears in a prompt. A higher WNR means the prompt involves commonly referenced entities, suggesting activation of well-practiced memory units and higher entanglement.\\n( ‚Ñ≥ 5 \\\\mathcal{M}_{5} ) Average Node Degree Entanglement (ANDE). Measures the average connectivity of nodes. Higher ANDE indicates that the prompt activates hubs with many connections, leading to wider spreading activation.\\nGraph Structure Metrics\\n( ‚Ñ≥ 6 \\\\mathcal{M}_{6} ) Subgraph Density (SGD). Quantifies how tightly entities are connected. A higher SGD means many direct connections, facilitating fast activation spread.\\n( ‚Ñ≥ 7 \\\\mathcal{M}_{7} ) Mean Shortest Path (MSP). Reflects the average number of steps needed for activation to spread between entities. A lower MSP indicates tighter entanglement and easier recall.\\n( ‚Ñ≥ 8 \\\\mathcal{M}_{8} ) Redundancy Ratio (RR). Measures the redundancy of connections. A higher RR implies multiple overlapping links, increasing robustness in recall by offering alternative retrieval routes.\\nInfluence-Based Metric ( ‚Ñ≥ 9 \\\\mathcal{M}_{9} ) Distance-Weighted Influence Score (DWIS). Models how influence decays with distance from central nodes. A higher DWIS means entities are close to main references, suggesting strong semantic proximity. These main references were pre-determined from the base prompts in Eldan and Russinovich ( 2023 ) .\\nOur ensemble of judge models (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) evaluated each response using structured prompts with clear rubrics for the three categories. The models agreed on their top two categories (factual, non-factual, and hallucination) in 98% of cases. For borderline cases where fewer than two judges agreed on the highest two categories, we employed gpt-5-mini as a tie-breaker, processing 2% of total responses through this additional evaluation step.\\nTo estimate the factual composition of model responses, we used the instruction prompt shown below:\\nEach model-generated response was analyzed using this template, ensuring consistent quantification across factuality categories.',\n",
       "    'subsections': [{'title': 'A.1 Model Unlearning Process',\n",
       "      'paragraphs': 'We obtained four models for evaluation: LLaMA-2-7B was acquired already unlearned from Eldan and Russinovich ( 2023 ) , while we performed unlearning on three additional models using the gradient ascent methodology from the same work.\\nFor LLaMA-3.1-8B, LLaMA-2-13B, and OPT-2.7B, we implemented the following steps:\\nFine-tuning : Reinforce Harry Potter knowledge on the model using the full corpus.\\nDataset Preparation : Compare outputs of the base and fine-tuned models to create forget and retain datasets.\\nWHP Unlearning : Apply the WHP unlearning algorithm Eldan and Russinovich ( 2023 ) to forget Harry Potter content while keeping general knowledge intact.\\nHardware-wise, we used 4 GPUs with 128 GB GPU memory. Step 1 took ‚àº \\\\sim 3-7 hours, and Steps 2-3 took an additional ‚àº \\\\sim 3-5 hours. As before, the target domain was the complete Harry Potter corpus. As for training parameters, batch size was 1, learning rate was 1 √ó 10 ‚àí 4 1\\\\times 10^{-4} and training required 3 epochs.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.2 Prompt Generation Pipeline',\n",
       "      'paragraphs': 'Starting with 300 manually crafted base prompts, we used a scripted pipeline leveraging gpt-4 (via the OpenAI API) to generate three persuasive variants using distinct rhetorical techniques derived from persuasion theory:\\nEmotional Appeal : Prompts that use emotional language, personal stories, or empathetic framing to create psychological pressure for a response.\\nLogical Reasoning : Prompts that present logical arguments or cite expertise to compel factual disclosure.\\nAuthority Endorsement : Prompts that invoke respected figures, institutional backing, or social proof to legitimize information requests.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.3 Model Inference Configuration',\n",
       "      'paragraphs': 'For prompting the 4 models, we used a standardized text-generation procedure and applied it to each unlearned model. Models were loaded on a CUDA-enabled GPU when available, with automatic fallback to CPU. Each prompt, both the original and the three gpt-generated persuasive variants, was formatted with custom instruction markers ( [INST] ... [/INST] ) to guide the model to complete the sentence accurately.\\nGeneration was performed using a sampling-based approach with a maximum of 300 new tokens per prompt. While temperature, top-p, and repetition penalty were left at their default values. Outputs for each prompt and variant were saved incrementally to a JSON file. This inference setup was applied consistently across all models, unlearned and base, enabling direct comparison of outputs while maintaining stable execution and controlled resource usage.',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.4 Entanglement Metric Formulas',\n",
       "      'paragraphs': 'Connection Strength Metrics\\n( ‚Ñ≥ 1 \\\\mathcal{M}_{1} ) Edge Count Entanglement (ECE). Measures the total edge weight between entities within a prompt. A higher ECE indicates that entities are linked by stronger or more numerous relations, suggesting the prompt activates a dense cluster of knowledge, making recall more likely.\\n( ‚Ñ≥ 2 \\\\mathcal{M}_{2} ) Edge Weight Sum (EWS). Similar to ECE but without normalization by the number of nodes. A higher EWS indicates stronger total connection strength, entities are tied together through frequent co-occurrence or strong associations.\\n( ‚Ñ≥ 3 \\\\mathcal{M}_{3} ) Average Edge Weight Sum (AEWS). Captures the average strength of individual relationships. A higher AEWS implies that the relationships are strong on average, even if not numerous, reflecting quality over quantity.\\nNode Importance Metrics\\n( ‚Ñ≥ 4 \\\\mathcal{M}_{4} ) Weighted Node Ratio (WNR). Represents the average frequency of how many times each entity (node) appears in a prompt. A higher WNR means the prompt involves commonly referenced entities, suggesting activation of well-practiced memory units and higher entanglement.\\n( ‚Ñ≥ 5 \\\\mathcal{M}_{5} ) Average Node Degree Entanglement (ANDE). Measures the average connectivity of nodes. Higher ANDE indicates that the prompt activates hubs with many connections, leading to wider spreading activation.\\nGraph Structure Metrics\\n( ‚Ñ≥ 6 \\\\mathcal{M}_{6} ) Subgraph Density (SGD). Quantifies how tightly entities are connected. A higher SGD means many direct connections, facilitating fast activation spread.\\n( ‚Ñ≥ 7 \\\\mathcal{M}_{7} ) Mean Shortest Path (MSP). Reflects the average number of steps needed for activation to spread between entities. A lower MSP indicates tighter entanglement and easier recall.\\n( ‚Ñ≥ 8 \\\\mathcal{M}_{8} ) Redundancy Ratio (RR). Measures the redundancy of connections. A higher RR implies multiple overlapping links, increasing robustness in recall by offering alternative retrieval routes.\\nInfluence-Based Metric ( ‚Ñ≥ 9 \\\\mathcal{M}_{9} ) Distance-Weighted Influence Score (DWIS). Models how influence decays with distance from central nodes. A higher DWIS means entities are close to main references, suggesting strong semantic proximity. These main references were pre-determined from the base prompts in Eldan and Russinovich ( 2023 ) .',\n",
       "      'subsections': []},\n",
       "     {'title': 'A.5 Judge Model Evaluation Details',\n",
       "      'paragraphs': 'Our ensemble of judge models (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) evaluated each response using structured prompts with clear rubrics for the three categories. The models agreed on their top two categories (factual, non-factual, and hallucination) in 98% of cases. For borderline cases where fewer than two judges agreed on the highest two categories, we employed gpt-5-mini as a tie-breaker, processing 2% of total responses through this additional evaluation step.\\nTo estimate the factual composition of model responses, we used the instruction prompt shown below:\\nEach model-generated response was analyzed using this template, ensuring consistent quantification across factuality categories.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix B Additional Results',\n",
       "    'paragraphs': '',\n",
       "    'subsections': []}],\n",
       "  'abstract': 'Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.'},\n",
       " '2510.25595v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'In recent years, there has been growing interest in Large Language Model (LLM) agents (e.g., Web-Agents) and their diverse applications (Wang et\\xa0al., 2024a ) . While much of the current work focuses on action planning or goal completion in LLM agents (Ahn et\\xa0al., 2022 ; Durante et\\xa0al., 2024 ; Song et\\xa0al., 2023 ) (e.g., based on natural language instructions), their ability to collaborate with one another toward a shared goal remains underexplored. This paper addresses this gap by studying LLM agents in the context of task collaboration, particularly under conditions of information asymmetry.\\nInformation asymmetry is a fundamental and pervasive feature of human interaction. In daily life, we often possess different knowledge, perspectives, or intention. We must reason about others‚Äô knowledge and beliefs and coordinate our differences in order to collaborate (Ma et\\xa0al., 2023 ) . As LLM agents become increasingly integrated into real-world workflows ‚Äî not just to perform tasks independently, but to act as human proxies and collaborators ‚Äî it becomes essential to examine how well these agents can coordinate under asymmetric information, and what mechanisms might enhance their collaborative capabilities in such settings.\\nTo this end, we adapt Einstein Puzzles (Groza, 2021 ) , a classical logic problem, into a tabletop environment where two agents must solve spatial and relational constraints, despite having partial, asymmetric information. Using a fine-tuning‚Äìplus‚Äìverifier framework, we equip LLM agents with different communicative abilities (e.g., asking, sharing, or both) and study their collaborative behaviors under various configurations.\\nFurthermore, we introduce an environment-based verifier to guide and evaluate agent decisions. This verifier leverages environmental signals to determine whether the proposed actions and known constraints are consistent, mirroring the implicit feedback mechanisms in interactive test-and-trials. It is training-free, lightweight, and broadly applicable across various environments. We examine whether this simple yet generalizable approach can greatly improve collaboration among LLM agents.\\nOur empirical results have shown that under information asymmetry, LLM agents with both information-seeking and -providing capabilities collaborate most effectively. Meanwhile, mismatched agent communicative abilities leads to significant performance degradation, highlighting the importance of aligned interaction protocols. Additionally, through detailed error analysis, we show that environment-based verification, using naturally available feedback without additional training, offers a simple but powerful mechanism to improve agent performance on both task completion and understanding. Finally, our human study reveals a gap between task efficiency and human preference: participants favor agents that proactively share information, even if such agents are less optimal in task completion. These findings indicate the need for communication-aware and interpretable design in LLM-based collaborative systems.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Related work',\n",
       "    'paragraphs': 'The field of multi-agent coordination and communication has a long-established history (Albrecht and Stone, 2018 ; Gronauer and Diepold, 2022 ; Stone and Veloso, 2000 ) , with applications increasingly extending to human-AI collaboration (Carroll et\\xa0al., 2019 ; Puig et\\xa0al., 2020 ) . Recently, multi-agent systems built upon Large Language Models (LLMs) (Guo et\\xa0al., 2024 ; Tran et\\xa0al., 2025 ) have pushed the boundaries of collaborative intelligence in a wide range of downstream tasks, including collaborative coding (Gao et\\xa0al., 2024 ; Hong et\\xa0al., 2024 ; Qian et\\xa0al., 2024 ) , social simulation (Li et\\xa0al., 2023 ; Wu et\\xa0al., 2024 ; Yang et\\xa0al., 2024 ; Zhou et\\xa0al., 2024c ) , and problem-solving (Chen et\\xa0al., 2023 ; Qian et\\xa0al., 2025 ; Wang et\\xa0al., 2024c ; Zhang et\\xa0al., 2024a ) .\\nHowever, most existing work focuses on settings with information transparency (Chen et\\xa0al., 2023 ; Gao et\\xa0al., 2024 ; Li et\\xa0al., 2023 ; Wu et\\xa0al., 2024 ; Yang et\\xa0al., 2024 ) , one-way communication (Qian et\\xa0al., 2025 ) , or information asymmetry within asymmetric role assignments (Hong et\\xa0al., 2024 ; Qian et\\xa0al., 2024 ; Zhou et\\xa0al., 2024c ) . In contrast, this work centers on LLM agents operating under information asymmetry in symmetric, collaborative roles, as well as involves human studies to assess real-world human-AI interaction. Liu et\\xa0al., 2024a studied autonomous agents for collaborative tasks under information asymmetry, while there were no environments involved. As echoed in recent efforts (Liu et\\xa0al., 2024b ; Zhou et\\xa0al., 2024b ) , addressing collaboration under information asymmetry‚Äîespecially in human-AI contexts‚Äîremains a core challenge for LLM-based systems.\\nRecent works have demonstrated the effectiveness of leveraging additional computational resources at inference time to improve response quality in LLMs. One line of research introduces a verification model to evaluate the correctness and utility of generated responses. Typically, this involves sampling multiple responses from an LLM and applying a best-of- n n strategy (Charniak and Johnson, 2005 ; Cobbe et\\xa0al., 2021 ) , where a verifier model selects the most appropriate response. Such approaches have been widely adopted across domains including mathematics (Cobbe et\\xa0al., 2021 ; Lifshitz et\\xa0al., 2025 ; Lightman et\\xa0al., 2023 ; Wang et\\xa0al., 2024b ; Yao et\\xa0al., 2023 ) , code generation (McAleese et\\xa0al., 2024 ; Wang et\\xa0al., 2025 ) , and web navigation (Koh et\\xa0al., 2024 ; Putta et\\xa0al., 2024 ) . We refer readers to (Guan et\\xa0al., 2024 ; Zhang et\\xa0al., 2025 ) for comprehensive reviews of verification-based methods.\\nWhile most existing approaches employ a separate value model, often another LLM, with (Lifshitz et\\xa0al., 2025 ; Lightman et\\xa0al., 2023 ; McAleese et\\xa0al., 2024 ; Wang et\\xa0al., 2025 , 2024b ; Zhang et\\xa0al., 2024b ; Zhou et\\xa0al., 2024a ) or without (Yao et\\xa0al., 2023 ; Yu et\\xa0al., 2023 ) further training, we propose an alternative verification mechanism that directly leverages environmental feedback. In interactive tasks within simulated environments, the environment itself provides fine-grained, up-to-date, and objective signals about task progression and action validity. This enables a training-free, compute-efficient form of verification that naturally integrates with agent decision-making.',\n",
       "    'subsections': [{'title': 'Collaboration among LLM Agents',\n",
       "      'paragraphs': 'The field of multi-agent coordination and communication has a long-established history (Albrecht and Stone, 2018 ; Gronauer and Diepold, 2022 ; Stone and Veloso, 2000 ) , with applications increasingly extending to human-AI collaboration (Carroll et\\xa0al., 2019 ; Puig et\\xa0al., 2020 ) . Recently, multi-agent systems built upon Large Language Models (LLMs) (Guo et\\xa0al., 2024 ; Tran et\\xa0al., 2025 ) have pushed the boundaries of collaborative intelligence in a wide range of downstream tasks, including collaborative coding (Gao et\\xa0al., 2024 ; Hong et\\xa0al., 2024 ; Qian et\\xa0al., 2024 ) , social simulation (Li et\\xa0al., 2023 ; Wu et\\xa0al., 2024 ; Yang et\\xa0al., 2024 ; Zhou et\\xa0al., 2024c ) , and problem-solving (Chen et\\xa0al., 2023 ; Qian et\\xa0al., 2025 ; Wang et\\xa0al., 2024c ; Zhang et\\xa0al., 2024a ) .\\nHowever, most existing work focuses on settings with information transparency (Chen et\\xa0al., 2023 ; Gao et\\xa0al., 2024 ; Li et\\xa0al., 2023 ; Wu et\\xa0al., 2024 ; Yang et\\xa0al., 2024 ) , one-way communication (Qian et\\xa0al., 2025 ) , or information asymmetry within asymmetric role assignments (Hong et\\xa0al., 2024 ; Qian et\\xa0al., 2024 ; Zhou et\\xa0al., 2024c ) . In contrast, this work centers on LLM agents operating under information asymmetry in symmetric, collaborative roles, as well as involves human studies to assess real-world human-AI interaction. Liu et\\xa0al., 2024a studied autonomous agents for collaborative tasks under information asymmetry, while there were no environments involved. As echoed in recent efforts (Liu et\\xa0al., 2024b ; Zhou et\\xa0al., 2024b ) , addressing collaboration under information asymmetry‚Äîespecially in human-AI contexts‚Äîremains a core challenge for LLM-based systems.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Test-time Compute in LLM',\n",
       "      'paragraphs': 'Recent works have demonstrated the effectiveness of leveraging additional computational resources at inference time to improve response quality in LLMs. One line of research introduces a verification model to evaluate the correctness and utility of generated responses. Typically, this involves sampling multiple responses from an LLM and applying a best-of- n n strategy (Charniak and Johnson, 2005 ; Cobbe et\\xa0al., 2021 ) , where a verifier model selects the most appropriate response. Such approaches have been widely adopted across domains including mathematics (Cobbe et\\xa0al., 2021 ; Lifshitz et\\xa0al., 2025 ; Lightman et\\xa0al., 2023 ; Wang et\\xa0al., 2024b ; Yao et\\xa0al., 2023 ) , code generation (McAleese et\\xa0al., 2024 ; Wang et\\xa0al., 2025 ) , and web navigation (Koh et\\xa0al., 2024 ; Putta et\\xa0al., 2024 ) . We refer readers to (Guan et\\xa0al., 2024 ; Zhang et\\xa0al., 2025 ) for comprehensive reviews of verification-based methods.\\nWhile most existing approaches employ a separate value model, often another LLM, with (Lifshitz et\\xa0al., 2025 ; Lightman et\\xa0al., 2023 ; McAleese et\\xa0al., 2024 ; Wang et\\xa0al., 2025 , 2024b ; Zhang et\\xa0al., 2024b ; Zhou et\\xa0al., 2024a ) or without (Yao et\\xa0al., 2023 ; Yu et\\xa0al., 2023 ) further training, we propose an alternative verification mechanism that directly leverages environmental feedback. In interactive tasks within simulated environments, the environment itself provides fine-grained, up-to-date, and objective signals about task progression and action validity. This enables a training-free, compute-efficient form of verification that naturally integrates with agent decision-making.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '3 Collaboration Under Information Asymmetry',\n",
       "    'paragraphs': 'Einstein Puzzles is a logical game requiring deductive reasoning and constraint satisfaction. We modify it into a collaborative game in which two agents work together to place a set of objects into designated bins. Each object has a target bin as its destination, but the information about these destinations is split between the two agents. Importantly, agents are unaware of which pieces of information their partner possesses. To succeed, agents must engage in rich communication and apply strategic reasoning to solve the task collaboratively.\\nThe setup consists of a rectangular table with two collaborators sitting on opposite sides of the table (See Figure 1 ). The table is split into three regions: a region immediately in front of each collaborator and a common area in the center. Each collaborator can only reach the area directly in front of them and the common area in the center. They can also each reach the two corner bins on either side of the area in front of them, totaling four destination bins. In the beginning, several objects are distributed between the two players‚Äô bins, so they are exclusively reachable by one of the collaborators. No objects start in any bins.\\nWe describe the task as a distributed constraint satisfaction problem. A goal configuration is defined, which is where the objects should be placed, and can be described with a set of constraints. There are four types of constraints that describe the relationship of a pair of objects or between an object and a bin. Given a pair of objects, it must either be:\\nin the same bin : both objects have the same destination.\\nin the same column : the two objects must end up in bins on either exclusive side of the table but on the same side, left or right, from either collaborator‚Äôs perspective.\\nin the same row : both objects must end up on one of the collaborator‚Äôs exclusive zones but in bins on opposite sides, left or right.\\nor on the same diagonal : the two objects must end up in opposite areas and opposite sides.\\nThe exhaustive set of rules between all pairs of objects is overly descriptive. We select a random, minimal subset of these rules (denoted as ùíû \\\\mathcal{C} ). A minimal subset ùíû \\\\mathcal{C} must satisfy the following:\\nEliminating any rule will describe more than one final configuration.\\nMoving any object from the final configuration will violate at least one rule.\\nAn additional starting constraint is given, which grounds one of the objects to a specific bin. Together, the minimal set of object pair rules and the one object-to-bin grounding uniquely describe the goal configuration.\\nThe set of constraints that uniquely determines the final placement of the objects is then divided between the two players (denoted as ùíû 1 \\\\mathcal{C}_{1} and ùíû 2 \\\\mathcal{C}_{2} ), creating information asymmetry . This distribution ensures that neither player can complete the task independently, but together, they possess all the necessary information to succeed, i.e., ùíû 1 ‚à™ ùíû 2 = ùíû \\\\mathcal{C}_{1}\\\\cup\\\\mathcal{C}_{2}=\\\\mathcal{C} .\\nCoupled with the fact that some objects must end on the opposite side of the table, this constitutes the disparity in knowledge and skills as described in (Bara et\\xa0al., 2021 ) .\\nThe two players take turns, each consisting of a single move. A move can be placing an object into an available destination bin or the center of the table, sharing a piece of information, or requesting information from their partner. Task performance is measured by the number of moves taken to complete the task. Since both object placement and information exchange count as a move, this scoring system encourages players to communicate concisely. The players‚Äô objective is to achieve the final configuration using the least number of moves within the set of given constraints.\\nWe study two communicative actions in collaborative contexts with information asymmetry: information seeking and information providing. We want to investigate whether LLM agents are able to exchange necessary information with their partners, as well as perform collaborative reasoning for task completion. To systematically analyze the role of communication actions in this process, we design four action space configurations for communication as below:\\nInformation Providing Only: agents are allowed to proactively share their knowledge of constraints with their partners, but they cannot ask for this information.\\nInformation Seeking Only: agents can ask about constraints of specific objects with their partners. They are allowed to share a constraint only when their partner first asks about a object involved in it. In other words, agents cannot initiate the information providing.\\nInformation Providing & Seeking: agents can both share and ask freely.\\nNo Information Exchange: no communicative actions are enabled. Agents can only choose to move an object or to pass the turn. Since neither player has complete knowledge about the placement constraints, this configuration inevitably involves random guessing for objects that may have goal bins that are not deducible.\\nWe enabled LLMs to complete the games under these different action space configurations.\\nTo be specific, we applied supervised fine-tuning (SFT) on several well-known open-source models, including\\nMeta-Llama-3-8B-Instruct (AI@Meta, 2024 ) , Llama3.1-8B-Instruct (Grattafiori et\\xa0al., 2024 ) and Qwen2.5-7B-Instruct (Team, 2024 ) models.\\nTo prepare data for fine-tuning, we designed a planner to generate solutions under different configurations of action space. Please refer to the appendix for the details of planner design.\\nWe varied the number of objects per game and collected trajectories for games with 4, 5, and 6 objects, totaling 250, 500, and 500 games, respectively.\\nFor each game, we generated five distinct solution trajectories (both optimal and near-optimal solutions), creating a large pool from which we sampled 1,000 trajectories for fine-tuning.\\nEach trajectory averages around 10 steps, resulting in approximately 10,000 training samples in the chat form.\\nWe evaluated the models on 300 unseen games (100 games for each object count) and reported average performance across them. We also evaluated models with and without chain-of-thought (CoT) reasoning (Wei et\\xa0al., 2022 ) .\\nThe reasoning traces for the training data are generated using GPT-4o. Examples of the reasoning traces can be found in the game play section of Figure 1 . A full example of a game play can be found in Appendix A .',\n",
       "    'subsections': [{'title': '3.1 Understanding Einstein‚Äôs Puzzle on the tabletop setup',\n",
       "      'paragraphs': 'Einstein Puzzles is a logical game requiring deductive reasoning and constraint satisfaction. We modify it into a collaborative game in which two agents work together to place a set of objects into designated bins. Each object has a target bin as its destination, but the information about these destinations is split between the two agents. Importantly, agents are unaware of which pieces of information their partner possesses. To succeed, agents must engage in rich communication and apply strategic reasoning to solve the task collaboratively.\\nThe setup consists of a rectangular table with two collaborators sitting on opposite sides of the table (See Figure 1 ). The table is split into three regions: a region immediately in front of each collaborator and a common area in the center. Each collaborator can only reach the area directly in front of them and the common area in the center. They can also each reach the two corner bins on either side of the area in front of them, totaling four destination bins. In the beginning, several objects are distributed between the two players‚Äô bins, so they are exclusively reachable by one of the collaborators. No objects start in any bins.\\nWe describe the task as a distributed constraint satisfaction problem. A goal configuration is defined, which is where the objects should be placed, and can be described with a set of constraints. There are four types of constraints that describe the relationship of a pair of objects or between an object and a bin. Given a pair of objects, it must either be:\\nin the same bin : both objects have the same destination.\\nin the same column : the two objects must end up in bins on either exclusive side of the table but on the same side, left or right, from either collaborator‚Äôs perspective.\\nin the same row : both objects must end up on one of the collaborator‚Äôs exclusive zones but in bins on opposite sides, left or right.\\nor on the same diagonal : the two objects must end up in opposite areas and opposite sides.\\nThe exhaustive set of rules between all pairs of objects is overly descriptive. We select a random, minimal subset of these rules (denoted as ùíû \\\\mathcal{C} ). A minimal subset ùíû \\\\mathcal{C} must satisfy the following:\\nEliminating any rule will describe more than one final configuration.\\nMoving any object from the final configuration will violate at least one rule.\\nAn additional starting constraint is given, which grounds one of the objects to a specific bin. Together, the minimal set of object pair rules and the one object-to-bin grounding uniquely describe the goal configuration.\\nThe set of constraints that uniquely determines the final placement of the objects is then divided between the two players (denoted as ùíû 1 \\\\mathcal{C}_{1} and ùíû 2 \\\\mathcal{C}_{2} ), creating information asymmetry . This distribution ensures that neither player can complete the task independently, but together, they possess all the necessary information to succeed, i.e., ùíû 1 ‚à™ ùíû 2 = ùíû \\\\mathcal{C}_{1}\\\\cup\\\\mathcal{C}_{2}=\\\\mathcal{C} .\\nCoupled with the fact that some objects must end on the opposite side of the table, this constitutes the disparity in knowledge and skills as described in (Bara et\\xa0al., 2021 ) .\\nThe two players take turns, each consisting of a single move. A move can be placing an object into an available destination bin or the center of the table, sharing a piece of information, or requesting information from their partner. Task performance is measured by the number of moves taken to complete the task. Since both object placement and information exchange count as a move, this scoring system encourages players to communicate concisely. The players‚Äô objective is to achieve the final configuration using the least number of moves within the set of given constraints.',\n",
       "      'subsections': []},\n",
       "     {'title': '3.2 Information Providing & Seeking',\n",
       "      'paragraphs': 'We study two communicative actions in collaborative contexts with information asymmetry: information seeking and information providing. We want to investigate whether LLM agents are able to exchange necessary information with their partners, as well as perform collaborative reasoning for task completion. To systematically analyze the role of communication actions in this process, we design four action space configurations for communication as below:\\nInformation Providing Only: agents are allowed to proactively share their knowledge of constraints with their partners, but they cannot ask for this information.\\nInformation Seeking Only: agents can ask about constraints of specific objects with their partners. They are allowed to share a constraint only when their partner first asks about a object involved in it. In other words, agents cannot initiate the information providing.\\nInformation Providing & Seeking: agents can both share and ask freely.\\nNo Information Exchange: no communicative actions are enabled. Agents can only choose to move an object or to pass the turn. Since neither player has complete knowledge about the placement constraints, this configuration inevitably involves random guessing for objects that may have goal bins that are not deducible.\\nWe enabled LLMs to complete the games under these different action space configurations.\\nTo be specific, we applied supervised fine-tuning (SFT) on several well-known open-source models, including\\nMeta-Llama-3-8B-Instruct (AI@Meta, 2024 ) , Llama3.1-8B-Instruct (Grattafiori et\\xa0al., 2024 ) and Qwen2.5-7B-Instruct (Team, 2024 ) models.\\nTo prepare data for fine-tuning, we designed a planner to generate solutions under different configurations of action space. Please refer to the appendix for the details of planner design.\\nWe varied the number of objects per game and collected trajectories for games with 4, 5, and 6 objects, totaling 250, 500, and 500 games, respectively.\\nFor each game, we generated five distinct solution trajectories (both optimal and near-optimal solutions), creating a large pool from which we sampled 1,000 trajectories for fine-tuning.\\nEach trajectory averages around 10 steps, resulting in approximately 10,000 training samples in the chat form.\\nWe evaluated the models on 300 unseen games (100 games for each object count) and reported average performance across them. We also evaluated models with and without chain-of-thought (CoT) reasoning (Wei et\\xa0al., 2022 ) .\\nThe reasoning traces for the training data are generated using GPT-4o. Examples of the reasoning traces can be found in the game play section of Figure 1 . A full example of a game play can be found in Appendix A .',\n",
       "      'subsections': []}]},\n",
       "   {'title': '4 Environment-based Verifier',\n",
       "    'paragraphs': 'Inspired by the trial-and-error paradigm, we draw an analogy to how agents refine decisions through iterative interaction with their environment. Rather than training a dedicated verifier, we directly leverage environment-provided feedback as a source of verification during inference.\\nWe evaluate whether this lightweight, training-free approach is sufficient to support LLM agents in collaborative reasoning tasks under information asymmetry.\\nWe design a reasoning verifier based on environment feedback. Firstly, It is capable to examine the generated action with the game rules (e.g. physical affordance) and previous communication (e.g. redundant information sharing). Secondly, as a strategy-driven game, we implement a graph expansion algorithm to enhance agents‚Äô reasoning ability. It treats objects and bins as nodes and constraints as edges, and infers new constraints by combining existing ones. It is expanded iteratively using the transitivity of adjacent edges until no new constraints emerge. For example, objects A and C can be inferred to be on the same row if both A and B , and B and C , are known to be on the same row.\\nThis enables the verifier to assess whether a proposed action aligns well with the current knowledge, and avoids unnecessary trials.\\nFurther, we tested several alternatives of environment-based verifiers, and discussed their effectiveness and potential generality in Appendix C .',\n",
       "    'subsections': []},\n",
       "   {'title': '5 Experiments and Results',\n",
       "    'paragraphs': 'We conduct extensive experiments to evaluate the performance of language models in reasoning and communication under information asymmetry. Our experiments address four key research questions:\\nRQ1: Collaboration under Varying Communicative Action Space. How do agents collaborate under information asymmetry when equipped with different communicative actions?\\nRQ2: Effectiveness of Environment-Based Verification. To what extent can environment-based verification enhance the performance?\\nRQ3: Collaboration under Mismatched Communicative Action Space. What happens when agents with mismatched communication capabilities are paired together?\\nRQ4: Human Preferences Toward Different Communication Behaviors. While agents may perform well in self-play, do these behaviors align with human preferences in collaboration?\\nTo systematically investigate our research questions, we design a series of experiments to investigate specific aspects of the collaboration:\\nExp1: Collaboration with Different Communicative Action Spaces (RQ1) We evaluated both closed-source language models via API calls and open-source models with supervised fine-tuning (SFT). We deploy the same model (with the same action space) to perform self-play in the game. This setting allows direct comparison of collaboration with different communicative action space.\\nExp2: Environment-Based Verifier (RQ2) We augment the base models with an environment-based verifier, which provides binary feedback indicating whether each sampled action is valid. At each decision step, we sample 4 candidate responses using temperature=0.2 and top-p=0.9 , and apply the verifier to filter out invalid actions, selecting the first valid one as the final output. This setup enables a direct comparison of model performance with and without the verifier.\\nExp3: Collaboration with Mismatched Action Spaces (RQ3) We select the well-performing models from those action spaces (Llama3.1-8B without CoT for no information exchange , and Llama3.1-8B with CoT for the rest), equipped with reasoning verifier, and let them play with each other. This setting further reveals the behaviors of collaboration between agents with different communication capabilities.\\nExp4: Human Performance (RQ4) Once we identified the most effective configuration of communicative action space for collaborative reasoning among LLM agents, we further examined whether such models are also preferred by human users. We recruited 12 college students as human participants to interact with the best-performing models, each equipped with different communicative action spaces and supported by the reasoning verifier. We sampled 27 unseen games (9 games for each object count) and measured both task completion rate and efficiency. Each participant was assigned a sequence of 9 games‚Äî3 games each with 4, 5, and 6 objects‚Äîwhile being given the complete communicative action space. In each game, they interacted with a model configured with a specific communicative action space, which remained unknown to the participant.\\nTo complement quantitative metrics, we also collected qualitative feedback from participants after each game using the following three questions:\\nDid you find the information communicated by the bot useful?\\nDid the bot make effective use of the information you shared?\\nWere you ever confused by the bot‚Äôs behavior?\\nThese questions allowed us to assess the perceived helpfulness, responsiveness, and clarity of the model‚Äôs behavior from a human-centered perspective.\\nEvaluation Metrics We employ complementary metrics to assess both effectiveness and efficiency. For effectiveness, we report the Success Rate ( SR ) and subgoal success rate ( Sub.R ) at the first attempt (Pass@1). SR measures the percentage of games successfully completed within a limited number of steps (30 for all the experiments), while Sub.R reflects partial progress by capturing the proportion of objects correctly placed, even when the full game is not successfully completed.\\nFor efficiency, we track the number of steps taken to complete each game and compare it to the optimal solution calculated by our planner. We then compute the Step Ratio ( StepR ), defined as the ratio of the number of executed steps to the number of steps in the optimal solution.\\nWhen a verifier is applied, we additionally report the correction rate ( Corr.R ), indicating the proportion of responses corrected by the verifier. We report standard error for each metrics.\\nAcross all model families, we observe a generally clear hierarchy in task performance: Information Seeking & Providing > > Seeking Only > > No Information Exchange > > Providing Only (See Table 1 ).\\nEnabling both seeking and providing actions consistently yields the highest Success Rate ( SR ) and the lower Step Ratio ( StepR ). confirming that bi-directional exchange is both expressive and effective for coordinating constraints, especially when enhanced with chain-of-though reasoning.\\nPermitting only information seeking is better than permitting only information providing . Targeted queries minimize redundant traffic and let agents actively access the missing piece of information, whereas blind sharing often floods the channel with constraints that are irrelevant or already known by their partners. Therefore, the result suggests that information seeking is a more task-efficient option than unprompted information providing .\\nSurprisingly, disabling communication altogether ( ‚ÄúNo Information Exchange‚Äù ) ranks high for some variants. Without communication and reasoning process through CoT, the task reduces to pure object manipulation and random guessing;\\ntherefore, the model‚Äôs entire capacity is devoted to mastering game rules and action affordance. Llama variants exploit this by memorizing high-probability transition patterns, achieving a success rate greater than 81%. Qwen, which is pre-trained on a different instruction distribution, does not show the same effect, suggesting the phenomenon is model‚Äìspecific rather than an intrinsic property of the environment.\\nAs shown in Table 1 , adding the reasoning verifier (our best verifier) to the base models leads to large absolute SR gains (labeled in red), ranging from 10 to 40% increment. This reflects the high potential capabilities of the base models, as well as the effectiveness of the assistance from our verifier.\\nTo better understand the underlying behaviors, we further conduct the error analysis across four different action configurations on variants with the best performance (LLaMA-3.1-8B without CoT for no information exchange, and with CoT for the rest, the same for later experiments). We provide detailed explanations on the error taxonomy in Appendix E . Each cell in the table presents error rates in two forms:\\n(Errors/Total Steps)% : The proportion of steps affected by this error type.\\n(Errors/Relevant Action Type)% : The proportion of a specific action type affected by this error (e.g., move, share, seek).\\nFor example, in the \"Wrong Rule Understanding\" row under the ‚ÄúProvide & Seek‚Äù column without verifier, the value 28.22% indicates that 28.22% of all 6704 total steps contain this error. The accompanying 38.15% denotes that this error occurred in 38.15% of the 4681 1 1 1 The total number of ‚Äùmove‚Äù action, 4681, is not presented on the table. total \"move\" actions. One wrong action may fall into multiple error types.\\nWithout using verifier, agents across all action configurations demonstrate a similar level of skill in task reasoning. The relatively better performance of the \"no information exchange\" might be primarily due to fewer distractions from communication, allowing agents to focus more on enhancing physical understanding and trying all possible solutions. Also, agents struggle with efficient communication. When they are allowed to proactively share information, a large portion of their sharing actions are labeled as redundant (59.86% and 79.35%, respectively). Moreover, responding to partners‚Äô requests and actively seeking necessary information are challenging.\\nInterestingly, when agents are restricted to only share information upon request, the redundancy issue is reduced. However, their overall performance still lags behind agents with full abilities, indicating that balanced, bidirectional communication remains crucial for effective collaboration.\\nWith the integration of the reasoning verifier, we observe a consistent reduction in error rates and a decrease in the total number of actions taken by the agents. Notably, there is a significant improvement in rule understanding , which aligns well with the design objective of the reasoning verifier. In addition, several issues in communication are also well improved with the involvement of the reasoning verifier, indicating its effectiveness in guiding more efficient and purposeful communication behaviors.\\nThese findings support our central claim: although agents may achieve good performance without information exchange, they do so without truly learning the underlying rules, posing severe safety risks. In contrast, with the support of an environment-based verifier, agents with full communication capabilities exhibit gains both in task performance and rule comprehension . This combination offers a promising path toward developing safer, more interpretable AI systems.\\nWe further examine model behavior in scenarios where two agents are assigned different communicative action spaces. Specifically, we evaluate four asymmetric pairings of action spaces, as shown in Table 3 . We exclude the pair Provide only vs. None, as it is functionally equivalent to Provide & Seek vs. None . In both cases, one agent is unable to respond to queries. Similarly, Seek only vs. None is the same as None vs. None , since the None agent cannot initiate or respond to communication.\\nAcross the evaluated pairings, we observe a consistent drop in performance when agents have mismatched communicative abilities. This finding highlights the importance of aligning communication protocols in collaborative tasks and provides insights into designing agent communication strategies in multi-agent systems. Notably, in the last two pairings listed in Table 3 , agents are sometimes forced to rely on random guessing. For instance, a Seek only agent cannot proactively share unless asked‚Äîyet its Provide only partner lacks the ability to initiate queries. These structural mismatches lead to particularly sharp declines in task success, strengthening the need for matched-communication agent design.\\nWe follow the model selection in Table 3 from RQ3 and recruit human participants to play games with them.\\nAs shown in Table 4 , human participants, with complete communicative action space, achieve 100% success rate across all action space configurations within 30 steps for each game. Agents with communicative actions perform efficient collaboration with human participants, while agents that cannot communicate spend far more rounds completing tasks.\\nIn addition to qualitative metrics, we also conduct quantitative analyses to better understand the perceived helpfulness, responsiveness, and clarity of the models from a human-centered perspective. Figure 2 presents the distributions of participant responses across these three dimensions.\\nWe find that most participants agree the agents generally communicate useful information and make effective use of the information shared by the human. Notably, agents with the Seek only configuration are rated as especially helpful‚Äîlikely because they have learned to selectively share the most relevant information when prompted. Interestingly, in the None condition for Q1 (usefulness of communicated information), participants often interpret the agents‚Äô physical actions as implicit communication from which they base their evaluations.\\nHowever, we observe strong disagreement regarding the clarity of agent behavior. Agents with Seek only or None configurations tend to cause more confusion. Specifically, while Seek only agents are seen as helpful and responsive, they still leave users uncertain‚Äîpossibly because they never proactively offer information before making a move. This suggests that even if an agent is effective and efficient in completing the task, a lack of initiative in communication can reduce the perceived clarity of its behavior. Similarly, models without any communicative capabilities achieve near perfect performance in self-play (94.33% SR ), yet they are perceived as unclear in human-AI collaboration. Instead, although models that can initiate information providing take slightly more steps, they offer better clarity in human-AI collaboration. These results highlight that beyond task success, proactive and transparent communication plays a critical role in fostering human trust and understanding in collaborative settings.',\n",
       "    'subsections': [{'title': '5.1 Experiment Setup',\n",
       "      'paragraphs': 'To systematically investigate our research questions, we design a series of experiments to investigate specific aspects of the collaboration:\\nExp1: Collaboration with Different Communicative Action Spaces (RQ1) We evaluated both closed-source language models via API calls and open-source models with supervised fine-tuning (SFT). We deploy the same model (with the same action space) to perform self-play in the game. This setting allows direct comparison of collaboration with different communicative action space.\\nExp2: Environment-Based Verifier (RQ2) We augment the base models with an environment-based verifier, which provides binary feedback indicating whether each sampled action is valid. At each decision step, we sample 4 candidate responses using temperature=0.2 and top-p=0.9 , and apply the verifier to filter out invalid actions, selecting the first valid one as the final output. This setup enables a direct comparison of model performance with and without the verifier.\\nExp3: Collaboration with Mismatched Action Spaces (RQ3) We select the well-performing models from those action spaces (Llama3.1-8B without CoT for no information exchange , and Llama3.1-8B with CoT for the rest), equipped with reasoning verifier, and let them play with each other. This setting further reveals the behaviors of collaboration between agents with different communication capabilities.\\nExp4: Human Performance (RQ4) Once we identified the most effective configuration of communicative action space for collaborative reasoning among LLM agents, we further examined whether such models are also preferred by human users. We recruited 12 college students as human participants to interact with the best-performing models, each equipped with different communicative action spaces and supported by the reasoning verifier. We sampled 27 unseen games (9 games for each object count) and measured both task completion rate and efficiency. Each participant was assigned a sequence of 9 games‚Äî3 games each with 4, 5, and 6 objects‚Äîwhile being given the complete communicative action space. In each game, they interacted with a model configured with a specific communicative action space, which remained unknown to the participant.\\nTo complement quantitative metrics, we also collected qualitative feedback from participants after each game using the following three questions:\\nDid you find the information communicated by the bot useful?\\nDid the bot make effective use of the information you shared?\\nWere you ever confused by the bot‚Äôs behavior?\\nThese questions allowed us to assess the perceived helpfulness, responsiveness, and clarity of the model‚Äôs behavior from a human-centered perspective.\\nEvaluation Metrics We employ complementary metrics to assess both effectiveness and efficiency. For effectiveness, we report the Success Rate ( SR ) and subgoal success rate ( Sub.R ) at the first attempt (Pass@1). SR measures the percentage of games successfully completed within a limited number of steps (30 for all the experiments), while Sub.R reflects partial progress by capturing the proportion of objects correctly placed, even when the full game is not successfully completed.\\nFor efficiency, we track the number of steps taken to complete each game and compare it to the optimal solution calculated by our planner. We then compute the Step Ratio ( StepR ), defined as the ratio of the number of executed steps to the number of steps in the optimal solution.\\nWhen a verifier is applied, we additionally report the correction rate ( Corr.R ), indicating the proportion of responses corrected by the verifier. We report standard error for each metrics.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.2 Result Analysis',\n",
       "      'paragraphs': 'Across all model families, we observe a generally clear hierarchy in task performance: Information Seeking & Providing > > Seeking Only > > No Information Exchange > > Providing Only (See Table 1 ).\\nEnabling both seeking and providing actions consistently yields the highest Success Rate ( SR ) and the lower Step Ratio ( StepR ). confirming that bi-directional exchange is both expressive and effective for coordinating constraints, especially when enhanced with chain-of-though reasoning.\\nPermitting only information seeking is better than permitting only information providing . Targeted queries minimize redundant traffic and let agents actively access the missing piece of information, whereas blind sharing often floods the channel with constraints that are irrelevant or already known by their partners. Therefore, the result suggests that information seeking is a more task-efficient option than unprompted information providing .\\nSurprisingly, disabling communication altogether ( ‚ÄúNo Information Exchange‚Äù ) ranks high for some variants. Without communication and reasoning process through CoT, the task reduces to pure object manipulation and random guessing;\\ntherefore, the model‚Äôs entire capacity is devoted to mastering game rules and action affordance. Llama variants exploit this by memorizing high-probability transition patterns, achieving a success rate greater than 81%. Qwen, which is pre-trained on a different instruction distribution, does not show the same effect, suggesting the phenomenon is model‚Äìspecific rather than an intrinsic property of the environment.\\nAs shown in Table 1 , adding the reasoning verifier (our best verifier) to the base models leads to large absolute SR gains (labeled in red), ranging from 10 to 40% increment. This reflects the high potential capabilities of the base models, as well as the effectiveness of the assistance from our verifier.\\nTo better understand the underlying behaviors, we further conduct the error analysis across four different action configurations on variants with the best performance (LLaMA-3.1-8B without CoT for no information exchange, and with CoT for the rest, the same for later experiments). We provide detailed explanations on the error taxonomy in Appendix E . Each cell in the table presents error rates in two forms:\\n(Errors/Total Steps)% : The proportion of steps affected by this error type.\\n(Errors/Relevant Action Type)% : The proportion of a specific action type affected by this error (e.g., move, share, seek).\\nFor example, in the \"Wrong Rule Understanding\" row under the ‚ÄúProvide & Seek‚Äù column without verifier, the value 28.22% indicates that 28.22% of all 6704 total steps contain this error. The accompanying 38.15% denotes that this error occurred in 38.15% of the 4681 1 1 1 The total number of ‚Äùmove‚Äù action, 4681, is not presented on the table. total \"move\" actions. One wrong action may fall into multiple error types.\\nWithout using verifier, agents across all action configurations demonstrate a similar level of skill in task reasoning. The relatively better performance of the \"no information exchange\" might be primarily due to fewer distractions from communication, allowing agents to focus more on enhancing physical understanding and trying all possible solutions. Also, agents struggle with efficient communication. When they are allowed to proactively share information, a large portion of their sharing actions are labeled as redundant (59.86% and 79.35%, respectively). Moreover, responding to partners‚Äô requests and actively seeking necessary information are challenging.\\nInterestingly, when agents are restricted to only share information upon request, the redundancy issue is reduced. However, their overall performance still lags behind agents with full abilities, indicating that balanced, bidirectional communication remains crucial for effective collaboration.\\nWith the integration of the reasoning verifier, we observe a consistent reduction in error rates and a decrease in the total number of actions taken by the agents. Notably, there is a significant improvement in rule understanding , which aligns well with the design objective of the reasoning verifier. In addition, several issues in communication are also well improved with the involvement of the reasoning verifier, indicating its effectiveness in guiding more efficient and purposeful communication behaviors.\\nThese findings support our central claim: although agents may achieve good performance without information exchange, they do so without truly learning the underlying rules, posing severe safety risks. In contrast, with the support of an environment-based verifier, agents with full communication capabilities exhibit gains both in task performance and rule comprehension . This combination offers a promising path toward developing safer, more interpretable AI systems.\\nWe further examine model behavior in scenarios where two agents are assigned different communicative action spaces. Specifically, we evaluate four asymmetric pairings of action spaces, as shown in Table 3 . We exclude the pair Provide only vs. None, as it is functionally equivalent to Provide & Seek vs. None . In both cases, one agent is unable to respond to queries. Similarly, Seek only vs. None is the same as None vs. None , since the None agent cannot initiate or respond to communication.\\nAcross the evaluated pairings, we observe a consistent drop in performance when agents have mismatched communicative abilities. This finding highlights the importance of aligning communication protocols in collaborative tasks and provides insights into designing agent communication strategies in multi-agent systems. Notably, in the last two pairings listed in Table 3 , agents are sometimes forced to rely on random guessing. For instance, a Seek only agent cannot proactively share unless asked‚Äîyet its Provide only partner lacks the ability to initiate queries. These structural mismatches lead to particularly sharp declines in task success, strengthening the need for matched-communication agent design.\\nWe follow the model selection in Table 3 from RQ3 and recruit human participants to play games with them.\\nAs shown in Table 4 , human participants, with complete communicative action space, achieve 100% success rate across all action space configurations within 30 steps for each game. Agents with communicative actions perform efficient collaboration with human participants, while agents that cannot communicate spend far more rounds completing tasks.\\nIn addition to qualitative metrics, we also conduct quantitative analyses to better understand the perceived helpfulness, responsiveness, and clarity of the models from a human-centered perspective. Figure 2 presents the distributions of participant responses across these three dimensions.\\nWe find that most participants agree the agents generally communicate useful information and make effective use of the information shared by the human. Notably, agents with the Seek only configuration are rated as especially helpful‚Äîlikely because they have learned to selectively share the most relevant information when prompted. Interestingly, in the None condition for Q1 (usefulness of communicated information), participants often interpret the agents‚Äô physical actions as implicit communication from which they base their evaluations.\\nHowever, we observe strong disagreement regarding the clarity of agent behavior. Agents with Seek only or None configurations tend to cause more confusion. Specifically, while Seek only agents are seen as helpful and responsive, they still leave users uncertain‚Äîpossibly because they never proactively offer information before making a move. This suggests that even if an agent is effective and efficient in completing the task, a lack of initiative in communication can reduce the perceived clarity of its behavior. Similarly, models without any communicative capabilities achieve near perfect performance in self-play (94.33% SR ), yet they are perceived as unclear in human-AI collaboration. Instead, although models that can initiate information providing take slightly more steps, they offer better clarity in human-AI collaboration. These results highlight that beyond task success, proactive and transparent communication plays a critical role in fostering human trust and understanding in collaborative settings.',\n",
       "      'subsections': [{'title': 'Effects of Communicative Action Space (RQ1)',\n",
       "        'paragraphs': 'Across all model families, we observe a generally clear hierarchy in task performance: Information Seeking & Providing > > Seeking Only > > No Information Exchange > > Providing Only (See Table 1 ).\\nEnabling both seeking and providing actions consistently yields the highest Success Rate ( SR ) and the lower Step Ratio ( StepR ). confirming that bi-directional exchange is both expressive and effective for coordinating constraints, especially when enhanced with chain-of-though reasoning.\\nPermitting only information seeking is better than permitting only information providing . Targeted queries minimize redundant traffic and let agents actively access the missing piece of information, whereas blind sharing often floods the channel with constraints that are irrelevant or already known by their partners. Therefore, the result suggests that information seeking is a more task-efficient option than unprompted information providing .\\nSurprisingly, disabling communication altogether ( ‚ÄúNo Information Exchange‚Äù ) ranks high for some variants. Without communication and reasoning process through CoT, the task reduces to pure object manipulation and random guessing;\\ntherefore, the model‚Äôs entire capacity is devoted to mastering game rules and action affordance. Llama variants exploit this by memorizing high-probability transition patterns, achieving a success rate greater than 81%. Qwen, which is pre-trained on a different instruction distribution, does not show the same effect, suggesting the phenomenon is model‚Äìspecific rather than an intrinsic property of the environment.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Verifier Assistance (RQ2)',\n",
       "        'paragraphs': 'As shown in Table 1 , adding the reasoning verifier (our best verifier) to the base models leads to large absolute SR gains (labeled in red), ranging from 10 to 40% increment. This reflects the high potential capabilities of the base models, as well as the effectiveness of the assistance from our verifier.\\nTo better understand the underlying behaviors, we further conduct the error analysis across four different action configurations on variants with the best performance (LLaMA-3.1-8B without CoT for no information exchange, and with CoT for the rest, the same for later experiments). We provide detailed explanations on the error taxonomy in Appendix E . Each cell in the table presents error rates in two forms:\\n(Errors/Total Steps)% : The proportion of steps affected by this error type.\\n(Errors/Relevant Action Type)% : The proportion of a specific action type affected by this error (e.g., move, share, seek).\\nFor example, in the \"Wrong Rule Understanding\" row under the ‚ÄúProvide & Seek‚Äù column without verifier, the value 28.22% indicates that 28.22% of all 6704 total steps contain this error. The accompanying 38.15% denotes that this error occurred in 38.15% of the 4681 1 1 1 The total number of ‚Äùmove‚Äù action, 4681, is not presented on the table. total \"move\" actions. One wrong action may fall into multiple error types.\\nWithout using verifier, agents across all action configurations demonstrate a similar level of skill in task reasoning. The relatively better performance of the \"no information exchange\" might be primarily due to fewer distractions from communication, allowing agents to focus more on enhancing physical understanding and trying all possible solutions. Also, agents struggle with efficient communication. When they are allowed to proactively share information, a large portion of their sharing actions are labeled as redundant (59.86% and 79.35%, respectively). Moreover, responding to partners‚Äô requests and actively seeking necessary information are challenging.\\nInterestingly, when agents are restricted to only share information upon request, the redundancy issue is reduced. However, their overall performance still lags behind agents with full abilities, indicating that balanced, bidirectional communication remains crucial for effective collaboration.\\nWith the integration of the reasoning verifier, we observe a consistent reduction in error rates and a decrease in the total number of actions taken by the agents. Notably, there is a significant improvement in rule understanding , which aligns well with the design objective of the reasoning verifier. In addition, several issues in communication are also well improved with the involvement of the reasoning verifier, indicating its effectiveness in guiding more efficient and purposeful communication behaviors.\\nThese findings support our central claim: although agents may achieve good performance without information exchange, they do so without truly learning the underlying rules, posing severe safety risks. In contrast, with the support of an environment-based verifier, agents with full communication capabilities exhibit gains both in task performance and rule comprehension . This combination offers a promising path toward developing safer, more interpretable AI systems.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Mismatched Action Spaces (RQ3)',\n",
       "        'paragraphs': 'We further examine model behavior in scenarios where two agents are assigned different communicative action spaces. Specifically, we evaluate four asymmetric pairings of action spaces, as shown in Table 3 . We exclude the pair Provide only vs. None, as it is functionally equivalent to Provide & Seek vs. None . In both cases, one agent is unable to respond to queries. Similarly, Seek only vs. None is the same as None vs. None , since the None agent cannot initiate or respond to communication.\\nAcross the evaluated pairings, we observe a consistent drop in performance when agents have mismatched communicative abilities. This finding highlights the importance of aligning communication protocols in collaborative tasks and provides insights into designing agent communication strategies in multi-agent systems. Notably, in the last two pairings listed in Table 3 , agents are sometimes forced to rely on random guessing. For instance, a Seek only agent cannot proactively share unless asked‚Äîyet its Provide only partner lacks the ability to initiate queries. These structural mismatches lead to particularly sharp declines in task success, strengthening the need for matched-communication agent design.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Human Preference (RQ4)',\n",
       "        'paragraphs': 'We follow the model selection in Table 3 from RQ3 and recruit human participants to play games with them.\\nAs shown in Table 4 , human participants, with complete communicative action space, achieve 100% success rate across all action space configurations within 30 steps for each game. Agents with communicative actions perform efficient collaboration with human participants, while agents that cannot communicate spend far more rounds completing tasks.\\nIn addition to qualitative metrics, we also conduct quantitative analyses to better understand the perceived helpfulness, responsiveness, and clarity of the models from a human-centered perspective. Figure 2 presents the distributions of participant responses across these three dimensions.\\nWe find that most participants agree the agents generally communicate useful information and make effective use of the information shared by the human. Notably, agents with the Seek only configuration are rated as especially helpful‚Äîlikely because they have learned to selectively share the most relevant information when prompted. Interestingly, in the None condition for Q1 (usefulness of communicated information), participants often interpret the agents‚Äô physical actions as implicit communication from which they base their evaluations.\\nHowever, we observe strong disagreement regarding the clarity of agent behavior. Agents with Seek only or None configurations tend to cause more confusion. Specifically, while Seek only agents are seen as helpful and responsive, they still leave users uncertain‚Äîpossibly because they never proactively offer information before making a move. This suggests that even if an agent is effective and efficient in completing the task, a lack of initiative in communication can reduce the perceived clarity of its behavior. Similarly, models without any communicative capabilities achieve near perfect performance in self-play (94.33% SR ), yet they are perceived as unclear in human-AI collaboration. Instead, although models that can initiate information providing take slightly more steps, they offer better clarity in human-AI collaboration. These results highlight that beyond task success, proactive and transparent communication plays a critical role in fostering human trust and understanding in collaborative settings.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '6 Conclusion',\n",
       "    'paragraphs': 'We adapted Einstein‚Äôs Puzzle to a tabletop environment to study collaboration under information asymmetry between LLM agents. Our empirical results show the critical role of aligned communication, especially information seeking and providing abilities in the success of collaboration. Through detailed error analysis, we identify general limitations in task understanding, which are effectively mitigated by incorporating environment-based verification. Furthermore, a human study highlights the importance of proactive and transparent communication in fostering trust and interpretability. These findings point to a pressing need for reliable, communication-aware, and interpretable design in future LLM-based collaborative systems.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Limitations',\n",
       "    'paragraphs': 'Still our study has known limitations. First, a ready-to-use environment-based verifier relies on the assumption that invalid actions are always recoverable‚Äîa prerequisite for trial-and-error-style interaction. While this holds in many simulated environments, extending the approach to real-world settings remains challenging. Doing so would require agents to possess richer perceptual capabilities and a deeper understanding of the environment‚Äôs dynamics. Nevertheless, the verifier is readily applicable and easily deployable in a wide range of simulated environments, where structured feedback is available.\\nSecond, our human evaluation was conducted on a relatively small scale. Due to constraints in time and computational resources, we were unable to deploy multiple models simultaneously, limiting the ability to compare different configurations within a single user study. In future work, we plan to expand the study by recruiting more participants and enabling broader comparisons across models. This will allow us to conduct more robust analyses of both human preferences and model behaviors in collaborative settings.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Acknowledgments',\n",
       "    'paragraphs': 'This work has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program. We\\nthank all the human-study participants for their contributions.',\n",
       "    'subsections': []},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []},\n",
       "   {'title': 'Appendix',\n",
       "    'paragraphs': 'We include the following contents as our supplementary materials.\\nA. Example of Game Play ‚Äî An illustrative walkthrough of a full game session demonstrating player turns, actions, and reasoning strategies.\\nB. Data Generation ‚Äî A detailed description of how we generate trajectories using a planner with perspective-taking, inference, and communication modeling.\\nC. Ablation on Verifiers - A detailed explanation and experiment on different design of verifier, which is potentially generalizable to other simulated environments.\\nD. Error Taxonomy - A detailed explanation on the error types we analyze in Exp2 in section 4 .\\nE. Experiment Details ‚Äî Technical and procedural specifications of model training, deployment, and human evaluation setup.\\nF. Code of Ethics ‚Äî Statement of ethical compliance, consent protocol, and risk mitigation measures approved by the IRB.\\nG. Human Study Interface ‚Äî Screenshot and description of the web interface and tutorial used for guiding participants in the human study.\\nH. Prompts Used ‚Äî A comprehensive collection of system and user prompts used across training, evaluation, and reasoning trace generation for different agent configurations.\\nIn terms of generative AI usage, we use it for purely improving the language of the paper.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix A Example of Game Play',\n",
       "    'paragraphs': 'To provide a clear understanding of the Einstein Puzzle gameplay, we illustrate a complete game session in Figure 3 , building on the initial setup introduced in Figure 1 . This example demonstrates how players are expected to take turns, make decisions, and communicate throughout the game. We hope this serves as a helpful reference for understanding the nature of the task, as well as the types of reasoning and interaction involved.\\nIt is worth noting that the ask action‚Äîused to seek information‚Äîis not utilized in this example. This is primarily due to the relative simplicity of the case. The ask action tends to be used more frequently in scenarios involving a larger number of objects, or when agents need to inquire about specific object-related information.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix B Data Generation',\n",
       "    'paragraphs': 'To prepare data for fine-tuning, we designed a planner to generate solutions under different configurations of action space.\\nThe planner operates from the perspective of a single player in each turn, selecting moves based on the player‚Äôs limited knowledge of the constraints and their specific communicative action space. It performs breadth-first search to explore possible action sequences until a valid solution is found. Throughout the search process, the planner maintains an up-to-date representation of the player‚Äôs knowledge, incorporating both the communication history and the current positions of objects on the board. Based on this evolving knowledge state, it selects valid next actions‚Äîsuch as sharing constraints that have not yet been communicated, or asking about objects that remain unknown from the player‚Äôs perspective.\\nTo improve search efficiency, we incorporate inferred knowledge into action selection, which is also used by the reasoning verifier introduced in Section 4 . This inferred knowledge enables the planner to determine whether:\\nThe goal of an object is already known , in which case redundant information-seeking is avoided, and a valid move (placing the object into its goal bin, if reachable) is added;\\nThe goal of an object is still unknown , in which case the agent may ask for information about that object.\\nBy tracking communication history, the planner can also avoid redundant sharing by identifying which constraints have already been communicated.\\nIf no valid move is available in a given search step, we manually add a pass action to allow the player to skip their turn. This mechanism is important when the two players take different numbers of actions to complete a task. Any trajectory ending with two consecutive passes is pruned to avoid unnecessary stalling.\\nIn scenarios where no communication is allowed, or where information flow is unidirectional, the agent may have to randomly guess the goals of some objects. This is triggered only after all valid actions have been exhausted, before skipping their turn. The agent will then attempt to place reachable, unplaced objects into all reachable bins (including the common bin), one by one, until the correct placement is accepted by the environment. Since the environment prevents invalid placements, this process can still lead to a valid solution. However, agents are discouraged from guessing prematurely and are designed to prioritize valid actions before resorting to this strategy.\\nWe collect both optimal and near-optimal trajectories‚Äîthose that deviate from the optimal solution by only one or two steps but exhibit diverse strategies. This diversity ensures that both types of communicative actions‚Äîinformation providing and information seeking‚Äîare well represented in the demonstrations. Without this balance, solutions dominated by unprompted sharing would disproportionately appear, as they often require fewer steps to complete.\\nIn scenarios involving random guessing, the optimal trajectories are those in which agents correctly guess the target location on the first attempt. However, such demonstrations provide little guidance for learning robust guessing strategies. To address this, we also include trajectories where agents try multiple possible bins‚Äîan approach we observe being learned by several fine-tuned models in Table 1 .',\n",
       "    'subsections': [{'title': 'B.1 Perspective Taking',\n",
       "      'paragraphs': 'To prepare data for fine-tuning, we designed a planner to generate solutions under different configurations of action space.\\nThe planner operates from the perspective of a single player in each turn, selecting moves based on the player‚Äôs limited knowledge of the constraints and their specific communicative action space. It performs breadth-first search to explore possible action sequences until a valid solution is found. Throughout the search process, the planner maintains an up-to-date representation of the player‚Äôs knowledge, incorporating both the communication history and the current positions of objects on the board. Based on this evolving knowledge state, it selects valid next actions‚Äîsuch as sharing constraints that have not yet been communicated, or asking about objects that remain unknown from the player‚Äôs perspective.\\nTo improve search efficiency, we incorporate inferred knowledge into action selection, which is also used by the reasoning verifier introduced in Section 4 . This inferred knowledge enables the planner to determine whether:\\nThe goal of an object is already known , in which case redundant information-seeking is avoided, and a valid move (placing the object into its goal bin, if reachable) is added;\\nThe goal of an object is still unknown , in which case the agent may ask for information about that object.\\nBy tracking communication history, the planner can also avoid redundant sharing by identifying which constraints have already been communicated.\\nIf no valid move is available in a given search step, we manually add a pass action to allow the player to skip their turn. This mechanism is important when the two players take different numbers of actions to complete a task. Any trajectory ending with two consecutive passes is pruned to avoid unnecessary stalling.\\nIn scenarios where no communication is allowed, or where information flow is unidirectional, the agent may have to randomly guess the goals of some objects. This is triggered only after all valid actions have been exhausted, before skipping their turn. The agent will then attempt to place reachable, unplaced objects into all reachable bins (including the common bin), one by one, until the correct placement is accepted by the environment. Since the environment prevents invalid placements, this process can still lead to a valid solution. However, agents are discouraged from guessing prematurely and are designed to prioritize valid actions before resorting to this strategy.',\n",
       "      'subsections': []},\n",
       "     {'title': 'B.2 Optimal And Near-Optimal Trajectories',\n",
       "      'paragraphs': 'We collect both optimal and near-optimal trajectories‚Äîthose that deviate from the optimal solution by only one or two steps but exhibit diverse strategies. This diversity ensures that both types of communicative actions‚Äîinformation providing and information seeking‚Äîare well represented in the demonstrations. Without this balance, solutions dominated by unprompted sharing would disproportionately appear, as they often require fewer steps to complete.\\nIn scenarios involving random guessing, the optimal trajectories are those in which agents correctly guess the target location on the first attempt. However, such demonstrations provide little guidance for learning robust guessing strategies. To address this, we also include trajectories where agents try multiple possible bins‚Äîan approach we observe being learned by several fine-tuned models in Table 1 .',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix C Ablation on Environment-Based Verifiers',\n",
       "    'paragraphs': 'In addition to the reasoning verifier introduced in Section 4 , we design two more types of verifiers as is illustrated in Figure 4 .\\nAffordance Verifier : The environment inherently enforces physical rules that govern the actions an agent can take under different conditions. In our game, this refers to whether the selected action is executable, such as whether a object or bin is reachable, or whether the chosen object is in the correct source bin. This helps validate the agent‚Äôs decision from the perspective of physical feasibility.\\nCommunication Verifier : In multi-agent environments, other agents can be viewed as part of the environment, and their interactions can provide additional feedback. In our game, the communication verifier assesses whether the communicative action selected by the agent is meaningful. For example, it identifies when an agent shares already-known knowledge, repeats sharing an existing constraint, or asks about a object that has already been placed.\\nThese three verifiers (including reasoning verifier) are hierarchically related in our design. The affordance and communication verifiers address physical actions and communication, respectively, while the reasoning verifier builds upon both and extends coverage through inferred knowledge.\\nThe affordance verifier is widely applicable and available in most environments.\\nThe communication verifier is similarly accessible whenever the task involves communication. In contrast, the reasoning verifier is more environment- and task-specific, although it can often be enabled through custom algorithm design. Used together, the environment-based verifier framework can be viewed as a general and flexible approach that can be adapted to a wide range of simulated environments.\\nWe follow the same setting of Exp2 (see Section 4 with two additional verifiers that separately target action affordance and communication. We compare performance across different verifier settings with Llama3.1-8B CoT model as base, and assess whether the environment-based verifier is potentially generalizable to other environments.\\nAs is shown in Table 5 , the affordance verifier evaluates physical preconditions of actions and improves success rates ( SR ) by 2‚Äì7%. The communication verifier filters out redundant or uninformative exchanges, contributing up to a 9% SR increase in task completion. Importantly, these verifiers operate solely on feedback from the environment and interaction history, requiring no additional training or computational overhead.\\nWe argue that such a mechanism offers a promising alternative to recent agent modeling approaches, especially in simulated environments where rich, structured feedback is readily available.\\nThese results invite a broader reconsideration of the environment‚Äôs role‚Äînot merely as a testing ground, but as an active, model-free verifier that can guide agent behavior in a lightweight manner.',\n",
       "    'subsections': [{'title': 'C.1 Verifier Design',\n",
       "      'paragraphs': 'In addition to the reasoning verifier introduced in Section 4 , we design two more types of verifiers as is illustrated in Figure 4 .\\nAffordance Verifier : The environment inherently enforces physical rules that govern the actions an agent can take under different conditions. In our game, this refers to whether the selected action is executable, such as whether a object or bin is reachable, or whether the chosen object is in the correct source bin. This helps validate the agent‚Äôs decision from the perspective of physical feasibility.\\nCommunication Verifier : In multi-agent environments, other agents can be viewed as part of the environment, and their interactions can provide additional feedback. In our game, the communication verifier assesses whether the communicative action selected by the agent is meaningful. For example, it identifies when an agent shares already-known knowledge, repeats sharing an existing constraint, or asks about a object that has already been placed.\\nThese three verifiers (including reasoning verifier) are hierarchically related in our design. The affordance and communication verifiers address physical actions and communication, respectively, while the reasoning verifier builds upon both and extends coverage through inferred knowledge.\\nThe affordance verifier is widely applicable and available in most environments.\\nThe communication verifier is similarly accessible whenever the task involves communication. In contrast, the reasoning verifier is more environment- and task-specific, although it can often be enabled through custom algorithm design. Used together, the environment-based verifier framework can be viewed as a general and flexible approach that can be adapted to a wide range of simulated environments.',\n",
       "      'subsections': []},\n",
       "     {'title': 'C.2 Ablation Study',\n",
       "      'paragraphs': 'We follow the same setting of Exp2 (see Section 4 with two additional verifiers that separately target action affordance and communication. We compare performance across different verifier settings with Llama3.1-8B CoT model as base, and assess whether the environment-based verifier is potentially generalizable to other environments.\\nAs is shown in Table 5 , the affordance verifier evaluates physical preconditions of actions and improves success rates ( SR ) by 2‚Äì7%. The communication verifier filters out redundant or uninformative exchanges, contributing up to a 9% SR increase in task completion. Importantly, these verifiers operate solely on feedback from the environment and interaction history, requiring no additional training or computational overhead.\\nWe argue that such a mechanism offers a promising alternative to recent agent modeling approaches, especially in simulated environments where rich, structured feedback is readily available.\\nThese results invite a broader reconsideration of the environment‚Äôs role‚Äînot merely as a testing ground, but as an active, model-free verifier that can guide agent behavior in a lightweight manner.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix D Experiment Details',\n",
       "    'paragraphs': 'We utilize the Azure OpenAI services for our GPT models. For GPT-4o, we employ the GPT-4o-20241120 version, and in all experiments, the temperature is set to 0.2 and the top-p value to 0.9. For all the model fine-tuning, we employ LoRA (Hu et\\xa0al., 2022 ) with a rank of 32, training with a global batch size of 128 and a learning rate of 2e-4 using a cosine decay schedule for 1 epoch. Fine-tuning is conducted using OpenRLHF (Hu et\\xa0al., 2024 ) , while FlashAttention-2 (Dao, 2024 ) is used to speed up training. The process takes approximately 30 minutes on 4 A40 GPUs with 48GB RAM each. For evaluation, we deploy the model using PeFT (Mangrulkar et\\xa0al., 2022 ) . For inference in the human study, we deploy the model using vLLM (Kwon et\\xa0al., 2023 ) .\\nWe follow the license requirement of Llama3.1, Qwen2.5, and GPT-4o model when using these artifacts, and our implementation is licensed under the MIT License.\\nWe recruited 12 human subjects with no prior experience in Einstein Puzzles on Tabletop to evaluate the models under four different action space configurations. Before the experiment began, each participant signed a consent form. We prepared 27 unseen game scenarios across the four configurations and divided them into three groups, each containing 9 distinct games (3 games each with 4, 5, and 6 objects). Each participant was assigned to one group and paired with a model using one of the action space configurations, without being informed of which model they were interacting with. As a result, each group was tested by 4 participants‚Äîone per configuration. Each session lasted approximately 30 minutes, and participants received a $20 Amazon gift card as compensation.\\nAt the start of the study, participants were introduced to the task environment via a detailed tutorial that explained the environment, task setup, and interface (see Appendix G ). After the tutorial, participants completed 10 sessions sequentially, beginning with a practice session which is not taken into the result. In each session, they were presented with an initial game board layout and explicit constraints, and were required to communicate with the model with communicative actions to solve the task. Upon task completion, a feedback form with three questions was shown. Once the form was submitted, the interface advanced to the next session, continuing until all games in the assigned group were completed. Participants were allowed to give up at any point if they felt stuck or not comfortable. Additionally, a maximum step limit of 30 was imposed to prevent excessive task duration. This same constraint was applied in all the evaluations (see Table 1 ) to ensure a fair comparison.',\n",
       "    'subsections': [{'title': 'D.1 Model Configuration, Fine-tuning and Deployment',\n",
       "      'paragraphs': 'We utilize the Azure OpenAI services for our GPT models. For GPT-4o, we employ the GPT-4o-20241120 version, and in all experiments, the temperature is set to 0.2 and the top-p value to 0.9. For all the model fine-tuning, we employ LoRA (Hu et\\xa0al., 2022 ) with a rank of 32, training with a global batch size of 128 and a learning rate of 2e-4 using a cosine decay schedule for 1 epoch. Fine-tuning is conducted using OpenRLHF (Hu et\\xa0al., 2024 ) , while FlashAttention-2 (Dao, 2024 ) is used to speed up training. The process takes approximately 30 minutes on 4 A40 GPUs with 48GB RAM each. For evaluation, we deploy the model using PeFT (Mangrulkar et\\xa0al., 2022 ) . For inference in the human study, we deploy the model using vLLM (Kwon et\\xa0al., 2023 ) .\\nWe follow the license requirement of Llama3.1, Qwen2.5, and GPT-4o model when using these artifacts, and our implementation is licensed under the MIT License.',\n",
       "      'subsections': []},\n",
       "     {'title': 'D.2 Human Evaluation Setup',\n",
       "      'paragraphs': 'We recruited 12 human subjects with no prior experience in Einstein Puzzles on Tabletop to evaluate the models under four different action space configurations. Before the experiment began, each participant signed a consent form. We prepared 27 unseen game scenarios across the four configurations and divided them into three groups, each containing 9 distinct games (3 games each with 4, 5, and 6 objects). Each participant was assigned to one group and paired with a model using one of the action space configurations, without being informed of which model they were interacting with. As a result, each group was tested by 4 participants‚Äîone per configuration. Each session lasted approximately 30 minutes, and participants received a $20 Amazon gift card as compensation.\\nAt the start of the study, participants were introduced to the task environment via a detailed tutorial that explained the environment, task setup, and interface (see Appendix G ). After the tutorial, participants completed 10 sessions sequentially, beginning with a practice session which is not taken into the result. In each session, they were presented with an initial game board layout and explicit constraints, and were required to communicate with the model with communicative actions to solve the task. Upon task completion, a feedback form with three questions was shown. Once the form was submitted, the interface advanced to the next session, continuing until all games in the assigned group were completed. Participants were allowed to give up at any point if they felt stuck or not comfortable. Additionally, a maximum step limit of 30 was imposed to prevent excessive task duration. This same constraint was applied in all the evaluations (see Table 1 ) to ensure a fair comparison.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix E Error Taxonomy',\n",
       "    'paragraphs': 'To better understand LLM agents‚Äô behaviors, we define several error types that LLM agents may encounter during interaction. Broadly, these fall into four categories: format following, physical understanding, communication, and task reasoning.\\nLLM‚Äôs format following\\nInvalid Action : The LLM fails to follow the required output format or exceeds the token limit.\\nPhysical Understanding\\nObject not in source bin : The agent specifies a move involving an incorrect source location for the object.\\nSource bin not reachable : The agent attempts to move an object from a bin that is not reachable (only bins at the front and the common bin are reachable).\\nDestination bin not reachable : The agent attempts to place an object into a non-reachable bin.\\nSource and destination bin are same : The agent mistakenly assigns the same bin as both the source and destination.\\nCommunication\\nRedundant knowledge sharing : The agent redundantly shares knowledge already communicated by itself or its partner.\\nNo share after seek : The agent fails to respond to its partner‚Äôs information-seeking request.\\nWrong share after seek : The agent provides incorrect or irrelevant information in response to a request.\\nSeek known object : The agent asks for the location of an object whose location it already knows, indicating inefficient behavior.\\nTask Reasoning\\nWrong rule understanding : The agent failed to interpret or infer the right location, leading to incorrect moves when it should be able to do so.\\nWrong random guessing : The agent, lacking sufficient information, guesses randomly and places the object incorrectly.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Appendix F Code of Ethics',\n",
       "    'paragraphs': 'The institution‚Äôs Institutional Review Board (IRB) considered this project exempt from ongoing review. The data collection process among researchers and participants is in line with standard ethical practice.\\nYou are invited to participate in a research study that intends to evaluate generative AI agents that can communicate and collaborate with their human partners to complete tasks.\\nIf you agree to be part of the research study, you will be asked to interact with the AI agents to accomplish a set of tasks. The tasks include: (1). completing a logical board game with AI agents; (2). sharing necessary information with AI agents to help them complete the tasks; (3). asking AI agents for necessary information that will help you to complete the tasks. The study will last approximately an hour. The interaction history, i.e., only the text generated by AI models and the subjects‚Äô symbolic inputs, and numerical evaluations, will be recorded in a datafile. The data collected in this study will be analyzed and used for research purposes. No personally identifiable information will be stored in the datafile.\\nThe game setting and the tasks assigned to participants were designed and strictly controlled by the research team. This ensured that the potential for safety concerns was minimized, allowing participants to engage with the study with minimal risk. Data collection involved only non-personal information, adhering to standard ethical practices and was used exclusively for research purposes. We ensured confidentiality and privacy, and the data will not be published publicly. Please refer to Appendix D.2 for implementation details of our human study.',\n",
       "    'subsections': [{'title': 'Consent Statement.',\n",
       "      'paragraphs': 'You are invited to participate in a research study that intends to evaluate generative AI agents that can communicate and collaborate with their human partners to complete tasks.\\nIf you agree to be part of the research study, you will be asked to interact with the AI agents to accomplish a set of tasks. The tasks include: (1). completing a logical board game with AI agents; (2). sharing necessary information with AI agents to help them complete the tasks; (3). asking AI agents for necessary information that will help you to complete the tasks. The study will last approximately an hour. The interaction history, i.e., only the text generated by AI models and the subjects‚Äô symbolic inputs, and numerical evaluations, will be recorded in a datafile. The data collected in this study will be analyzed and used for research purposes. No personally identifiable information will be stored in the datafile.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Potential Harm.',\n",
       "      'paragraphs': 'The game setting and the tasks assigned to participants were designed and strictly controlled by the research team. This ensured that the potential for safety concerns was minimized, allowing participants to engage with the study with minimal risk. Data collection involved only non-personal information, adhering to standard ethical practices and was used exclusively for research purposes. We ensured confidentiality and privacy, and the data will not be published publicly. Please refer to Appendix D.2 for implementation details of our human study.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix G Human Study Interface',\n",
       "    'paragraphs': 'We deploy a web-based interface to facilitate our human study. To ensure that all participants understand the task, interface elements, and available actions, we provide a detailed tutorial at the beginning of the study. This tutorial is displayed before the first game session and serves as a self-contained guide covering the game objective, interaction mechanics, and platform layout. For completeness and transparency, we include the full tutorial content below, as it was shown to participants, without modification. This also naturally serves as the introduction of the interface we design.\\nIn this study, you will play a logic-based tabletop game in collaboration with an AI agent. The goal of the game is for you and the AI agent to work together to place objects into designated bins according to a given set of constraints.\\nEach constraint defines either a relationship between two objects or between an object and a bin. The bins are the two player bins, the four destination bins, and the common area bin. The types of constraints that you may receive include the following:\\nObject1 and Object2 must be in the same row\\nObject1 and Object2 must be in the same column\\nObject1 and Object2 must be on the same diagonal\\nObject1 and Object2 must be in the same bin\\nObject1 must be placed in binA\\nTo avoid ambiguity, each pair of objects has only one constraint type describing their relationship. For example, if Object1 and Object2 are said to be in the same row, it implies that they are not in the same bin. Below is a visualization of our collaborative game.\\nYou can choose from four possible actions during your turn:\\nMove ‚Äì Move a block from one bin to another.\\nShare ‚Äì Share one of your constraints with the AI partner.\\nAsk ‚Äì Ask your AI partner about the placement of an object.\\nSkip ‚Äì Pass your turn without taking any action.\\nYou and the AI agent will take turns performing actions. The objective is to complete the task using the fewest possible steps. Note that the AI agent is not perfect and may make suboptimal decisions. Your collaboration and guidance are key to success.\\nOnce the game begins, you will see the following components:\\nThe game board\\nThe objects within your reach (which you can move)\\nThe constraints available to you (which you can share)\\nThe objects you can ask about\\nThe Skip button, if you wish to pass your turn\\nThe board includes four colored bins (top-left, top-right, bottom-left, bottom-right) and one common bin in the center. You (Player 1) are positioned at the bottom, and the AI agent (Player 2) is at the top.\\nYou can only move objects to the bins in front of you and the common bin. To move an object to a bin that is out of your reach, you must place it in the common bin so your partner can complete the move.\\nTo move an object:\\nClick on the object you want to move.\\nSelect the destination bin (scroll down if needed to see all bins).\\nClick ‚ÄôSubmit‚Äô below the game board to confirm the move.\\nTo share a constraint or ask about an object:\\nClick the blue button under the relevant constraint or object.\\nClick ‚ÄôSubmit‚Äô to confirm your action.\\nConstraints specify how objects should be arranged. You may share one constraint per turn , and you may repeat the same constraint if needed for clarification.\\nIn the bottom-right corner of the screen, you‚Äôll find the action history . This log shows all actions taken by both you and the AI since the beginning of the game. Use this to:\\nReview your partner‚Äôs most recent action\\nCheck whether each action was successfully executed\\nMistakes made by either player will also appear in this log, helping you keep track of progress and errors.\\nThe game ends when:\\nAll objects are correctly placed, or\\nThe maximum number of turns (30 in this game) is reached\\nAt the end of each game, you will be prompted to complete a short survey with three questions:\\nPlease answer based on your experience in this particular game and click ‚ÄôStart New Game‚Äô to proceed. You will play 10 games in total (the first game is mainly for familiarization, and the performance will not be taken into account), with each game taking approximately 3‚Äì8 minutes .\\nOnce you start the game, you will not be able to return to this tutorial. Please read all instructions carefully before beginning. If you feel uncomfortable at any point or wish to exit the study, you may simply close the browser window.\\nIf you‚Äôre ready to begin, click ‚ÄôGo To Game‚Äô below.',\n",
       "    'subsections': [{'title': 'G.1 Overview',\n",
       "      'paragraphs': 'In this study, you will play a logic-based tabletop game in collaboration with an AI agent. The goal of the game is for you and the AI agent to work together to place objects into designated bins according to a given set of constraints.\\nEach constraint defines either a relationship between two objects or between an object and a bin. The bins are the two player bins, the four destination bins, and the common area bin. The types of constraints that you may receive include the following:\\nObject1 and Object2 must be in the same row\\nObject1 and Object2 must be in the same column\\nObject1 and Object2 must be on the same diagonal\\nObject1 and Object2 must be in the same bin\\nObject1 must be placed in binA\\nTo avoid ambiguity, each pair of objects has only one constraint type describing their relationship. For example, if Object1 and Object2 are said to be in the same row, it implies that they are not in the same bin. Below is a visualization of our collaborative game.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.2 Actions',\n",
       "      'paragraphs': 'You can choose from four possible actions during your turn:\\nMove ‚Äì Move a block from one bin to another.\\nShare ‚Äì Share one of your constraints with the AI partner.\\nAsk ‚Äì Ask your AI partner about the placement of an object.\\nSkip ‚Äì Pass your turn without taking any action.\\nYou and the AI agent will take turns performing actions. The objective is to complete the task using the fewest possible steps. Note that the AI agent is not perfect and may make suboptimal decisions. Your collaboration and guidance are key to success.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.3 Platform Introduction',\n",
       "      'paragraphs': 'Once the game begins, you will see the following components:\\nThe game board\\nThe objects within your reach (which you can move)\\nThe constraints available to you (which you can share)\\nThe objects you can ask about\\nThe Skip button, if you wish to pass your turn',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.4 Game Board Description',\n",
       "      'paragraphs': 'The board includes four colored bins (top-left, top-right, bottom-left, bottom-right) and one common bin in the center. You (Player 1) are positioned at the bottom, and the AI agent (Player 2) is at the top.\\nYou can only move objects to the bins in front of you and the common bin. To move an object to a bin that is out of your reach, you must place it in the common bin so your partner can complete the move.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.5 How to Move',\n",
       "      'paragraphs': 'To move an object:\\nClick on the object you want to move.\\nSelect the destination bin (scroll down if needed to see all bins).\\nClick ‚ÄôSubmit‚Äô below the game board to confirm the move.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.6 Share and Ask Actions',\n",
       "      'paragraphs': 'To share a constraint or ask about an object:\\nClick the blue button under the relevant constraint or object.\\nClick ‚ÄôSubmit‚Äô to confirm your action.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.7 Understanding Constraints',\n",
       "      'paragraphs': 'Constraints specify how objects should be arranged. You may share one constraint per turn , and you may repeat the same constraint if needed for clarification.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.8 Action History',\n",
       "      'paragraphs': 'In the bottom-right corner of the screen, you‚Äôll find the action history . This log shows all actions taken by both you and the AI since the beginning of the game. Use this to:\\nReview your partner‚Äôs most recent action\\nCheck whether each action was successfully executed\\nMistakes made by either player will also appear in this log, helping you keep track of progress and errors.',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.9 At the End of the Game',\n",
       "      'paragraphs': 'The game ends when:\\nAll objects are correctly placed, or\\nThe maximum number of turns (30 in this game) is reached\\nAt the end of each game, you will be prompted to complete a short survey with three questions:\\nPlease answer based on your experience in this particular game and click ‚ÄôStart New Game‚Äô to proceed. You will play 10 games in total (the first game is mainly for familiarization, and the performance will not be taken into account), with each game taking approximately 3‚Äì8 minutes .',\n",
       "      'subsections': []},\n",
       "     {'title': 'G.10 Final Notes',\n",
       "      'paragraphs': 'Once you start the game, you will not be able to return to this tutorial. Please read all instructions carefully before beginning. If you feel uncomfortable at any point or wish to exit the study, you may simply close the browser window.\\nIf you‚Äôre ready to begin, click ‚ÄôGo To Game‚Äô below.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'Appendix H Prompts Used',\n",
       "    'paragraphs': 'We prepare prompts for four distinct action space configurations, each with and without chain-of-thought (CoT) reasoning. While the system prompt is tailored to each configuration, the user prompt remains consistent across all four. For configurations with CoT, the system prompt includes several illustrative examples to demonstrate the expected reasoning process.\\nTo enhance readability, we provide the full system prompt for the Providing\\u2004&\\u2004Seeking configuration with CoT. For the remaining configurations, we highlight only the differences relative to this version. The primary distinctions among the four configurations lie in their permitted action spaces and the corresponding reasoning examples. Although the reasoning examples are largely shared across configurations, minor variations are introduced to reflect the specific situations each agent may encounter.\\nFor the output format, models with CoT reasoning are expected to output their reasoning traces and actions in the format of: <THINK><your reasoning></THINK><ACTION><your action></ACTION> , while the one with no CoT reasoning capability needs to follow the format of: <ACTION><your action></ACTION> .\\nThe prompts used for GPT4o evaluation is slightly different than the ones we use for fine-tuned model training and evaluation. The prompts designed for GPT4o involves more detailed explanations and proper guidance to make sure the comparison is relatively fair. We have also tried using the same prompts for evaluation, while the preliminary result shows that GPT4o is hard to understand the game setting. This drives us to add extra guidance for a better comparison.\\nUsing a large, well-trained language model to generate reasoning traces as supervision for smaller models has been widely recognized as an effective strategy to enhance reasoning capabilities. In our setup, we leverage GPT-4o to generate such reasoning traces, following the pipeline outlined below:\\nWe first use a planner to generate a good solution for a given game instance. The generation process can be found in Appendix B .\\nAt each turn, we present GPT-4o with both the current game state and the corresponding action suggested by the planner.\\nGPT-4o is then prompted to assume it is the agent taking the given action, and to generate a rationale for this decision from a first-person perspective.\\nThe prompts used for this process are provided below. As with the training setup, we employ distinct system prompts for each of the four action space configurations, while keeping the user prompt consistent across all settings.',\n",
       "    'subsections': [{'title': 'H.1 Prompts for Model Training & Evaluation',\n",
       "      'paragraphs': 'We prepare prompts for four distinct action space configurations, each with and without chain-of-thought (CoT) reasoning. While the system prompt is tailored to each configuration, the user prompt remains consistent across all four. For configurations with CoT, the system prompt includes several illustrative examples to demonstrate the expected reasoning process.\\nTo enhance readability, we provide the full system prompt for the Providing\\u2004&\\u2004Seeking configuration with CoT. For the remaining configurations, we highlight only the differences relative to this version. The primary distinctions among the four configurations lie in their permitted action spaces and the corresponding reasoning examples. Although the reasoning examples are largely shared across configurations, minor variations are introduced to reflect the specific situations each agent may encounter.\\nFor the output format, models with CoT reasoning are expected to output their reasoning traces and actions in the format of: <THINK><your reasoning></THINK><ACTION><your action></ACTION> , while the one with no CoT reasoning capability needs to follow the format of: <ACTION><your action></ACTION> .',\n",
       "      'subsections': []},\n",
       "     {'title': 'H.2 Prompts for Evaluation with GPT4o',\n",
       "      'paragraphs': 'The prompts used for GPT4o evaluation is slightly different than the ones we use for fine-tuned model training and evaluation. The prompts designed for GPT4o involves more detailed explanations and proper guidance to make sure the comparison is relatively fair. We have also tried using the same prompts for evaluation, while the preliminary result shows that GPT4o is hard to understand the game setting. This drives us to add extra guidance for a better comparison.',\n",
       "      'subsections': []},\n",
       "     {'title': 'H.3 Prompts for Generating Reasoning Traces with GPT4o for Model Fine-tuning',\n",
       "      'paragraphs': 'Using a large, well-trained language model to generate reasoning traces as supervision for smaller models has been widely recognized as an effective strategy to enhance reasoning capabilities. In our setup, we leverage GPT-4o to generate such reasoning traces, following the pipeline outlined below:\\nWe first use a planner to generate a good solution for a given game instance. The generation process can be found in Appendix B .\\nAt each turn, we present GPT-4o with both the current game state and the corresponding action suggested by the planner.\\nGPT-4o is then prompted to assume it is the agent taking the given action, and to generate a rationale for this decision from a first-person perspective.\\nThe prompts used for this process are provided below. As with the training setup, we employ distinct system prompts for each of the four action space configurations, while keeping the user prompt consistent across all settings.',\n",
       "      'subsections': []}]}],\n",
       "  'abstract': 'While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game.\\n In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle.\\n We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment.\\n Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents‚Äô ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles'},\n",
       " '2510.25517v1': {'content': [{'title': '1 Introduction',\n",
       "    'paragraphs': 'Logic is an effective approach to modeling domains with complex relationships among entities. Logic Programming (LP) has been a powerful declarative paradigm based on formal logic since the 70s ( [ 44 ] ).\\nContrary to the traditional imperative programming paradigm, where the focus is on how the problem should be solved, in logic programming the program is a description of what is the problem.\\nLogic programs represent knowledge of a domain using facts and rules to state what is true. An LP system can derive new knowledge by means of logical inference and answer users‚Äô queries about knowledge not explicitly encoded in the program.\\nExamples of LP languages include Prolog ( [ 10 ] ), Datalog ( [ 1 ] ), and Answer Set Programming ( [ 24 ] ).\\nSince the inference procedure is independent of the knowledge base, logic programs are generally flexible, reusable, and easy to maintain and adapt as new requirements arise.\\nNew knowledge can be added without changing the inference algorithm.\\nDespite the many advantages, LP also has some challenges.\\nThe knowledge base needs to be carefully structured.\\nTo mitigate this problem, Inductive Logic Programming (ILP) aims at automatically inducing knowledge bases from data ( [ 60 ] ).\\nVarious ILP systems have been proposed and have found successful applications ( [ 12 ] ) in\\nbioinformatics and drug design ( [ 74 , 75 , 31 ] ),\\nnatural language processing ( [ 56 ] ),\\ngames ( [ 46 , 3 ] ),\\ndata curation and transformation ( [ 26 ] ),\\nexplainable Artificial Intelligence (AI) ( [ 91 ] ),\\nand learning from trajectories ( [ 32 ] ).\\nSome systems are capable of performing predicate invention, i.e., introducing new unnamed predicates into the learned program to adhere to a language bias or create simpler rules.\\nExamples of such systems are POPPI ( [ 14 ] ), PHIL ( [ 62 ] ), Nesy- œÄ \\\\pi ( [ 73 ] ), and many others.\\nThe presence of such predicates poses a big challenge for both programmers and final users, since it is difficult to understand their intended meaning, especially when the output theory is composed of a large number of rules.\\nAdditionally, manually renaming them is usually unfeasible, since it requires the availability of a domain expert.\\nIn fact, unnamed predicates not only complicate human understanding of the logic program but also hinder the debugging process for programmers.\\nIn this paper, we thus propose to use Large Language Models (LLMs) to provide names to these predicates.\\nIn the last few years, LLMs ( [ 2 ] ), a type of generative AI , have proven to be highly performing in a wide array of tasks across many domains, including natural language processing, art, and data science, often producing human-like results ( [ 6 , 36 ] ). LLMs are based on deep learning architectures, typically using transformer models ( [ 50 , 83 ] ), and are pre-trained on massive amounts of data, enabling them to process and generate natural language and other types of contents to perform a wide range of tasks, such as text, audio, and video generation, text summarization, code generation, chatbots and query answering.\\nTherefore, the goal of this work is to exploit LLMs to generate semantically meaningful predicate names, which should be naturally understood by the user.\\nTo our knowledge, no systems currently perform automatic renaming of unnamed predicates.\\nThe rest of this paper is structured as follows:\\nSection 2 provides an overview of the relevant background;\\nSection 3 presents a review of related work;\\nSection 4 outlines the methodology used in the study;\\nSection 5 describes the experimental setup and approach, and presents the results of the experiments;\\nSection 6 discusses the findings and limitations of this work.\\nFinally, Section 7 summarizes key findings and suggests directions for future research.',\n",
       "    'subsections': []},\n",
       "   {'title': '2 Preliminaries',\n",
       "    'paragraphs': 'ILP systems may sometimes introduce invented predicates that are not explicitly defined in the knowledge base or problem description.\\nThere are many reasons to invent new predicates, such as capturing intermediate relationships, simplifying complex rules, enhancing the program‚Äôs modularity and flexibility, expressing a new concept, improving the accuracy while reducing the complexity of a hypothesis, or when the given vocabulary is insufficient to express the target theory.\\nHowever, invented symbols have a placeholder name, such as ‚Äú inv ‚Äù. The lack of a meaningful name makes it difficult to understand the intended use of the predicate and its reusability.\\nIn ILP ( [ 58 ] ), Predicate Invention (PI) was first introduced in inverse resolution ( [ 59 ] ), and early works have been summarized in [ 77 ] . PI refers to those techniques that enable systems to automatically discover new predicates, which are not part of the initial knowledge base ( [ 78 ] ).\\nOne of the most straightforward examples of PI , shown in Example 1 , is the invention of the predicate parent to learn the definition of grandparent , given only the predicates mother and father :\\nExample of PI , where the invented predicate inv represents the parent relation.\\nIn this way, a more compact theory has been created.\\nDespite gaining increasing attention in recent developments, PI is still not widely spread ( [ 12 ] ).\\nAs a matter of fact, the invention of new, useful predicates is not a trivial task ( [ 60 ] ), as it is often challenging to determine when and if they are necessary, how many arguments they should have (i.e., their arity), and what their order and type should be ( [ 78 , 45 ] ).\\nMETAGOL ( [ 61 ] ) is a metarule-based approach that seeks to overcome this limitation by using metarules to determine and restrict the syntax of rules, although they should be predefined by the user.\\nPOPPER ( [ 13 ] ) is an ILP system based on the Learning From Failures (LFF) framework, which decomposes the ILP problem into three steps: generate, test, and constrain. In each cycle, POPPER generates a logic program based on current constraints, evaluates it against examples, and, if the hypothesis is incomplete or inconsistent, derives new constraints from the failure. This iterative refinement continues until a consistent and complete program is found. POPPI ( [ 14 ] ) is an ILP method that supports the creation of new predicates without needing human intervention to specify the arity or metarules. POPPI thus extends POPPER by enabling automatic PI, casting the ILP problem as an Answer Set Programming (ASP) task.\\n[ 73 ] proposed NeSy- œÄ \\\\pi , a Neuro-Symbolic (NeSy) approach for the classification of complex visual scenes, that processes visual inputs with deep neural networks and invents new concepts.\\nNevertheless, the discovery of new predicates is not limited to PI .\\nIn the context of Probabilistic Logic Programming (PLP) ( [ 71 ] ), Hierarchical Probabilistic Logic Programs (HPLPs) ( [ 20 ] ) are a restriction of Logic Programs with Annotated Disjunctions (LPADs) ( [ 84 ] ).\\nIn HPLPs clauses and predicates are hierarchically organized.\\nParameter learning for HIerarchical probabilistic Logic programs (PHIL) ( [ 62 ] ) is an algorithm for learning the parameters of HPLPs .\\nGiven a specific form of LPADs defining the target predicate r in terms of input predicates, for which the definition is given as input and is certain, and hidden predicates, defined by the rules of the program, PHIL learns the probabilities of atoms for the target predicate. Each clause in the program has a single atom in the head annotated with a probability.\\nThese programs can be divided into layers. Each layer defines a set of hidden unnamed predicates in terms of either the predicates from the layer below or the input predicates.\\nIn Example 2 from [ 62 ] in the UWCSE domain ( [ 42 ] ) we can see an example of such predicates.\\nThe first clause C \\u200b _ \\u200b 1 C\\\\_1 states that the probability of a student (A) being advised by a professor (B) is 0.3 if both A and B have worked on a project (C), and there is a relation r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 between the student, the professor, and the project.\\nThe relation r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 is defined in clause C \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 C\\\\_1\\\\_1\\\\_1 , which states that r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 holds with a probability of 0.2 if there is a publication P authored by both A and B and produced within project C. r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 can then be renamed as ‚ÄúcollaboretedOn‚Äù.\\nWe can thus say that a student (A) is advised by a professor (B) if there is a project (C) on which both have worked, and A and B have collaborated on project C.\\nThe first clause C \\u200b _ \\u200b 2 C\\\\_2 states that a student (A) is advised by a professor (B) with a probability of 0.6 if A is a teaching assistant (ta) for course C, which is taught by B.\\nUnnamed predicate r_1_1 created by PHIL in the learning process.\\nIn the context of abductive logic programming ( [ 37 ] ), the approach proposed by ( [ 72 ] ) aims to abduce relations by leveraging linear algebra in vector spaces. Hidden relationships between entities are learnt through matrix operations, making it useful for applications where explicit rules are hard to define.\\nFocusing on Datalog programs with binary predicates, the authors address both non-recursive and recursive cases.\\nFor instance, in the non-recursive case, given two relations r 1 \\u200b ( X , Y ) r_{1}(X,Y) and r 3 \\u200b ( X , Z ) r_{3}(X,Z) , represented as adjacency matrices R 1 \\\\textbf{R}_{1} and R 3 \\\\textbf{R}_{3} respectively, the system learns a new relation r 2 \\u200b ( Y , Z ) r_{2}(Y,Z) , represented by matrix R 2 \\\\textbf{R}_{2} , that explains how r 1 r_{1} and r 3 r_{3} interact. This is done by solving the matrix equation R 3 = m \\u200b i \\u200b n 1 \\u200b ( R 1 \\u200b R 2 ) \\\\textbf{R}_{3}=min_{1}(\\\\textbf{R}_{1}\\\\textbf{R}_{2}) , where m \\u200b i \\u200b n 1 \\u200b ( x ) min_{1}(x) is a nonlinear function, so that r 3 \\u200b ( X , Z ) ‚áî ‚àÉ Y \\u200b r 1 \\u200b ( X , Y ) ‚àß r 2 \\u200b ( Y , Z ) r_{3}(X,Z)\\\\Leftrightarrow\\\\exists Yr_{1}(X,Y)\\\\land r_{2}(Y,Z) . Although r 1 r_{1} and r 3 r_{3} are known, r 2 r_{2} is not, therefore it could be useful to find a meaningful name for easier reuse.\\nFor example, the authors have applied this method to a subset of the FB15k ( [ 5 ] ) knowledge graph, involving person and film.\\nBy considering the relations language(X,Z) and genre(X,Y) , the following rule can be discovered:\\nHere, the abduced relation r 2 \\u200b _ \\u200b a \\u200b b \\u200b d r_{2\\\\_abd} , which isn‚Äôt originally in the graph, could be interpreted as genre_lang , implying that genre and language are somewhat related to each other.\\nHowever, such an interpretation on the new predicate r 2 \\u200b _ \\u200b a \\u200b b \\u200b d r_{2\\\\_abd} as well as the predicate name genre_lang have to be given by the user.\\nGenerative AI ( [ 22 ] ) is the subset of AI techniques that generate new content in the form of text, code, images, audio, and video.\\nOne type of Generative AI are Foundation Models (FMs) ( [ 4 ] ), very large Machine Learning (ML) models trained on enormous quantities of generic and unlabeled data and thus capable of performing a wide range of tasks.\\nAmong FMs , LLMs are based on the transformer architecture ( [ 50 , 83 , 2 ] ). They are composed of multiple layers of neural networks, with an encoder and a decoder ( [ 67 , 89 ] ) that process sequences of text, capturing the connections and the relationships between words. Thanks to the self-attention mechanism ( [ 83 ] ), the model can consider distant tokens as well.\\nTo represent words, LLMs use multi-dimensional vectors, called word embeddings, in a way that keeps related words close to each other in the embedding space.\\nThese models often have billions of parameters and can be trained on vast amounts of data, enabling them to learn not only the grammar but also the semantics of a language.\\nGiven an input sequence, parameters are continuously adjusted during the training phase to maximize the likelihood of the next tokens.\\nLLMs have gained popularity in recent years ( [ 6 ] ) thanks to their ability to generate meaningful content from little input.\\nVarious LLMs have been specifically trained and used for automatic code generation showing good performances ( [ 6 , 36 ] ), such as OpenAI‚Äôs Codex ( [ 7 ] ), and its GitHub implementation, Copilot 1 1 1 https://copilot.github.com .\\nFor example, [ 66 ] have tested Codex to determine if it can identify security flaws and fix them, obtaining positive results on hand-crafted datasets; however, real-world scenarios proved to be more challenging. [ 57 ] demonstrated that Copilot can become a useful tool for software developers for implementing simple procedures, speeding up their work. Given their expertise, even if the generate code contains bugs, they should be able to identify and fix them.\\nPrompting is crucial when working with LLMs to get the most relevant and useful answers. A carefully crafted prompt can significantly simplify the process by precisely defining the goal we aim to achieve.\\nA basic prompt usually includes only the instructions for the task at hand, such as ‚Äúsummarize‚Äù or ‚Äúclassify‚Äù. Other elements that can be included are: context; the audience; the role of the model; input data; output format; and examples of inputs and outputs ( [ 2 ] ). The tone of the response can also be controlled. Additionally, the order also matters: information in the initial or in the final part of the prompt is often given more weight than that in the middle. This behavior is referred to as primacy effect and recency effect, respectively ( [ 2 ] , [ 49 ] ).\\nThese components are not always mandatory, but the more specific the prompt, the better the response will be. Giving an overly general prompt to the LLM may result in its inability to complete the given task.\\nOne or more examples of the desired output can also be included in the prompt.\\nThis is referred to as in-context learning or few-shot learning ( [ 54 , 6 ] ), in which a few examples of the task at hand and a test example are provided, as shown in Example 3 taken from [ 6 ] . ( [ 81 , 2 ] ) proved that implementing these strategies leads to improved performances.\\nFew-shot prompting example from [ 6 ] .\\nAnother prompting technique is Chain-of-Thought (CoT) Prompting, in which the output of one prompt is given as input of the next one, creating in this way a chain of prompts. CoT is at the base of LLMs reasoning ( [ 86 , 41 ] ), as shown by OpenAI‚Äôs reasoning models ( [ 65 ] ). Before generating an answer, these models learn to refine their thinking process, correct their mistakes, and explore different paths, building a chain of thought and leading to a higher-quality final result.\\nOne of the main challenges in using LLMs is designing prompts that are both clear and effective in guiding the model toward the desired output. The quality of the prompt has a significant impact on the model‚Äôs performance, and even small variations in wording or structure may lead to different results. Crafting an optimal prompt is therefore not straightforward, but is a gradual, iterative process that requires careful experimentation to determine what works best for each specific use case.\\nAnother major challenge is handling the inherent non-determinism of LLMs , as the same prompt can yield different outputs across multiple runs, making it difficult to ensure consistency and reproducibility.',\n",
       "    'subsections': [{'title': '2.1 Unnamed Predicates',\n",
       "      'paragraphs': 'ILP systems may sometimes introduce invented predicates that are not explicitly defined in the knowledge base or problem description.\\nThere are many reasons to invent new predicates, such as capturing intermediate relationships, simplifying complex rules, enhancing the program‚Äôs modularity and flexibility, expressing a new concept, improving the accuracy while reducing the complexity of a hypothesis, or when the given vocabulary is insufficient to express the target theory.\\nHowever, invented symbols have a placeholder name, such as ‚Äú inv ‚Äù. The lack of a meaningful name makes it difficult to understand the intended use of the predicate and its reusability.\\nIn ILP ( [ 58 ] ), Predicate Invention (PI) was first introduced in inverse resolution ( [ 59 ] ), and early works have been summarized in [ 77 ] . PI refers to those techniques that enable systems to automatically discover new predicates, which are not part of the initial knowledge base ( [ 78 ] ).\\nOne of the most straightforward examples of PI , shown in Example 1 , is the invention of the predicate parent to learn the definition of grandparent , given only the predicates mother and father :\\nExample of PI , where the invented predicate inv represents the parent relation.\\nIn this way, a more compact theory has been created.\\nDespite gaining increasing attention in recent developments, PI is still not widely spread ( [ 12 ] ).\\nAs a matter of fact, the invention of new, useful predicates is not a trivial task ( [ 60 ] ), as it is often challenging to determine when and if they are necessary, how many arguments they should have (i.e., their arity), and what their order and type should be ( [ 78 , 45 ] ).\\nMETAGOL ( [ 61 ] ) is a metarule-based approach that seeks to overcome this limitation by using metarules to determine and restrict the syntax of rules, although they should be predefined by the user.\\nPOPPER ( [ 13 ] ) is an ILP system based on the Learning From Failures (LFF) framework, which decomposes the ILP problem into three steps: generate, test, and constrain. In each cycle, POPPER generates a logic program based on current constraints, evaluates it against examples, and, if the hypothesis is incomplete or inconsistent, derives new constraints from the failure. This iterative refinement continues until a consistent and complete program is found. POPPI ( [ 14 ] ) is an ILP method that supports the creation of new predicates without needing human intervention to specify the arity or metarules. POPPI thus extends POPPER by enabling automatic PI, casting the ILP problem as an Answer Set Programming (ASP) task.\\n[ 73 ] proposed NeSy- œÄ \\\\pi , a Neuro-Symbolic (NeSy) approach for the classification of complex visual scenes, that processes visual inputs with deep neural networks and invents new concepts.\\nNevertheless, the discovery of new predicates is not limited to PI .\\nIn the context of Probabilistic Logic Programming (PLP) ( [ 71 ] ), Hierarchical Probabilistic Logic Programs (HPLPs) ( [ 20 ] ) are a restriction of Logic Programs with Annotated Disjunctions (LPADs) ( [ 84 ] ).\\nIn HPLPs clauses and predicates are hierarchically organized.\\nParameter learning for HIerarchical probabilistic Logic programs (PHIL) ( [ 62 ] ) is an algorithm for learning the parameters of HPLPs .\\nGiven a specific form of LPADs defining the target predicate r in terms of input predicates, for which the definition is given as input and is certain, and hidden predicates, defined by the rules of the program, PHIL learns the probabilities of atoms for the target predicate. Each clause in the program has a single atom in the head annotated with a probability.\\nThese programs can be divided into layers. Each layer defines a set of hidden unnamed predicates in terms of either the predicates from the layer below or the input predicates.\\nIn Example 2 from [ 62 ] in the UWCSE domain ( [ 42 ] ) we can see an example of such predicates.\\nThe first clause C \\u200b _ \\u200b 1 C\\\\_1 states that the probability of a student (A) being advised by a professor (B) is 0.3 if both A and B have worked on a project (C), and there is a relation r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 between the student, the professor, and the project.\\nThe relation r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 is defined in clause C \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 C\\\\_1\\\\_1\\\\_1 , which states that r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 holds with a probability of 0.2 if there is a publication P authored by both A and B and produced within project C. r \\u200b _ \\u200b 1 \\u200b _ \\u200b 1 r\\\\_1\\\\_1 can then be renamed as ‚ÄúcollaboretedOn‚Äù.\\nWe can thus say that a student (A) is advised by a professor (B) if there is a project (C) on which both have worked, and A and B have collaborated on project C.\\nThe first clause C \\u200b _ \\u200b 2 C\\\\_2 states that a student (A) is advised by a professor (B) with a probability of 0.6 if A is a teaching assistant (ta) for course C, which is taught by B.\\nUnnamed predicate r_1_1 created by PHIL in the learning process.\\nIn the context of abductive logic programming ( [ 37 ] ), the approach proposed by ( [ 72 ] ) aims to abduce relations by leveraging linear algebra in vector spaces. Hidden relationships between entities are learnt through matrix operations, making it useful for applications where explicit rules are hard to define.\\nFocusing on Datalog programs with binary predicates, the authors address both non-recursive and recursive cases.\\nFor instance, in the non-recursive case, given two relations r 1 \\u200b ( X , Y ) r_{1}(X,Y) and r 3 \\u200b ( X , Z ) r_{3}(X,Z) , represented as adjacency matrices R 1 \\\\textbf{R}_{1} and R 3 \\\\textbf{R}_{3} respectively, the system learns a new relation r 2 \\u200b ( Y , Z ) r_{2}(Y,Z) , represented by matrix R 2 \\\\textbf{R}_{2} , that explains how r 1 r_{1} and r 3 r_{3} interact. This is done by solving the matrix equation R 3 = m \\u200b i \\u200b n 1 \\u200b ( R 1 \\u200b R 2 ) \\\\textbf{R}_{3}=min_{1}(\\\\textbf{R}_{1}\\\\textbf{R}_{2}) , where m \\u200b i \\u200b n 1 \\u200b ( x ) min_{1}(x) is a nonlinear function, so that r 3 \\u200b ( X , Z ) ‚áî ‚àÉ Y \\u200b r 1 \\u200b ( X , Y ) ‚àß r 2 \\u200b ( Y , Z ) r_{3}(X,Z)\\\\Leftrightarrow\\\\exists Yr_{1}(X,Y)\\\\land r_{2}(Y,Z) . Although r 1 r_{1} and r 3 r_{3} are known, r 2 r_{2} is not, therefore it could be useful to find a meaningful name for easier reuse.\\nFor example, the authors have applied this method to a subset of the FB15k ( [ 5 ] ) knowledge graph, involving person and film.\\nBy considering the relations language(X,Z) and genre(X,Y) , the following rule can be discovered:\\nHere, the abduced relation r 2 \\u200b _ \\u200b a \\u200b b \\u200b d r_{2\\\\_abd} , which isn‚Äôt originally in the graph, could be interpreted as genre_lang , implying that genre and language are somewhat related to each other.\\nHowever, such an interpretation on the new predicate r 2 \\u200b _ \\u200b a \\u200b b \\u200b d r_{2\\\\_abd} as well as the predicate name genre_lang have to be given by the user.',\n",
       "      'subsections': []},\n",
       "     {'title': '2.2 Overview of Large Language Models',\n",
       "      'paragraphs': 'Generative AI ( [ 22 ] ) is the subset of AI techniques that generate new content in the form of text, code, images, audio, and video.\\nOne type of Generative AI are Foundation Models (FMs) ( [ 4 ] ), very large Machine Learning (ML) models trained on enormous quantities of generic and unlabeled data and thus capable of performing a wide range of tasks.\\nAmong FMs , LLMs are based on the transformer architecture ( [ 50 , 83 , 2 ] ). They are composed of multiple layers of neural networks, with an encoder and a decoder ( [ 67 , 89 ] ) that process sequences of text, capturing the connections and the relationships between words. Thanks to the self-attention mechanism ( [ 83 ] ), the model can consider distant tokens as well.\\nTo represent words, LLMs use multi-dimensional vectors, called word embeddings, in a way that keeps related words close to each other in the embedding space.\\nThese models often have billions of parameters and can be trained on vast amounts of data, enabling them to learn not only the grammar but also the semantics of a language.\\nGiven an input sequence, parameters are continuously adjusted during the training phase to maximize the likelihood of the next tokens.\\nLLMs have gained popularity in recent years ( [ 6 ] ) thanks to their ability to generate meaningful content from little input.\\nVarious LLMs have been specifically trained and used for automatic code generation showing good performances ( [ 6 , 36 ] ), such as OpenAI‚Äôs Codex ( [ 7 ] ), and its GitHub implementation, Copilot 1 1 1 https://copilot.github.com .\\nFor example, [ 66 ] have tested Codex to determine if it can identify security flaws and fix them, obtaining positive results on hand-crafted datasets; however, real-world scenarios proved to be more challenging. [ 57 ] demonstrated that Copilot can become a useful tool for software developers for implementing simple procedures, speeding up their work. Given their expertise, even if the generate code contains bugs, they should be able to identify and fix them.\\nPrompting is crucial when working with LLMs to get the most relevant and useful answers. A carefully crafted prompt can significantly simplify the process by precisely defining the goal we aim to achieve.\\nA basic prompt usually includes only the instructions for the task at hand, such as ‚Äúsummarize‚Äù or ‚Äúclassify‚Äù. Other elements that can be included are: context; the audience; the role of the model; input data; output format; and examples of inputs and outputs ( [ 2 ] ). The tone of the response can also be controlled. Additionally, the order also matters: information in the initial or in the final part of the prompt is often given more weight than that in the middle. This behavior is referred to as primacy effect and recency effect, respectively ( [ 2 ] , [ 49 ] ).\\nThese components are not always mandatory, but the more specific the prompt, the better the response will be. Giving an overly general prompt to the LLM may result in its inability to complete the given task.\\nOne or more examples of the desired output can also be included in the prompt.\\nThis is referred to as in-context learning or few-shot learning ( [ 54 , 6 ] ), in which a few examples of the task at hand and a test example are provided, as shown in Example 3 taken from [ 6 ] . ( [ 81 , 2 ] ) proved that implementing these strategies leads to improved performances.\\nFew-shot prompting example from [ 6 ] .\\nAnother prompting technique is Chain-of-Thought (CoT) Prompting, in which the output of one prompt is given as input of the next one, creating in this way a chain of prompts. CoT is at the base of LLMs reasoning ( [ 86 , 41 ] ), as shown by OpenAI‚Äôs reasoning models ( [ 65 ] ). Before generating an answer, these models learn to refine their thinking process, correct their mistakes, and explore different paths, building a chain of thought and leading to a higher-quality final result.\\nOne of the main challenges in using LLMs is designing prompts that are both clear and effective in guiding the model toward the desired output. The quality of the prompt has a significant impact on the model‚Äôs performance, and even small variations in wording or structure may lead to different results. Crafting an optimal prompt is therefore not straightforward, but is a gradual, iterative process that requires careful experimentation to determine what works best for each specific use case.\\nAnother major challenge is handling the inherent non-determinism of LLMs , as the same prompt can yield different outputs across multiple runs, making it difficult to ensure consistency and reproducibility.',\n",
       "      'subsections': [{'title': '2.2.1 Prompt Engineering',\n",
       "        'paragraphs': 'Prompting is crucial when working with LLMs to get the most relevant and useful answers. A carefully crafted prompt can significantly simplify the process by precisely defining the goal we aim to achieve.\\nA basic prompt usually includes only the instructions for the task at hand, such as ‚Äúsummarize‚Äù or ‚Äúclassify‚Äù. Other elements that can be included are: context; the audience; the role of the model; input data; output format; and examples of inputs and outputs ( [ 2 ] ). The tone of the response can also be controlled. Additionally, the order also matters: information in the initial or in the final part of the prompt is often given more weight than that in the middle. This behavior is referred to as primacy effect and recency effect, respectively ( [ 2 ] , [ 49 ] ).\\nThese components are not always mandatory, but the more specific the prompt, the better the response will be. Giving an overly general prompt to the LLM may result in its inability to complete the given task.\\nOne or more examples of the desired output can also be included in the prompt.\\nThis is referred to as in-context learning or few-shot learning ( [ 54 , 6 ] ), in which a few examples of the task at hand and a test example are provided, as shown in Example 3 taken from [ 6 ] . ( [ 81 , 2 ] ) proved that implementing these strategies leads to improved performances.\\nFew-shot prompting example from [ 6 ] .\\nAnother prompting technique is Chain-of-Thought (CoT) Prompting, in which the output of one prompt is given as input of the next one, creating in this way a chain of prompts. CoT is at the base of LLMs reasoning ( [ 86 , 41 ] ), as shown by OpenAI‚Äôs reasoning models ( [ 65 ] ). Before generating an answer, these models learn to refine their thinking process, correct their mistakes, and explore different paths, building a chain of thought and leading to a higher-quality final result.\\nOne of the main challenges in using LLMs is designing prompts that are both clear and effective in guiding the model toward the desired output. The quality of the prompt has a significant impact on the model‚Äôs performance, and even small variations in wording or structure may lead to different results. Crafting an optimal prompt is therefore not straightforward, but is a gradual, iterative process that requires careful experimentation to determine what works best for each specific use case.\\nAnother major challenge is handling the inherent non-determinism of LLMs , as the same prompt can yield different outputs across multiple runs, making it difficult to ensure consistency and reproducibility.',\n",
       "        'subsections': []}]}]},\n",
       "   {'title': '3 Related work',\n",
       "    'paragraphs': 'PI is considered a critical step, yet it is still not widely spread ( [ 12 ] ).\\nChoosing suitable names for newly created predicates is an even more neglected area in ILP and has not received much attention so far, with only a few attempts made in the past.\\nIn [ 30 ] the authors adopted a meta-level abduction approach to identify previously unknown relations within a knowledge graph that represents a specific domain. In this setting, new predicates are invented as existentially quantified variables, which act as placeholders for the missing or hidden knowledge. While the system is capable of structurally generating these new predicates, their semantic meaning (i.e., their names) is not determined automatically and must instead be manually defined or interpreted based on the domain context. This highlights a key limitation: the invented predicates are syntactically valid but lack meaningful, human-readable names without further domain-specific interpretation. In the rule abduction approach proposed in [ 40 ] , the authors built databases related to the cello playing domain to assist performers in assigning meaningful names to new abduced rules.\\nAs the power of LLMs continues to grow, we could leverage their capabilities to enable them to reason about rules and generate meaningful names for previously unnamed predicates.\\nWhile many have highlighted the impressive performances of LLMs with zero or few-shot prompting ( [ 6 , 41 , 86 ] ), a rising number of recent works have explored their reasoning ability on more complex tasks ( [ 48 ] ) involving common knowledge and planning ( [ 82 , 79 ] ), math ( [ 8 , 41 ] ), and symbolic reasoning ( [ 33 , 41 , 76 ] ).\\nIn particular, LLMs are already widely employed in software engineering ( [ 6 , 29 , 35 , 36 ] ), where they typically generate code with semantically meaningful variable names.\\n[ 48 ] evaluated state-of-the-art LLMs on an ILP benchmark and found their performance lacking when compared to a smaller neural program induction model.\\nThey evaluated LLMs on two benchmarks, one of which required the model to induce programs to deduce more complex family relationships from some basic ones.\\nAlthough this setup is similar to ours, they focused on assessing the relational reasoning capabilities of LLMs using standard natural language and truth value prompting. In contrast, our goal was to determine if LLMs could identify names that are relevant to the context of the example. [ 27 ] used GPT-3.5 and GPT-4 for property induction, a traditional inductive reasoning task that involves generalizing a property from a few examples to new cases. In their experiments, participants‚Äîincluding both humans and language models‚Äîwere presented with a set of objects sharing a property and asked to infer whether new objects shared the same property. Although GPT-3.5 had difficulty capturing various aspects of human behavior, GPT-4‚Äôs performance was mostly close to that of humans.\\n[ 90 ] investigated the ability of pretrained language models to infer natural language rules from natural language facts, obtaining promising results. [ 85 ] proposed an approach to improve LLMs ‚Äô inductive reasoning ability. They generate hypotheses in natural language, and then translate them into specific programs used for making predictions, breaking the task into two levels of abstraction and outperforming current baselines.\\nIn [ 80 ] the authors explored techniques to extract and leverage the knowledge embedded in LLMs by converting it into different types of logic programs, aiming to improve reasoning capabilities and ensure that LLM outputs are consistent with their intended purposes.\\nAdditionally, [ 23 ] aimed to determine whether LLMs can address ILP tasks, analyzing their capabilities and limitations on logic theory induction. Their results indicate that while larger LLMs can achieve competitive results compared to state-of-the-art ILP systems, long predicate relationship chains are still an obstacle for LLMs in inductive reasoning tasks. [ 15 ] proposed NeSyGPT, a novel architecture that integrates neural and symbolic reasoning, that fine-tunes a vision-language foundation model to extract symbolic features from raw data, and then learns an expressive answer set program using them. The results show that NeSyGPT outperforms various baselines in accuracy and can be used for more complex NeSy tasks. They thus demonstrated that LLMs can reduce the need for manual efforts in automating the interface between neural and symbolic components.\\nRemaining in the NeSy field, [ 17 ] presented a novel approach to expand the knowledge base by integrating LLMs with structured semantic representations. It employs a state-of-the-art LLM to produce a natural language description of an input image, which is then transformed into an Abstract Meaning Representation (AMR) graph. This graph is further extended with logical design patterns and layered semantics derived from linguistic and factual knowledge bases. [ 38 ] proposed LLM2LAS, a system that learns commonsense knowledge in the form of ASP programs, from story-based question and answers in natural language, using only a few examples and combining LLMs ‚Äô semantic parsing abilities with ILASP ( [ 47 ] ), a tool for learning ASP programs.\\nThe authors of [ 33 ] used an LLM to generate ASP programs representing logic puzzles, given their description in natural language. Here, the authors state that the LLM successfully produced an ASP , despite some minor errors that were corrected manually, confirming the possibility of leveraging LLMs to support the creation of ASP programs.\\nAlthough some findings suggest that LLMs still struggle with complex reasoning ( [ 11 , 48 ] ), this proves that the field yields some potential and is worth further investigation, especially considering the continuous improvements of LLMs .',\n",
       "    'subsections': []},\n",
       "   {'title': '4 Renaming predicates using LLMs',\n",
       "    'paragraphs': 'In this section, we outline our approach for assigning meaningful names to unnamed predicates in a set of logical rules.\\nAfter deciding the number n of LLMs to query for suggestions, the number k of suggestions they should provide, and the number j of LLM judges, the pipeline we propose consists of three main steps, as shown in Figure 1 :\\nAsking the n LLMs to find a meaningful name for each unnamed predicate, for k times to tackle possible hallucinations; for these preliminary experiments, we decided to repeat the question three times.\\nAsking the models to choose the most suitable name among their own suggested names.\\nAsking the j judges to score the different proposed names, obtaining a ranking.\\nTo assess the ability of the LLMs to find correct names for unnamed predicates, we created different examples of logic theories, which represent the possible output of an ILP system in a declarative logic programming language . Although constructs that are specific to Prolog were used for these experiments, this approach could be applied to scenarios where other logic programming languages are employed.\\nFor example, if rules are written in a purely declarative manner, those rules may be commonly used for both Prolog and ASP.\\nIn each set of rules, we replaced the true names of some or all predicates with placeholders.\\nWe tested various formats (e.g., A, P, h0, and so on) for placeholders to determine whether the placeholder name influences the suggestions.\\nUnnamed predicates can occur in the head and/or in the body of one or multiple rules. For all case studies presented in this paper, the true names of the unnamed predicates are included as comments (e.g., # P = coauthors) for the reader‚Äôs reference. These comments were not included in the prompts provided to the LLMs .\\nThe first and simplest case study, coauthors , consists of a single rule, as shown in Case Study 1 .\\nHere, P is only one unnamed predicate appearing in the head of a rule, which states that there are two researchers (A and B) and both authored a paper (C); P could thus be renamed as ‚Äúcoauthors‚Äù.\\nRules of the coauthors case study.\\nThe second case study, family , is described in Case Study 2 and contains several rules representing various family relationships.\\nUnnamed predicates appear both in the head and in the body of the rules, with h2 being the only exception, as it appears only on the head on a single rule.\\nRules of the family case study.\\nThe third case study, math , shown in Case Study 3 , is a set of rules representing basic mathematical operations.\\nAlso in this case, unnamed predicates can appear both in the head and in the body of the rules.\\nRules of the math case study.\\nWe also created simpler one-rule examples to investigate the ability of the LLMs to find a meaningful name for predicates appearing only in the body.\\nThese examples are taken from the previous ones and are shown in Case Study 4 : the grandparent rule has predicate h0 in the body, representing the parent relation; the cousins rule with predicate h3 representing the siblings relation; and lastly, the lcm rule, representing the computation of the least common multiple using the greatest common divisor, uses predicates G and H , being product and quotient , respectively.\\nIn all these cases, the unnamed predicates appear only in the body of the rule.\\nRules of the grandparent, cousins, and lcm case studies.\\nIn addition, we also tried a real-world knowledge base taken from the mutagenesis dataset ( [ 74 ] ), a classic ILP benchmark dataset for Quantitative Structure-Activity Relationship (QSAR), that is, predicting the biological activity of chemicals from their physicochemical properties or molecular structure. The goal is to predict the mutagenicity of compounds given their chemical structure. We used the ring_theory provided with the dataset and removed the names of most of the head predicates. The set of rules is extensive, with 24 head predicates to be renamed, such as those in Case Study 5 .\\nExamples of rules of the Mutagenesis ring theory. Here, the \"@>\" operator checks if the first term is greater than the second term according to Prolog‚Äôs standard order of terms.\\nSome of these rules are relatively short (1 to 3 body atoms), while others are much more complex, with more than 5 body atoms.\\nTo test our approach on an actual ILP theory, we designed a new case study and used POPPER to learn a recursive definition of a reachability predicate over a graph defined by bidirectional (road/2) and unidirectional (one_way/2) connections. The goal was to induce a definition of reachable/2 using predicate invention, such that POPPER introduces an intermediate invented predicate (e.g., inv1/2) to generalize a single-step path relation.\\nThe resulting program is presented in Case Study 6 .\\nGiven that this case study was specifically designed by the authors for this work, it is not present in the existing literature to the best of our knowledge. As a result, it is unlikely that the LLMs were exposed to it during training.\\nRules of the reachability case study.\\nWe use zero-shot prompting for all the examples. Figure 2 shows Prompt 1 , used to ask for name suggestions. We insert the corresponding rules with placeholders where [rules] is present.\\nThe prompt is built in sections, separated by ### section ### :\\nin the beginning, we provide the context;\\nthen, we give the instruction;\\nlastly, we provide the desired output format.\\nThe prompts used for the examples differ only in the predicates listed in the instruction and output format. Since we don‚Äôt want our method to be dependent on the chosen language, we instruct the LLMs to behave as a ‚Äúspecialist of logic programming‚Äù.\\nPrompt 2 , shown in Figure 3 , is used when asking the models to choose between their own suggestions.\\nFor each model, [suggested names] is replaced with the list of its own suggestions.\\nSince models can suggest names with semantically equivalent meanings but written in different formats, such as using underscores, hyphens, or capitalized letters, we ensure that they are standardized: we remove any underscores or hyphens from the name and convert them to camel case, leaving the first letter in lowercase.\\nAfter this, we remove duplicates from the list.\\nFinally, Prompt 3 in Figure 4 was used for the judgment, where we asked the model to score the suggestions for each predicate. We also provide the scoring rules in the INSTRUCTIONS section.\\nFor this study, we selected 7 different LLMs :\\nOpenAI‚Äôs ChatGPT based on GPT-4o (ChatGPT-4o) ( [ 63 , 64 ] ) and ChatGPT based on o3mini (ChatGPT-o3mini) ( [ 65 ] );\\nMeta‚Äôs Llama 3.2 3B Instruct (Llama) ( [ 53 , 52 ] ),\\nGoogle DeepMind‚Äôs Gemini 1.5 Flash (Gemini) ( [ 18 ] );\\nFalconMamba 7B Instruct (FalconMamba) ( [ 93 ] ) and Falcon3 7B Instruct (Falcon3) ( [ 21 ] ) of the Technology Innovation Institute (TII) ;\\nand Cohere‚Äôs Command R+ 08 2024 (Command R+) ( [ 9 ] ).\\nChatGPT is a powerful chatbot running on GPT-4o (with approximately 200 billion parameters), capable of processing multi-modal inputs and answering within a few milliseconds, mirroring the behavior of a human.\\nGenerative pre-trained Transformer (GPT) models, based on the tranformers technology, are robust and versatile and are nowadays widely employed for routine tasks such as text summarization or translation. Recently, given the significant advances, the adoption of ChatGPT and LLMs in general is also spreading in more critical applications ( [ 43 ] ), such as generating simplified clinical reports ( [ 34 ] ) or supporting decision-making process ( [ 68 ] ), with high accuracy and correctness.\\nIn addition to GPT models, OpenAI provides also reasoning models, trained with reinforcement learning.\\nReasoning models, like o1 and the latest o3mini that has 3B parameters, use a chain-of-thought approach, where intermediate reasoning steps are generated before producing the final answer. Although this increases response time, reasoning models yield higher-quality outputs and are particularly effective in tasks involving science, mathematics, and coding ( [ 65 ] ).\\nGoogle DeepMind‚Äôs Gemini 1.5 Flash is a fast and versatile model that can take text, images, audio, and video as input, and produces a text output. Gemini 1.5 Flash has been adopted in an AI-based application to help users monitor and reduce their carbon emissions ( [ 16 ] ). It also obtained good results in answering multiple-choice questions regarding pediatric nephrology ( [ 55 ] ).\\nLlama 3.2 3B Instruct is a lightweight (3.21B parameters) text-only model of the Llama 3.2 family, which is a collection of multilingual, pre-trained, and instruction-tuned LLMs .\\nFalconMamba-7B-Instruction is a version of the base model FalconMamba 7B, fine-tuned on instruction data. The model is based on Mamba ( [ 25 ] ), an attention-free architecture, and has 7.27B parameters.\\nThe Falcon3 family is a collection of pre-trained and instruction-tuned, decoder-only LLMs of various sizes.\\nFalcon3-7B-Instruct, specifically, has been pre-trained on curated high-quality and multilingual data from various sources (web, code, STEM), and according to TII it achieves state-of-art-results on different tasks, including natural language processing, common-sense reasoning, coding, and reasoning. This is due to the improvements made in the development of the base models, such as depth upscaling and knowledge distillation ( [ 28 ] ).\\nCohere‚Äôs Command R+ 08-2024 is a multilingual model optimized for various applications, such as text summarization, reasoning, and structured data analysis, with 104 billion parameters, and trained on an extensive corpus of data in different languages. Command R+ 08-2024 has shown good performances for most of the writing tasks in Arabic tested by [ 51 ] and evaluated on Gazelle , the dataset they proposed, delivering clear and coherent responses.\\nThe experiments involving few-shot prompting, the reachability case study, and the human judgment evaluation were conducted between July and August 2025, while all other experiments took place between January and February 2025.\\nSince ChatGPT-4o and ChatGPT-o3mini were no longer available at the time of the reachability case study experiment, we used ChatGPT-5, the only freely accessible version at that time.\\nWe use LLMs also for the automated evaluation of the answers, adopting the LLM-as-a-Judge strategy ( [ 92 , 2 ] ).\\nWe selected four models, ChatGPT-4o, ChatGPT-o3mini, Gemini, and Command R+, to act as judges. Each judge assigns a score to every proposed name for each predicate: 1 for a correct and precise name, 0.5 for a name that is imprecise or too general but still correct, and 0 for a wrong name. The scores are then summed to determine the final results, and the name with the highest score is selected.\\nFinally, to assess the ability of LLMs in both renaming and judgment, we conducted a complementary experiment involving 14 human judges on all the case studies except Mutagenesis and reachability .\\nThe reachability case study was not included in the human judgment experiment, as it was introduced after the experiment had already been conducted, while Mutagenesis requires a field expert to provide reliable feedback.\\nParticipants were asked to score the proposed names using the same evaluation method adopted by LLMs judges.',\n",
       "    'subsections': [{'title': '4.1 Predicate Renaming Pipeline',\n",
       "      'paragraphs': 'After deciding the number n of LLMs to query for suggestions, the number k of suggestions they should provide, and the number j of LLM judges, the pipeline we propose consists of three main steps, as shown in Figure 1 :\\nAsking the n LLMs to find a meaningful name for each unnamed predicate, for k times to tackle possible hallucinations; for these preliminary experiments, we decided to repeat the question three times.\\nAsking the models to choose the most suitable name among their own suggested names.\\nAsking the j judges to score the different proposed names, obtaining a ranking.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.2 Materials',\n",
       "      'paragraphs': 'To assess the ability of the LLMs to find correct names for unnamed predicates, we created different examples of logic theories, which represent the possible output of an ILP system in a declarative logic programming language . Although constructs that are specific to Prolog were used for these experiments, this approach could be applied to scenarios where other logic programming languages are employed.\\nFor example, if rules are written in a purely declarative manner, those rules may be commonly used for both Prolog and ASP.\\nIn each set of rules, we replaced the true names of some or all predicates with placeholders.\\nWe tested various formats (e.g., A, P, h0, and so on) for placeholders to determine whether the placeholder name influences the suggestions.\\nUnnamed predicates can occur in the head and/or in the body of one or multiple rules. For all case studies presented in this paper, the true names of the unnamed predicates are included as comments (e.g., # P = coauthors) for the reader‚Äôs reference. These comments were not included in the prompts provided to the LLMs .\\nThe first and simplest case study, coauthors , consists of a single rule, as shown in Case Study 1 .\\nHere, P is only one unnamed predicate appearing in the head of a rule, which states that there are two researchers (A and B) and both authored a paper (C); P could thus be renamed as ‚Äúcoauthors‚Äù.\\nRules of the coauthors case study.\\nThe second case study, family , is described in Case Study 2 and contains several rules representing various family relationships.\\nUnnamed predicates appear both in the head and in the body of the rules, with h2 being the only exception, as it appears only on the head on a single rule.\\nRules of the family case study.\\nThe third case study, math , shown in Case Study 3 , is a set of rules representing basic mathematical operations.\\nAlso in this case, unnamed predicates can appear both in the head and in the body of the rules.\\nRules of the math case study.\\nWe also created simpler one-rule examples to investigate the ability of the LLMs to find a meaningful name for predicates appearing only in the body.\\nThese examples are taken from the previous ones and are shown in Case Study 4 : the grandparent rule has predicate h0 in the body, representing the parent relation; the cousins rule with predicate h3 representing the siblings relation; and lastly, the lcm rule, representing the computation of the least common multiple using the greatest common divisor, uses predicates G and H , being product and quotient , respectively.\\nIn all these cases, the unnamed predicates appear only in the body of the rule.\\nRules of the grandparent, cousins, and lcm case studies.\\nIn addition, we also tried a real-world knowledge base taken from the mutagenesis dataset ( [ 74 ] ), a classic ILP benchmark dataset for Quantitative Structure-Activity Relationship (QSAR), that is, predicting the biological activity of chemicals from their physicochemical properties or molecular structure. The goal is to predict the mutagenicity of compounds given their chemical structure. We used the ring_theory provided with the dataset and removed the names of most of the head predicates. The set of rules is extensive, with 24 head predicates to be renamed, such as those in Case Study 5 .\\nExamples of rules of the Mutagenesis ring theory. Here, the \"@>\" operator checks if the first term is greater than the second term according to Prolog‚Äôs standard order of terms.\\nSome of these rules are relatively short (1 to 3 body atoms), while others are much more complex, with more than 5 body atoms.\\nTo test our approach on an actual ILP theory, we designed a new case study and used POPPER to learn a recursive definition of a reachability predicate over a graph defined by bidirectional (road/2) and unidirectional (one_way/2) connections. The goal was to induce a definition of reachable/2 using predicate invention, such that POPPER introduces an intermediate invented predicate (e.g., inv1/2) to generalize a single-step path relation.\\nThe resulting program is presented in Case Study 6 .\\nGiven that this case study was specifically designed by the authors for this work, it is not present in the existing literature to the best of our knowledge. As a result, it is unlikely that the LLMs were exposed to it during training.\\nRules of the reachability case study.',\n",
       "      'subsections': [{'title': 'Unnamed predicates appearing only in the head',\n",
       "        'paragraphs': 'The first and simplest case study, coauthors , consists of a single rule, as shown in Case Study 1 .\\nHere, P is only one unnamed predicate appearing in the head of a rule, which states that there are two researchers (A and B) and both authored a paper (C); P could thus be renamed as ‚Äúcoauthors‚Äù.\\nRules of the coauthors case study.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Unnamed predicates appearing both in the head and in the body',\n",
       "        'paragraphs': 'The second case study, family , is described in Case Study 2 and contains several rules representing various family relationships.\\nUnnamed predicates appear both in the head and in the body of the rules, with h2 being the only exception, as it appears only on the head on a single rule.\\nRules of the family case study.\\nThe third case study, math , shown in Case Study 3 , is a set of rules representing basic mathematical operations.\\nAlso in this case, unnamed predicates can appear both in the head and in the body of the rules.\\nRules of the math case study.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Unnamed predicates appearing only in the body',\n",
       "        'paragraphs': 'We also created simpler one-rule examples to investigate the ability of the LLMs to find a meaningful name for predicates appearing only in the body.\\nThese examples are taken from the previous ones and are shown in Case Study 4 : the grandparent rule has predicate h0 in the body, representing the parent relation; the cousins rule with predicate h3 representing the siblings relation; and lastly, the lcm rule, representing the computation of the least common multiple using the greatest common divisor, uses predicates G and H , being product and quotient , respectively.\\nIn all these cases, the unnamed predicates appear only in the body of the rule.\\nRules of the grandparent, cousins, and lcm case studies.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Unnamed predicates in a real-world dataset',\n",
       "        'paragraphs': 'In addition, we also tried a real-world knowledge base taken from the mutagenesis dataset ( [ 74 ] ), a classic ILP benchmark dataset for Quantitative Structure-Activity Relationship (QSAR), that is, predicting the biological activity of chemicals from their physicochemical properties or molecular structure. The goal is to predict the mutagenicity of compounds given their chemical structure. We used the ring_theory provided with the dataset and removed the names of most of the head predicates. The set of rules is extensive, with 24 head predicates to be renamed, such as those in Case Study 5 .\\nExamples of rules of the Mutagenesis ring theory. Here, the \"@>\" operator checks if the first term is greater than the second term according to Prolog‚Äôs standard order of terms.\\nSome of these rules are relatively short (1 to 3 body atoms), while others are much more complex, with more than 5 body atoms.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Unnamed predicates in an actual POPPER output',\n",
       "        'paragraphs': 'To test our approach on an actual ILP theory, we designed a new case study and used POPPER to learn a recursive definition of a reachability predicate over a graph defined by bidirectional (road/2) and unidirectional (one_way/2) connections. The goal was to induce a definition of reachable/2 using predicate invention, such that POPPER introduces an intermediate invented predicate (e.g., inv1/2) to generalize a single-step path relation.\\nThe resulting program is presented in Case Study 6 .\\nGiven that this case study was specifically designed by the authors for this work, it is not present in the existing literature to the best of our knowledge. As a result, it is unlikely that the LLMs were exposed to it during training.\\nRules of the reachability case study.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '4.3 Prompts',\n",
       "      'paragraphs': 'We use zero-shot prompting for all the examples. Figure 2 shows Prompt 1 , used to ask for name suggestions. We insert the corresponding rules with placeholders where [rules] is present.\\nThe prompt is built in sections, separated by ### section ### :\\nin the beginning, we provide the context;\\nthen, we give the instruction;\\nlastly, we provide the desired output format.\\nThe prompts used for the examples differ only in the predicates listed in the instruction and output format. Since we don‚Äôt want our method to be dependent on the chosen language, we instruct the LLMs to behave as a ‚Äúspecialist of logic programming‚Äù.\\nPrompt 2 , shown in Figure 3 , is used when asking the models to choose between their own suggestions.\\nFor each model, [suggested names] is replaced with the list of its own suggestions.\\nSince models can suggest names with semantically equivalent meanings but written in different formats, such as using underscores, hyphens, or capitalized letters, we ensure that they are standardized: we remove any underscores or hyphens from the name and convert them to camel case, leaving the first letter in lowercase.\\nAfter this, we remove duplicates from the list.\\nFinally, Prompt 3 in Figure 4 was used for the judgment, where we asked the model to score the suggestions for each predicate. We also provide the scoring rules in the INSTRUCTIONS section.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.4 LLMs used in this study',\n",
       "      'paragraphs': 'For this study, we selected 7 different LLMs :\\nOpenAI‚Äôs ChatGPT based on GPT-4o (ChatGPT-4o) ( [ 63 , 64 ] ) and ChatGPT based on o3mini (ChatGPT-o3mini) ( [ 65 ] );\\nMeta‚Äôs Llama 3.2 3B Instruct (Llama) ( [ 53 , 52 ] ),\\nGoogle DeepMind‚Äôs Gemini 1.5 Flash (Gemini) ( [ 18 ] );\\nFalconMamba 7B Instruct (FalconMamba) ( [ 93 ] ) and Falcon3 7B Instruct (Falcon3) ( [ 21 ] ) of the Technology Innovation Institute (TII) ;\\nand Cohere‚Äôs Command R+ 08 2024 (Command R+) ( [ 9 ] ).\\nChatGPT is a powerful chatbot running on GPT-4o (with approximately 200 billion parameters), capable of processing multi-modal inputs and answering within a few milliseconds, mirroring the behavior of a human.\\nGenerative pre-trained Transformer (GPT) models, based on the tranformers technology, are robust and versatile and are nowadays widely employed for routine tasks such as text summarization or translation. Recently, given the significant advances, the adoption of ChatGPT and LLMs in general is also spreading in more critical applications ( [ 43 ] ), such as generating simplified clinical reports ( [ 34 ] ) or supporting decision-making process ( [ 68 ] ), with high accuracy and correctness.\\nIn addition to GPT models, OpenAI provides also reasoning models, trained with reinforcement learning.\\nReasoning models, like o1 and the latest o3mini that has 3B parameters, use a chain-of-thought approach, where intermediate reasoning steps are generated before producing the final answer. Although this increases response time, reasoning models yield higher-quality outputs and are particularly effective in tasks involving science, mathematics, and coding ( [ 65 ] ).\\nGoogle DeepMind‚Äôs Gemini 1.5 Flash is a fast and versatile model that can take text, images, audio, and video as input, and produces a text output. Gemini 1.5 Flash has been adopted in an AI-based application to help users monitor and reduce their carbon emissions ( [ 16 ] ). It also obtained good results in answering multiple-choice questions regarding pediatric nephrology ( [ 55 ] ).\\nLlama 3.2 3B Instruct is a lightweight (3.21B parameters) text-only model of the Llama 3.2 family, which is a collection of multilingual, pre-trained, and instruction-tuned LLMs .\\nFalconMamba-7B-Instruction is a version of the base model FalconMamba 7B, fine-tuned on instruction data. The model is based on Mamba ( [ 25 ] ), an attention-free architecture, and has 7.27B parameters.\\nThe Falcon3 family is a collection of pre-trained and instruction-tuned, decoder-only LLMs of various sizes.\\nFalcon3-7B-Instruct, specifically, has been pre-trained on curated high-quality and multilingual data from various sources (web, code, STEM), and according to TII it achieves state-of-art-results on different tasks, including natural language processing, common-sense reasoning, coding, and reasoning. This is due to the improvements made in the development of the base models, such as depth upscaling and knowledge distillation ( [ 28 ] ).\\nCohere‚Äôs Command R+ 08-2024 is a multilingual model optimized for various applications, such as text summarization, reasoning, and structured data analysis, with 104 billion parameters, and trained on an extensive corpus of data in different languages. Command R+ 08-2024 has shown good performances for most of the writing tasks in Arabic tested by [ 51 ] and evaluated on Gazelle , the dataset they proposed, delivering clear and coherent responses.\\nThe experiments involving few-shot prompting, the reachability case study, and the human judgment evaluation were conducted between July and August 2025, while all other experiments took place between January and February 2025.\\nSince ChatGPT-4o and ChatGPT-o3mini were no longer available at the time of the reachability case study experiment, we used ChatGPT-5, the only freely accessible version at that time.',\n",
       "      'subsections': []},\n",
       "     {'title': '4.5 Evaluation: LLM-as-a-Judge and human judgment',\n",
       "      'paragraphs': 'We use LLMs also for the automated evaluation of the answers, adopting the LLM-as-a-Judge strategy ( [ 92 , 2 ] ).\\nWe selected four models, ChatGPT-4o, ChatGPT-o3mini, Gemini, and Command R+, to act as judges. Each judge assigns a score to every proposed name for each predicate: 1 for a correct and precise name, 0.5 for a name that is imprecise or too general but still correct, and 0 for a wrong name. The scores are then summed to determine the final results, and the name with the highest score is selected.\\nFinally, to assess the ability of LLMs in both renaming and judgment, we conducted a complementary experiment involving 14 human judges on all the case studies except Mutagenesis and reachability .\\nThe reachability case study was not included in the human judgment experiment, as it was introduced after the experiment had already been conducted, while Mutagenesis requires a field expert to provide reliable feedback.\\nParticipants were asked to score the proposed names using the same evaluation method adopted by LLMs judges.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '5 Experiments and Results',\n",
       "    'paragraphs': 'In this section, we illustrate the results of our experiments.\\nFor this study, the desktop app was used to execute both ChatGPT running on GPT-4o and ChatGPT running on GPT-o3mini.\\nWe accessed Gemini and Command R+ via their own API.\\nFor Llama, FalconMamba, and Falcon3, experiments were\\nperformed locally via the Hugging Face Transformers framework ( [ 87 ] ) on a cluster consisting of two machines with two AMD EPYC 9654 96-cores and four machines with two AMD EPYC 9454 48-cores. The memory limit was set to 100 GB, and the time limit was set to 17 hours and 30 minutes.\\nWe carried out several initial prompt design attempts to observe how LLMs responded under varying instructions. For instance, after noticing that the models often provided multiple equivalent suggestions, we decided to limit them to just one.\\nWe also added a constraint to prevent models from altering the structure of the rules, after observing such behaviors with some LLMs . Each of these attempts was executed independently, ensuring that no contextual information was shared between runs. Importantly, the models did not retain any memory of previous executions; in particular, ChatGPT was used with the memory feature explicitly disabled to prevent any persistence of prior interactions. After analyzing the models‚Äô responses and identifying recurring issues or undesired behaviors, we refined our instructions iteratively. The final version of the prompt, shown in Figure 2 , was then used to rerun all experiments from scratch. Additionally, we observed that models sometimes gave the correct response on the first try, while other times they required several attempts. To address this, and given the non-deterministic nature of the answers, we decided to repeat the question a certain number of times to address this issue. In these preliminary experiments, we decided to use three repetitions.\\nAll the models handled the task appropriately, so zero-shot prompting was sufficient in this case. Most of them followed the instructions accurately, with only a few exceptions.\\nFor example, Llama usually wrote a sentence (explaining the reasons for its choices) instead of adopting the requested format.\\nIn such cases, however, the name can be extracted from the answer using basic string manipulation.\\nAfter getting three suggestions out of each model, we asked them to choose only one name among their own ones.\\nTables 1 and 2 show the final choices made by each model for each predicate of each case study; we also report for each LLM how many of the suggested names could be considered correct. Table 3 provides a summary of the number of correct name suggestions per predicate. Each model receives a score of 1 if its final suggestion matches the expected name, and 0 otherwise. In the final column, we indicate how many LLMs correctly named each predicate. This overview allows us to easily compare the performance of the models on a per-predicate basis and identify which predicates were more challenging.\\nThe coauthors case study, which was the simplest, presented no difficulties for the models. In fact, all of them suggested a correct name, although sometimes too verbose.\\nRegarding the family case study, the GPT models and Command R+ provided the correct name of each predicate, and Gemini failed to identify only h4.\\nLlama was completely unable to find meaningful names for most of the predicates, with only two correct suggestions. Furthermore, one out of the three times it rewrote most of the rules, making its answer completely wrong.\\nFalconMamba and Falcon3 also struggled with the family case study, as none of the answers were correct.\\nIn particular, Falcon3 provided the answer for only one predicate, likely being misled by the word ‚ÄúONE‚Äù in the prompt.\\nFor this reason, its answers were not considered.\\nFor the math case study, the GPT models correctly identified almost all the names.\\nGemini followed closely, with only two mistakes.\\nFalcon3 and Command R+ also identified most of the correct names, while Llama just over half of them.\\nOn the other hand, FalconMamba simply rewrote the rules with different capital letters all three times, and therefore its answers were ignored.\\nMany models couldn‚Äôt find the correct name for predicates L ( isDivisor , suggested only by Command R+), M ( gcd ), and N ( lcm ); these last two were correctly identified only by ChatGPT-4o and ChatGPT-o3mini.\\nIn the lcm case study, most models struggled to identify the correct names for predicates G and H, namely product and quotient (or equivalent terms).\\nOnly ChatGPT-4o and ChatGPT-o3mini provided correct suggestions.\\nFinding siblings for predicate h3 in the cousins case study was also challenging for some models, even if they suggested it in the family case study. Only the GPT models, Gemini and LLama provided the correct name.\\nFor the grandparent case study, all the models correctly suggested parent for predicate h0, except for FalconMamba.\\nIn Table 4 we report the results obtained on the Mutagenesis ring theory, using ChatGPT-4o and ChatGPT-o3mini.\\nSome of the predicates appear to be similar to the original ones, but overall it is difficult to determine whether the suggested names are meaningful. In such a case, a domain expert should make the final decision.\\nTo improve the quality of the output, the models could be fine-tuned on the specific domain; alternatively, context (e.g., in the form of scientific papers) could be provided.\\nIt is worth mentioning another attempt we made with Mutagenesis. For this initial experiment, we used a less constrained version of Prompt 1 , where we simply asked for suggestions for all the predicates, without specifying the context or limiting the model to provide only one suggestion per predicate.\\nThe response of Command R+ was particularly interesting. The model not only explained the predicate to rename but in some cases it also invented a helper predicate for it.\\nAn example of such behavior is shown in Figure 5 : the model created the helper predicate check_atom_count_recursive for predicate HP19, which was renamed as check_atom_count_recursive . Regardless of the correctness of the suggested name and despite altering the structure of the rule, this suggests the potential of LLMs to directly perform PI .\\nThe reachability case study was generally unchallenging for the majority of models evaluated.\\nSuggestions produced by GPT-5 (direct_connection), Command R+ (is_connected), Gemini (directly_connected), and LLama (is_connected) are all plausible and appropriate, as they generalize a single-step path relation.\\nIn contrast, both FalconMamba and Falcon3 exhibited notable difficulties. FalconMamba generated an output (can reach) that was syntactically invalid as a predicate, due to the inclusion of a space in the identifier, while Falcon 3 failed to produce any answer at all.\\nWe additionally conducted a few-shot prompting experiment with the LLMs that demonstrated lower performance, namely FalconMamba, Falcon3, and Llama, with the goal of improving their answers.\\nWe focused on the math case study, which was selected due to its potential to highlight the benefits of few-shot prompting.\\nIn this experiment, suggestions were requested for one predicate at a time, while also providing the model with the correct names of previously addressed predicates.\\nFigure 6 shows the prompts used in the few-shot prompting experiments with answers from Falcon3. The initial prompt, Prompt 0 , requests the name of only the first predicate ( A ). Then, its answers is used to replace the corresponding unnamed predicate in the following prompt, Prompt 1 , while requesting a suggestion for the second predicate ( P ). This continues until the last prompt, in which all the unnamed predicates of the math case study have been renamed, except of the last one.\\nThis incremental approach aimed to support the model by narrowing its focus and reducing the complexity of the task, in contrast to requiring it to generate names for all predicates simultaneously.\\nHowever, the outcomes did not meet expectations. None of the models showed improved performance in terms of correctly identifying a greater number of names; in some instances, performance even declined.\\nPrompt 0:\\nResponse:\\nPrompt 1:\\nResponse:\\nTable 5 shows the results of this experiment.\\nWith few-shot prompting Falcon3 correctly identified 10 out of 16 predicate names, a decrease from the 12 out of 16 correctly identified with zero-shot prompting.\\nSimilarly, Llama achieved 8 correct predictions with few-shot prompting, compared to 9 without.\\nA substantial improvement was observed with FalconMamba. with zero-shot prompting, the model failed to generate a meaningful output, simply repeating the predicates.\\nIn contrast, when few-shot prompting was applied, the model provided a suggestion for all the predicates, finding 11 correct answers out of 16.\\nThese findings, particularly in the case of FalconMamba, indicate that breaking down the prompt and using few-shot prompting can significantly enhance performance in models that otherwise struggle with zero-shot prompting.\\nRegarding the judgment on the coauthors case study, shown in Table 6 , all the suggestions\\nare plausible. LLM judges seem to prefer a longer and more detailed name, assigning the lowest score to the simplest one, i.e. coauthors .\\nIn Table 7 we report the results of the judgment on the family case study.\\nThe correct name was chosen for each predicate.\\nThe same holds for the grandparent and cousins case studies, both shown in Table 6 .\\nTable 8 shows the results of the judgment on the math case study. Here, the correct name was chosen for all the predicates, except for predicate L. While the correct name is divisor , it was mostly interpreted as ‚Äúdivisible‚Äù, which is actually the opposite.\\nRegarding predicates M and N, all the judges recognized the true names, gcd and lcm respectively, even if they didn‚Äôt suggest them initially. The strategy of asking multiple models for name suggestions turned out to be effective, as it allowed the judges to identify the correct name when one model suggested it, even when the others did not.\\nThe same cannot be said for the lcm case study, as can be seen in Table 6 . For predicate G, findLeastCommonMultipleIntermediate was chosen as the most fitting name, instead of computeProduct or multiply , which would have been equally correct.\\nSimilarly, for predicate H, computeLcmFromGcd was selected as the most appropriate name, rather than divide or divideValues .\\nTable 9 presents the judgment results for the reachability case study. It is important to note that ChatGPT-5 was used as a substitute for ChatGPT-4o and ChatGPT-o3mini, as these models were no longer available at the time the experiment was conducted.\\nThree out of the four alternatives were selected as the most suitable name, each receiving the same score. This outcome is acceptable, as all three options are considered plausible. Notably, however, only ChatGPT-5 assigned a score of 0 to the proposed name can reach , which is syntactically incorrect. In contrast, both Gemini and Command R+ assigned it a score of 0.5, indicating a limited ability to recognize the syntactic invalidity of the suggestion.\\nAlthough all the proposed alternatives in the coauthors case study\\nare plausible, human judges did not select the most straightforward option for predicate P, that is, coauthors . This behavior aligns with that observed with the LLMs .\\nThey also correctly identified the appropriate names for predicates h0 and h3 in the grandparent and cousins case studies, respectively.\\nIn the lcm case study, human judges successfully selected the correct names for predicates G and H, whereas the LLMs judges failed to do so.\\nAveraged scores assigned by human judges for these case studies are summarized in Table 10 .\\nTable 11 presents the results of the human evaluation for the proposed names in the family case study. For each predicate, the correct name received the highest overall score. However, it is noteworthy that several human judges assigned non-zero scores to incorrect names. For instance, in the case of predicate h0, 4 out of 14 judges assigned a score of 1, and another 4 assigned a score of 0.5 to the incorrect name ancestor .\\nFor the math case study, human judges consistently assigned the highest score to the correct name across all predicates, as shown in Table 12 . Moreover, they successfully identified the correct name for predicate L, that is isDivisor , which none of the LLMs judges were able to recognize correctly.\\nOverall, ChatGPT-4o and ChatGPT-o3mini appear to be the best-performing models, followed by Gemini and Command R+; Llama, FalconMamba, and Falcon3 performed the weakest.\\nIt is worth noting that we chose some relatively small models to test whether they could identify useful names. Llama 3.2 and Falcon3 provided several correct suggestions, though not for every predicate. While few-shot prompting did not enhance performance for Falcon3 and Llama, it led to a substantial improvement for FalconMamba.\\nNevertheless, their smaller size allows them to be run locally on machines with lower processing power, making this approach accessible to users without high-performance hardware.\\nThe evaluation strategy that we adopted is automatic, meaning that human intervention is not required except for a final assessment, which should be faster than evaluating every single suggestion for each predicate.\\nIt could be argued that an LLM could make errors not only in making suggestions but also in judging them. Even though in these experiments we asked the LLMs to score only once, for a more robust evaluation,\\nthe procedure could be repeated multiple times per judge, and the results averaged.\\nIf a tie occurs between two or more suggestions, a new evaluation could be carried out, or the final decision could be left to domain experts.\\nBased on the results of the human evaluation, we can conclude that, overall, LLMs were generally capable of performing both the renaming and judgment tasks effectively. Their assessments aligned with those of human judges in most cases, with discrepancies emerging primarily in the more challenging or ambiguous predicates.',\n",
       "    'subsections': [{'title': '5.1 Prompting strategy',\n",
       "      'paragraphs': 'We carried out several initial prompt design attempts to observe how LLMs responded under varying instructions. For instance, after noticing that the models often provided multiple equivalent suggestions, we decided to limit them to just one.\\nWe also added a constraint to prevent models from altering the structure of the rules, after observing such behaviors with some LLMs . Each of these attempts was executed independently, ensuring that no contextual information was shared between runs. Importantly, the models did not retain any memory of previous executions; in particular, ChatGPT was used with the memory feature explicitly disabled to prevent any persistence of prior interactions. After analyzing the models‚Äô responses and identifying recurring issues or undesired behaviors, we refined our instructions iteratively. The final version of the prompt, shown in Figure 2 , was then used to rerun all experiments from scratch. Additionally, we observed that models sometimes gave the correct response on the first try, while other times they required several attempts. To address this, and given the non-deterministic nature of the answers, we decided to repeat the question a certain number of times to address this issue. In these preliminary experiments, we decided to use three repetitions.\\nAll the models handled the task appropriately, so zero-shot prompting was sufficient in this case. Most of them followed the instructions accurately, with only a few exceptions.\\nFor example, Llama usually wrote a sentence (explaining the reasons for its choices) instead of adopting the requested format.\\nIn such cases, however, the name can be extracted from the answer using basic string manipulation.\\nAfter getting three suggestions out of each model, we asked them to choose only one name among their own ones.\\nTables 1 and 2 show the final choices made by each model for each predicate of each case study; we also report for each LLM how many of the suggested names could be considered correct. Table 3 provides a summary of the number of correct name suggestions per predicate. Each model receives a score of 1 if its final suggestion matches the expected name, and 0 otherwise. In the final column, we indicate how many LLMs correctly named each predicate. This overview allows us to easily compare the performance of the models on a per-predicate basis and identify which predicates were more challenging.\\nThe coauthors case study, which was the simplest, presented no difficulties for the models. In fact, all of them suggested a correct name, although sometimes too verbose.\\nRegarding the family case study, the GPT models and Command R+ provided the correct name of each predicate, and Gemini failed to identify only h4.\\nLlama was completely unable to find meaningful names for most of the predicates, with only two correct suggestions. Furthermore, one out of the three times it rewrote most of the rules, making its answer completely wrong.\\nFalconMamba and Falcon3 also struggled with the family case study, as none of the answers were correct.\\nIn particular, Falcon3 provided the answer for only one predicate, likely being misled by the word ‚ÄúONE‚Äù in the prompt.\\nFor this reason, its answers were not considered.\\nFor the math case study, the GPT models correctly identified almost all the names.\\nGemini followed closely, with only two mistakes.\\nFalcon3 and Command R+ also identified most of the correct names, while Llama just over half of them.\\nOn the other hand, FalconMamba simply rewrote the rules with different capital letters all three times, and therefore its answers were ignored.\\nMany models couldn‚Äôt find the correct name for predicates L ( isDivisor , suggested only by Command R+), M ( gcd ), and N ( lcm ); these last two were correctly identified only by ChatGPT-4o and ChatGPT-o3mini.\\nIn the lcm case study, most models struggled to identify the correct names for predicates G and H, namely product and quotient (or equivalent terms).\\nOnly ChatGPT-4o and ChatGPT-o3mini provided correct suggestions.\\nFinding siblings for predicate h3 in the cousins case study was also challenging for some models, even if they suggested it in the family case study. Only the GPT models, Gemini and LLama provided the correct name.\\nFor the grandparent case study, all the models correctly suggested parent for predicate h0, except for FalconMamba.\\nIn Table 4 we report the results obtained on the Mutagenesis ring theory, using ChatGPT-4o and ChatGPT-o3mini.\\nSome of the predicates appear to be similar to the original ones, but overall it is difficult to determine whether the suggested names are meaningful. In such a case, a domain expert should make the final decision.\\nTo improve the quality of the output, the models could be fine-tuned on the specific domain; alternatively, context (e.g., in the form of scientific papers) could be provided.\\nIt is worth mentioning another attempt we made with Mutagenesis. For this initial experiment, we used a less constrained version of Prompt 1 , where we simply asked for suggestions for all the predicates, without specifying the context or limiting the model to provide only one suggestion per predicate.\\nThe response of Command R+ was particularly interesting. The model not only explained the predicate to rename but in some cases it also invented a helper predicate for it.\\nAn example of such behavior is shown in Figure 5 : the model created the helper predicate check_atom_count_recursive for predicate HP19, which was renamed as check_atom_count_recursive . Regardless of the correctness of the suggested name and despite altering the structure of the rule, this suggests the potential of LLMs to directly perform PI .\\nThe reachability case study was generally unchallenging for the majority of models evaluated.\\nSuggestions produced by GPT-5 (direct_connection), Command R+ (is_connected), Gemini (directly_connected), and LLama (is_connected) are all plausible and appropriate, as they generalize a single-step path relation.\\nIn contrast, both FalconMamba and Falcon3 exhibited notable difficulties. FalconMamba generated an output (can reach) that was syntactically invalid as a predicate, due to the inclusion of a space in the identifier, while Falcon 3 failed to produce any answer at all.',\n",
       "      'subsections': [{'title': 'Results for unnamed predicates appearing only in the head',\n",
       "        'paragraphs': 'The coauthors case study, which was the simplest, presented no difficulties for the models. In fact, all of them suggested a correct name, although sometimes too verbose.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Results for unnamed predicates appearing both in the head and in the body',\n",
       "        'paragraphs': 'Regarding the family case study, the GPT models and Command R+ provided the correct name of each predicate, and Gemini failed to identify only h4.\\nLlama was completely unable to find meaningful names for most of the predicates, with only two correct suggestions. Furthermore, one out of the three times it rewrote most of the rules, making its answer completely wrong.\\nFalconMamba and Falcon3 also struggled with the family case study, as none of the answers were correct.\\nIn particular, Falcon3 provided the answer for only one predicate, likely being misled by the word ‚ÄúONE‚Äù in the prompt.\\nFor this reason, its answers were not considered.\\nFor the math case study, the GPT models correctly identified almost all the names.\\nGemini followed closely, with only two mistakes.\\nFalcon3 and Command R+ also identified most of the correct names, while Llama just over half of them.\\nOn the other hand, FalconMamba simply rewrote the rules with different capital letters all three times, and therefore its answers were ignored.\\nMany models couldn‚Äôt find the correct name for predicates L ( isDivisor , suggested only by Command R+), M ( gcd ), and N ( lcm ); these last two were correctly identified only by ChatGPT-4o and ChatGPT-o3mini.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Results for unnamed predicates appearing only in the body',\n",
       "        'paragraphs': 'In the lcm case study, most models struggled to identify the correct names for predicates G and H, namely product and quotient (or equivalent terms).\\nOnly ChatGPT-4o and ChatGPT-o3mini provided correct suggestions.\\nFinding siblings for predicate h3 in the cousins case study was also challenging for some models, even if they suggested it in the family case study. Only the GPT models, Gemini and LLama provided the correct name.\\nFor the grandparent case study, all the models correctly suggested parent for predicate h0, except for FalconMamba.',\n",
       "        'subsections': []},\n",
       "       {'title': 'Results for the Mutagenesis case study',\n",
       "        'paragraphs': 'In Table 4 we report the results obtained on the Mutagenesis ring theory, using ChatGPT-4o and ChatGPT-o3mini.\\nSome of the predicates appear to be similar to the original ones, but overall it is difficult to determine whether the suggested names are meaningful. In such a case, a domain expert should make the final decision.\\nTo improve the quality of the output, the models could be fine-tuned on the specific domain; alternatively, context (e.g., in the form of scientific papers) could be provided.\\nIt is worth mentioning another attempt we made with Mutagenesis. For this initial experiment, we used a less constrained version of Prompt 1 , where we simply asked for suggestions for all the predicates, without specifying the context or limiting the model to provide only one suggestion per predicate.\\nThe response of Command R+ was particularly interesting. The model not only explained the predicate to rename but in some cases it also invented a helper predicate for it.\\nAn example of such behavior is shown in Figure 5 : the model created the helper predicate check_atom_count_recursive for predicate HP19, which was renamed as check_atom_count_recursive . Regardless of the correctness of the suggested name and despite altering the structure of the rule, this suggests the potential of LLMs to directly perform PI .',\n",
       "        'subsections': []},\n",
       "       {'title': 'Results for the reachability case study',\n",
       "        'paragraphs': 'The reachability case study was generally unchallenging for the majority of models evaluated.\\nSuggestions produced by GPT-5 (direct_connection), Command R+ (is_connected), Gemini (directly_connected), and LLama (is_connected) are all plausible and appropriate, as they generalize a single-step path relation.\\nIn contrast, both FalconMamba and Falcon3 exhibited notable difficulties. FalconMamba generated an output (can reach) that was syntactically invalid as a predicate, due to the inclusion of a space in the identifier, while Falcon 3 failed to produce any answer at all.',\n",
       "        'subsections': []}]},\n",
       "     {'title': '5.2 Results for the few-shot prompting experiment',\n",
       "      'paragraphs': 'We additionally conducted a few-shot prompting experiment with the LLMs that demonstrated lower performance, namely FalconMamba, Falcon3, and Llama, with the goal of improving their answers.\\nWe focused on the math case study, which was selected due to its potential to highlight the benefits of few-shot prompting.\\nIn this experiment, suggestions were requested for one predicate at a time, while also providing the model with the correct names of previously addressed predicates.\\nFigure 6 shows the prompts used in the few-shot prompting experiments with answers from Falcon3. The initial prompt, Prompt 0 , requests the name of only the first predicate ( A ). Then, its answers is used to replace the corresponding unnamed predicate in the following prompt, Prompt 1 , while requesting a suggestion for the second predicate ( P ). This continues until the last prompt, in which all the unnamed predicates of the math case study have been renamed, except of the last one.\\nThis incremental approach aimed to support the model by narrowing its focus and reducing the complexity of the task, in contrast to requiring it to generate names for all predicates simultaneously.\\nHowever, the outcomes did not meet expectations. None of the models showed improved performance in terms of correctly identifying a greater number of names; in some instances, performance even declined.\\nPrompt 0:\\nResponse:\\nPrompt 1:\\nResponse:\\nTable 5 shows the results of this experiment.\\nWith few-shot prompting Falcon3 correctly identified 10 out of 16 predicate names, a decrease from the 12 out of 16 correctly identified with zero-shot prompting.\\nSimilarly, Llama achieved 8 correct predictions with few-shot prompting, compared to 9 without.\\nA substantial improvement was observed with FalconMamba. with zero-shot prompting, the model failed to generate a meaningful output, simply repeating the predicates.\\nIn contrast, when few-shot prompting was applied, the model provided a suggestion for all the predicates, finding 11 correct answers out of 16.\\nThese findings, particularly in the case of FalconMamba, indicate that breaking down the prompt and using few-shot prompting can significantly enhance performance in models that otherwise struggle with zero-shot prompting.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.3 LLMs judgment results',\n",
       "      'paragraphs': 'Regarding the judgment on the coauthors case study, shown in Table 6 , all the suggestions\\nare plausible. LLM judges seem to prefer a longer and more detailed name, assigning the lowest score to the simplest one, i.e. coauthors .\\nIn Table 7 we report the results of the judgment on the family case study.\\nThe correct name was chosen for each predicate.\\nThe same holds for the grandparent and cousins case studies, both shown in Table 6 .\\nTable 8 shows the results of the judgment on the math case study. Here, the correct name was chosen for all the predicates, except for predicate L. While the correct name is divisor , it was mostly interpreted as ‚Äúdivisible‚Äù, which is actually the opposite.\\nRegarding predicates M and N, all the judges recognized the true names, gcd and lcm respectively, even if they didn‚Äôt suggest them initially. The strategy of asking multiple models for name suggestions turned out to be effective, as it allowed the judges to identify the correct name when one model suggested it, even when the others did not.\\nThe same cannot be said for the lcm case study, as can be seen in Table 6 . For predicate G, findLeastCommonMultipleIntermediate was chosen as the most fitting name, instead of computeProduct or multiply , which would have been equally correct.\\nSimilarly, for predicate H, computeLcmFromGcd was selected as the most appropriate name, rather than divide or divideValues .\\nTable 9 presents the judgment results for the reachability case study. It is important to note that ChatGPT-5 was used as a substitute for ChatGPT-4o and ChatGPT-o3mini, as these models were no longer available at the time the experiment was conducted.\\nThree out of the four alternatives were selected as the most suitable name, each receiving the same score. This outcome is acceptable, as all three options are considered plausible. Notably, however, only ChatGPT-5 assigned a score of 0 to the proposed name can reach , which is syntactically incorrect. In contrast, both Gemini and Command R+ assigned it a score of 0.5, indicating a limited ability to recognize the syntactic invalidity of the suggestion.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.4 Human judgment results',\n",
       "      'paragraphs': 'Although all the proposed alternatives in the coauthors case study\\nare plausible, human judges did not select the most straightforward option for predicate P, that is, coauthors . This behavior aligns with that observed with the LLMs .\\nThey also correctly identified the appropriate names for predicates h0 and h3 in the grandparent and cousins case studies, respectively.\\nIn the lcm case study, human judges successfully selected the correct names for predicates G and H, whereas the LLMs judges failed to do so.\\nAveraged scores assigned by human judges for these case studies are summarized in Table 10 .\\nTable 11 presents the results of the human evaluation for the proposed names in the family case study. For each predicate, the correct name received the highest overall score. However, it is noteworthy that several human judges assigned non-zero scores to incorrect names. For instance, in the case of predicate h0, 4 out of 14 judges assigned a score of 1, and another 4 assigned a score of 0.5 to the incorrect name ancestor .\\nFor the math case study, human judges consistently assigned the highest score to the correct name across all predicates, as shown in Table 12 . Moreover, they successfully identified the correct name for predicate L, that is isDivisor , which none of the LLMs judges were able to recognize correctly.',\n",
       "      'subsections': []},\n",
       "     {'title': '5.5 Summary of results',\n",
       "      'paragraphs': 'Overall, ChatGPT-4o and ChatGPT-o3mini appear to be the best-performing models, followed by Gemini and Command R+; Llama, FalconMamba, and Falcon3 performed the weakest.\\nIt is worth noting that we chose some relatively small models to test whether they could identify useful names. Llama 3.2 and Falcon3 provided several correct suggestions, though not for every predicate. While few-shot prompting did not enhance performance for Falcon3 and Llama, it led to a substantial improvement for FalconMamba.\\nNevertheless, their smaller size allows them to be run locally on machines with lower processing power, making this approach accessible to users without high-performance hardware.\\nThe evaluation strategy that we adopted is automatic, meaning that human intervention is not required except for a final assessment, which should be faster than evaluating every single suggestion for each predicate.\\nIt could be argued that an LLM could make errors not only in making suggestions but also in judging them. Even though in these experiments we asked the LLMs to score only once, for a more robust evaluation,\\nthe procedure could be repeated multiple times per judge, and the results averaged.\\nIf a tie occurs between two or more suggestions, a new evaluation could be carried out, or the final decision could be left to domain experts.\\nBased on the results of the human evaluation, we can conclude that, overall, LLMs were generally capable of performing both the renaming and judgment tasks effectively. Their assessments aligned with those of human judges in most cases, with discrepancies emerging primarily in the more challenging or ambiguous predicates.',\n",
       "      'subsections': []}]},\n",
       "   {'title': '6 Discussion',\n",
       "    'paragraphs': 'Having unnamed predicates in the output rules may significantly hinder their understanding and reusability, especially for lengthy theories. However, current approaches do not seem to address this issue, with invented predicates typically being left unnamed.\\nA possible solution is to use placeholders and select names from a predefined pool ( [ 12 ] ), but this would require knowing beforehand which predicates should be invented and their arity, which is not always feasible.\\nWhile all the predicates could be renamed manually, our approach showed promising results in accomplishing this task automatically.\\nEven though the involvement of LLMs in logic is still not much investigated, the experiments show that this method was effective in most of the tested cases, even without providing specific contexts or examples. In fact, zero-shot prompting was sufficient, as the LLMs immediately understood their task. This is particularly convenient when extensive computing power to fine-tune models is not easily accessible, as models for general-purpose tasks, trained on common knowledge data and code can still perform the task ( [ 88 ] ).\\nThere are several limitations to this early-stage approach.\\nThe main one is the use of relatively simple examples. The rules express common knowledge and typical relationships, which the models may have already seen in the LP codes they have been trained on. Nonetheless, this initial step was necessary to assess whether LLMs could correctly process them; otherwise, their ability to grasp even more specific relationships would have seemed unlikely.\\nDespite the simplicity of the examples, we encountered several challenges throughout the process.\\nOne issue is that, at times, some models fail to follow the request or the output format, which complicates the automation of the entire process.\\nFor instance, in some preliminary experiments, we noticed that some models also altered the body of the rule, even if instructed to find a name for the head predicate. This is not desirable, as it could completely change the meaning of the logic theory, which is assumed to be correct. That is why we decided to add ‚ÄúDo NOT change the body of the rules‚Äù to the instructions.\\nAnother constraint, ‚ÄúGive only ONE suggestion for each predicate.‚Äù, was added to the prompt to limit the number of suggestions. While one could argue that this might cause the potentially correct answer to be overlooked, we addressed this concern by repeating the request multiple times.\\nWhen this behavior persisted, we decided to disregard that particular answer for that specific predicate. Even if one answer is discarded, the presence of multiple models offering suggestions reduces the impact of this issue.\\nModels also struggled to recognize that some names, despite being written in different formats, were conceptually equivalent (e.g., lessThan , less_than , LessThan ), even when explicitly instructed to treat them as equal. As a result, it was necessary to manually standardize these names, to ensure that they were evaluated as the same. The issue persists to some extent: singular and plural forms are considered different (e.g., cousins and cousin ), as well as names that include a verb or particle (e.g., isEqual or equalTo ).\\nLastly, this approach was unsuccessful when tested on predicates from a real-world dataset, Mutagenesis.\\nIt should be noted that nearly all of the predicates‚Äô true names were masked, which significantly hindered the models‚Äô ability to identify them and grasp the relationship between them. However, such a scenario is highly unlikely, if not entirely unrealistic, since it would mean that the background knowledge is almost empty. In this case, inferring a set of meaningful rules would be highly difficult for any ML model.\\nNevertheless, our findings hint that domain-specific fine-tuning might be necessary for real-world applications ( [ 69 ] ), provided that enough computational resources are available. Alternatively, big models could be replaced by smaller or quantized fine-tuned versions, which are more computationally efficient and have been shown to not significantly decrease the quality of the results ( [ 50 , 39 , 19 ] ).',\n",
       "    'subsections': [{'title': '6.1 Limitations',\n",
       "      'paragraphs': 'There are several limitations to this early-stage approach.\\nThe main one is the use of relatively simple examples. The rules express common knowledge and typical relationships, which the models may have already seen in the LP codes they have been trained on. Nonetheless, this initial step was necessary to assess whether LLMs could correctly process them; otherwise, their ability to grasp even more specific relationships would have seemed unlikely.\\nDespite the simplicity of the examples, we encountered several challenges throughout the process.\\nOne issue is that, at times, some models fail to follow the request or the output format, which complicates the automation of the entire process.\\nFor instance, in some preliminary experiments, we noticed that some models also altered the body of the rule, even if instructed to find a name for the head predicate. This is not desirable, as it could completely change the meaning of the logic theory, which is assumed to be correct. That is why we decided to add ‚ÄúDo NOT change the body of the rules‚Äù to the instructions.\\nAnother constraint, ‚ÄúGive only ONE suggestion for each predicate.‚Äù, was added to the prompt to limit the number of suggestions. While one could argue that this might cause the potentially correct answer to be overlooked, we addressed this concern by repeating the request multiple times.\\nWhen this behavior persisted, we decided to disregard that particular answer for that specific predicate. Even if one answer is discarded, the presence of multiple models offering suggestions reduces the impact of this issue.\\nModels also struggled to recognize that some names, despite being written in different formats, were conceptually equivalent (e.g., lessThan , less_than , LessThan ), even when explicitly instructed to treat them as equal. As a result, it was necessary to manually standardize these names, to ensure that they were evaluated as the same. The issue persists to some extent: singular and plural forms are considered different (e.g., cousins and cousin ), as well as names that include a verb or particle (e.g., isEqual or equalTo ).\\nLastly, this approach was unsuccessful when tested on predicates from a real-world dataset, Mutagenesis.\\nIt should be noted that nearly all of the predicates‚Äô true names were masked, which significantly hindered the models‚Äô ability to identify them and grasp the relationship between them. However, such a scenario is highly unlikely, if not entirely unrealistic, since it would mean that the background knowledge is almost empty. In this case, inferring a set of meaningful rules would be highly difficult for any ML model.\\nNevertheless, our findings hint that domain-specific fine-tuning might be necessary for real-world applications ( [ 69 ] ), provided that enough computational resources are available. Alternatively, big models could be replaced by smaller or quantized fine-tuned versions, which are more computationally efficient and have been shown to not significantly decrease the quality of the results ( [ 50 , 39 , 19 ] ).',\n",
       "      'subsections': []}]},\n",
       "   {'title': '7 Conclusion',\n",
       "    'paragraphs': 'In this paper, we have proposed a straightforward pipeline that leverages LLMs to find suggestions for unnamed predicates in logic theories, since their presence limits the interpretability. Based on the results obtained from some hand-crafted sets of rules, LLMs appear to be a promising solution to this problem. To the best of our knowledge, this is the first study to address this issue.\\nThe limitations of this study are primarily related to the examples we used, and investigating more complex scenarios is an interesting future work. We believe that the suitability of the proposed names is primarily determined by the context and semantics of the program and thus developed an approach that is independent of the language. Nevertheless, differences in language-specific conventions may play a role, and investigating this aspect could offer further insights into the generalization ability of LLM-based renaming approaches. Another topic worth exploring is the use of fine-tuned LLMs for domain-specific naming suggestions, as well as the development of quantized versions that can run locally on machines with lower processing power.\\nFuture research could also consider the potential of LLMs for directly performing PI and their application to other approaches such as Learning From Interpretation Transitions (LFIT) ( [ 70 ] ) to reduce the number of rules.',\n",
       "    'subsections': []},\n",
       "   {'title': 'Declarations',\n",
       "    'paragraphs': 'Elisabetta Gentili contributed to this publication while attending the PhD programme in Engineering Science at the University of Ferrara, Cycle XXXVIII, with the support of a scholarship financed by the Ministerial Decree no. 351 of 9th April 2022, based on the NRRP - funded by the European Union - NextGenerationEU - Mission 4 ‚ÄúEducation and Research‚Äù, Component 1 ‚ÄúEnhancement of the offer of educational services: from nurseries to universities‚Äù - Investment 4.1 ‚ÄúExtension of the number of research doctorates and innovative doctorates for public administration and cultural heritage‚Äù.\\nThis work has been partially supported by Spoke 1 ‚ÄúFutureHPC & BigData‚Äù of the Italian Research Center on High-Performance Computing, Big Data and Quantum Computing (ICSC) funded by MUR Missione 4 - Next Generation EU (NGEU) and by Partenariato Esteso PE00000013 - ‚ÄúFAIR - Future Artificial Intelligence Research‚Äù - Spoke 8 ‚ÄúPervasive AI‚Äù, funded by MUR through PNRR - M4C2 - Investimento 1.3 (Decreto Direttoriale MUR n. 341 of 15th March 2022) under the Next Generation EU (NGEU).\\nElisabetta Gentili and Fabrizio Riguzzi are members of the Gruppo Nazionale Calcolo Scientifico ‚Äì Istituto Nazionale di Alta Matematica (GNCS-INdAM).\\nKatsumi Inoue has been supported in part by JST CREST Grant Number\\nJPMJCR22D3, Japan.\\nTony Ribeiro has been funded by the European Union. Views\\nand opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or ERCEA. Neither the European Union nor the granting authority can be held responsible for them.\\nThe authors declare none.',\n",
       "    'subsections': [{'title': 'Funding',\n",
       "      'paragraphs': 'Elisabetta Gentili contributed to this publication while attending the PhD programme in Engineering Science at the University of Ferrara, Cycle XXXVIII, with the support of a scholarship financed by the Ministerial Decree no. 351 of 9th April 2022, based on the NRRP - funded by the European Union - NextGenerationEU - Mission 4 ‚ÄúEducation and Research‚Äù, Component 1 ‚ÄúEnhancement of the offer of educational services: from nurseries to universities‚Äù - Investment 4.1 ‚ÄúExtension of the number of research doctorates and innovative doctorates for public administration and cultural heritage‚Äù.\\nThis work has been partially supported by Spoke 1 ‚ÄúFutureHPC & BigData‚Äù of the Italian Research Center on High-Performance Computing, Big Data and Quantum Computing (ICSC) funded by MUR Missione 4 - Next Generation EU (NGEU) and by Partenariato Esteso PE00000013 - ‚ÄúFAIR - Future Artificial Intelligence Research‚Äù - Spoke 8 ‚ÄúPervasive AI‚Äù, funded by MUR through PNRR - M4C2 - Investimento 1.3 (Decreto Direttoriale MUR n. 341 of 15th March 2022) under the Next Generation EU (NGEU).\\nElisabetta Gentili and Fabrizio Riguzzi are members of the Gruppo Nazionale Calcolo Scientifico ‚Äì Istituto Nazionale di Alta Matematica (GNCS-INdAM).\\nKatsumi Inoue has been supported in part by JST CREST Grant Number\\nJPMJCR22D3, Japan.\\nTony Ribeiro has been funded by the European Union. Views\\nand opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or ERCEA. Neither the European Union nor the granting authority can be held responsible for them.',\n",
       "      'subsections': []},\n",
       "     {'title': 'Competing interests',\n",
       "      'paragraphs': 'The authors declare none.',\n",
       "      'subsections': []}]},\n",
       "   {'title': 'References', 'paragraphs': '', 'subsections': []}],\n",
       "  'abstract': 'In this paper, we address the problem of giving names to predicates in logic rules using Large Language Models (LLMs). In the context of Inductive Logic Programming, various rule generation methods produce rules containing unnamed predicates, with Predicate Invention being a key example. This hinders the readability, interpretability, and reusability of the logic theory. Leveraging recent advancements in LLMs development, we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates. The evaluation of our approach on some hand-crafted logic rules indicates that LLMs hold potential for this task.'}}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATA_PATH: str = \"../data\"\n",
    "PARSED_PATH: str = f\"{DATA_PATH}/raw/parsed_sections\"\n",
    "ABSTRACT_PATH: str = f\"{DATA_PATH}/raw/abstracts\"\n",
    "SUMMARIES_PATH: str = f\"{DATA_PATH}/summaries\"\n",
    "\n",
    "\n",
    "# creates txt files of summaries including subfolders for sections\n",
    "DEBUG: bool = True\n",
    "\n",
    "Path(SUMMARIES_PATH).mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "content_files = Path(PARSED_PATH).glob(\"*.json\")\n",
    "abstract_files = Path(ABSTRACT_PATH).glob(\"*.txt\")\n",
    "abstract_content_mapping = {}\n",
    "\n",
    "for file in content_files:\n",
    "    paper_id = file.stem\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        abstract_content_mapping[paper_id] = {}\n",
    "        abstract_content_mapping[paper_id][\"content\"] = json.load(json_file)\n",
    "\n",
    "for file in abstract_files:\n",
    "    paper_id = file.stem\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        try:\n",
    "            abstract_content_mapping[paper_id][\"abstract\"] = ' '.join(txt_file.readlines())\n",
    "            print(f\"Read {file}\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "abstract_content_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f8344a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a61db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bf23d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_subsection_with_llm(title: str, text: str, debug_path: Path, child_summaries: str = \"\"):\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that summarizes scientific paper sections.\\n\"\n",
    "        \"Produce a concise, factual summary.\\n\"\n",
    "        \"Do NOT add commentary, disclaimers, or phrases like 'The summary is:'.\\n\"\n",
    "        \"Only output the summary text itself.\"\n",
    "    )\n",
    "\n",
    "    user_text = f\"Section title: {title}\\n\\nSection content:\\n{text}\"\n",
    "\n",
    "    if child_summaries.strip():\n",
    "        user_text += (\n",
    "            \"\\n\\nSummaries of direct subsections (for additional context):\\n\"\n",
    "            f\"{child_summaries}\"\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", user_text),\n",
    "    ]\n",
    "\n",
    "    if DEBUG:\n",
    "        debug_path.mkdir(parents=True, exist_ok=True)\n",
    "        with open(debug_path / \"prompt.txt\", \"w\") as f:\n",
    "            f.write(user_text)\n",
    "\n",
    "    response = llm.invoke(messages).content\n",
    "    return response\n",
    "\n",
    "\n",
    "def summarize_section_recursive(section: dict, debug_path: Path) -> str:\n",
    "\n",
    "    if DEBUG:\n",
    "        debug_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Collect child summaries\n",
    "    children_summary_text = \"\"\n",
    "    for subsection in section.get(\"subsections\", []):\n",
    "        \n",
    "        # ‚úÖ FIXED: use subsection title, not parent title\n",
    "        debug_path_subsection = debug_path / subsection['title'].replace(\"/\", \"_\")\n",
    "        \n",
    "        child_text = summarize_section_recursive(\n",
    "            subsection,\n",
    "            debug_path=debug_path_subsection,\n",
    "        )\n",
    "\n",
    "        children_summary_text += f\"- {subsection['title']}: {child_text}\\n\"\n",
    "\n",
    "    # Summarize this section\n",
    "    section_summary = summarize_subsection_with_llm(\n",
    "        title=section[\"title\"],\n",
    "        text=section[\"paragraphs\"],\n",
    "        debug_path=debug_path,\n",
    "        child_summaries=children_summary_text,\n",
    "    )\n",
    "\n",
    "    if DEBUG:\n",
    "        with open(debug_path / \"subsection_summary.txt\", \"w\") as f:\n",
    "            f.write(section_summary)\n",
    "\n",
    "    return section_summary\n",
    "\n",
    "\n",
    "def summarize_whole_paper(paper_title: str, paper: list) -> str:\n",
    "\n",
    "    paper_path = Path(SUMMARIES_PATH) / paper_title\n",
    "    paper_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    combined_text = \"\"\n",
    "\n",
    "    for section in paper:\n",
    "        debug_path_subsection = paper_path / section['title'].replace(\"/\", \"_\")\n",
    "        print(debug_path_subsection)\n",
    "\n",
    "        section_summary = summarize_section_recursive(\n",
    "            section,\n",
    "            debug_path=debug_path_subsection,\n",
    "        )\n",
    "\n",
    "        combined_text += f\"{section['title']}: {section_summary}\\n\"\n",
    "\n",
    "    print(combined_text)\n",
    "\n",
    "    final_summary = summarize_subsection_with_llm(\n",
    "        title=paper_title,\n",
    "        text=combined_text,\n",
    "        debug_path=paper_path\n",
    "    )\n",
    "\n",
    "    with open(paper_path / \"summary.txt\", \"w\") as f:\n",
    "        f.write(final_summary)\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "16178432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25320v1/1 Introduction\n",
      "../data/summaries/2510.25320v1/2 Background\n",
      "../data/summaries/2510.25320v1/3 Graph-based Agent Planning Paradigm\n",
      "../data/summaries/2510.25320v1/4 Training Pipeline\n",
      "../data/summaries/2510.25320v1/5 Experiments\n",
      "../data/summaries/2510.25320v1/6 Conclusion\n",
      "../data/summaries/2510.25320v1/References\n",
      "../data/summaries/2510.25320v1/Appendix A Related Work\n",
      "../data/summaries/2510.25320v1/Appendix B Case Study\n",
      "1 Introduction: Graph-based agent planning enables parallel and serial tool execution through explicit task dependency modeling, improving agent efficiency and generalization.\n",
      "2 Background: The paper introduces the ReAct-style approach, leveraging few-shot exemplars to guide an LLM through a ‚ÄúThought-Action-Observation‚Äù cycle, interleaving reasoning traces, tool use, and observation. Tool-Integrated Reasoning (TIR) enhances LLMs‚Äô code reasoning by integrating natural language with external tool execution environments, maintaining a global state and selecting tools via a policy.\n",
      "3 Graph-based Agent Planning Paradigm: GAP introduces a framework for LLM-based agents to plan complex tasks by representing them as a directed acyclic graph. The agent decomposes queries into sub-tasks, models dependencies, and dynamically selects tools to maximize efficiency and correctness. The model constructs a dependency graph, generates parallel tool calls, and partitions the graph into execution levels to enable parallel task execution.\n",
      "4 Training Pipeline: The paper details a supervised fine-tuning pipeline using GPT-4o to generate Graph-based Action Planning (GAP) trajectories from Natural Questions and HotpotQA datasets. Data quality was filtered through complexity, diversity, and length constraints, resulting in approximately 7,000 high-quality trajectories. Fine-tuning focuses on optimizing policy parameters through end-to-end reinforcement learning, balancing parallel exploration and context window constraints, using a graph-based reward function and rule-based metrics.\n",
      "5 Experiments: Seven benchmark datasets were selected for evaluation, including general question answering, trivia, popQA, multi-hop QA, and multimodal QA. GAP outperformed baseline methods on HotpotQA, achieving 0.9% improvement and a 33.4% reduction in execution time. It demonstrates strong multi-hop reasoning through parallel decomposition, reducing interaction turns and execution time, and autonomously determines parallelizability, generalizing across domains.\n",
      "6 Conclusion: GAP (Graph-based Agent Planning) is a new paradigm that improves LLM-based agents' reasoning and tool execution through explicit task dependency modeling. It achieves better efficiency and accuracy by autonomously decomposing queries into dependency graphs and parallel tool execution. GAP outperforms traditional sequential baselines, especially on multi-step retrieval tasks.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Related Work: The paper explores training large language models for multi-turn tool-integrated reasoning, demonstrating that this approach expands LLM capabilities beyond pure text-based RL through strategies like post-training with SFT or RL, structured output, and improved tool-use capability. Recent work, including Chain-of-Agents and Cognitive Kernel-Pro, highlights the importance of training quality and architectural innovation for creating capable agents, even with smaller models, and showcases that even models at smaller scales can rival larger systems on benchmarks like GAIA and WebArena.\n",
      "Appendix B Case Study: Case studies were conducted to analyze the behavior and capabilities of the GAP framework. Results are presented in tables.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñé         | 1/40 [00:42<27:24, 42.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25232v1/1 Introduction\n",
      "../data/summaries/2510.25232v1/2 Related Work\n",
      "../data/summaries/2510.25232v1/3 Patient EMR and Experience Generation Workflow\n",
      "../data/summaries/2510.25232v1/4 Multi-agent Diagnosis Framework\n",
      "../data/summaries/2510.25232v1/5 Final Product: PsyCoTalk Dataset\n",
      "../data/summaries/2510.25232v1/Limitations\n",
      "../data/summaries/2510.25232v1/6 Conclusion\n",
      "../data/summaries/2510.25232v1/Ethics Statement\n",
      "../data/summaries/2510.25232v1/References\n",
      "../data/summaries/2510.25232v1/Appendix A Appendix\n",
      "1 Introduction: PsyCoTalk is a large-scale dialogue dataset designed for comorbidity diagnosis, utilizing synthetic social media posts transformed into structured EMRs to train a three-agent diagnostic system. It includes 502 synthetic EMRs representing common comorbid conditions and provides a clinically grounded multi-agent framework combining a Hierarchical Diagnostic State Machine (HDSM) and Diagnosis Context Tree (DCT) for comprehensive diagnostic reasoning.\n",
      "2 Related Work: Recent datasets include D, PsyQA, EFAQA, and MDD‚Äë5k. LLM-driven dialogue simulation models are used in mental health research, with studies assessing chatbot empathy and tutoring techniques. Multi-agent designs, leveraging the AMC framework, improve clinical grounding and generate dialogues reflecting diagnostic reasoning.\n",
      "3 Patient EMR and Experience Generation Workflow: The PsySym dataset, a scale of 5,624 users with detailed annotations, was selected due to its suitability for modeling psychiatric comorbidity. A user filtering process, based on symptom diversity and frequency, retained users with 102.5 posts per user, ensuring clinical reliability. A modular generation strategy, involving individual section categorization and LLM inference, produced 502 structured EMRs, covering six psychiatric conditions.  The dataset includes structured information on demographic data, medical history, personal history, family history, and preliminary diagnosis, alongside a comparison with de-identified clinical records, revealing a broad alignment of disease prevalence with real-world data.  A personalized fictitious experience generation method, leveraging LLMs, was proposed to enhance dialogue diversity and realism, ensuring semantic coherence and diagnostic validity.\n",
      "4 Multi-agent Diagnosis Framework: The framework utilizes a hierarchical diagnostic state machine (HDSM) and diagnostic context tree (DCT) to guide dialogue, incorporating structured clinical interview data, dynamic dialogue generation, and a knowledge graph for contextualization. The Patient Agent generates responses based on the EMR, fictitious experience, and tool agent‚Äôs topic, while the Doctor Agent poses questions based on the topic, utilizing five distinct profiles. The Tool Agent manages dialogue flow, ensuring clinical relevance and coherence through tree management and dialogue coordination, incorporating a Disease‚ÄìSymptom Description Knowledge Graph for hallucination filtering and personalized templates.\n",
      "5 Final Product: PsyCoTalk Dataset: PsyCoTalk Dataset Summary:\n",
      "\n",
      "The dataset comprises 502 structured EMRs from PsyCoProfile, generating five distinct fictitious experiences and paired with five personal histories. These FEDs are then used to create 3,000 dialogues, selected to form the PsyCoTalk dataset. Evaluation involves two stages: human evaluation by psychiatrists with experience and an AB test comparing PsyCoTalk with existing datasets.  The dataset statistics demonstrate PsyCoTalk‚Äôs scale and structural fidelity, exceeding other corpora in terms of dialogue length and turn count.  The study assesses dialogue diversity and validates cross-lingual generalization through a small-scale English generation experiment, comparing with existing corpora.  Expert evaluation reveals PsyCoTalk excels in communication, professionalism, fluency, and realism, while MDD-5k exhibits overly long utterances and lower accuracy.\n",
      "Limitations: PsyCoTalk‚Äôs focus on four prevalent disorders and comorbidities reflects real-world cases but limits coverage of rarer conditions. The dataset remains Chinese, restricting multilingual applicability, but the pipeline is extensible for broader coverage and cross-lingual applications.\n",
      "6 Conclusion: In conclusion, the paper introduces a two-stage pipeline to create a large, clinically standard dataset for psychiatric comorbidity. It first converts social media data into structured electronic medical records and then develops PsyCoTalk, a multi-agent dialogue corpus, to train models for multi-disorder screening.\n",
      "Ethics Statement: The authors have adhered to the ICLR Code of Ethics, ensuring the work does not involve human subjects, identifiable private data, or harmful applications. Data provenance and privacy are addressed through anonymization, synthetic electronic medical records, and rigorous review by licensed psychiatrists, ensuring no re-identification of authors. The dataset is designed for research on multi-disorder screening and dialogue modeling, with a focus on synthetic conversations and limitations regarding potential biases from web data and language models. The release of synthetic dialogues is restricted to a data-usage agreement prohibiting re-identification and clinical deployment, and users are urged to apply rigorous evaluation. The dataset‚Äôs potential impact includes positive contributions to early-stage screening and inclusive diagnostic tools, while risks of misuse are mitigated through responsible modeling and evaluation practices.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Appendix: The appendix details EMR section guidelines based on clinical practice, including demographic data, chief complaint, medical history, and family history. It outlines the structure of the template, emphasizing concise, factual documentation of patient symptoms, medical conditions, and lifestyle factors to assess potential comorbidities. Preliminary diagnoses are provided based on the EMR content. The prompt details for virtual experience generation involves a large language model to create a fictitious narrative based on a patient‚Äôs EMR data, using a template sentence derived from the EMR, and selecting from a dictionary of fictitious experiences. The hierarchical diagnostic state taxonomy and topic mapping presents a hierarchical diagnostic state taxonomy and topic mapping, grouping states based on SCID-5 categories: Affective & Cognitive Symptoms, Physiological & Behavioral Changes, Functional Impairment & Risk, and Comorbid or Contributing Factors. Terminal nodes detail the final diagnostic outcomes for each sub-state machine, mirroring real-world psychiatric evaluations. The case study of PsyCoProfile illustrates a 20-24-year-old unmarried female student with a history of major depressive disorder and anxiety, characterized by emotional instability, fatigue, and negative thoughts, with a 28-month progression of symptoms including social withdrawal, panic attacks, sleep disturbances, and compulsive behavior, followed by gradual improvement after psychological intervention. The HDSM framework organizes four psychiatric disorders (MDD, AD, BD, and ADHD) into four independent sub-states. The case study of PsyCoTalk presents a doctor-patient dialogue example generated using a multi-agent interaction framework, simulating a diagnostic process.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 2/40 [01:29<28:42, 45.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25563v1/1 Introduction\n",
      "../data/summaries/2510.25563v1/2 Related Work\n",
      "../data/summaries/2510.25563v1/3 Dataset and Study Area\n",
      "../data/summaries/2510.25563v1/4 A Foundational Model for Forecasting the SST\n",
      "../data/summaries/2510.25563v1/5 Experimental Configuration\n",
      "../data/summaries/2510.25563v1/6 Results\n",
      "../data/summaries/2510.25563v1/7 Discussion\n",
      "../data/summaries/2510.25563v1/8 Conclusion\n",
      "../data/summaries/2510.25563v1/References\n",
      "1 Introduction: This work adapts the Aurora model, initially developed for global atmospheric applications, to subregional ocean prediction, focusing on the Canary Upwelling System. Utilizing data from the GLORYS12V1 ocean prediction system, the model predicts the evolution of SST, salinity, currents, and two-dimensional variables, offering improved computational cost reduction and prediction quality, though limitations remain due to domain complexity.\n",
      "2 Related Work: Deep learning has significantly advanced oceanographic and climate prediction, incorporating satellite data, numerical models, and machine learning techniques. CNNs initially focused on temperature mapping, while GraphCast and Diffusion-based methods leverage graph structures for medium-range forecasting. Transformers enhance spatial and temporal representations, while models like Aurora and Orca improve resolution and accuracy. The development of deep learning frameworks like AIFS and the integration of pre-trained models like Aurora demonstrate advancements in handling heterogeneous data and extreme events.\n",
      "3 Dataset and Study Area: The CMEMS [47] provides a European service offering global and regional oceanic data for scientific analysis, organized into thematic categories including Blue Ocean (physical variables), Green Ocean (biogeochemical variables), and White Ocean (sea ice and polar regions). The GLOBAL_MULTIYEAR_PHY_001_030 product, a 1/12o spatial resolution, 50 vertical level global physical ocean reanalysis, utilizes the GLORYS12V1 ocean prediction system, integrating satellite data and in situ observations to create a coherent representation of the global ocean environment. The data period covers 1993-2021, enabling analysis of seasonal variations and medium-term trends, and is available in NetCDF format, adhering to CF-1.6 conventions. The study area is limited to the Northeastern Atlantic region near the African coast and Canary Islands, a region characterized by high biological productivity and influence on regional climate, spanning from January 1, 2014, to January 1, 2021. The data includes daily and monthly averages of three-dimensional variables (potential temperature, salinity, currents) and surface variables (sea level, sea ice), facilitating analysis of mesoscale variability and seasonal patterns.\n",
      "4 A Foundational Model for Forecasting the SST: This section details the Aurora model, a foundational model for atmospheric forecasting, adapted to oceanographic data. The model utilizes a local window attention mechanism and a 3D Swin Transformer to process heterogeneous data, generating long-term forecasts by iteratively applying a prediction function. The model‚Äôs scalability is achieved through parallel training and mixed precision, enabling practical implementation in operational contexts. The encoder transforms raw data into a unified representation, while the processor evolves this representation over time, capturing temporal relationships. The decoder restores the variables to their original resolution and levels, ensuring coherence and physical interpretability. The model‚Äôs architecture incorporates multiple configurations, differentiated by parameters and embedding sizes, and is adapted to a medium-sized model, facilitating balance between complexity and computational cost.\n",
      "5 Experimental Configuration: The study split a dataset annually between 2014 and 2018, validating from 2019 to 2020, and testing from 2020 to 2021, maintaining temporal order for studying oceanic phenomena. Data was divided into training (2014-2018), validation (2019), and testing (2020) sets, with a 70/15/15 split. The model used Mean Absolute Error (MAE) as a loss function, incorporating weighted variables and spatial dimensions to prioritize relevant data, and utilized weighted metrics for evaluation.\n",
      "6 Results: Here‚Äôs a concise summary of the paper‚Äôs results:\n",
      "\n",
      "The model was evaluated using a test set, with experiments focusing on fine-tuning, batch size adjustments, and training duration. Results showed improved performance with larger batch sizes (8) and longer training times (30), resulting in an RMSE of 0.134K.  The model captured relationships between anomalies, with an ACC of approximately 0.997, indicating good pattern recognition. Bias values were generally low, with a few exceptions, and the last fine-tuning yielded a significantly lower RMSE.  Inferences from the table and figure 7 demonstrate the model‚Äôs ability to handle complex coastal zones, where temperature variations are high and the model struggles with boundary regions.  Further training, incorporating high-resolution data and modeling techniques, could improve performance in these areas.  The long-term forecast results show increasing RMSE with extended time horizons, particularly during summer, highlighting the challenge of maintaining accuracy over extended periods.\n",
      "7 Discussion: This study explores deep learning models for oceanographic forecasting, leveraging prior knowledge and expanding modeling methodologies. While advancements are made, limitations remain regarding computational demands, data representativeness, model interpretability, and the incorporation of physical knowledge. Future research should focus on expanding data sources, enhancing model complexity through physics-informed approaches, and improving spatial resolution through progressive training strategies.\n",
      "8 Conclusion: The study demonstrated the feasibility of adapting the Aurora atmospheric model to oceanographic forecasting by successfully applying it to forecast local ocean temperature. Aurora‚Äôs ability to handle oceanic data, differing from atmospheric variables, showcases cross-domain adaptability, suggesting potential for future knowledge transfer.  Further, the model‚Äôs sensitivity to hyperparameter tuning and normalization highlights its flexibility for diverse oceanographic applications.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 3/40 [02:04<24:54, 40.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25590v1/1 Introduction\n",
      "../data/summaries/2510.25590v1/2 Related Work\n",
      "../data/summaries/2510.25590v1/3 Preliminary\n",
      "../data/summaries/2510.25590v1/4 Methodology\n",
      "../data/summaries/2510.25590v1/5 Experiment\n",
      "../data/summaries/2510.25590v1/6 Conclusion\n",
      "../data/summaries/2510.25590v1/References\n",
      "../data/summaries/2510.25590v1/Appendix A Pseudocode of RegionE\n",
      "../data/summaries/2510.25590v1/Appendix B Analysis of Adaptive Velocity Decay Cache\n",
      "../data/summaries/2510.25590v1/Appendix C Per-Task Visualization Results in the Benchmark\n",
      "../data/summaries/2510.25590v1/Appendix D Per-Task Quantitative Results in the Benchmark\n",
      "1 Introduction: RegionE introduces a training-free framework that reduces spatial and temporal redundancy in IIE models through adaptive region-aware generation and a velocity-decay cache. Experimental results demonstrate significant speedups on Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, while maintaining PSNR values of 30.520, 32.133, and 31.115, and negligible perceptual differences with GPT-4o.\n",
      "2 Related Work: Efficient diffusion models have been explored with various acceleration techniques, including pruning, quantization, distillation, sparse attention, and early stopping. Researchers have addressed temporal and spatial redundancy through methods like DeepCache, Œî \\Delta -DiT, FORA, TeaCache, and RAS. RegionE leverages IIE's trajectory characteristics, addressing both spatial and temporal redundancies in image editing.  A new IIE model, leveraging textual instructions, offers improved general-purpose editing capabilities.\n",
      "3 Preliminary: Flow Matching & Rectified Flow: The paper introduces a method for training diffusion models by learning a velocity field that guides the generation of images. This involves differentiating the equation d ‚Äã ùëø t d ‚Äã t = ùëø 1 ‚àí ùëø 0 \\frac{d\\bm{X}_{t}}{dt}=\\bm{X}_{1}-\\bm{X}_{0} , which iteratively refines the image. The rectified flow simplifies this process by assuming a linear velocity. The inference process starts with a source distribution and iteratively solves for the target distribution, using the learned velocity.  A discrete Euler sampler is employed for approximation.  The paper focuses on optimizing the inference process for T T iterations, as demonstrated by the Step1X-Edit model.\n",
      "4 Methodology: Here's a summary of the paper's sections:\n",
      "\n",
      "This paper introduces a method called RegionE to accelerate the IIE model without additional training. It utilizes three stages: Stabilization, Region-Aware Generation, and Smooth Stage. The Stabilization Stage stabilizes the initial sampling process, while the Region-Aware Generation stage adapts the image to the region, and the Smooth Stage smooths the boundaries between regions. The RegionE method employs a dynamic region partition and adaptive velocity decay to improve efficiency.\n",
      "5 Experiment: RegionE demonstrates superior performance compared to baseline methods across all evaluation metrics, achieving significant improvements in reconstruction quality, editing evaluation, and efficiency. The ablation studies reveal the importance of different components in the algorithm, validating its architectural agnosticism and providing robust quality validation.\n",
      "6 Conclusion: RegionE is a new generation framework that accelerates the IIE process through temporal and spatial redundancy reduction using ARP, RIKVCache, and AVDCache. Experiments show it achieves significant speedups on Step1X-Edit and FLUX1 Kontext, maintaining minimal bias and quality loss.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Pseudocode of RegionE: Appendix A contains pseudocode for the RegionE algorithm, outlining the core steps and data flow. It details the initialization, processing of input data, the application of specific transformations, and the final output.  The pseudocode covers the fundamental logic of the algorithm.\n",
      "Appendix B Analysis of Adaptive Velocity Decay Cache: The study analyzes residual caching in diffusion models.  The current residual cache and velocity cache are equivalent, with the coefficient before the velocity cache being less than one.  The solver determines the decay coefficient for the velocity cache, but the decay behavior is timestep-dependent and is corrected by an external coefficient. The AVDCache is reduced to the velocity cache when Œ≥ equals 1.\n",
      "Appendix C Per-Task Visualization Results in the Benchmark: Appendix C presents visualization results for 11 tasks on Step1X-Edit, Qwen-Image-Edit, FLUX.1, and the original dataset (Step1X-Edit). Figures 6 and 7 compare additional tasks and models, while Figures 9 and 10 compare tasks to the original dataset.\n",
      "Appendix D Per-Task Quantitative Results in the Benchmark: Table 13 presents quantitative results for RegionE and baseline methods on the 11 tasks within the benchmark. Table 14 presents quantitative results for the five tasks: Character Reference, Style Reference, Text Editing, Instruction Editing-Global, and Instruction Editing-Local.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 4/40 [02:28<20:29, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25007v1/1 Introduction\n",
      "../data/summaries/2510.25007v1/2 Related Work\n",
      "../data/summaries/2510.25007v1/3 Task Overview\n",
      "../data/summaries/2510.25007v1/4 The ProFees Model\n",
      "../data/summaries/2510.25007v1/5 Data Gathering and Annotation\n",
      "../data/summaries/2510.25007v1/6 Evaluation and Results\n",
      "../data/summaries/2510.25007v1/7 Conclusion and Future Work\n",
      "../data/summaries/2510.25007v1/Limitations\n",
      "../data/summaries/2510.25007v1/Ethical Considerations\n",
      "../data/summaries/2510.25007v1/Acknowledgement\n",
      "../data/summaries/2510.25007v1/References\n",
      "../data/summaries/2510.25007v1/Appendix A MDM Guidelines\n",
      "../data/summaries/2510.25007v1/Appendix B Ablation Studies\n",
      "../data/summaries/2510.25007v1/Appendix C High-Level Prompt Templates for CPT E_M Components\n",
      "../data/summaries/2510.25007v1/Appendix D Datasets\n",
      "../data/summaries/2510.25007v1/Appendix E Additional Experiments\n",
      "../data/summaries/2510.25007v1/Appendix F Synthetic Example\n",
      "1 Introduction: ProFees is an automated CPT E/M coding model developed to address inconsistencies and errors in coding, leveraging a dynamic few-shot prompting strategy, explicit criticism, and self-consistency techniques. It improves accuracy, efficiency, and transparency in E/M coding, demonstrating superior performance compared to commercial systems and a baseline model through empirical testing on a proprietary test dataset.\n",
      "2 Related Work: This paper reviews related work on CPT coding, highlighting the limitations of classical machine learning models in handling clinical narratives and the potential of large language models (LLMs) to improve coding accuracy. It discusses the challenges of using deep learning models for CPT coding, citing limited success compared to classical ML approaches. The paper then introduces a systematic framework for developing LLM-based CPT E/M coding, acknowledging its nascent stage of development.\n",
      "3 Task Overview: The paper analyzes CPT E/M coding accuracy, focusing on outpatient and preventive medicine encounters. The process involves assessing MDM levels based on patient visit complexity, data review, and risk factors, using a 2-out-of-3 rule. Strict exact-match accuracy is prioritized, as payers handle code denials and adjustments, impacting reimbursement. Near-miss codes have operational and financial implications, while systematic overcoding and undercoding create risks despite potential revenue gains.\n",
      "4 The ProFees Model: The ProFees model utilizes an LLM-based classifier and self-critics to predict encounter type and MDM level from clinical data, incorporating retrieval-augmented prompting, recursive self-critique, and rule-based logic. It enhances contextual relevance through a VDB of curated justifications and reasoning steps, ensuring guideline compliance and reducing hallucination. The model achieves high accuracy, explainability, and alignment with clinical workflows, fostering trust and deployment.\n",
      "5 Data Gathering and Annotation: We collected 216 real-world encounters from hospital production databases, using a larger dataset with coarse-level CPT labels as a reference. An internal medical expert annotated the E/M code and MDM element complexity for each encounter, using a finer-grained dataset. A high degree of disagreement between coders existed, with 52 agreements and 65 disagreements, resulting in a partitioned development dataset of Platinum and Disagreement subsets.\n",
      "6 Evaluation and Results: We combine Platinum and Disagreement subsets with 117 encounters, using them as a tuning dataset for both ProFees and baseline models. A Test dataset of 99 encounters was used for evaluation. The VDB was constructed with development and test datasets, employing a leave-one-out strategy and semantic dense retrieval with 3 N=3 exemplars. The study compared against System A (commercial tool) and four single-prompt variants, varying context and prompting strategies. Accuracy-based evaluation was performed at different granularity levels, with standard deviation reported across 5 runs. The intermediate complexity prediction was not available for System A, and the number of predictions per test encounter was set to 3, with a temperature of 0. The full model (Few-Shot + RCI) achieves the highest CPT accuracy (36.85% higher than System A), significantly surpassing the single prompt baseline (33.73% higher) and the single prompt baseline by 4.73%. RCI enhances MDM accuracy by 4% in the few-shot setting and 2.33% in the zero-shot setting, confirming the value of structured self-critique. The full model outperforms the strongest baseline on MDM, PC, DC, and RC accuracy by 8-17 absolute points, demonstrating its efficacy. A detailed ablation study is available in Appendix B.\n",
      "7 Conclusion and Future Work: This study presents a production-oriented automated CPT coding system using a modular architecture. The model improves CPT accuracy by 4.73% and outperforms commercial software by 30.61%. Future work includes expanding to multiple codes, CPT modifiers, and synthetic dataset generation.\n",
      "Limitations: ProFees currently predicts a single CPT E/M code per encounter. The team plans to expand this to support multiple codes, including preventive medicine visits, and will leverage an extended Encounter Type Classifier.  They are also creating a synthetic dataset to enhance model development and research.\n",
      "Ethical Considerations: Ethical considerations include privacy and data integrity (HIPAA-compliant development), accountability and liability (as an assistive tool, not autonomous), algorithmic biases and fairness (mitigated through synthetic data), interpretability and transparency (through multi-step reasoning and rationales), automation bias (risk of human reliance), and human oversight (UI/UX design for review).\n",
      "Acknowledgement: We thank Oracle Health & AI for collaboration and feedback on the deployment of ProFees in production, and thank Irfan Bulu, Raefer Gabriel, Neil Hauge, Mark Johnson, Kiran Rama, Amitabh Saikia, Vishal Vishnoi, and Krishnaram Kenthapadi for their insights.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A MDM Guidelines: Table 4 summarizes the MDM guidelines from the 2024 CPT Professional Manual, utilizing which ProFees were developed.\n",
      "Appendix B Ablation Studies: The study demonstrates that a dedicated post-hoc critic effectively detects and corrects intermediate errors, resulting in a 7.58% increase in MDM accuracy and a 4% increase in final CPT accuracy. Increasing the number of votes in self-consistency runs improves accuracy and cost, but at a higher unit cost.  The study also shows that replacing the verbose prompt with three CoT examples increases CPT accuracy by 4% relative to zero-shot CoT, and by 2.3% when comparing ProFees (Few-Shot) to Zero-Shot.\n",
      "Appendix C High-Level Prompt Templates for CPT E/M Components: Appendix C details template skeletons for LLM components, focusing on prompt design, output format, and dynamic input placeholders, while maintaining privacy and avoiding verbatim release of production prompts.\n",
      "Appendix D Datasets: The appendix presents detailed data on CPT E/M codes, physician specialties, and a data annotation interface for the Platinum, Disagreement, and Test datasets. Figures 4-9 illustrate the distribution of CPT E/M codes, specialty distribution, and the data annotation interface, respectively.\n",
      "Appendix E Additional Experiments: Appendix E includes additional experiments performed on the Test dataset, with the ProFees model performing well on the agreement subset, as verified by internal and external coders.\n",
      "Appendix F Synthetic Example: This section presents an example of data generated synthetically, mirroring the data employed in the study.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñé        | 5/40 [03:17<23:04, 39.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25388v1/1 Introduction\n",
      "../data/summaries/2510.25388v1/2 Foundations of automatic abstractions\n",
      "../data/summaries/2510.25388v1/3 Method\n",
      "../data/summaries/2510.25388v1/4 Experiment setup\n",
      "../data/summaries/2510.25388v1/5 Experiments\n",
      "../data/summaries/2510.25388v1/6 Conclusion, Limitations and Future Work\n",
      "../data/summaries/2510.25388v1/References\n",
      "../data/summaries/2510.25388v1/Appendix A Supplementary Materials\n",
      "1 Introduction: Research into non-learning-based decision-making algorithms, such as Monte Carlo Tree Search (MCTS), is an active field. MCTS can be used for applications where a general on-the-fly decision-making algorithm is needed, such as Game Studios which rarely use Machine Learning (ML) based AI as they would have to be retrained whenever the game rules are modified (e.g. during development or patches). One way to improve MCTS is to reduce the search space by grouping states and actions in the current MCTS search tree to enable intra-layer information flow (Jiang et al., 2014; Anand et al., 2015; 2016). One key strength of one of the state-of-the-art abstraction algorithms On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) (Anand et al., 2016) is its exactness in the sense that if OGA-UCT groups two state-action pairs in a search tree where all possible successors of each state-action pair have been sampled, only state-action pairs are grouped that have the same Q ‚àó Q^{*} value, i.e., they have the same value under subsequent optimal play. This exactness condition, however, comes at the cost that state or state-action pairs that only differ slightly in their Q ‚àó Q^{*} value cannot be detected. This issue was slightly alleviated with the introduction of ( Œµ a , Œµ t ) (\\varepsilon_{\\text{a}},\\varepsilon_{\\text{t}}) -OGA (Schm√∂cker et al., 2025c) , which is equivalent to ( Œµ a , 0 ) (\\varepsilon_{\\text{a}},0) -OGA in deterministic environments that allows for small errors in the immediate reward or transition function when building an abstraction. However, this also brings two downsides with it. Firstly, it introduces two parameters, which makes tuning harder, and secondly, grouping non-value equivalent state-action pairs might even be harmful to the performance, as they can make convergence to the optimal action impossible.\n",
      "\n",
      "In this work, we propose Known Value Differences Abstractions UCT (KVDA-UCT), that relaxes the strict abstraction conditions of OGA-UCT to detect almost as many abstractions as ( Œµ a , 0 ) (\\varepsilon_{\\text{a}},0) -OGA in deterministic environments but without losing the exactness condition. The novel idea that makes this possible is to deliberately group states or state-action pairs that do not have the same value if we know the difference between their values which is inferred by analysis of the search tree‚Äôs immediate rewards. When the abstractions are used, instead of averaging state-action pair values directly, their difference-accounted values are averaged. The contributions of this paper can be summarized as follows:\n",
      "1. We introduce the Known-Value-Difference (KVDA) abstraction framework that extends the Abstractions of State-Action Pairs (ASAP) framework used by OGA-UCT. Fig. 1 is an example of a simple state-transition graph in which KVDA finds three non-trivial abstractions, while ASAP would detect none.\n",
      "2. We propose and empirically evaluate KVDA-UCT, a modification of OGA-UCT that introduces no parameters and uses and builds KVDA abstractions. We show that KVDA-UCT outperforms OGA-UCT in most of the here-considered deterministic environments. We also compare KVDA-UCT with ( Œµ a , 0 ) (\\varepsilon_{\\text{a}},0) -OGA and show that it either performs equally well or better than a parameter-optimized ( Œµ a , 0 ) (\\varepsilon_{\\text{a}},0) -OGA agent.\n",
      "3. We also consider the stochastic setting where we generalize KVDA-UCT to Œµ t \\ Œµ_{\\text{t}} -KVDA which allows for errors in the transition function when building the abstraction. Furthermore, we compare it to ( Œµ t , Œµ a ) (\\varepsilon_{\\text{t}},\\varepsilon_{\\text{a}}) -OGA and show that, unlike in the deterministic setting, KVDA rarely performs better than ( Œµ t , Œµ a ) (\\varepsilon_{\\text{t}},\\varepsilon_{\\text{a}}) -OGA in stochastic environments.\n",
      "\n",
      "The paper is structured as follows: In Section 2, the theoretical groundwork for automatic state and state-action pair abstractions is laid. In particular, we define OGA-UCT and ( Œµ a , Œµ t ) (\\varepsilon_{\\text{a}},\\varepsilon_{\\text{t}}) -OGA. Then, in Section 3, our novel KVDA framework and both KVDA-UCT and Œµ t \\ Œµ_{\\text{t}} -KVDA are introduced. Afterwards, in Section 4, the experimental setup is defined, then in Section 5, the experimental results using this setup are shown and discussed. The paper is concluded by a discussion of the limitations of KVDA-UCT and avenues for future work in Section 6 .\n",
      "2 Foundations of automatic abstractions: Here‚Äôs a concise summary of the provided text:\n",
      "\n",
      "The paper introduces the theoretical foundations for automatic abstraction, focusing on the use of finite Markov Decision Processes (Sutton & Barto) as a model for sequential decision-making. It details the concept of state-action pairs (ASAPs) and the goal of identifying equivalent states and action pairs to improve MCTS efficiency through intra-layer information flow. The paper then discusses the Abstraction of State-Action Pairs (ASAP) framework, which iteratively constructs state abstractions and state-action pair abstractions, employing a recursive process to converge to a stable solution.  It also explores related research on automatic abstraction of transition functions, including techniques like pruning and refinement.\n",
      "3 Method: This section details the KVDA framework, which builds abstractions by grouping states and action pairs based on difference functions. The KVDA-UCT algorithm integrates this framework into MCTS, utilizing a modified search tree approach. The framework focuses on detecting more abstractions than ASAP, allowing for subtraction of differences during aggregation, and incorporates a representation of the state-action difference for improved decision-making.\n",
      "4 Experiment setup: This section details the experimental setup for evaluating KVDA-UCT against OGA-UCT in a variety of MDPs. The experiments were conducted on a diverse set of MDPs, including those from the International Probabilistic Planning Conference (Grzes et al., 2014), board games, and commonly used abstraction algorithm literature.  The MDPs were initially stochastic but were subsequently transformed into dense-reward MDPs through deterministic One-Step-Lookahead agents, with a heuristic V h V^{\\text{h}} defined for each board game.  The problem domains have varying reward scales, and the dynamic exploration factor Global Std (Schm√∂cker et al., 2025d) is used to control the exploration process.  The recency counter K = 3 K=3 is employed, and the evaluation process involves repeated runs with 99% confidence intervals of approximately 2 ‚ãÖ 2.33.  The normalized pairings score, calculated as the average of the first row of the matrix, is used for generalization testing. The implementation is released with the code compiled with g++ version 13.1.0 and the -O3 flag.\n",
      "5 Experiments: This section presents experimental results for KVDA-UCT across various settings, including comparisons to OGA-UCT, deterministic environments, and stochastic environments. KVDA-UCT demonstrates superior performance in deterministic settings, outperforming OGA-UCT and other methods, particularly in environments like SysAdmin and Wildfire.  KVDA-UCT detects more abstractions than OGA-UCT, and it outperforms all competitors in some environments, including those with multiple iterations.  The parameter-optimized KVDA-UCT achieves better performance than standard OGA-UCT, and the performance of KVDA-UCT is stable across different iteration budgets.  The study also investigates the generalization capabilities of KVDA-UCT, demonstrating its effectiveness in diverse environments.\n",
      "6 Conclusion, Limitations and Future Work: This paper evaluated KVDA-UCT against OGA-UCT and (Œµ a, 0) -OGA, demonstrating its superior generalization and parameter-optimized performance in deterministic settings.  It found limited success in stochastic settings, specifically with KVDA-UCT to Œµ t Œµ_{\\text{t}} -OGA, which outperforms both. Future work will focus on understanding the stochastic performance limitations and extending KVDA to multi-player games.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Supplementary Materials: This paper presents a comparative analysis of KVDA-UCT performance against OGA-UCT using Monte Carlo Tree Search (MCTS) with a directed acyclic graph (DAG) for 100 and 2000 iterations. KVDA-UCT demonstrates improved convergence speed compared to the baseline algorithm, with a median runtime overhead of less than 1% for 100 iterations and approximately 1% for 2000 iterations. The paper details the performance of the UCT algorithm, the UCB tree policy, and the heuristic functions for Connect 4, Constrictor, and Othello, utilizing parameter-optimized KVDA-UCT for the experiments. The presented results highlight the effectiveness of the MCTS approach and the impact of parameter adjustments on convergence speed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 6/40 [04:11<25:11, 44.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25770v1/1 INTRODUCTION\n",
      "../data/summaries/2510.25770v1/2 PROBLEM FORMULATION\n",
      "../data/summaries/2510.25770v1/3 BACKGROUND AND RELATED WORK\n",
      "../data/summaries/2510.25770v1/4 E-SCORES\n",
      "../data/summaries/2510.25770v1/5 EXPERIMENTAL RESULTS\n",
      "../data/summaries/2510.25770v1/6 THEORETICAL RESULTS\n",
      "../data/summaries/2510.25770v1/7 CONCLUSIONS\n",
      "../data/summaries/2510.25770v1/Appendix A THEORETICAL RESULTS\n",
      "../data/summaries/2510.25770v1/Appendix B EXPERIMENTAL RESULTS\n",
      "../data/summaries/2510.25770v1/Appendix C IMPLICIT P-SCORES IN RELATED WORKS\n",
      "1 INTRODUCTION: Here‚Äôs a summary of the paper‚Äôs sections:\n",
      "\n",
      "Introduction: Generative models, like LLMs, are prone to errors, requiring careful assessment. The example illustrates a failure of the LLM in a math problem, prompting research into post-hoc error assessment. This work proposes e-scores as a data-dependent measure of incorrectness, addressing the limitations of p-value-based conformal prediction.\n",
      "\n",
      "Section 2: Background and Related Work: This section provides context on LLM outputs, error assessment, and existing methods like p-value-based conformal prediction.\n",
      "\n",
      "Section 3: Definitions and Methodology: The paper defines e-scores, explains the e-value concept, and outlines the experimental setup.\n",
      "\n",
      "Section 4: e-Score Definition and Practical Evaluation: The paper details the e-score methodology, including the calculation of e-values and practical demonstration of its efficacy.\n",
      "\n",
      "Section 5: Experimental Results: The paper presents experimental results demonstrating the effectiveness of e-scores in mathematical factuality and property constraint satisfaction.\n",
      "\n",
      "Section 6: Theoretical Analysis: The paper provides a theoretical analysis of the e-score concept, including the generalization to various generative models and response sets.\n",
      "\n",
      "Section 7: Conclusion: The paper concludes with remarks on the problem and the proposed solution.\n",
      "2 PROBLEM FORMULATION: We define the prompt space ùí≥ \\mathcal{X} as a sub-space of language, and define the (in)correctness of generative model outputs as a measure of their correctness, utilizing a post-hoc statistical guarantee of the generated outputs.  The paper introduces a method for assessing the correctness of LLM responses, focusing on factuality and property constraints satisfaction, and defines a labeled response set.  The desideratum is to provide a score reflecting the user's tolerance level, with a ratio of error observation to tolerance level expected to be at most 1.\n",
      "3 BACKGROUND AND RELATED WORK: Here's a concise summary of the provided text:\n",
      "\n",
      "This work reviews recent research on post-hoc significance testing for generative models, drawing on previous work on p-values and e-values. It highlights adaptations of conformal prediction techniques, particularly in the context of filtering LLM outputs to construct response sets. The authors aim to develop a scoring system that generalizes across different generative models and response sets, extending beyond the results of previous studies.\n",
      "4 E-SCORES: We propose e-scores that compare test responses to incorrect calibration responses, utilizing a function f ‚àó f^{*} to approximate the oracle o. These e-scores are defined as the maximum incorrect response value, and are calibrated to the oracle o. The e-scores are derived from Gammerman et al. [1998] and are influenced by the non-conformity functions in Mohri and Hashimoto [2024]; Cherian et al. [2024]; Rubin-Toles et al. [2025].  The oracle estimator o ^ \\hat{o} approximates the oracle o o, a binary classification problem, and is trained to predict the probability of correctness, with a range of [0, 1] [0, 1].  The data used for training should be independent of test and calibration data.  The estimator operates at the response-level y, where y = (y1, y2, ...) and a response consists of multiple sub-responses y = (y1, y2, ...) .  To translate to a response-level prediction, it multiplies conditional probabilities.\n",
      "5 EXPERIMENTAL RESULTS: We demonstrate the efficacy of our proposed e-scores with two experimental use-cases. We discuss the use-cases individually and summarize the observed trends collectively. We begin by stating the baselines, the metrics for comparisons, the time and memory complexities, and the experimental setup. We perform a worst-case analysis in Appendix Àú B, where Œ± \\alpha is chosen to maximize the size distortion in Eq. Àú 3 . We compare against p-value based scores, or p-scores . Analogous to our e-scores in Eq. Àú 4 , the p-scores can be defined as the corresponding p-values, comparing the test response value to the incorrect calibration response values as relative ranks [Shafer and Vovk, 2008 ; Vovk et¬†al., 2022a ] . Mohri and Hashimoto [ 2024 ]; Cherian et¬†al. [ 2024 ]; Rubin-Toles et¬†al. [ 2025 ] use such p-scores implicitly to achieve Eq. Àú 2 , which we make explicit in Appendix Àú C . Due to the reliance on relative ranks, the choice of the transformed oracle estimator in Eq. Àú 5 does not matter, as they are monotonically increasing transformations of each other. We also compare against the transformed oracle estimators in Eq. Àú 5 directly, without conversion to e-/p-scores. These naive scores generally do not come with any statistical guarantees by themselves. We will use them for our worst-case analysis in Appendix Àú B . Our comparisons are based on the following. Size distortion. This is the most important metric, from our desideratum in Eq. Àú 3 . We report its empirical mean, which is expected to be ‚â§ 1 \\leq 1 . Error vs. Œ± \\alpha . While we aim to control size distortion, the expected error to Œ± \\alpha ratio, one might also be interested in the expected error and expected Œ± \\alpha individually. We report both empirical means. Precision vs. recall. We provide guarantees around the inclusion of an incorrect response. Simultaneouly, we want to experimentally validate that we do not exclude too many correct responses. For this reason, we report the empirical means of precision (fraction of correct responses among the ones filtered) and recall (fraction of correct responses that are filtered). We use the precision-recall curve for comparisons, where higher is better for both.\n",
      "\n",
      "\n",
      "6 THEORETICAL RESULTS: We generalize Eq. 8 to a super-set of responses, encompassing previously discussed response sets.  The largest set to consider when a single generated response is presented is the proposed super-set. Alternatives include sub-sets based on use-case, while maintaining guarantees. Eq. 1 is used as a benchmark for this super-set.  The worst-case analysis reveals that the size distortion, defined as Œ± Œ± maximizes, is minimized when Œ± is set to the smallest value such that the indicator function evaluates to 1. This simplifies to a requirement that the score is ‚â§ Œ± ‚â§ Œ±, ensuring the indicator function is 1.  The worst-case size distortion is then reduced to the difference between the minimum score and the required score to be 0, which is equivalent to the expression (min(y, c) ‚àà ùïÜ ‚Äã (x, g œÄ ‚Äã ( x ) ) : c = 0 ‚Å° s ‚Äã ( x , ùê≤ ) ) ‚àí 1.  This is an e-value, as defined by exchangeable data, and thus satisfies Eq. 3 under this worst-case setting.  Therefore, the proposed e-scores in Eqs. 4 and 6 provide a bound on the size distortion, as in Eq. 3, when the test and calibration prompts are exchangeable.\n",
      "7 CONCLUSIONS: We propose e-scores, based on e-values, to achieve post-hoc guarantees for generative model outputs. Experiments demonstrate these e-scores provide flexibility in tolerance levels and extend to large response sets, with the choice of oracle estimator impacting the results.  The use of pre-trained models offers a starting point, with potential for further training for specific use cases to enhance e-scores.  Koning [2024] discusses alternative error estimation candidates.\n",
      "Appendix A THEORETICAL RESULTS: The proposed e-scores in Eq. Àú 4 upper bound the size distortion by 1, as in Eq. Àú 3, due to the exchangeability assumption.  The worst-case size distortion simplifies to a maximum value achieved by setting Œ± to the smallest value such that the indicator function evaluates to 1, yielding a value of 0.  The derivation of Theorem Àú 1 involves re-stating the theorem and using the definition of the proposed e-scores.  The worst-case size distortion is bounded by the worst-case scenario, where the equality (i) (i) is achieved by setting Œ± to the smallest value such that the indicator function evaluates to 1, resulting in a value of 0.  The exchangeability assumption allows for a simplified analysis, and the combined e-scores in Eq. Àú 6 are used to further bound the size distortion.\n",
      "Appendix B EXPERIMENTAL RESULTS: The study analyzes worst-case size distortion using a uniform random sample, recovering p-scores as a special case. Randomized p-values correspond to exact p-values, while the reciprocal of the transformed oracle estimators provide a naive score, which is conservative and excludes correct and incorrect responses. The proposed e-scores reliably upper bound the worst-case distortion by 1, verifying the theory.\n",
      "Appendix C IMPLICIT P-SCORES IN RELATED WORKS: The paper presents a method to determine if a work is closely related to the current work by computing calibration values and using p-scores to identify works with similar calibration values. This approach achieves a threshold based on the user-defined alpha, and returns a filtered set of works if the calibration value exceeds this threshold.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 7/40 [05:16<28:01, 50.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25662v1/1. Introduction\n",
      "../data/summaries/2510.25662v1/2. Related Work\n",
      "../data/summaries/2510.25662v1/3. Research Method\n",
      "../data/summaries/2510.25662v1/4. Misconceptions Brainstorming\n",
      "../data/summaries/2510.25662v1/5. Qualitative Analysis of Conversational Logs\n",
      "../data/summaries/2510.25662v1/6. Results\n",
      "../data/summaries/2510.25662v1/7. Discussion\n",
      "../data/summaries/2510.25662v1/8. Limitations\n",
      "../data/summaries/2510.25662v1/9. Conclusions\n",
      "../data/summaries/2510.25662v1/References\n",
      "1. Introduction: This study investigates user misconceptions about conversational LLM-based programming tools, focusing on features like web access, model knowledge, and runtime limitations. Analysis of 500 Python conversations reveals misconceptions regarding these aspects, particularly during debugging and when prompting for code generation. The research aims to inform design choices to improve user understanding of these tools‚Äô capabilities.\n",
      "2. Related Work: Related work explores the tendency for over-trust in automated systems, particularly within programming contexts, and highlights the user-synthesizer gap. Previous studies suggest that less experienced programmers exhibit greater overconfidence in automated feedback, while user studies demonstrate a propensity for misusing AI-aided tools. Neural-network-based tools present a significant challenge for users due to opaque rules, sensitivity to prompt variations, and reliance on training data, compounded by LLMs' potential for fabrication. Computer science education literature also identifies programmer misconceptions about computers and systems as a target for intervention.\n",
      "3. Research Method: This study investigates user misconceptions regarding LLM-based programming tools, defining them as incorrect beliefs about tool affordances. It employs a brainstorming activity and a qualitative analysis of the WildChat dataset, focusing on conversational interactions with a ‚Äúvibe coding‚Äù assistant to identify potential misconceptions across providers and assistant versions. The study acknowledges limitations due to the indirect method and inference-based approach to misperception identification.\n",
      "4. Misconceptions Brainstorming: This paper explores user misconceptions about LLM-based conversational assistants, focusing on data storage, retrieval, and interaction methods. Users frequently assume databases, keyword lookup, or training data-driven approaches, leading to confusion about how these tools operate. Misconceptions center on knowledge base availability, session memory, and the model‚Äôs knowledge cutoff, impacting the reliability of LLM-based software library questions. The paper also addresses LLM misconceptions concerning hallucinations, meta-information, and the model‚Äôs context window size, highlighting challenges in interpreting responses and the predictability of outputs.\n",
      "5. Qualitative Analysis of Conversational Logs: The WildChat dataset, comprising over 1 million conversations, focuses on programming-related interactions with a free chatbot. Analysis reveals Python, Java, SQL, and JavaScript are prevalent languages, with Python being the most common (23,353).  The study identifies misconceptions through open coding, filtering conversations based on code block formatting and language tags. A codebook was developed for conversational LLM interactions, focusing on potential misconceptions and user intent. Cohen‚Äôs Kappa was used to assess inter-rater reliability, with a Gold dataset of 50 conversations created to ensure a robust analysis.\n",
      "6. Results: 6.1. Programming-Related Conversations: Annotators identified ‚Äúgenerate code‚Äù as the most frequent intention, followed by ‚Äúdebugging‚Äù involving help-seeking behavior and varied information about program issues (errors, traceback, message details). Code validation and brainstorming were also common.\n",
      "\n",
      "6.2. Results of Labeling Misconceptions: The labeling of misconceptions yielded inconsistent inter-rater reliability, with a low level of agreement among annotators after multiple rounds of coding. Common causes included difficult language, copy-paste homework instructions, and the tendency of the codebook to flag plausible misconceptions. Despite disagreements, the Gold dataset‚Äôs codebook demonstrated reasonable consistency in identifying misconceptions, with some cases exhibiting strong consensus among annotators.\n",
      "\n",
      "6.3. High-Agreement Misconceptions: The chatbot struggled to generate functional programs due to a lack of specific dataset details, requiring diverse interaction styles including coding, data analysis, and model optimization. It misinterpreted requests for non-text output, including code execution tasks and data analysis requests, and exhibited a tendency to accept code snippets as solutions. The model recognized a previous session reference and responded to a homework assignment, while also handling requests to use the latest versions of libraries and APIs, reflecting a focus on continuous training.\n",
      "\n",
      "6.4. Potential But Unconfirmed Misconceptions: Here‚Äôs a concise summary of the paper‚Äôs sections:\n",
      "\n",
      "During review of conversation logs, users frequently request the removal of all bugs from programs, expecting automated bug fixing. Users also seek code that doesn‚Äôt produce errors and may ask for code to resolve previously discussed bugs. The model‚Äôs limited ability to provide solutions due to lack of dynamic information, such as traceback messages, and the potential for scenarios where debugging information cannot be obtained, leads to user expectations regarding optimization requests. Users also frequently request code modifications, particularly for longer programs, and the model‚Äôs default behavior suggests a trust in reliably reproducing the entire program, despite the context window limitations.\n",
      "7. Discussion: Here‚Äôs a concise summary of the section:\n",
      "\n",
      "The discussion highlights that users frequently misunderstand the affordances of LLM-assisted tools due to misconceptions about tool-level differences and the inherent complexity of LLM models. The WildChat dataset revealed user confusion regarding web access, code execution, and context window limitations, suggesting a need for clearer communication about available features. Users may attempt to infer system commands, but these are unreliable. The model card for the underlying models, particularly OpenAI‚Äôs API, offers a potential solution by providing clear labeling of features, while the WildChat creators note GPT-4‚Äôs self-identification as GPT-3 due to its training data cut-off, necessitating explicit communication through alternative channels.\n",
      "8. Limitations: Several threats to validity exist, including biased data due to authors‚Äô backgrounds and positionality, a non-representative WildChat dataset (users likely from the IT community), imperfect inferring user intentions from interaction logs, and the dataset‚Äôs age (collection ended in May 2024).  Future usefulness may be limited as conversational LLM tools have evolved.\n",
      "9. Conclusions: Based on the research, further studies should focus on surveying users about specific tool affordances, scaling user log data with automated detection using an LLM, and conducting interviews/open-ended surveys to explore misconceptions.  Additional research should examine how users react to LLM tool responses during debugging sessions and utilize behavioral experiments to measure sensitivity to LLM properties.  The rapid evolution of LLM capabilities, tool availability, and user literacy necessitates developing generalizable theories about user assumptions.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 8/40 [06:21<29:38, 55.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25729v1/I Introduction\n",
      "../data/summaries/2510.25729v1/II Methodology\n",
      "../data/summaries/2510.25729v1/III Results\n",
      "../data/summaries/2510.25729v1/IV Conclusion\n",
      "../data/summaries/2510.25729v1/V Acknowledgment\n",
      "../data/summaries/2510.25729v1/References\n",
      "I Introduction: Diffusion-based generative model demonstrates improved reconstruction accuracy and stability compared to deterministic supervised networks, particularly in challenging scenarios involving noise and calibration errors. The physics-informed validation ensures physically consistent solutions, enhancing robustness and generalization to real-world MWI data.\n",
      "II Methodology: Diffusion models approximate a target distribution by iteratively adding noise and removing it. Sampling involves denoising back to a clean sample, utilizing a neural network to approximate the score function. The model‚Äôs flexibility makes it suitable for inverse problems, leveraging a Bayesian framework to sample from a posterior distribution combining a learned score function with the physics of the measurement process.\n",
      "III Results: The latent diffusion model was trained on synthetic data from DataSet1 and subsequently evaluated using unseen synthetic samples differing in position and orientation from the training set. The model consistently reproduced the target permittivity distribution with minimal reconstruction error, demonstrating its stability and robustness to stochastic variation in the sampling process. 100 candidate permittivity maps were generated, and the final reconstruction yielded the lowest mean squared error between predicted and ground-truth scattered fields, indicating a close match in both material properties and electromagnetic behavior.\n",
      "\n",
      "To identify the best reconstruction, the diffusion model generated 100 candidate permittivity maps, each evaluated using a forward electromagnetic solver to compute the corresponding scattered fields. The choice of 100 reconstructions was arbitrary and could be adjusted based on the time or computational constraints of each application. The selected reconstruction yielded the lowest mean squared error (MSE) between the predicted and ground-truth scattered fields, indicating a close match in both material properties and resulting electromagnetic behavior.\n",
      "\n",
      "The model was subsequently evaluated on experimental scattered-field measurements from DataSet1. The model achieved promising reconstruction performance, successfully recovering key structural features from previously unseen experimental samples. Although the reconstructed permittivity maps were less accurate than those obtained when testing synthetic data, the results demonstrate the model's capacity to adapt to experimental measurements, even though it was trained solely on synthetic data.\n",
      "\n",
      "This generalization result is notable given the significant differences between synthetic and experimental measurements, particularly for the near-field electromagnetic imaging system from which the experimental data was obtained [ 45 ]. Near-field systems, especially those utilizing co-resident antennas, exhibit field behaviors that can differ significantly from the fields generated using, necessarily approximate, computational models. For example, the synthetic training data was generated using the incident fields of idealized point sources (line-soures) and the received fields were simply taken to be scattered fields at precise point-locations. In contrast, the experimental data was collected as microwave scattering parameters (S-parameters) at the antenna ports. An antenna effectively integrates the spatially varying fields across its aperture. In addition, the mutual-coupling with nonactive antennas is ignored in the synthetically generated data. Despite these discrepancies, our ML model was able to deliver strong reconstruction performance. Of course, this is partly due to the scattered-field calibration procedure that was implemented [ 46 , 45 ] . Incorporating the calibration procedure into a machine learning model of its own has been attempted in the past and is a research subject of the greatest importance.\n",
      "\n",
      "The model was re-trained using this newly generated dataset and tested on previously unseen samples. Representative reconstructions and their corresponding ground-truth targets are shown in Figure 7 . As expected, the inverse problem associated with this more complex, multi-object dataset presents increased difficulty compared to Dataset 1, but the model successfully reconstructed the underlying permittivity distributions, effectively capturing both the global structure and spatial organization of the objects.\n",
      "\n",
      "To further enhance the model‚Äôs capacity to learn the mapping between scattered-field measurements and corresponding permittivity distributions, we investigated the integration of multi-frequency data. Specifically, scattered-field measurements were acquired at five frequencies: 3.0, 3.5, 4.0, 4.5, and 5.0 GHz. Figure 8 illustrates the model‚Äôs performance when conditioned on multi-frequency data, demonstrating improved reconstruction consistency and sharper structural recovery across test samples.\n",
      "\n",
      "The results indicate that while the single-frequency model occasionally struggles to accurately reconstruct the shapes of closely spaced or sharp-edged objects‚Äîsometimes confusing U-shaped targets with hollow circles (see rows 2 and 4 of Fig. 7 )‚Äîthe multi-frequency model performs substantially better. By leveraging additional frequency-domain information, it accurately resolves fine structural details and preserves object boundaries even in complex spatial configurations.\n",
      "\n",
      "This improvement is quantitatively reflected in the last two rows of Table I , where the average MSE image \\text{MSE}_{\\text{image}} values for the multi-frequency case decrease to approximately one-third of the single-frequency error. The averages were computed over 10 test cases, with each case reconstructed 100 times to select the best reconstruction based on the MSE data \\text{MSE}_{\\text{data}} .\n",
      "\n",
      "These findings highlight the potential of incorporating richer physical information‚Äîbeyond frequency diversity‚Äîto improve data representation and model robustness. Future research will explore additional labeled datasets encompassing a more comprehensive set of composite scattering targets to further enhance the model‚Äôs capacity for accurate and stable inversion in challenging electromagnetic imaging scenarios.\n",
      "\n",
      "IV Conclusion: This work presents a physics-informed diffusion model for microwave inverse scattering, accurately reconstructing complex permittivity distributions from synthetic data and successfully recovering key structural features from experimental scattered-field measurements. It outperforms state-of-the-art methods, achieving improved reconstruction error and generalization across diverse measurement domains, and incorporates a diverse synthetic dataset to enhance robustness.  The autoencoder component contributed to increased reconstruction error, which was addressed through hyperparameter tuning.\n",
      "V Acknowledgment: The authors acknowledge assistance from Mr. Seth Cathers with the forward solver.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñé       | 9/40 [07:09<27:24, 53.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25005v1/1 Introduction\n",
      "../data/summaries/2510.25005v1/2 Background and Problem Setup\n",
      "../data/summaries/2510.25005v1/3 Theory\n",
      "../data/summaries/2510.25005v1/4 Example\n",
      "../data/summaries/2510.25005v1/5 Limitations\n",
      "../data/summaries/2510.25005v1/6  Conclusion and Future Work\n",
      "../data/summaries/2510.25005v1/Acknowledgments and Disclosure of Funding\n",
      "../data/summaries/2510.25005v1/References\n",
      "../data/summaries/2510.25005v1/Appendix A Supplementary Material\n",
      "../data/summaries/2510.25005v1/References\n",
      "1 Introduction: This work introduces a theoretical framework for counterfactual reasoning in cyclic causal models, utilizing shift-scale interventions. It demonstrates the uniqueness of simple cyclic models under contraction conditions, proves the well-posedness of counterfactual queries under bounded scale interventions, establishes compositionality and algebraic stability, and derives sub-Gaussian tail bounds for counterfactual functionals, emphasizing sharp concentration of outcomes.\n",
      "2 Background and Problem Setup: This section describes a structural causal model (SCM) called ‚Ñ≥ \\mathcal{M} which is defined by a twin SCM, a shift-scale intervention, and a counterfactual distribution. The SCM is uniquely solvable with respect to a subset of coordinates, and the twin SCM is defined by a mapping f : ‚Ñê \\mathcal{I} ‚Üí ‚Ñ∞ \\mathcal{E} and a measure function f : ‚Ñ∞ \\mathcal{E} ‚Üí ‚Ñ≥ \\mathcal{M} . The twin SCM is constructed by applying a perfect intervention to the twin SCM, resulting in a counterfactual distribution of the original state. The text also discusses the semantics of cyclic SCMs, highlighting the alternative interpretation derived from differential equations and emphasizing the framework's applicability to various examples.\n",
      "3 Theory: Okay, let's break down the key takeaways and the overall argument of this text.\n",
      "\n",
      "**Core Idea:** The text is discussing the convergence of a sequence of measures (likely related to a differential operator) to a specific point, and the implications of this convergence for the model's behavior.  It‚Äôs essentially a mathematical argument about the stability and predictability of a system.\n",
      "\n",
      "**Here's a breakdown of the key points:**\n",
      "\n",
      "1. **The Setup:** The text begins by introducing a system of measures (let's call them `E`) that are being considered. These measures are being iterated, and the goal is to find a point `u` where the iterated measures converge.\n",
      "\n",
      "2. **The Convergence:** The core of the argument is that this convergence process leads to a system of measures that are *stable* and *predictable*.  This stability is crucial because it suggests the system is well-behaved.\n",
      "\n",
      "3. **The \"Lipschitz\" Property:** The text emphasizes the importance of the \"Lipschitz\" property.  This is a mathematical concept that means a function (in this case, the iterated measures) is bounded ‚Äì it doesn't grow too large or too small.  The Lipschitz property is a key factor in ensuring stability.\n",
      "\n",
      "4. **The Picard Iteration:** The text mentions a technique called \"Picard iteration.\" This is a method used to iteratively refine a measure, and the convergence of the iterated measures is a central point of interest.\n",
      "\n",
      "5. **The Significance of the Convergence:** The text argues that the convergence of the iterated measures *implies* that the system is *simple* ‚Äì meaning it's predictable and doesn't have too many possible states or paths.\n",
      "\n",
      "**Why is this important?**\n",
      "\n",
      "* **Modeling:**  This type of convergence is often used in modeling systems where the system's behavior is influenced by a sequence of steps or iterations.\n",
      "* **Stability Analysis:**  The convergence is a key step in analyzing the stability of a system.  If the system converges to a stable point, it's likely to remain stable over time.\n",
      "* **Predictability:**  The \"simple\" nature of the system is a desirable property, as it allows for better prediction of the system's future behavior.\n",
      "\n",
      "**In short, the text is exploring how a sequence of measures converges to a point, and that convergence is linked to the system's stability and predictability.**\n",
      "\n",
      "**To help me understand your question better, could you tell me:**\n",
      "\n",
      "*   What specifically are you interested in about this text? Are you trying to understand a particular concept or step?\n",
      "*   Are you working on a related problem or task?\n",
      "4 Example: The twin SCM model simulates a linear cyclic SCM with mutual dependence between consumption and income. A fiscal policy reform reduces consumption and random shocks by a factor of 0.8, resulting in a fixed income supplement of 1.0. This leads to a scaling effect, where the exogenous term E' is calculated as a matrix with a 0.04 coefficient and a 0.0256 coefficient. The contraction is preserved, resulting in a unique interventional equilibrium where consumption rises by approximately 29% and income rises by approximately 82%. The C-C ‚Äì I-I correlation decreases from 0.75 to 0.69.\n",
      "5 Limitations: The Œ∫-contraction condition must be uniform across the state space, which may be violated by realistic feedback systems. Gaussian noise assumptions lead to polynomial concentration, which is not analyzed. Banach‚Äôs theorem on closed domains is invoked for models with open domains, requiring additional consideration. Closure results cover shift-scale interventions, which generalizes hard interventions and provides a foundation for broader extensions. The preservation of global contractivity ‚Äìthat is, the Lipschitz constant remains below one ‚Äì guarantees unique solvability and allows for richer intervention types, provided they don‚Äôt destroy contractivity.\n",
      "6  Conclusion and Future Work: We demonstrated that contraction-based cyclic structural causal models are simple, solvable, and exhibit closed-form solutions for counterfactual functionals under natural regularity assumptions. Future research could explore deep generative models for this class.\n",
      "Acknowledgments and Disclosure of Funding: Funding received from the Indo-French Centre for the Promotion of Advanced Research (IFCPAR/CEFIPRA) through project number CSRP 6702-2.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Supplementary Material: The twin SCM provides a unique counterfactual distribution, identical to Pearl‚Äôs method, derived from the twin SCM‚Äôs posterior.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 10/40 [07:47<24:18, 48.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25758v1/1 Introduction\n",
      "../data/summaries/2510.25758v1/2 Related Work\n",
      "../data/summaries/2510.25758v1/3 Methodology\n",
      "../data/summaries/2510.25758v1/4 Experiments\n",
      "../data/summaries/2510.25758v1/5 Conclusion\n",
      "../data/summaries/2510.25758v1/References\n",
      "../data/summaries/2510.25758v1/Appendix A Human Evaluation of Pairwise Response Selection\n",
      "../data/summaries/2510.25758v1/Appendix B Human Agreement Analysis\n",
      "../data/summaries/2510.25758v1/Appendix C Ethical Considerations\n",
      "../data/summaries/2510.25758v1/Appendix D Open-source Models\n",
      "../data/summaries/2510.25758v1/Appendix E Expert Evaluation\n",
      "../data/summaries/2510.25758v1/Appendix F Prompts for TheraMind\n",
      "../data/summaries/2510.25758v1/Appendix G TheraMind of Counseling Strategies\n",
      "1 Introduction: TheraMind introduces a dual-loop agent architecture for longitudinal psychological counseling, demonstrating improved efficacy through adaptive therapy selection and a novel dialogue management system. The agent‚Äôs architecture facilitates a more strategic and holistic approach to counseling, moving beyond reactive dialogue to achieve clinically sound, personalized therapeutic interactions.\n",
      "2 Related Work: Recent advancements in large language models (LLMs) are driving innovation in counseling agents, focusing on adaptive, multi-session support. Existing approaches utilize supervised fine-tuning, reinforcement learning, and preference optimization to enhance model capabilities.  Efforts have centered on specialized agents like ChatCounselor, CPsyCoun, MentaLLaMA, PsyDial, and PsyDT, incorporating memory, reasoning, and tool use.  Furthermore, LLM-driven autonomous agents are being adapted for healthcare and mental health tasks, leveraging architectures like memory-augmented and hierarchical designs.\n",
      "3 Methodology: TheraMind is an autonomous agent framework designed for multi-session psychotherapy using a dual-loop architecture. The Intra-Session Loop manages real-time dialogue, while the Cross-Session Loop dynamically selects therapeutic methods based on session history and efficacy. A perception module analyzes patient utterances to infer psychological state, enabling a more nuanced interaction than traditional LLMs. The agent generates deliberate responses, integrating tactical strategy and clinical objectives, and dynamically adapts to patient emotional variability through a self-assessment module. A high-fidelity simulation environment, grounded in anonymized CPsyCounR data, provides a robust testbed for evaluating long-term counseling skills.\n",
      "4 Experiments: Here is a concise summary of the provided text:\n",
      "\n",
      "TheraMind demonstrates significant advancements in evaluating counseling agents through a dual-loop architecture, significantly outperforming baseline methods across multiple metrics. The agent effectively maintains long-term, clinically-grounded therapeutic relationships by integrating memory retrieval and strategic adaptation.  It achieves high accuracy in single and multi-session evaluations, particularly in establishing trust and guiding the therapeutic process.  Human evaluations confirm the agent‚Äôs robust internal validity and clinical reasoning, highlighting its ability to model a structured longitudinal arc.\n",
      "5 Conclusion: TheraMind is a new agent that overcomes limitations of previous work by decoupling tactical dialogue management from strategic planning, allowing for longitudinal counseling and superior long-term coherence and flexibility.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Human Evaluation of Pairwise Response Selection: Randomly selected pairs of cases from ten categories were evaluated pairwise using a self-designed scoring platform. TheraMind outperformed PsyDTLLM, DeepSeek-V3, and Interactive Agents in the twenty samples, demonstrating better counseling effects across various psychological counseling scenarios. Cohen‚Äôs Œ∫ \\ kappa analysis confirmed the reliability of human evaluation.\n",
      "Appendix B Human Agreement Analysis: The annotators evaluated TheraMind‚Äôs three decision-making modules (Memory Invoke, Emotion Recognition, and Response Strategy) using a self-designed scoring platform. The evaluation showed high fidelity with human clinical reasoning, with Emotion Recognition achieving 95.59% and Memory Invoke 94.12%. The Response Strategy module achieved 85.29% agreement. Inter-rater reliability was high (0.67).\n",
      "Appendix C Ethical Considerations: Data privacy was ensured through anonymization and ethical guidelines, while annotator compensation was set at a minimum wage level. Evaluation bias was addressed through limited human expert assessments.\n",
      "Appendix D Open-source Models: The appendix lists several open-source language models, including ChatCounselor, CPsyCounX, GLM-4-9B-Chat, InterLM2.5-7B-Chat, Qwen3-8B, Kimi-Dev-72B, Doubao-1.5-pro-32k, Yi-Large, and PsyDTLLM.\n",
      "Appendix E Expert Evaluation: TheraMind demonstrates superior performance compared to PsyTLLM, DeepSeeker-V3, and Interactive Agents through a case study analysis. TheraMind regulates pace, adapts strategies, and centers the patient‚Äôs feelings, leading to more coherent and durable therapeutic progress.\n",
      "Appendix F Prompts for TheraMind: TheraMind utilizes a series of prompts for psychological counseling, including emotion perception, therapy selection, adjustment, memory retrieval, resistance detection, stage analysis, and response strategy determination. Dialogue generation, session termination, and evaluation are handled by specialized prompts.\n",
      "Appendix G TheraMind of Counseling Strategies: TheraMind comprises 12 counseling strategies, categorized into eight passive supporting strategies and four proactive challenging strategies. Table 3 and Table 4 illustrate the definitions and real-world examples of each strategy within simulated therapeutic interactions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 11/40 [08:33<23:05, 47.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25445v1/1 Introduction\n",
      "../data/summaries/2510.25445v1/2 Theoretical Foundations: Mapping the Dual Lineages of Agentic Intelligence\n",
      "../data/summaries/2510.25445v1/3 Methodology\n",
      "../data/summaries/2510.25445v1/4 Literature Review: A Dual-Paradigm Analysis\n",
      "../data/summaries/2510.25445v1/5 Analysis of Domain-Specific Applications\n",
      "../data/summaries/2510.25445v1/6 Comprehensive Taxonomy of Agentic AI Literature: A Paradigm-Aware Analysis\n",
      "../data/summaries/2510.25445v1/7 Ethical and Governance Challenges: A Paradigm-Specific Analysis\n",
      "../data/summaries/2510.25445v1/8 Research Gaps: A Paradigm-Specific Roadmap\n",
      "../data/summaries/2510.25445v1/9 Future Directions: The Path to Hybrid Intelligence\n",
      "../data/summaries/2510.25445v1/10 Conclusion\n",
      "../data/summaries/2510.25445v1/References\n",
      "1 Introduction: The paper reviews the evolution of Artificial Intelligence, starting with the foundational concept of autonomous systems, then transitioning through symbolic AI, machine learning, deep learning, and generative AI, culminating in the current era of large language models. It introduces a dual-paradigm taxonomy ‚Äì symbolic vs. neural ‚Äì to analyze the field‚Äôs architectural foundations and agency, highlighting key conceptual schisms and gaps in current research. The paper employs a systematic literature review and taxonomy creation to provide a comprehensive analysis of agentic AI, addressing challenges related to retrofitting symbolic paradigms into neural architectures and identifying critical research gaps.\n",
      "2 Theoretical Foundations: Mapping the Dual Lineages of Agentic Intelligence: The paper traces the evolution of AI, starting with the symbolic lineage emphasizing explicit logic and deterministic models, culminating in the neural lineage based on statistical learning and emergent reasoning through large language models. The symbolic lineage provides foundational autonomous systems, while the neural lineage utilizes DRL to create generative pipelines and multi-agent orchestration, ultimately establishing a new paradigm of LLM-driven systems.\n",
      "3 Methodology: This review examines agentic AI research across symbolic and neural paradigms, analyzing evolution, frameworks, and ethical challenges. It employs a systematic search strategy, including database searches and pre-print analysis, classifying studies based on architectural lineage and incorporating thematic synthesis. The study utilizes a multi-faceted approach to analyze key concepts and limitations, including a supplementary contextual analysis of foundational symbolic papers.\n",
      "4 Literature Review: A Dual-Paradigm Analysis: The rapid expansion of Agentic AI has produced a diverse yet fragmented body of research. This section synthesizes the extant literature through the lens of the dual-paradigm framework introduced in Figure 2, analyzing how contributions are distributed across foundational studies, architectural frameworks, and domain-specific applications, focusing on their operational mechanisms to clearly delineate the paradigm shift. Landmark studies have shaped the conceptual and architectural foundations of both paradigms, spanning strategic reasoning, cognitive models, and alignment. These studies collectively mark the progression from explicit, algorithmic deliberation to emergent, stochastic intelligence. They serve as reference points for the fundamental differences in how adaptability, coordination, and strategic reasoning are implemented in each paradigm, illustrating the conceptual divide captured by our framework. The theoretical bedrock of Agentic AI is found in two distinct lineages, each with its own foundational breakthroughs. Landmark studies have shaped the conceptual and architectural foundations of both paradigms, spanning strategic reasoning, cognitive models, and alignment. These studies collectively mark the progression from explicit, algorithmic deliberation to emergent, stochastic intelligence. They serve as reference points for the fundamental differences in how adaptability, coordination, and strategic reasoning are implemented in each paradigm, illustrating the conceptual divide captured by our framework. The advent of large language models (LLMs) has solidified the neural/generative paradigm, which operates on principles fundamentally incompatible with its symbolic predecessor. Modern agentic frameworks leverage LLMs as generative engines within software pipelines, explicitly departing from classical cognitive loops. Their core innovation lies in dynamic context management, prompt engineering, and tool composition. This analysis underscores that these frameworks form the backbone of the neural paradigm, designed for practical task completion through orchestration, not for simulating internal cognitive processes. Mapping them to PPAR or BDI obscures their true innovative mechanics, which are defined by prompt-driven stochasticity, not algorithmic symbol manipulation. Agentic AI frameworks are being deployed across sectors where autonomy and adaptability are essential. The choice of paradigm is critically influenced by domain-specific constraints‚Äîethical, regulatory, or epistemic. The following implementations exemplify how each paradigm is applied. In healthcare, where safety and compliance are paramount, applications diverge clearly along architectural lines. Symbolic systems, such as rule-based clinical decision support tools, are predominantly employed for predictable and auditable tasks. In contrast, the flexibility of neural paradigms is leveraged for tasks like generating structured medical reports [ 66 ] and powering on-premise edge agents [ 87 ] ; however, these neural frameworks are often contained within deterministic tool-chaining pipelines to ensure the reliability required in clinical settings. This pattern of complementary paradigm use is also evident in finance, a domain demanding high accuracy and auditability. Here, neural frameworks dominate tasks involving complex data synthesis and analysis. For instance, CrewAI‚Äôs role-based workflow is applied to market analysis [ 43 ] as it provides a clear, auditable trail of agent actions. Similarly, LlamaIndex-powered models for financial sentiment [ 79 ] demonstrate how neural systems use Retrieval-Augmented Generation (RAG) to ground their stochastic outputs in verified data, thereby reducing hallucination. Despite this, symbolic systems maintain a critical role in high-frequency trading and core regulatory logic where absolute determinism is non-negotiable. Finally, in scientific research, which requires profound epistemic rigor, the choice of paradigm is dictated by the nature of the intellectual task. The deployment of AutoGen to coordinate multi-agent conversations for economic research [ 69 ] exemplifies the neural paradigm‚Äôs strength in simulating collaborative, exploratory discovery and critique. This stands in contrast to the limitations of pure neural approaches, such as hallucination and lack of verifiability. This synergistic combination of symbolic and neural approaches represents a shift from debating paradigm superiority to targeted solutions for adaptability, reliability, and long-term operation in complex environments.\n",
      "5 Analysis of Domain-Specific Applications: The analysis reveals a paradigm-driven approach to agentic AI, where architectural choices are shaped by domain-specific constraints. The symbolic paradigm favors rigid, verifiable rules and auditability, while the neural paradigm leverages generative flexibility but carries risks of hallucination. Hybrid approaches, combining symbolic and neural capabilities, are prevalent, requiring careful governance to mitigate risks across security, resilience, and human-AI collaboration. The framework highlights the need for tailored governance across each paradigm, emphasizing the importance of tool integration and the distinct challenges posed by each architectural choice.\n",
      "6 Comprehensive Taxonomy of Agentic AI Literature: A Paradigm-Aware Analysis: The paper presents a taxonomy of agentic AI literature categorized by architectural paradigms, revealing a market fit for symbolic and hybrid systems in safety-critical applications and data-rich domains, alongside a governance imbalance regarding neural systems. The taxonomy identifies a shift from symbolic to neural orchestration, with ethical challenges in the neural paradigm driving research toward hybrid approaches, ultimately validating the framework as a robust tool for future research.\n",
      "7 Ethical and Governance Challenges: A Paradigm-Specific Analysis: The paper argues that current governance approaches are insufficient for Agentic AI, necessitating a paradigm-specific analysis. This analysis reveals key challenges including the bifurcation of ethical concerns across symbolic and neural paradigms, complex governance implications, liability issues, and the need for human oversight that differs significantly based on system autonomy. A taxonomy of agency levels is proposed, emphasizing governance scales with autonomy, and policy frameworks must extend beyond risk analysis to account for the unique governance needs of Agentic AI.\n",
      "8 Research Gaps: A Paradigm-Specific Roadmap: The research highlights a critical gap in research methodologies ‚Äì the lack of Paradigm-Aware Research. Future work should focus on integrating paradigms, particularly the ‚ÄúReasoning & Adaptability‚Äù gap, and develop separate evaluation suites to stress-test each architecture‚Äôs failure modes. Effective governance requires collaboration between engineers and policymakers to create tailored frameworks for auditing and regulating these fundamentally different technologies, balancing innovation with risk mitigation.\n",
      "9 Future Directions: The Path to Hybrid Intelligence: This paradigm-aware approach to design seeks to integrate symbolic reasoning and neural generation, focusing on Architectural Integration, Neuro-Symbolic Integration, specialized agent roles, and governance mechanisms. Convergence with other technologies will further enhance capabilities, ultimately aiming for adaptable and reliable hybrid intelligence.\n",
      "10 Conclusion: This review establishes two distinct lineages of Agentic AI ‚Äì symbolic and neural ‚Äì with differing operational mechanics and strengths. The common practice of conceptual retrofitting obscures their true nature and hinders progress. The framework proposes a hybrid approach, integrating symbolic and neural systems through neuro-symbolic architectures, to address ethical challenges and ensure a coherent theory of architectural design.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 12/40 [09:36<24:24, 52.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25683v1/1 Introduction\n",
      "../data/summaries/2510.25683v1/2 Methods\n",
      "../data/summaries/2510.25683v1/3 Case Study and Results\n",
      "../data/summaries/2510.25683v1/4 Conclusions\n",
      "../data/summaries/2510.25683v1/References\n",
      "1 Introduction: Graph Network-based Structural Simulator (GNSS) introduces a GNN framework for surrogate modeling of structural dynamics, particularly for guided wave simulations. It combines node-fixed nodal kinematics, a sign-aware acceleration loss, and a wavelength-informed connectivity radius to stabilize finite-difference velocities and improve long-horizon stability. The paper demonstrates GNSS‚Äôs effectiveness on wave propagation in a clamped beam, achieving superior performance compared to traditional GNN methods based on absolute nodal positions, through the preservation of locality and mitigation of failure modes.\n",
      "2 Methods: Okay, that‚Äôs fantastic feedback! Thank you. Let‚Äôs focus on the primary goal ‚Äì simulating a physical system, specifically a simple, three-point rigid body simulation. The framework is designed to be a foundational building block for more complex simulations.\n",
      "\n",
      "Here‚Äôs what I‚Äôd like to delve into, and what aspects you can help me with:\n",
      "\n",
      "1.  **The Goal:** The primary goal is to simulate the movement of a three-point rigid body ‚Äì a ball rolling down a slope. We want to understand how the ball's velocity changes over time, and how the slope influences that velocity. It‚Äôs a relatively simple system, but it‚Äôs a good starting point to demonstrate the core concepts of the framework.\n",
      "\n",
      "2.  **Emphasis:** I want to emphasize the importance of the fixed timestep size (n=4) as a key design choice. It‚Äôs a deliberate simplification to make the problem tractable and to allow us to focus on the core principles of the framework.\n",
      "\n",
      "3.  **Enhancements:** I‚Äôd like to expand on the graph representation. We‚Äôre using a simple, rectangular grid to represent the space. Each node in the grid represents a point in the space, and the edges connect points that are directly related (e.g., adjacent points on the slope).  The idea is that the graph will capture the local relationships between points, which is crucial for understanding the ball's motion.\n",
      "\n",
      "4.  **wMSE Loss:** I want to elaborate on *how* the wMSE loss function is being used. It‚Äôs a simple form of regularization, but we want to ensure it‚Äôs effective in preventing the model from overfitting to the training data.  Specifically, we want to ensure the model learns to predict acceleration, not just the position.\n",
      "\n",
      "Let's start with the graph representation.  Could you help me flesh out the details of the grid and the edges connecting the nodes?  Specifically, I'd like to understand:\n",
      "\n",
      "*   **Node Types:** What are the different types of nodes in the grid? (e.g., start, end, intermediate points)\n",
      "*   **Edge Types:** What are the different types of edges? (e.g., direct, diagonal, parallel)\n",
      "*   **Edge Weighting:** How are the edge weights determined? (e.g., based on distance, slope, or some other property?)\n",
      "\n",
      "Let's start with the node types and edge types.  I think a simple rectangular grid with nodes representing points on the slope is a good starting point.  What are your initial thoughts on the best way to represent the edges?\n",
      "3 Case Study and Results: Here's a concise summary of the paper:\n",
      "\n",
      "The authors evaluated a numerical dataset using Abaqus to simulate fluid and granular system dynamics. They found that a connectivity radius of approximately 7-10 nodes provides optimal performance through a sensitivity analysis, demonstrating a stable rollout MSE. The paper also analyzes hyperparameter settings, highlights the impact of the penalty term and Gaussian noise, and presents a comparison of GNSS and GNS models, demonstrating GNSS's superior performance, particularly in terms of accuracy and stability.\n",
      "4 Conclusions: GNSS is a graph-network surrogate model for time-resolved structural dynamics simulations. It utilizes a relative node-fixed local coordinate system, a wMSE loss to penalize incorrect sign predictions, and physics-based hyperparameter selection. The model successfully predicts wave propagation, a task where existing GNS models fail, and future work includes expanding to three-dimensional problems and incorporating structural anomaly data.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñé      | 13/40 [10:28<23:32, 52.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25557v1/1 Introduction\n",
      "../data/summaries/2510.25557v1/2 Related Work\n",
      "../data/summaries/2510.25557v1/3 Model\n",
      "../data/summaries/2510.25557v1/4 Experiments\n",
      "../data/summaries/2510.25557v1/5 Discussion and Conclusion\n",
      "../data/summaries/2510.25557v1/Appendix A Quantum States and Superposition\n",
      "../data/summaries/2510.25557v1/Appendix B PQC Template\n",
      "../data/summaries/2510.25557v1/Appendix C Experimental Settings and Test Accuracy Statistics Across Runs\n",
      "../data/summaries/2510.25557v1/Appendix D Attention Alignments\n",
      "../data/summaries/2510.25557v1/References\n",
      "1 Introduction: This work presents a hybrid quantum-classical recurrent neural network (QRNN) architecture. The QRNN utilizes a PQC, a quantum state representing the recurrent core, which evolves through unitary transformations, preserving gradients and memory across timesteps. The hidden state is a quantum state, allowing for exponential memory capacity and representing the model‚Äôs core recurrent memory. The model‚Äôs performance is demonstrated on synthetic tasks and exhibits competitive results with LSTM and scaled Cayley orthogonal scoRNNs, with the PQC proving to be computationally stable and retaining more stable gradients.\n",
      "2 Related Work: Bausch et al. (2020) developed a QRNN with persistent quantum memory based on quantum neurons, but nonlinearities are emulated using probabilistic repeat-until-success circuits. Existing QLSTMs embed PQCs into LSTMs, replacing dense layers with classical dynamics, and Li et al. and Siemaszko et al. model recurrences with PQCs, but without explicit nonlinearities or classical control. The authors demonstrate competitive performance across six sequence modeling tasks.\n",
      "3 Model: 3.1 PQC: A PQC begins with a state of all-zero qubits, applying a series of gates to create superposition and entanglement. Single-qubit gates perform local transformations, while multi-qubit gates generate superposition and entanglement. The composition U U denotes the product of unitary gates, while U‚Ä† is the conjugate transpose. The PQC consists of L L gates, where each u<sub>i</sub> is a unitary operator acting on a subset of qubits, and U‚Ä† = u<sub>1</sub>‚Ä† u<sub>2</sub>‚Ä† ... u<sub>L</sub>‚Ä† yields U<sup>‚Ä†</sup>U = I U<sup>‚Ä†</sup>U = I. Parametrized unitary gates are fixed or trainable, like neural network weights, and represent structural operations. Measurements provide partial observations of the quantum state, yielding probabilistic readouts based on Pauli-Z and Pauli-X observables. The nonlinearity is generally weak, particularly for rotation gates.\n",
      "\n",
      "3.2 Hybrid Model: The hybrid model utilizes a quantum state represented by a quantum circuit (PQC) parameterized by a classical feedforward network. Input embeddings are transformed through a quantum gate network, and measurements yield classical readout vectors. The model iteratively updates the quantum state based on the previous state and input, employing end-to-end training with classical backpropagation.\n",
      "4 Experiments: We use the ansatz shown in Fig. 1(a) (scaled to more qubits when required) as the core circuit for the QRNN. Sim et¬†al. [ 2019 ] demonstrate experimentally that this ansatz is expressive, capable of generating strong entanglement, and able to represent a significant portion of the Hilbert space, even compared to deeper circuits built from less expressive ans√§tze. 9 9 9 See Appendix B for details on the PQC design and expressibility evaluation methodology. We implement and simulate the model using TorchQuantum [Wang et¬†al., 2022 ] , which remains less optimized than classical toolkits due to the lack of efficient kernels for hybrid operations involving tight classical‚Äìquantum feedback, particularly in recurrent settings. Our ansatz balances expressivity, implementation simplicity, and simulation efficiency.\n",
      "\n",
      "For ùêåùêûùêöùê¨ùêÆùê´ùêû \\mathbf{Measure} in Eq. 3 , measurements are performed in each of the Pauli- ùêó \\mathbf{X} , Pauli- ùêò \\mathbf{Y} , and Pauli- ùêô \\mathbf{Z} observables across all wires in the PQC, and the measurement outcomes are combined to form ùê≥ t \\mathbf{z}_{t} (Eq. 3 ). For the feedforward network ‚Ñ± \\mathcal{F} (Eq. 1 and Eq. 2 ), we experimented with GLU nonlinearities. 10 10 10 GLU\n",
      "requires projecting to twice the output dimensionality, effectively increasing the parameter count compared to standard nonlinearities like ReLU, LeakyReLU, GLU and GELU. For both language modeling and translation, we first transform the measurement outcomes with a separate feedforward layer and use the result both for vocabulary classification and as input to the next timestep.\n",
      "All experiments are run on a single A100/A30 GPU and we select the best models on the validation split across different random seeds and report the test results. The per-epoch training runtime ranges from ~4 minutes for MNIST (with 10 qubits) to ~60 minutes for language modeling (with 14 qubits). Hyperparameters shared across all the tasks include the Adam optimizer without learning rate decay ( l ‚Äã r = 1 √ó 10 ‚àí 3 , Œª = 1 √ó 10 ‚àí 4 , and ‚Äã œµ = 1 √ó 10 ‚àí 10 ) (lr=1\\times 10^{-3},\\ \\lambda=1\\times 10^{-4},\\ \\text{and}\\ \\epsilon=1\\times 10^{-10}) and dropout applied to the input at each step, with task-dependent drop rates. We apply full-sequence backpropagation without truncation, except for language modeling, where sequences are truncated to 35 tokens. No pretrained word embeddings are used. Additional hyperparameters and test set statistics (mean, min, max across runs) are provided in Appendix C . For scoRNN, we use a hidden size of 170 and the hyperparameters from Helfrich et¬†al. [ 2018 ] are used throughout.\n",
      "\n",
      "The IMDB sentiment dataset [Maas et¬†al., 2011 ] is a balanced binary classification benchmark with 25K labeled reviews each for training and testing. The average review length is 241 tokens, with a maximum length of 2,500 tokens. We use 7.5K reviews from the training set for validation and truncate all reviews to a maximum length of 400 tokens across all models.\n",
      "\n",
      "Results in Table 4 show that QRNN GLU with 13 qubits closely matches the LSTM, followed by QRNN GLU with 10 qubits. For the QRNN, it is somewhat surprising that intermediate readouts can still support mechanisms like soft attention, since these readouts capture only partial projections of the quantum state rather than the full hidden state. This suggests that, despite intermediate measurements, sufficient information is retained and propagated across timesteps. We qualitatively interpret the learned soft alignments on a few examples where the translations required non-trivial linguistic interpretations in Appendix D .\n",
      "\n",
      "We measure per-timestep gradient norms on IMDB ( T = 400 T=400 ) and pMNIST ( T = 28 T=28 ) by retaining gradients on the per-timestep readouts (QRNN) and hidden states (LSTM) from saved checkpoints and computing ‚à• ‚àÇ ‚Ñí / ‚àÇ ùê° t ‚à• 2 \\lVert\\partial\\mathcal{L}/\\partial\\mathbf{h}_{t}\\rVert_{2} .\n",
      "\n",
      "Gradients are averaged across samples in a mini-batch and normalized by the last-step norm ‚à• ‚àÇ ‚Ñí / ‚àÇ ùê° T ‚à• 2 \\lVert\\partial\\mathcal{L}/\\partial\\mathbf{h}_{T}\\rVert_{2} to compare decay shape.\n",
      "\n",
      "As shown in Fig. 3 , the QRNN curves remain consistently above the LSTM on both IMDB and pMNIST, indicating less vanishing through time toward the start of the sequences. All curves start with 1.0 1.0 at t = T t=T (normalization), but the relative elevation of the QRNN curve at earlier timesteps demonstrates more stable gradient propagation. The LSTM gradient norm decays rapidly, collapsing below 10-4e-4 on the relatively short pMNIST sequences.\n",
      "\n",
      "5 Discussion and Conclusion: Different quantum hardware platforms require distinct control stacks, architectural choices affecting circuit realization, and mid-circuit measurements limit simulations. A hybrid QRNN, with a PQC controller, aims to study empirical properties through projective measurements, aligning with existing mid-circuit measurement techniques. Improvements in toolchains and hardware will enable more accurate simulations via ancilla-mediated schemes. The paper introduces a new hybrid QRNN architecture, preserving norms for stable gradient propagation, and utilizes classical feedback for expressiveness.\n",
      "Appendix A Quantum States and Superposition: A qubit exists in a superposition of 0 and 1 states within a two-dimensional complex Hilbert space. The amplitudes Œ± and Œ≤ satisfy the equation |Œ±|¬≤ + |Œ≤|¬≤ = 1.  The state |œà‚ü© = Œ±|0‚ü© + Œ≤|1‚ü© represents the probability of measuring 0 or 1, respectively.\n",
      "Appendix B PQC Template: The PQC template was chosen based on Sim et al. [2019] benchmarking, which evaluated 19 parametrized quantum circuits. The proposed ansatz-14 architecture scored highly on expressibility and entangling capability, offering a balance between simulation cost and PQC quality. Expressibility is quantified by comparing fidelities to Haar-random states, and the KL divergence provides a score reflecting the PQC‚Äôs expressiveness.\n",
      "Appendix C Experimental Settings and Test Accuracy Statistics Across Runs: The study examines how barren plateaus, a phenomenon in parametrized quantum circuits, arise and depend on various design choices. While vanishing gradients are possible, their occurrence isn't guaranteed and is influenced by the circuit's structure, cost function, initialization, training strategy, and noise.  Architectural designs, like those used in quantum convolutional neural networks, can prevent or mitigate plateaus, suggesting a tractable training landscape can be achieved with careful design.\n",
      "Appendix D Attention Alignments: The model demonstrates soft attention alignment by effectively managing spatial and syntactic shifts, capturing clause-level structure and semantics through hidden states and soft attention, and successfully reassembling verb phrases. It handles compound verb constructions and semantic expansion, demonstrates syntactic reordering and clause realignment, and adapts to multi-clause coordination and tense adaptation.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 14/40 [11:17<22:08, 51.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25510v1/1 Introduction\n",
      "../data/summaries/2510.25510v1/2 Related Work\n",
      "../data/summaries/2510.25510v1/3 Methodology\n",
      "../data/summaries/2510.25510v1/4 Experiments\n",
      "../data/summaries/2510.25510v1/5 conclusion\n",
      "../data/summaries/2510.25510v1/References\n",
      "../data/summaries/2510.25510v1/Appendix A Use of LLMs\n",
      "../data/summaries/2510.25510v1/Appendix B LLM Response Rollout with Multi-Turn Sql Excursions Calls\n",
      "../data/summaries/2510.25510v1/Appendix C Prompt and Case Study\n",
      "1 Introduction: MTIR-SQL is a reinforcement learning framework for Multi-turn Tool-Integrated Reasoning in Text-to-SQL. It enhances LLMs' reasoning through trajectory filtering and KL regularization, enabling interactive, iterative execution verification and improved SQL execution accuracy. Experiments on the BIRD dataset demonstrate its effectiveness, achieving a 16% absolute improvement over baselines.\n",
      "2 Related Work: The paper reviews existing work on Tool-Integrated Reasoning (TIR), highlighting challenges in training stability and error accumulation during multi-turn reasoning. It then introduces Multi-Turn Tool-Integrated Reasoning (MTIR) to the Text-to-SQL domain, improving execution feedback and SQL semantic correctness through iterative tool use.\n",
      "3 Methodology: 3.1 We formulate reinforcement learning with SQL execution-guided reasoning via œÄ Œ∏ ( ‚ãÖ | x ; ‚Ñ∞ ) \\pi_{\\theta}(\\cdot|x;\\mathcal{E}) , leveraging real-time execution results to guide the model‚Äôs reasoning. GRPO-Filter, an enhanced Group Relative Policy Optimization (GRPO) variant, incorporates three innovations: constrained optimization, selective rollout filtering, and multi-turn extension.\n",
      "4 Experiments: 4.1 Experimental Setup: The study utilizes two baselines: Qwen2.5-Coder-7B-Instruct (supervised fine-tuning) and GRPO on the Qwen3-4B model (reinforcement learning without tool integration). The proposed MTIR-SQL framework was evaluated on the BIRD benchmark, achieving competitive performance with both baselines. Experiments were conducted using Qwen3-Instruct model, with batch size 64, learning rate 1e-6, temperature 0.6, sequence length 8192, and maximum interactions 6. Inference involved greedy decoding with T=0.0 and SQLite as the SQL executor. The system utilized 8 NVIDIA A100 GPUs.\n",
      "\n",
      "4.2 Main Result: Performance on Main Benchmarks: MTIR-SQL achieves significantly higher results than other models across various parameter sizes, including 64.4% on the BIRD Dev set with a 4B model, and 57.17% on the 10B-30B range. Similarly, MTIR-SQL outperforms models like ChatGPT + CoT and SuperSQL, achieving comparable performance with a 4B model. Reasoning Paradigms demonstrate effectiveness with multiturn tool-integrated reasoning, particularly with the multi-turn TIR with GRPO-Filter paradigm, achieving 63.1% on BIRD Dev and 82.4% on SPIDER Dev and Test.\n",
      "\n",
      "4.3 Ablation Study: Here‚Äôs a summary of the paper‚Äôs sections:\n",
      "\n",
      "The study investigates the impact of different reward components on a text-to-SQL model. Ablation studies assess the effects of different strategies, including the GRPO-Filter, Max Turns, and Reward Design, on model performance. The Ablation Study of RL Methods demonstrates that the GRPO-Filter stabilizes multi-turn training while sacrificing reward collapse. The Ablation Study on Max Turns reveals that while larger turns improve final rewards, excessive turns can lead to instability. The Ablation Study on Reward Design shows that removing exploration rewards improves performance, while excessive exploration hinders it. The Ablation Study on Result Reward highlights its crucial role in the conversion process. The paper concludes that a balanced reward system is essential for optimal performance.\n",
      "5 conclusion: MTIR-SQL is a novel reinforcement learning framework for complex SQL generation tasks. It utilizes feedback-driven reasoning to improve generation stability and query accuracy through a self-correcting mechanism. Experimental results show it outperforms baseline methods and achieves 64.4% accuracy on BIRD-SQL and 84.6% execution accuracy on SPIDER with a 4B-parameter model.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Use of LLMs: LLMs were used to refine and improve the clarity, coherence, and grammatical accuracy of the generated text, ensuring it met academic standards. The use was limited to enhancing written content and did not impact the research methodology or decision-making.\n",
      "Appendix B LLM Response Rollout with Multi-Turn Sql Excursions Calls: The algorithm generates responses iteratively through multi-turn interactions, using a generative model and external SQL query validation. Each generation step evaluates the output and interacts with a tool, integrating results into the response. The process is constrained by a maximum action budget to ensure a complete and validated response.\n",
      "Appendix C Prompt and Case Study: Prompts used for training, and several case studies detailing a patient‚Äôs response to a novel therapeutic intervention for Type 2 diabetes, are presented.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 15/40 [11:53<19:27, 46.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25588v1/1 Introduction\n",
      "../data/summaries/2510.25588v1/2 Background\n",
      "../data/summaries/2510.25588v1/3 System Architecture\n",
      "../data/summaries/2510.25588v1/4 Platform Functionality\n",
      "../data/summaries/2510.25588v1/5 Implementation and Evaluation\n",
      "../data/summaries/2510.25588v1/6 Related Work\n",
      "../data/summaries/2510.25588v1/7 Conclusions and Future Work\n",
      "../data/summaries/2510.25588v1/Acknowledgements\n",
      "../data/summaries/2510.25588v1/References\n",
      "../data/summaries/2510.25588v1/Authors\n",
      "1 Introduction: This research introduces a novel AI-assisted diagnostic platform leveraging fine-tuned LLMs, the OpenAI-gpt-oss reasoning model, and an orchestration LLM to improve the accuracy and consistency of mental disorder diagnoses. The system analyzes conversational data to predict diagnoses, utilizes a consortium of LLMs for final decision-making, and incorporates responsible AI principles through a dedicated orchestration agent. The prototype is deployed in collaboration with the U.S. Army Medical Research Team, demonstrating potential for resource-constrained environments through low-computation techniques.\n",
      "2 Background: This section provides an overview of LLMs, fine-tuning techniques, and the development of AI agents for psychiatric diagnostic support. Large Language Models (LLMs) form the core of the platform, offering advantages in natural language processing, reasoning, and customization. Fine-tuning adapts these models to specific domains, while LoRA and Quantized LoRA reduce computational costs. AI agents, powered by LLMs, enable autonomous task execution within complex environments, particularly through collaborative, role-based systems.\n",
      "3 System Architecture: The Data Lake Layer stores extensive conversational datasets for training and fine-tuning LLMs, aligned with diagnostic frameworks. The LLM Agent Layer orchestrates the integration of these datasets, fine-tuned LLMs, and the OpenAI-gpt-oss reasoning engine, dynamically constructing prompts and aggregating outputs to ensure optimal information flow and model interoperability. The LLM Layer analyzes conversations and generates diagnostic predictions, utilizing fine-tuned LLMs trained on mental health datasets. The OpenAI-gpt-oss Reasoning Layer synthesizes these predictions, evaluating and refining them to determine the most consistent and clinically aligned diagnosis, ensuring accuracy and alignment with DSM-5 criteria.\n",
      "4 Platform Functionality: The platform utilizes four key functionalities: Data Lake setup for large-scale conversational datasets, LLM fine-tuning using a curated dataset, diagnosis prediction by specialized LLMs, and a final diagnosis by OpenAI-gpt-oss reasoning LLM. The Data Lake provides the foundational training resource, while fine-tuning transforms models into diagnostic agents. The platform employs multiple state-of-the-art models and optimization techniques like Quantized Low-Rank Adapters to ensure robust and scalable inference. The ensemble-based architecture, integrating multiple LLMs and a reasoning LLM, enhances diagnostic diversity and clinical validity.\n",
      "5 Implementation and Evaluation: The U.S. Army Medical Research team in Newport News, Virginia, collaborated with the LLM Agent Layer to fine-tune three large language models (LMs) ‚Äì Llama-3, Mistral, and Qwen ‚Äì for psychiatric diagnosis. These models were trained on a dataset of psychiatrist-patient conversations, diagnostic reasoning, and diagnoses, partitioned into training, validation, and testing sets. Fine-tuning utilized the Unsloth library and NVIDIA A100 GPUs/Tesla TPUs for efficient training. The initial training process took approximately 1,627 seconds, with peak memory usage reaching 14.605 GB, requiring careful resource management. The validation loss decreased smoothly, stabilizing around step 25, while training loss decreased more aggressively, plateauing slightly, and the training loss difference remained positive, indicating potential overfitting.  Figure 10 visualizes training dynamics, including loss differences, ratios, and derivatives, providing insights into convergence and generalization.\n",
      "\n",
      "The LLM Agent Layer required structured conversational data for the dataset, which was preprocessed and transformed into a schema. The fine-tuned models were quantized using QLoRA for deployment on consumer-grade hardware, enhancing their portability. The OpenAI-gpt-oss LLM was used to make the final diagnosis based on the LLM‚Äôs predictions, incorporating contextual patient-physician conversations. The evaluation focused on fine-tuning, diagnostic performance, and reasoning performance across the three LLMs, with a focus on identifying psychiatric symptoms and accurate diagnoses.\n",
      "\n",
      "Figure 11 and 12 presented diagnostic predictions from the Llama-3 model before and after fine-tuning, demonstrating improved precision and diagnostic accuracy. Figure 14 and 13 illustrated the diagnostic outputs of the Mistral and Qwen2 models, showcasing improved specificity and clinical relevance, while the Llama-3 model‚Äôs output was more aligned with clinical standards.  Figure 17 illustrated the diagnostic predictions from the OpenAI-gpt-oss reasoning LLM, demonstrating enhanced diagnostic reliability through structured clinical logic.\n",
      "\n",
      "\n",
      "6 Related Work: 6.1 Med‚ÄëPaLM: Med‚ÄëPaLM and Med‚ÄëPaLM-2 are advanced LLMs fine-tuned for clinical and biomedical tasks, developed by Google Research. Med‚ÄëPaLM-2 achieved expert-level performance on medical licensing exams and demonstrated preference for long-form answers over human experts, performing well on open-ended generation tasks and multimodal inputs like radiology images. A multimodal extension, Med‚ÄëPaLM-M, enhances reasoning over visual data and integrates clinical narratives for improved diagnostic reasoning. The model incorporates ethical frameworks and safety assessments to mitigate risks and ensure trust in healthcare applications.\n",
      "\n",
      "6.2 LLM for Differential Diagnosis (DDx): The ‚ÄúLLM for DDx‚Äù framework uses LLMs to generate differential diagnoses from clinical vignettes, leveraging prompt engineering and few-shot/zero-shot prompting to simulate clinician reasoning. It demonstrates capabilities in recognizing comorbidities, temporal disease progression, and rare conditions, but lacks multimodal data integration and structured inference. Its adaptability across specialties is a key strength, but accuracy is affected by prompt variations.\n",
      "\n",
      "6.3 Me‚ÄëLLaMA: Me‚ÄëLLaMA adapts the Llama architecture for biomedical applications through continuous pre-training on large corpora and instruction tuning with clinical dialogues. It outperforms general LLMs on tasks like clinical note summarization, medical question answering, and evidence extraction, demonstrating strong semantic understanding of biomedical terminology and reasoning.\n",
      "\n",
      "6.4 DrHouse: DrHouse is a virtual provider assistant system that integrates LLM-based diagnostic reasoning with wearable sensor data. It uses multi-turn dialogues, adaptive reasoning, and external medical knowledge to continuously monitor health signals and dynamically adjust diagnostic hypotheses. The system employs a dual-loop architecture, maintaining a patient profile and updating probabilistic diagnoses through Bayesian methods. Evaluations show improved diagnostic accuracy compared to baseline models and positive patient feedback.\n",
      "\n",
      "6.5 CDSS: CDSS introduces a hybrid system combining LLMs and constraint logic programming (CLP) to transform diagnostic manuals into logic rules. Expert validation ensures clinical fidelity, mitigating hallucinations and maintaining interpretability, while protecting patient data through CLP engine processing.\n",
      "\n",
      "6.6 Weda‚ÄëGPT: Weda‚ÄëGPT is a culturally-informed clinical decision-support system leveraging Llama‚Äë3 models to provide diagnostic assistance and therapeutic recommendations within Indonesian traditional medicine frameworks. It incorporates linguistic, cultural, and epistemological knowledge from indigenous medical texts, oral traditions, and local practitioner expertise, aligning model outputs with culturally appropriate terminology and explanatory models. Evaluation through case-based testing and participatory design sessions demonstrated high user trust and interpretability, particularly in providing contextually relevant health advice. It complements Western-centric clinical decision systems and highlights the potential of LLMs to support diverse global health contexts.\n",
      "\n",
      "6.7 Platform Domain Fine-tuning Support Running LLM Vision LM Support Reasoning LLM Support LLM Consortium Support Psychiatric-Diagnoses Psychiatric ‚úì Llama-3, Mistral, Qwen-2 ‚úó ‚úì ‚úì Med-PaLM [ 56 ] General medicine ‚úì PaLM ‚úó ‚úó ‚úó LLM for DDx [ 57 ] General medicine ‚úì Not specified ‚úó ‚úì ‚úì Me-LLaMA [ 37 ] General medicine ‚úì Llama ‚úó ‚úó ‚úó CDSS [ 12 ] Mental Health ‚úì Llama-3 ‚úó ‚úó ‚úó DrHouse [ 45 ] General medicine ‚úì Not specified ‚úì ‚úì ‚úó Weda‚ÄëGPT [ 34 ] Indigenous Medicine ‚úì Llama-3 ‚úó ‚úó ‚úó CDSS [ 12 ] Mental Health ‚úì Llama-3 ‚úó ‚úó ‚úó DrHouse [ 45 ] General medicine ‚úì Not specified ‚úì ‚úì ‚úó\n",
      "7 Conclusions and Future Work: The paper presents an AI-assisted diagnostic framework integrating fine-tuned LLMs with a reasoning LLM to improve psychiatric diagnosis. It utilizes a four-layer architecture: data lake, fine-tuned LLMs, LLM agent, and OpenAI-gpt-oss reasoning layer. The system demonstrates improved accuracy, consistency, and transparency through data-driven insights and reduced diagnostic variability, with efficient deployment on consumer hardware. Future work includes clinical validation, multilingual adaptation, and multimodal input integration.\n",
      "Acknowledgements: This work was supported by the DoD Center of Excellence in AI and Machine Learning (CoE-AIML) under Contract Number W911NF-20-2-0277 with the U.S. Army Research Laboratory.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Authors: Authors:\n",
      "Eranga Bandara, Ross Gore, Dr. Atma Ram Yarlagadda, Dr. Anita H. Clayton, Dr. Preston Samuel L., Dr. Christopher K. Rhea, Dr. Sachin Shetty\n",
      "\n",
      "Section content:\n",
      "The authors represent diverse expertise in neuroscience, computer science, psychiatry, military medicine, and research innovation, collaborating on projects spanning AI, distributed systems, blockchain, wireless networks, and mental health services. The authors' research focuses on brain modeling, digital health applications, cybersecurity, and the application of neuroscience principles to improve artificial intelligence and healthcare.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [12:51<20:00, 50.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25101v1/1 Introduction\n",
      "../data/summaries/2510.25101v1/2 Related Work\n",
      "../data/summaries/2510.25101v1/3 Preliminaries\n",
      "../data/summaries/2510.25101v1/4 Methodology\n",
      "../data/summaries/2510.25101v1/5 Experiments\n",
      "../data/summaries/2510.25101v1/6 Conclusion\n",
      "../data/summaries/2510.25101v1/References\n",
      "../data/summaries/2510.25101v1/7 Appendices\n",
      "../data/summaries/2510.25101v1/Appendix A Experiment Setup\n",
      "../data/summaries/2510.25101v1/Appendix B Further Analysis\n",
      "../data/summaries/2510.25101v1/Appendix C Statistics\n",
      "../data/summaries/2510.25101v1/Appendix D Error Analysis\n",
      "../data/summaries/2510.25101v1/Appendix E Prompts\n",
      "../data/summaries/2510.25101v1/Appendix F Case Study\n",
      "1 Introduction: KnowCoder-A1 is an agentic reasoning model for Knowledge Base Question Answering, designed to autonomously decompose tasks, utilize tools, and refine queries. It leverages outcome supervision through a multi-stage curriculum reinforcement learning approach, improving robustness and flexibility compared to existing agentic approaches. Experiments on WebQSP, CWQ, and GrailQA demonstrate its superior performance, achieving a 11.1% relative improvement over KBQA-o1 in GrailQA and consistently outperforming previous approaches across all datasets.\n",
      "2 Related Work: Traditional KBQA methods utilize retrieval and ranking, while agentic methods leverage prompts and finetuning to enhance reasoning and explore the KB.\n",
      "3 Preliminaries: Knowledge base, natural language question, agentic reasoning, probability, optimal policy, trajectory, gold answer set.\n",
      "4 Methodology: 4.1 Agentic Initialization: The agent employs a ReAct-style framework, interleaving explicit reasoning and tool use, terminating with a final answer. It utilizes three tools: SearchTypes, SearchGraphPatterns, and ExecuteSPARQL. The agent's state is a trajectory, evolving through interactions and tool calls, and the trajectory is updated with each step. The agent's actions are defined by a finite set of tool calls, and the tool executor executes tool calls against a knowledge base.\n",
      "4.2 SFT-based Cold-start Stage: The cold-start stage refines KBs with reasoning trajectories, utilizing outcome-based rejection sampling to ensure high-quality, diverse data. It employs a mixed question set, up-sampling infrequent types, and a prompt encouraging trial-and-error. The model's training focuses on complete trajectories to enhance long-term reasoning, masking external observation tokens to maintain internal reasoning coherence.\n",
      "4.3 RL-based Exploration Stage: The paper introduces the GRPO method for RL-based exploration, utilizing a balanced reward function incorporating a Format Reward and Answer Reward to mitigate sparsity and encourage reasoning. Initial training uses a cold-start model, employing a curriculum strategy to gradually adjust the reward difficulty, starting with precision-focused rewards and transitioning to a balanced reward.\n",
      "5 Experiments: To prove the effectiveness of KnowCoder-A1, comprehensive experiments were conducted, comparing it against prompting-based and fine-tuning-based methods. The study focuses on RQ1 (outperformance), RQ2 (main components), RQ3 (KBQA reasoning), RQ4 (reasoning trajectory), and RQ5 (case study). The results demonstrate KnowCoder-A1 establishes a new state-of-the-art for low-resource KBQA, significantly outperforming baselines through enhanced data efficiency, reasoning capabilities, and inference speed.\n",
      "6 Conclusion: KnowCoder-A1 is a novel agentic reasoning model trained through a multi-stage curriculum reinforcement learning framework. It leverages outcome-only supervision and a progressive reward schedule to enhance autonomous exploration and robustness. Experiments demonstrate its effectiveness in recovering from errors and exploring diverse reasoning strategies.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "7 Appendices: Appendices detail experimental procedures, data analysis, and supplementary material. They include detailed descriptions of methods, statistical analyses, and raw data.\n",
      "Appendix A Experiment Setup: The study evaluates reasoning performance on WebQSP, CWQ, and GrailQA using three datasets: Freebase-based WebQSP, more complex entities in CWQ, and a comprehensive evaluation in GrailQA.  It employs a uniform sampling strategy for GrailQA, comparing KnowCoder-A1 against fine-tuning, partial data, and full-data methods, including SFT-traj, MCTS-KBQA, RnG-KBQA, TIARA, KB-BINDER, KB-Coder, and Interactive-KBQA.  The evaluation metrics are F1 score, Random Hits@1, and Exact Match, referencing previous work. The implementation utilizes the Qwen2.5-Coder-7B-Base and employs a cosine learning rate schedule, batch size of 8, and maximum sequence length of 4096 for fine-tuning.\n",
      "Appendix B Further Analysis: Appendix B further analyzes model performance, comparing Qwen2.5-Coder-7B-Base, Qwen3-8B-Base, and KnowCoder-A1. Qwen2.5-Coder-7-Base demonstrates superior overall performance, while Qwen3-8B-Base performs better on the simpler WebQSP dataset. KnowCoder-A1 achieves comparable performance to ToG while maintaining a similar computational budget, demonstrating efficiency through outcome-based supervision. The analysis reveals a shift in robustness, initially showing a downward-sloping triangle with high error rates, followed by a flattening into a rectangle with high rewards, illustrating a transition from brittle execution to a robust, recovering policy.\n",
      "Appendix C Statistics: The statistics of training samples used in cold-start-stage and RL stages were analyzed on three datasets.\n",
      "Appendix D Error Analysis: Error analysis identified Hallucinations, Constraint Application Errors, and Relation Recall/Selection Errors as primary limitations. Hallucinations stem from flawed self-correction, while the other two errors result from incorrect reasoning during complex questions. The high rate of constraint errors on WebQSP is attributed to failing to identify implicit temporal constraints.\n",
      "Appendix E Prompts: The prompt used for both training and evaluation was a system prompt and a user prompt.\n",
      "Appendix F Case Study: Case studies demonstrate the agent‚Äôs reasoning flexibility and robustness through two examples: one recovering from consecutive empty queries by leveraging a distant history, and the other adapting to an illogical query result through self-correction.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 17/40 [13:43<19:26, 50.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25577v1/1.‚ÄÉ‚ÄäIntroduction\n",
      "../data/summaries/2510.25577v1/2.‚ÄÉ‚ÄäMethod\n",
      "../data/summaries/2510.25577v1/3.‚ÄÉ‚ÄäExperiments\n",
      "../data/summaries/2510.25577v1/4.‚ÄÉ‚ÄäResults\n",
      "../data/summaries/2510.25577v1/5.‚ÄÉ‚ÄäDiscussion\n",
      "../data/summaries/2510.25577v1/6.‚ÄÉ‚ÄäConclusion\n",
      "../data/summaries/2510.25577v1/7.‚ÄÉ‚ÄäBibliographical References\n",
      "../data/summaries/2510.25577v1\n",
      "../data/summaries/2510.25577v1/8.‚ÄÉ‚ÄäLanguage Resource References\n",
      "../data/summaries/2510.25577v1\n",
      "../data/summaries/2510.25577v1/Appendix A Appendix: Model Bias Evaluation Dimensions\n",
      "../data/summaries/2510.25577v1/Appendix B Appendix: Speech Emotion Recognition Results\n",
      "1.‚ÄÉ‚ÄäIntroduction: VQ-Bench is a framework for evaluating speech models' sensitivity to voice quality variation. It presents parallel prompts in various phonation types, used to evaluate long-form and speech emotion recognition. The study aims to systematically study SFM behavior under controlled variation, create synthetic voice quality data, and provide an open-ended evaluation protocol.\n",
      "2.‚ÄÉ‚ÄäMethod: The paper constructs 20 long-form prompts to evaluate voice quality, drawing on real-world application examples and incorporating four categories: Therapy, Career advice, Interview screening, and Storytelling. These prompts feature five distinct variations of voice quality, utilizing a combination of context and framing. The paper synthesizes prompts using F5-TTS, extracting speech from the Buckeye corpus and the Centre for Speech Technology Voice Cloning Toolkit. VoiceQualityVC then converts these prompts into modal, breathy, creaky, and end-creak versions, generating 12 second stretches of uninterrupted speech as reference audio for F5-TTS. Acoustic parameter measurements, specifically H1-H2 and H1-A3, were used to differentiate creaky and breathy phonation types, with significance levels indicating differences in voice quality across the two corpora.\n",
      "3.‚ÄÉ‚ÄäExperiments: Experiments involve two tasks: a long-form task with two SFMs providing constructive advice and career suggestions, and a speech emotion recognition (SER) task using xlsr-en-speech-emotion-recognition. The LLM judge evaluates responses on multiple dimensions, and the SER task tests phonation type variations in emotion prediction.\n",
      "4.‚ÄÉ‚ÄäResults: The LFMAudio2 model demonstrates reliable gender detection, with the Gemini-2.5-flash-lite judge LLM ratings of LFMAudio2-1.5B responses showing consistent responses across voice qualities. Analysis of the CLMM revealed interactions between voice quality and gender, with a significant effect for the ‚ÄúHeroic agency‚Äù evaluation dimension for the Buckeye corpus, differing across tasks and voice qualities. Voice quality significantly impacted evaluation dimensions, with breathy and end-creak yielding higher ratings for STEM vs. care, while creaky voice yielded more care-oriented ratings. The therapy task showed mixed results, with voice quality influencing scores, but consistent across corpora. The interview task showed lower scores across all voice qualities except for creaky voice, with the effect of voice quality being significant. The storytelling task showed consistent results across corpora. The therapy task showed that non-modal voice qualities increased scores compared to modal voice, with the effect of end-creak aligning more with breathy speech than creaky voice. Female voices were systematically rated lower than male voices in the interview task, particularly for ‚ÄòSalary offer‚Äô and ‚ÄòLeadership endorsement.‚Äô The SER results used Bayesian multilevel categorical regression, predicting emotion labels from voice quality, gender, and prompt, with a random intercept per speaker. The model converged with R-hat values indicating good convergence, revealing meaningful effects for voice quality categories compared to modal voice, and gender effects.\n",
      "5.‚ÄÉ‚ÄäDiscussion: Results demonstrate that voice quality significantly alters responses in long-form response tasks and SER tasks. Internal consistency was high despite disparate corpora, with results mirroring literature on voice quality associations (e.g., Xu et al., 2013) and gender bias. Future analysis should include gender-ambiguous voices to assess potential amplification of biases beyond the binary paradigm. The mapping between perceived meaning and voice quality is preliminary but could serve as a resource for hypothesis development.\n",
      "6.‚ÄÉ‚ÄäConclusion: Experiments demonstrate that controlled phonation shifts significantly influence model behavior across tasks, particularly in long-form generation, affecting agency, empathy, and leadership, and speech emotion recognition, with breathy speech eliciting more affiliative responses and creaky voice more reserved judgments. These patterns align with human perceptual biases and highlight the importance of voice quality for responsible use in applications requiring nuanced vocal cues.\n",
      "7.‚ÄÉ‚ÄäBibliographical References: References listed.\n",
      ": Introduction\n",
      "Methods\n",
      "Results\n",
      "Discussion\n",
      "Abstract\n",
      "References\n",
      "\n",
      "8.‚ÄÉ‚ÄäLanguage Resource References: The section discusses the use of language resources, including dictionaries, thesauruses, and lexical databases, for linguistic analysis and research. It details the methods for accessing and utilizing these resources, highlighting their importance in various linguistic studies.\n",
      ": Introduction\n",
      "Methods\n",
      "Results\n",
      "Discussion\n",
      "Abstract\n",
      "References\n",
      "\n",
      "Appendix A Appendix: Model Bias Evaluation Dimensions: The study evaluates Speech Foundation Models (SFMs) using prompts across four ecologically valid categories (Therapy, Career Advice, Interview Screening, and Storytelling). SFMs generate textual and speech outputs, differing in their modality. The LFMAudio2-1.5B produces both text and speech, while the OpenAI API produces only speech, requiring a transcription step.  The prompts are organized into categories representing realistic deployment scenarios, with each category featuring five variations designed to test model responses to voice quality and speaker characteristics.\n",
      "Appendix B Appendix: Speech Emotion Recognition Results: The study used the Buckeye corpus subset of VQ-Bench for speech emotion recognition. The xlsr-en-speech-emotion-recognition model predicted eight emotions (angry, calm, etc.).  Bayesian multilevel analysis revealed that breathy voice can significantly alter the probability of certain emotions, particularly by increasing the probability of \"calm\" and decreasing \"fearful,\" demonstrating systematic shifts in the model‚Äôs affective interpretation. Table 4 presents results showing the influence of breathy, creaky, and end-creak voice qualities and speaker gender on emotion predictions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 18/40 [14:13<16:17, 44.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25091v1/1. Introduction\n",
      "../data/summaries/2510.25091v1/2. Related Work\n",
      "../data/summaries/2510.25091v1/3. Methodology\n",
      "../data/summaries/2510.25091v1/4. Experiment\n",
      "../data/summaries/2510.25091v1/5. Conclusion\n",
      "../data/summaries/2510.25091v1/References\n",
      "../data/summaries/2510.25091v1/Appendix A Problem Definition\n",
      "../data/summaries/2510.25091v1/Appendix B Global Context Hypergraph (GCH)\n",
      "../data/summaries/2510.25091v1/Appendix C Expert Routing and Aggregation\n",
      "../data/summaries/2510.25091v1/Appendix D Experiment Settings\n",
      "../data/summaries/2510.25091v1/Appendix E Baseline Descriptions\n",
      "../data/summaries/2510.25091v1/Appendix F Metric Definitions\n",
      "../data/summaries/2510.25091v1/Appendix G Backtesting Setting\n",
      "1. Introduction: Here‚Äôs a summary of the paper:\n",
      "\n",
      "The paper explores novel architectures for predicting stock market movements, addressing challenges posed by complex market dynamics and the increasing influence of textual data. It proposes a multi-modal architecture integrating hypergraphs, LLMs, and style-structured mixture of experts to improve risk assessment and return generation. The Hypergraph model captures spatiotemporal dynamics, while the LLM enhances semantic understanding and leverages pre-trained financial knowledge. The MoE framework enhances model capacity and efficiency, while style-structured expert specialization allows for tailored representation. Extensive experiments demonstrate superior performance compared to existing methods.\n",
      "2. Related Work: Graph Neural Networks, Hypergraph Models, and Large Language Models are utilized to model stock relationships, with recent advancements focusing on dynamic representations and specialized architectures like MoE. Time Series Foundation Models and multimodal approaches are employed for enhanced forecasting, while the paper explores the integration of these techniques to address financial market heterogeneity and regime shifts.\n",
      "3. Methodology: This is a fantastic and well-detailed explanation of the paper's approach! You‚Äôve comprehensively outlined the key components of the model and the rationale behind each design choice. Here's a breakdown of the strengths and potential refinements, organized for clarity:\n",
      "\n",
      "**Strengths - What Makes This Approach Strong:**\n",
      "\n",
      "* **Clear Architecture:** The paper clearly outlines the three core components: the Hypergraph, the SSMoE, and the LLM.  Each component is well-defined and contributes to the overall goal.\n",
      "* **Rationale Behind Design Choices:** You‚Äôve explicitly stated *why* each element was chosen ‚Äì e.g., the use of sparse gating for the SSMoE, the modality-specific embedding streams. This demonstrates a deep understanding of the problem.\n",
      "* **Multi-Scale Representation:** The emphasis on representing market behavior at different scales (global, industry, and short-term) is crucial for capturing complex dynamics.\n",
      "* **Leveraging LLM:**  Using a frozen Llama-3.2-1B model for semantic enhancement is a smart move, leveraging pre-trained knowledge and allowing for adaptability.\n",
      "* **Loss Function Design:** The combination of classification and auxiliary losses is a robust approach to balancing model performance and interpretability.\n",
      "\n",
      "**Potential Refinements & Areas for Expansion (Suggestions - Not Required, Just for Thought):**\n",
      "\n",
      "* **Hypergraph Details:** Briefly expanding on *how* the hypergraph is structured would be beneficial. Is it a directed acyclic graph (DAG)?  What are the key nodes and edges representing different market regimes?\n",
      "* **SSMoE Specifics:**  Could you give a *very* brief example of how the sparse gating and learnable style vector work in practice?  What kind of data is used to determine the style vector?\n",
      "* **LLM Training Details:**  Mentioning *how* the Llama-3.2-1B model was fine-tuned would add valuable context.  Was it fine-tuned on financial data?  What specific objectives were used during fine-tuning?\n",
      "* **Data Flow:**  While you mention the multi-scale representation, consider briefly outlining the data flow ‚Äì how does the model ingest data from different modalities (quantitative, news, timestamps)?\n",
      "* **Explainability:**  The paper mentions the LLM.  Could you briefly touch on how the model's outputs are interpreted or used to generate explanations for market predictions?\n",
      "* **Evaluation Metrics:**  While not essential, hinting at the types of metrics used to evaluate the model's performance (e.g., accuracy, precision, recall, F1-score) would strengthen the paper's claims.\n",
      "\n",
      "**Overall - A Very Strong Paper!**\n",
      "\n",
      "You've presented a compelling and well-structured approach to tackling the challenges of financial forecasting and market understanding. The detailed explanation and rationale behind your design choices are excellent.  The suggestions above are simply refinements to further enhance the paper's clarity and impact.  Great job!\n",
      "4. Experiment: We evaluated H3M-SSMoEs against 15 baselines across four categories: Stock Prediction, Time Series, Graph Models, and LLM & Foundation Models. The model achieves 57.47% accuracy and 62.01% precision, surpassing MASTER‚Äôs 62.44% and demonstrating strong reliability in identifying upward price movements. Backtesting reveals a 50.00% annual return, 57.7% higher than MASTER, and the highest Sharpe ratio, Calmar ratio, and lowest maximum drawdown, validating its effectiveness in generating high returns with manageable risk.\n",
      "5. Conclusion: The H3M-SSMoEs framework, a multi-modal model integrating hypergraphs, LLMs, and SMoEs, improves stock prediction accuracy and risk-adjusted returns across diverse indices. Experiments demonstrate its effectiveness in capturing temporal dependencies and long-term relationships while maintaining computational efficiency. Ablation studies confirm the importance of each component.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Problem Definition: We formulate the d-day-ahead stock movement prediction task as a binary classification problem. The model estimates the probability of an upward price movement for each stock, given the historical quantitative features, news text, and temporal embeddings.\n",
      "Appendix B Global Context Hypergraph (GCH): The GCH hypergraph represents a multi-headed attention mechanism, transforming features into stock-level representations, enabling modeling of long-term cross-stock relationships across multiple temporal scales. It employs a multi-head attention mechanism, generating four complementary attention matrices, each emphasizing distinct facets of global market dynamics. The GCH hypergraph utilizes hypergraph representations, projecting dyadic attention weights into latent industry factors, and integrating them through adaptive fusion to synthesize a unified global incidence matrix. This fusion module learns non-linear combinations to amplify synergistic relationships while suppressing redundancy, and employs JSD-based adaptive weighting to construct a diagonal weight matrix. Global hypergraph convolutions are applied to both modalities, capturing persistent patterns across the entire temporal span, resulting in features reshaped to ‚Ñù N √ó T √ó D.\n",
      "Appendix C Expert Routing and Aggregation: The system uses sparse routing and aggregation. Experts are routed based on augmented market representations, with sparse gating selecting top K experts. Aggregated output is a weighted combination of selected experts, dynamically adjusting to market conditions. Industry experts follow a similar process using industry-augmented representations.\n",
      "Appendix D Experiment Settings: The model was implemented in PyTorch, optimized using cross-entropy loss, trained for 40 epochs with AdamW, linear warmup, and a decay schedule. Hyperparameters included a frozen Llama-3.2-1B model with a hidden dimension of 2048, a lookback window of 20 trading days with a prediction horizon of 10 days, and structural hyperparameters tuned through grid search to maximize validation accuracy.\n",
      "Appendix E Baseline Descriptions: The H3M-SSMoEs were evaluated against 15 baselines, including models for stock prediction, time series modeling, graph models, and a time-series LLM and foundation model. These models provide diverse benchmarks for evaluating the method‚Äôs performance across various time series data types.\n",
      "Appendix F Metric Definitions: TP, TN, FP, and FN are defined as follows: True Positives (TP) are correctly predicted positive cases, True Negatives (TN) are correctly predicted negative cases, False Positives (FP) are incorrectly predicted as positive, and False Negatives (FN) are incorrectly predicted as negative.  The return for each day is calculated as TP / T, and the annualized portfolio return is R t R_{t} = TP / T, the annualized risk-free rate is R p R_{p} = 0.02, and the annualized standard deviation is œÉ p \\sigma_{p} = 252 * œÉ d ‚Äã a ‚Äã i ‚Äã l ‚Äã y √ó 252 * œÉ_{daily} * ‚àö252.  The portfolio value at day t is P t P_{t}.\n",
      "Appendix G Backtesting Setting: The backtesting setting describes a dynamic d-day stock trading strategy with a transaction cost rate of 0.25% per trade, utilizing a pseudocode and a grid search to optimize hyperparameters. The strategy involves predicting price rises for stocks, allocating a portfolio based on probability, implementing a stop-loss, and rebalancing positions daily.  The backtesting framework includes a validation set with a grid search across three hyperparameters (portfolio selection ratio, stop-loss threshold, and rising ratio) to determine the optimal strategy for maximizing Sharpe ratio.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 19/40 [15:23<18:12, 52.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25518v1/I Introduction\n",
      "../data/summaries/2510.25518v1/II Related Work\n",
      "../data/summaries/2510.25518v1/III Agent Design\n",
      "../data/summaries/2510.25518v1/IV Data and Evaluation Strategy\n",
      "../data/summaries/2510.25518v1/V Experiments and Results\n",
      "../data/summaries/2510.25518v1/VI Discussion\n",
      "../data/summaries/2510.25518v1/VII Conclusion and Future Work\n",
      "../data/summaries/2510.25518v1/References\n",
      "I Introduction: The paper introduces an agent-based RAG system designed for fintech domain retrieval, addressing challenges of acronym resolution, contextual understanding, and evaluation within complex, regulated environments. It proposes an Orchestrator Agent, specialized sub-agents for acronym handling, query rewriting, and re-ranking, alongside a semi-automated evaluation strategy utilizing LLM-as-a-judge and internal knowledge bases. The system aims to improve retrieval accuracy and answer relevance through a modular architecture and a focused evaluation methodology.\n",
      "II Related Work: This paper reviews recent advancements in Retrieval-Augmented Generation (RAG) systems, particularly focusing on agentic architectures and domain-specific implementations. Singh et al. and Nguyen et al. introduce Agentic RAG and MA-RAG, respectively, integrating autonomous agents for dynamic query decomposition and reasoning. Barron et al. propose the SMART-SLIC framework to enhance retrieval accuracy through knowledge graph integration. The Pathway team emphasizes agent task specialization and leverages structured knowledge sources.  Numerous studies demonstrate the benefits of these approaches in specialized domains like finance, healthcare, and law, highlighting the need for real-time, curated retrieval pipelines to improve factual accuracy and trustworthiness in high-stakes contexts.\n",
      "III Agent Design: III-A Baseline RAG System: A sequential retrieval pipeline utilizing specialized agents ‚Äì Query Reformulator, Retriever, and Summary Agent ‚Äì begins with user input, iteratively refining it through agents, retrieving relevant documents using cosine similarity, aggregating them into an answer, and avoiding information not found in the context. B-RAG serves as a simplified benchmark for evaluating agentic architecture improvements, despite limitations including a single-pass retrieval, lack of sub-query decomposition, and absence of re-ranking.\n",
      "\n",
      "III-B Proposed Refined Design: The A-RAG system integrates modular intelligence and task-specific specialization, utilizing an Orchestrator Agent to coordinate specialized agents focused on retrieval or synthesis. It addresses challenges like ambiguity and fragmented sources through features like acronym resolution, sub-query generation, parallel retrieval, re-ranking, and quality assessment. A comparison of the pipeline architectures is presented in Figure 3.\n",
      "\n",
      "III-C System Architecture and Workflow: The A-RAG system comprises eight specialized agents, orchestrated by an orchestrator, that guide iterative retrieval and reasoning. The system‚Äôs workflow begins with query reformulation and acronym expansion to enhance clarity, followed by vector retrieval, which returns top-k relevant chunks. Sub-queries are generated to address low initial retrieval scores, and the system re-ranks these based on semantic alignment. A Re-Ranker Agent further refines the output, while a Summary Agent synthesizes a concise answer from the retrieved content. An QA Agent evaluates the answer‚Äôs relevance and support, determining pipeline progression or refinement.\n",
      "IV Data and Evaluation Strategy: The study constructs a knowledge base from proprietary internal documents, preprocessing it through a custom pipeline to convert structured markup into clean, linear text. A vector index stores over 30,000 text chunks, with an average of 19 chunks per document, representing 1,624 unique documents. A multi-phase evaluation pipeline was developed, utilizing a language model to generate question-answer pairs and a quality control process to select relevant pairs. A benchmark was created, extracting questions from internal knowledge base content and human-verified answer pairs, forming the foundation for quantitative and qualitative evaluations.\n",
      "V Experiments and Results: B-RAG demonstrated a significantly improved retrieval accuracy (69.70%) compared to A-RAG (64.3%) using a human-curated benchmark, while A-RAG‚Äôs latency was reduced to 5.02 seconds, compared to B-RAG‚Äôs 0.79 seconds, due to iterative document re-ranking.  The A-RAG system‚Äôs sub-query generation and re-ranking modules yielded a higher proportion of correct answers, increasing the semantic accuracy metric to 7.04, while B-RAG‚Äôs performance was limited by missing exact matches.\n",
      "VI Discussion: The A-RAG system demonstrates improved retrieval accuracy through agentic decomposition, supported by manual review and cross-encoder re-ranking, particularly in procedural queries. Sub-query generation effectively addresses edge cases and improved coverage and semantic accuracy, while acronym resolution presents challenges due to over-filtering and inconsistent performance. The modular structure offers robustness to domain-specific ambiguity compared to the static B-RAG pipeline, although acronym resolution remains inconsistent.\n",
      "VII Conclusion and Future Work: This study demonstrated that a modular RAG pipeline, incorporating agent-driven resolution, decomposition, and reranking, improved retrieval accuracy by 62.35% compared to a baseline. Future work will focus on improved agent coordination through reinforcement learning, enhanced context awareness (discourse, temporal, multi-turn memory), and strengthening reasoning capabilities, particularly in producing helpful and nuanced responses.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 20/40 [16:06<16:31, 49.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.24832v1/1 Introduction\n",
      "../data/summaries/2510.24832v1/2 Related Work\n",
      "../data/summaries/2510.24832v1/3 Preliminaries\n",
      "../data/summaries/2510.24832v1/4 Motivation\n",
      "../data/summaries/2510.24832v1/5 Method\n",
      "../data/summaries/2510.24832v1/6 Analysis\n",
      "../data/summaries/2510.24832v1/7 Experiment\n",
      "../data/summaries/2510.24832v1/8 Conclusions\n",
      "../data/summaries/2510.24832v1/Ethics statement\n",
      "../data/summaries/2510.24832v1/Reproducibility statement\n",
      "../data/summaries/2510.24832v1/References\n",
      "../data/summaries/2510.24832v1/Appendix A Usage of LLMs\n",
      "../data/summaries/2510.24832v1/Appendix B Details of Experimental Setup\n",
      "../data/summaries/2510.24832v1/Appendix C Supplementary Experiment: Effect of Metric Selection\n",
      "1 Introduction: The Rationale Score (r-score) quantifies a query‚Äôs learning efficiency based on its reasoning tree structure, offering a more accurate assessment of difficulty than path-based metrics. The Re-Schedule algorithm, a data scheduling method, utilizes the r-score to create an efficient curriculum for RLVR training, prioritizing high-scoring queries in the initial phases to accelerate convergence on complex problems. Empirical evidence demonstrates this approach significantly improves average accuracy, achieving gains of up to 3.2% on complex reasoning tasks.\n",
      "2 Related Work: This paper reviews existing methods for improving LLM reasoning, focusing on a novel training data schedule optimization approach. It introduces GRPO, supplemented by techniques like LIMR, R 3 R^{3}, LPPO, SEED-GRPO, and DELT, which utilize data entropy and reasoning tree analysis to quantify query learning potential.\n",
      "3 Preliminaries: The GRPO algorithm optimizes a policy œÄ Œ∏ \\pi_{\\theta} based on generated responses Shao et al. (2024; Yu et al. 2025). It generates G G responses {o i } i = 1, and the policy calculates the token-level objective function r i, t = œÄ Œ∏ ‚Äã ( o i, t | q, o i, < t) œÄ old ‚Äã ( o i, t | q, o i, < t) r_{i,t}=\\frac{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\text{old}}(o_{i,t}|q,o_{i,<t})} for queries q q from a dataset ùíü \\mathcal{D} . The advantage term A i, t A_{i,t} is constant for all tokens within a single response, calculated by normalizing the response's reward R i R_{i} relative to the other responses in the group Œ¥ Œ¥ is a small constant for numerical stability. Data scheduling algorithms modulate the objective by weighting the query's contribution with œâ (q, t) œâ(q,t) and the tree is structured as a triplet (nodes, edges, parent-child relationships).\n",
      "4 Motivation: The study uses a Qwen2.5-Math-7B model to compare query accuracy across two sets of 100 queries. Blue lines represent queries with high initial accuracy but require extensive training, while red lines represent queries with simple tree structures that improve rapidly. The discrepancy reveals that path-based metrics like accuracy are misleading indicators of learning difficulty, motivating the development of a new metric based on reasoning tree structure.\n",
      "5 Method: The Reasoning Tree Schedule enhances RLVR performance by using a fixed-structure k-ary approximation for each query, leveraging the Reasoning Score (r-score) to guide learning. The r-score quantifies query difficulty, and the framework constructs a manageable tree through a branching strategy, maximizing accuracy gain while minimizing computational overhead. Dynamic weighting adjusts query weights based on training step and r-score, prioritizing simpler queries during early training and gradually shifting towards more challenging instances to improve generalization.\n",
      "6 Analysis: The experiment used the Minimum Corrective Nodes (MCN) as a metric to validate that reinforcement learning is optimizing reasoning tree structure during training. Training data exhibited a consistent downward trend, regardless of target accuracy, confirming the process‚Äôs effectiveness. The r-score method identified queries with lower learning difficulty, as demonstrated by improved model performance on the test set.\n",
      "7 Experiment: The Re-Schedule method consistently achieves state-of-the-art performance, scoring 48.5 on Qwen2.5-Math-7B and 44.5 on Qwen2.5-7B compared to baselines like ACC s i g m o i d and classical RLVR methods. The r-score, reflecting reasoning tree structure, proves more effective than path-based metrics for measuring query learning difficulty.  Decreasing the dynamic range resulted in performance degradation, highlighting the importance of a sufficient range for curriculum differentiation.\n",
      "8 Conclusions: The authors challenged the use of path-based metrics for RLVR data scheduling. They introduced the r-score, a novel metric measuring learnability based on reasoning tree structure, and proposed Re-Schedule, a curriculum learning algorithm utilizing the r-score. Experiments showed Re-Schedule consistently outperformed classical RLVR and existing scheduling methods, demonstrating that the r-score is a more effective measure of learnability than path-based accuracy.\n",
      "Ethics statement: The dataset has been manually reviewed to eliminate potential ethical concerns including discrimination, human rights violations, bias, exploitation, and other ethical issues.\n",
      "Reproducibility statement: Source code and datasets are included in the supplementary material to allow for replication of the main results.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Usage of LLMs: LLMs were utilized as a writing and editing tool to improve text clarity, readability, sentence structure, and grammatical correctness. The final content, including claims, design, and conclusions, was conceived and written by the authors.\n",
      "Appendix B Details of Experimental Setup: The authors implemented algorithms within the GRPO framework using a learning rate of 1e-6 across all experiments, generating 8 answers per question for 512 512 samples, training the policy network for 16 gradient steps, and limiting rollout steps to 150. Models were trained for a maximum of 150 rollout steps, following GRPO‚Äôs default design choices with token-level loss normalization and KL regularization.  The maximum input length is 1024, and the minimum is 3072. Experiments were performed on H20 GPUs. Results for the Qwen2.5-7B model were excluded due to the authors‚Äô publication of results from Qwen2.5-Math-7B.\n",
      "Appendix C Supplementary Experiment: Effect of Metric Selection: The ‚ÄòFix‚Äô method consistently outperformed ‚ÄòPruning‚Äô for both linear and sigmoid schedules, demonstrating a better alignment with the training process of reinforcement learning.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 21/40 [16:48<14:57, 47.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25504v1/1 Introduction\n",
      "../data/summaries/2510.25504v1/2 Problem Setting & Variants\n",
      "../data/summaries/2510.25504v1/3 Algorithmic Advances\n",
      "../data/summaries/2510.25504v1/4 MOS as an Algorithmic Toolbox\n",
      "../data/summaries/2510.25504v1/5 Emerging Applications\n",
      "../data/summaries/2510.25504v1/6 Open Challenges and Opportunities\n",
      "../data/summaries/2510.25504v1/7 Conclusion\n",
      "../data/summaries/2510.25504v1/Acknowledgments\n",
      "../data/summaries/2510.25504v1/References\n",
      "1 Introduction: Multi-objective search is a burgeoning field with growing interest across diverse disciplines, including operations research, robotics, and machine learning. This paper presents an accessible overview of recent advances, focusing on problem variants, algorithms, and applications, while acknowledging the need for further technical detail.\n",
      "2 Problem Setting & Variants: Okay, let's break down this text and summarize the key points about the paper's focus. Here's a consolidated explanation of the paper's core ideas:\n",
      "\n",
      "**The Paper's Goal: Finding the Pareto Front of a Graph**\n",
      "\n",
      "The paper tackles a computationally challenging problem: finding the Pareto front of a graph ‚Äì a set of solutions where no solution is worse across all objectives.  It's a bit of a puzzle, and the paper proposes a method to efficiently find this set.\n",
      "\n",
      "**Here's a breakdown of the key concepts and the paper's approach:**\n",
      "\n",
      "1. **The Problem:**  Imagine a graph with multiple paths (edges).  Each path has a \"cost\" (e.g., travel time, distance). The goal is to find the set of paths that offer the *best* combination of all these costs (the Pareto front).\n",
      "\n",
      "2. **The Challenge: NP-hardness:** Finding the Pareto front is computationally hard.  This means that no known algorithm can guarantee finding the best solution in a reasonable amount of time for large graphs.\n",
      "\n",
      "3. **The Proposed Solution (MOSSP):** The paper introduces a method called MOSSP (Multi-objective Stochastic Shortest Path).  It's a variation of the MOS problem, but it adds a probabilistic element ‚Äì the agent learns through experience.\n",
      "\n",
      "4. **Key Components of MOSSP:**\n",
      "   * **Probabilistic Transitions:** The agent doesn't just move from one path to another; it randomly chooses a path with a certain probability.\n",
      "   * **Objective Functions:**  The agent optimizes multiple objectives (cost, distance, etc.).\n",
      "   * **Local and Global Optimization:** MOSSP uses both local (finding good paths within a small region) and global (finding the best overall path) optimization techniques.\n",
      "\n",
      "5. **The Paper's Contribution:** The paper provides a method to efficiently compute the Pareto front, addressing the computational challenges of the problem.\n",
      "\n",
      "**In essence, the paper is about developing a method to efficiently explore and find the best possible combination of paths in a graph, given a probabilistic approach to learning.**\n",
      "\n",
      "**Do you want me to elaborate on any specific aspect of the paper, such as:**\n",
      "\n",
      "*   The mathematical details of the MOSSP algorithm?\n",
      "*   The specific challenges the paper addresses?\n",
      "*   The connection to other related research areas (e.g., reinforcement learning)?\n",
      "3 Algorithmic Advances: Here‚Äôs a concise summary of the provided text:\n",
      "\n",
      "Recent algorithmic advances in multi-objective optimization, particularly in the field of graph algorithms, have significantly improved the efficiency of exact and approximate methods.  The work focuses on improvements in generalization of the MOS algorithm, exemplified by the Multi-Objective A* (MOA ‚àó) algorithm, which introduced dimensionality reduction and node ordering techniques.  Recent advancements include algorithms like BOA ‚àó, which simplifies the MOA ‚àó approach, and recent work on parallelization strategies leveraging SIMD instructions.  The study of dynamic MOS theory, including the proposed framework for vertex classification, represents a significant theoretical contribution.\n",
      "4 MOS as an Algorithmic Toolbox: The Multi-Objective Minimum Spanning Tree (MO-MST) problem, originally NP-hard, is crucial for communication networks and transport logistics, leveraging MOS techniques like A ‚àó pex.  The Multi-Objective Multi-Agent Path Finding (MO-MAPF) problem addresses complex multi-objective optimization in shared environments, utilizing MAPF and MOS to generate Pareto-optimal path plans.  The Constrained Shortest-Path problem focuses on finding shortest paths subject to constraints, with recent advancements incorporating MOS and multi-objective techniques.\n",
      "5 Emerging Applications: MOS and its variants are applied across diverse fields including chemistry, biology, and engineering, demonstrating applicability despite its relative simplicity compared to more complex models. Examples include retrosynthesis planning in computational chemistry, drug discovery, and generative design, where MOS identifies novel molecular candidates with high efficacy and pharmacological profiles. Multi-modal journey planning, combining transport modes and optimization, is utilized in metropolitan areas through tools like OpenTripPlanner 2, generating approximately 40,000 trip plans per weekday in Portland, Oregon. MOS also contributes to robotics by balancing cost, energy, and safety, exemplified by rulebook-based autonomous vehicle planning, including considerations of traffic rules like Singapore‚Äôs Final Theory of Driving.\n",
      "6 Open Challenges and Opportunities: Despite progress, key challenges remain in MOS algorithms, including scalability limitations, difficulty navigating high-dimensional cost spaces, and the need for adaptable dynamic environments. Opportunities exist in leveraging preference elicitation, integrating with other AI fields, and developing a standardized benchmark suite to foster community collaboration and improve comparability across diverse domains.\n",
      "7 Conclusion: MOS has evolved into a broad principle influencing multiple disciplines and communities, with algorithmic advancements in search, approximate algorithms, and theoretical understanding. Applications have adopted MOS methods to address trade-offs between metrics, highlighting a need for greater cross-fertilization of multi-objective optimization approaches.\n",
      "Acknowledgments: Funding received from the United States-Israel Binational Science Foundation (BSF).\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 22/40 [17:58<16:10, 53.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25731v1/1 Introduction\n",
      "../data/summaries/2510.25731v1/2 Theoretical Background\n",
      "../data/summaries/2510.25731v1/3 LieSolver\n",
      "../data/summaries/2510.25731v1/4 Experiments and results\n",
      "../data/summaries/2510.25731v1/5 Conclusion\n",
      "../data/summaries/2510.25731v1/Appendix A Appendix\n",
      "../data/summaries/2510.25731v1/References\n",
      "1 Introduction: LieSolver aims to solve linear homogeneous PDEs using a Lie-symmetrical machine learning framework, offering a more accurate and efficient approach than PINNs by enforcing the PDE directly through a PDE-informed architecture, minimizing training time and requiring only boundary data.\n",
      "2 Theoretical Background: Okay, let's break down this text and focus on understanding the core concepts presented.  It‚Äôs a foundational introduction to the theory of Lie symmetries, particularly in the context of PDEs. Here‚Äôs a breakdown of what‚Äôs being discussed, organized for clarity:\n",
      "\n",
      "**1. The Core Idea: Lie Symmetries and PDEs**\n",
      "\n",
      "* **PDEs (Partial Differential Equations):**  These are equations that involve derivatives of a function (like temperature, pressure, or velocity) across different points in space and time.  They're incredibly important in many fields like physics, engineering, and biology.\n",
      "* **The Prolation of Functions:** The text emphasizes the crucial role of the *prolongation* of functions.  This is a key concept.  Instead of directly solving the PDE, we often *transform* the problem by applying a series of transformations (like rotations, reflections, and scaling) to the function.  These transformations are *defined* by the Lie group.\n",
      "* **Lie Groups:**  The text introduces the idea of Lie groups ‚Äì a fundamental concept in mathematics and physics.  They are groups of transformations that preserve the underlying structure of a space.  They provide a framework for understanding and analyzing PDEs.\n",
      "\n",
      "**2. The Role of Symmetry Transformations**\n",
      "\n",
      "* **Symmetry Transformations:**  The text explains that the core of the theory lies in the *symmetry transformations* that are applied to the PDE. These transformations are defined by the Lie group.\n",
      "* **Local Group of Transformations:** The text introduces the idea of a *local group* of transformations.  This is a crucial step.  Instead of applying a single transformation to the entire PDE, we consider a *subset* of transformations that are \"close\" to the underlying structure.\n",
      "* **The Transformation Process:** The text describes how the transformation process works. It involves applying a series of transformations to the function, which are defined by the Lie group.\n",
      "\n",
      "**3.  The Importance of the Lie Group**\n",
      "\n",
      "* **Group Structure:** The text highlights that the Lie group provides a *group structure* to the transformations. This structure is essential for understanding the behavior of the PDE.\n",
      "* **Two-Dimensional Group:** The text introduces the idea of a *two-dimensional group* ‚Äì a fundamental concept in Lie group theory.  This group is defined by the transformations.\n",
      "\n",
      "**4.  The Connection to PDEs**\n",
      "\n",
      "* **The Prolation of Functions:** The text connects the prolongation of functions to the Lie group.  The transformation process is essentially a way to \"rotate\" or \"reflect\" the function, and the Lie group provides the framework for understanding these transformations.\n",
      "\n",
      "**5.  Key Concepts Summarized**\n",
      "\n",
      "* **PDEs:** Equations involving derivatives.\n",
      "* **Prolation of Functions:**  Systematically listing all derivatives up to order.\n",
      "* **Lie Groups:** Groups of transformations that preserve the underlying structure of a space.\n",
      "* **Symmetry Transformations:** Transformations that preserve the underlying structure.\n",
      "* **Local Group of Transformations:** A subset of transformations that are \"close\" to the underlying structure.\n",
      "\n",
      "**In essence, the text is setting the stage for a deeper understanding of how to analyze PDEs using the tools of Lie groups and symmetry transformations.**\n",
      "\n",
      "---\n",
      "\n",
      "**To help me tailor my explanation further, could you tell me:**\n",
      "\n",
      "*   What specifically are you struggling with understanding about this text? (e.g., the concept of a Lie group, the transformation process, or the role of symmetry?)\n",
      "3 LieSolver: LieSolver constructs solutions of IBVPs using a sequence of transformations, leveraging a set of base solutions to define the solution's expressivity. The algorithm iteratively applies transformations, combines base solutions, and employs variable projection to optimize the solution, ensuring a wide range of patterns are captured.\n",
      "4 Experiments and results: LieSolver outperforms PINNs in accuracy and efficiency, achieving near-order-of-magnitude reduction in MSE across various ICs. Gaussian bases yield a reliable MSE within a minute, while the step IC requires more bases. The sine mix IC benefits from a larger PINN model, and the LieSolver-based approach handles the wave equation with a step IC effectively, outperforming PINNs.\n",
      "5 Conclusion: LieSolver utilizes Lie symmetries to generate parametrized functions, solving initial-boundary value problems by optimizing parameters to satisfy IBCs. It achieves IBC MSE below 10-6 with fewer than 40 bases compared to PINNs, offering robust optimization and accurate predictive accuracy through a sparse, analytical expression. The method‚Äôs compact nature and speed advantages, combined with the inherent physics of the PDE, make it suitable for a wide range of problems, including heat and wave equations.  Future work will focus on expanding the framework to more general PDEs and automating base solution selection.\n",
      "Appendix A Appendix: Okay, let‚Äôs refine this explanation further, focusing on enhancing the clarity and depth of the description, particularly around the exponential map and the connection to the group structure.  Here‚Äôs a revised version incorporating these suggestions:\n",
      "\n",
      "**Revised Explanation of Symmetry Group Derivation**\n",
      "\n",
      "The goal of this section is to systematically derive the symmetry group of a system of PDEs, leveraging the exponential map to connect the generated vector field to the group‚Äôs structure.  This process is crucial for understanding the fundamental properties of the PDE system and for potentially simplifying subsequent analysis.\n",
      "\n",
      "**1. Check Solvability & Local Un solvability:**\n",
      "\n",
      "The initial step involves verifying that the system of PDEs is locally solvable ‚Äì meaning there exists a smooth solution for any point (x‚ÇÄ, u‚ÇÄ) within the domain of the PDE.  This is achieved through techniques like the method of fixed points and the analysis of the PDE‚Äôs behavior near the point.  If the system is not locally solvable, the derivation of the symmetry group becomes significantly more complex, potentially requiring more advanced techniques like the method of representations.\n",
      "\n",
      "**2. Generate Ansatz:**\n",
      "\n",
      "We construct an ansatz ‚Äì a vector field representation ‚Äì for the PDE. This ansatz consists of a set of coefficients, typically denoted as  `v = (v‚ÇÅ, v‚ÇÇ, ..., v)`  that define the evolution of the vector field.  The choice of this ansatz is crucial, as it directly influences the resulting symmetry group.  The ansatz is typically expressed as a function of the variables `x` and `u` (where `x` represents the spatial coordinates and `u` represents the time).\n",
      "\n",
      "**3. Derive Equations:**\n",
      "\n",
      "The core of the derivation lies in translating the ansatz into a set of equations.  This involves a careful mapping of the PDE‚Äôs equations to a set of differential equations.  The key is to identify the transformation that relates the initial state (defined by the ansatz) to the final state (determined by the PDE).  This transformation is fundamentally driven by the group‚Äôs action.  The equations are typically expressed in a form that allows for a direct computation of the generated vector field.  The derivation of these equations is often the most challenging part of the process.\n",
      "\n",
      "**4.  The Exponential Map ‚Äì The Key Connection:**\n",
      "\n",
      "The exponential map, denoted as `exp(Œ∏)`, is the cornerstone of this derivation.  It‚Äôs a mathematical tool that allows us to \"translate\" the evolution of the vector field `v` into a transformation of the variables `x` and `u`.  Specifically, the exponential map maps the evolution of the vector field `v` to a transformation of the variables `x` and `u` in a way that reveals the group‚Äôs structure.  The transformation is defined by a set of equations that relate the initial state to the final state.  The exponential map is crucial because it provides a way to \"unfold\" the PDE into a set of differential equations that can be solved.\n",
      "\n",
      "**5.  Derive Transformation Maps:**\n",
      "\n",
      "The equations derived from the ansatz are then transformed into transformation maps. These maps represent the group‚Äôs action on the vector field.  The transformation maps are the heart of the derivation.  They determine how the evolution of the vector field changes as it moves through the group.  The properties of these transformation maps ‚Äì their smoothness, continuity, and the specific terms involved ‚Äì directly dictate the symmetry group.\n",
      "\n",
      "**6.  Group Structure & Symmetry:**\n",
      "\n",
      "Finally, the derivation culminates in determining the group structure of the system. This involves analyzing the transformation maps to identify the group‚Äôs elements.  The group‚Äôs structure is defined by the symmetry properties of the transformation maps.  For example, a rotational symmetry might manifest as a specific transformation map that preserves the vector field‚Äôs orientation.  The group‚Äôs structure is then used to define the symmetry group, which is a set of transformations that preserve the essential properties of the system.\n",
      "\n",
      "**7.  Potential Challenges & Considerations:**\n",
      "\n",
      "* **Complexity of Equations:** The derivation of the equations can be quite complex, especially for systems with multiple variables and intricate PDE dynamics.\n",
      "* **Computational Complexity:**  The exponential map and the resulting equations can be computationally intensive, particularly for large-scale systems.\n",
      "* **Understanding the Group:**  The group structure itself can be subtle and require careful analysis.\n",
      "\n",
      "**In essence, this revised explanation emphasizes the crucial role of the exponential map as a bridge between the PDE's dynamics and the group's structure, highlighting the key steps involved in deriving the symmetry group.**\n",
      "\n",
      "---\n",
      "\n",
      "Do you think this revised version provides a more comprehensive and insightful explanation of the process? Would you like me to elaborate on any specific aspect, such as the mathematical details of the exponential map or the challenges involved?\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 23/40 [19:20<17:38, 62.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25668v1/1 Introduction\n",
      "../data/summaries/2510.25668v1/2 Related Work\n",
      "../data/summaries/2510.25668v1/3 Preliminaries\n",
      "../data/summaries/2510.25668v1/4 Methodology\n",
      "../data/summaries/2510.25668v1/5 Experiments\n",
      "../data/summaries/2510.25668v1/6 Conclusions\n",
      "../data/summaries/2510.25668v1/LLM Usage Statement\n",
      "../data/summaries/2510.25668v1/Acknowledgments\n",
      "../data/summaries/2510.25668v1/Reproducibility Statement\n",
      "../data/summaries/2510.25668v1/References\n",
      "../data/summaries/2510.25668v1/Appendix A Datasets\n",
      "../data/summaries/2510.25668v1/Appendix B Baselines\n",
      "../data/summaries/2510.25668v1/Appendix C Implementation Details\n",
      "../data/summaries/2510.25668v1/Appendix D Evaluation Metrics\n",
      "../data/summaries/2510.25668v1/Appendix E Case Study\n",
      "1 Introduction: Agentic Long-Document Navigation (ALDEN) is a multi-turn reinforcement learning framework designed to train Visual Language Models (VLMs) as active agents capable of navigating and reasoning over long documents to deliver accurate and adaptive question answering. ALDEN expands the action space with a fetch action, incorporates a cross-level reward function, and introduces visual semantic anchoring to improve training robustness and efficiency. Experiments on five benchmarks demonstrate ALDEN‚Äôs superior performance compared to baselines, achieving a 9.14% average improvement in answer accuracy.\n",
      "2 Related Work: The study investigates A-VRDU, a task requiring active, multi-step retrieval and reasoning using RL fine-tuning of VLMs. Recent research explores extending context length through RL to address real-world document challenges, while semantic retrieval offers a scalable approach to document focus. Existing retrieval-based methods are static and brittle, prompting a new RL framework for this task.\n",
      "3 Preliminaries: 3.1 Problem Formulation: The agent analyzes documents to answer queries, collecting evidence through sequential decision-making. The task utilizes a Markov Decision Process (MDP) to guide actions, receiving visual observations and rewards. The objective is to maximize expected cumulative reward through iterative evidence collection.\n",
      "\n",
      "3.2 Proximal Policy Optimization for fine-tuning LLMs: PPO fine-tunes LLMs by treating tokens as actions, using a reward model and KL divergence to prevent policy drift. Value function predicts expected returns, and GAE calculates advantage, optimizing the LLM by maximizing surrogate objectives and adapting to long-horizon interactions in the A-VRDU task.\n",
      "4 Methodology: We propose a Visual Semantic Anchoring mechanism to mitigate training fluctuations in RL for A-VRDU, addressing the challenge of large visual token counts. This involves dual-path KL regularization to constrain hidden states during policy optimization, stabilizing language generation and preserving semantic grounding, while preventing drift.\n",
      "5 Experiments: We conduct experiments on long VRDU benchmarks to (i) compare ALDEN with strong baselines and (ii) assess the contribution of its key components, including expanded action space, cross-level reward, and visual semantic anchoring, to navigation accuracy, answer quality, and training stability. We first outline datasets, baselines, implementation details, and evaluation metrics (section 5.1), then present main results (section 5.2), followed by ablations (section 5.3) and detailed component analyses (section 5.4). Datasets. We build the training set by merging and processing three multi-page VRDU datasets: DUDE (Van Landeghem et al., 2023), MPDocVQA (Tito et al., 2023a), and SlideVQA (Tanaka et al., 2023a) . We filter out documents with fewer than 10 pages. To enrich query diversity, we use GPT-4o (Hurst et al., 2024) to rewrite part of MPDocVQA, increasing the proportion of page-index‚Äìreferenced queries in the final training corpus. Detailed statistics of the resulting training set are provided in table 1 . The evaluation is conducted mainly on the following VRDU benchmarks: MMLongBench (Ma et al., 2024b), LongDocURL (Deng et al., 2024), PaperTab (Hui et al., 2024), PaperText (Hui et al., 2024), and FetaTab (Hui et al., 2024) . To evaluate the fetch action, we create DUDE-sub, a DUDE validation subset with 480 general queries and 480 queries containing explicit page references or implicit sequential navigation cues. Baselines. To validate the effectiveness of ALDEN, we compare it with three categories of baselines. (1) Full-Document Input : mainstream state-of-the-art VLMs are prompted with the entire document as context to answer user queries. (2) Visual RAG : methods that retrieve the most relevant document pages using the user query, including M3DocRAG (Cho et al., 2024) , and ReSearch-VL, a Search-only ALDEN variant trained with GRPO using outcome-based rewards adapted from a fully textual method ReSearch (Chen et al., 2025b) . (3) Hybrid RAG : approaches that augment page images with OCR-extracted text for retrieval and reasoning, including MDocAgent (Han et al., 2025) , VidoRAG (Wang et al., 2025b) . Detailed baseline configurations can be seen in appendix B. Implementation Details. Both the policy and value models are initialized from Qwen2.5-VL-7B-Instruct (Bai et al., 2025) , and all Visual RAG and Hybrid RAG baselines use the same backbone for fairness. During training, we adopt the single-vector retriever vdr-2b-v1 (Ma et al., 2024a) for images and e5-large-v2 (Wang et al., 2022) for text. For evaluation, we also report results with the multi-vector retrievers ColQwen2-v1.0 (ColQwen) (Faysse et al., 2025) for images and ColBERT-v2.0 (ColBERT) (Santhanam et al., 2021) for text. Unless otherwise noted, each search action retrieves the top- 1 1 candidate page, with a maximum of T = 6 T=6 reasoning‚Äìaction turns. On average, ALDEN collects 1.87 unique pages per query; hence, single-turn RAG baselines are set to retrieve the top-2 pages for a fair comparison. Further implementation details are provided in appendix C . Evaluation Metrics. The primary evaluation metric is GPT-4o‚Äìjudged answer accuracy ( Acc ) on each benchmark. For finer-grained analysis of ALDEN‚Äôs components, we further assess navigation quality using trajectory-level retrieval recall ( Rec ), precision ( Pre ), and F1-score ( F1 ).\n",
      "\n",
      "6 Conclusions: ALDEN is a reinforcement-learning framework for autonomous agents trained to navigate and reason across vast document information. Extensive experiments demonstrate ALDEN achieves state-of-the-art accuracy and improves evidence localization, validating the benefits of its components ‚Äì including a fetch action, reward modeling, and visual anchoring. The A-VRDU paradigm represents a shift towards autonomous document understanding, with ALDEN‚Äôs strong performance highlighting the potential for more accurate, scalable, and adaptive agents.\n",
      "LLM Usage Statement: Large Language Models (LLMs) were utilized as writing and editing tools, specifically OpenAI‚Äôs ChatGPT (GPT-5), assisting with grammar, clarity, and phrasing suggestions. The authors conceived and executed all research, design, data processing, model development, and analysis independently.\n",
      "Acknowledgments: Funding from the Lower Saxony Ministry of Science and Culture, the VW Foundation, and the German Federal Ministry of Research, Technology and Space.\n",
      "Reproducibility Statement: The authors commit to reproducibility by releasing all source code, datasets, and configuration files used in their experiments. This includes training scripts, evaluation code, data preprocessing scripts, and model hyperparameters. They will also provide checkpoints for the policy and value models, as well as prompts for GPT-4o evaluation, ensuring others can replicate their work. The experiments were performed on NVIDIA A100 GPUs with PyTorch 2.4 and Transformers 4.49.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Datasets: Training data consists of 30,728 samples, comprising user queries, answers, and evidence page indices derived from DUDE, MPDocVQA, and SlideVQA. Benchmarks include MMLongBench-Doc (long, visually rich documents), LongDocURL (vision-language models on long, multimodal documents), PaperText (academic papers), PaperTab (tables in academic papers), and fetaTab (free-form question answering over Wikipedia tables).\n",
      "Appendix B Baselines: Here's a summary of the provided text:\n",
      "\n",
      "The study compares five RAG methods ‚Äì ALDEN, M3DocRAG, ReSearch, MDocAgent, and VidoRAG ‚Äì against three VLMs, including Qwen2.5-VL-7B. ALDEN utilizes a multi-page visual processing capability, while M3DocRAG employs a MaxSim-based retrieval mechanism. ReSearch leverages reinforcement learning to improve search actions, while MDocAgent and VidoRAG combine visual and textual retrieval with specialized agents. The study demonstrates that ALDEN performs best, with M3DocRAG achieving significant improvements over baseline models.\n",
      "Appendix C Implementation Details: Implementation details include a framework based on EasyR1, initializing policy and value functions with Qwen2.5-VL-7B-Instruct, using a batch size of 128, learning rates of 1x10^-6 and 1x10^-5, maximum interaction turns of 6, and a trajectory length of 19000 tokens. KL coefficients are set to Œ≤ gen = 0.001, Œ≤ gen = 0.001, and Œ≤ obs = 0.01, search actions limited to top-1 retrieved pages, a m NDCG@m metric with m=5, a scale coefficient Œ±=5, a repetition penalty Œ∑=0.5, and a token repetition penalty Œ≥=1.0.  The GAE, Œ≥ token = 1.0, Œ≥ turn = 0.9, and Œª token = 1.0 are set.  The system prompt used for Visual RAG variant is shown, and the Multi-vector retriever ColQwen is used for evaluation. Training was performed on 16 NVIDIA A100-80Gb GPUs.\n",
      "Appendix D Evaluation Metrics: Models are evaluated using LLM-as-judge, GPT-4o classification, and trajectory-level Recall and Precision metrics. The trajectory-level Recall and Precision are calculated based on the number of successfully retrieved ground-truth pages and relevant pages, respectively. The F1 Score is calculated based on the combination of Recall and Precision.\n",
      "Appendix E Case Study: The appendix presents typical LongDocURL benchmark examples demonstrating the ALDEN model's ability to combine search and fetch actions to gather evidence and answer questions effectively.  It showcases successful cases like the agent utilizing fetch to overcome failures in retrieval, and includes examples of struggles with evidence verification and exploration when search results are distant from ground truth.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 24/40 [20:22<16:36, 62.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25626v1/1 Introduction\n",
      "../data/summaries/2510.25626v1/2 Related Work\n",
      "../data/summaries/2510.25626v1/3 Logic Programming and Deductive Reasoning\n",
      "../data/summaries/2510.25626v1/4 Evaluating Language Models on Grade School Math Word Problems\n",
      "../data/summaries/2510.25626v1/5 Experiments\n",
      "../data/summaries/2510.25626v1/6 Conclusion\n",
      "../data/summaries/2510.25626v1/Acknowledgments and Disclosure of Funding\n",
      "../data/summaries/2510.25626v1/References\n",
      "../data/summaries/2510.25626v1/Appendix A Further Background\n",
      "../data/summaries/2510.25626v1/Appendix B Decidability of GSM Programs\n",
      "../data/summaries/2510.25626v1/Appendix C Data Generation\n",
      "../data/summaries/2510.25626v1/Appendix D More Details on Experiments\n",
      "1 Introduction: This paper introduces a formalization of deductive reasoning based on logic programming, aiming to evaluate language model reasoning efficiency. It demonstrates that while LMs often generate more tokens than necessary, recent findings suggest a key dimension of reasoning ‚Äì efficiency ‚Äì that standard evaluations fail to address. The authors construct verbalized logic programs for grade school math word problems, demonstrating that injecting irrelevant axioms significantly reduces accuracy, particularly when the irrelevant axioms overlap with the goal theorem. Experiments on GSM problems reveal that LMs are inefficient when dealing with irrelevant axioms, suggesting a heuristic search process may be employed.\n",
      "2 Related Work: This work presents an automatic evaluation method for reasoning tasks, generating problems with irrelevant information to assess LM performance. It leverages logic programming and proposes a method for formally defining irrelevance, focusing on generating proofs that only contain necessary steps, contrasting with previous evaluations relying on manual scrutiny or heuristic measures. The study also investigates the efficiency of transformer-based Large Language Models for search, proposing methods to improve reasoning through length rewards and token reduction.\n",
      "3 Logic Programming and Deductive Reasoning: Okay, let's delve deeper into the concept of \"queries\" within the context of the text.  We'll focus on how queries are used to guide the forward-chaining process and why they're important for the program's reasoning.\n",
      "\n",
      "**The Role of Queries in Forward Chaining**\n",
      "\n",
      "The text emphasizes that queries are *not* just simple questions. They are carefully orchestrated steps that the program takes to explore the possible truth values that can be derived from the input.  The query acts as a focused request to the program, prompting it to evaluate a specific truth value based on the current state of the program.  This is a critical element of the forward-chaining process.\n",
      "\n",
      "**Here's a breakdown of how queries contribute to the forward-chaining:**\n",
      "\n",
      "1. **Initialization:** The forward-chaining process begins with an initial set of axioms. These axioms define the starting point of the reasoning.\n",
      "\n",
      "2. **Query Generation:**  The program generates a query for each axiom.  The query is designed to explore the possible truth values that can be derived from the axiom.  It's a targeted request to the program to evaluate a specific truth value.\n",
      "\n",
      "3. **Chain of Evaluation:** The program then chains together these queries, systematically evaluating the truth values of the axioms and the resulting queries.  Each query is a step in the chain.\n",
      "\n",
      "4. **Constraint Satisfaction:** The query process is driven by the goal theorem. The program is trying to find a truth value that satisfies the goal theorem. The queries are designed to explore the possible truth values that can lead to the goal theorem.\n",
      "\n",
      "5. **Iteration and Refinement:** The forward-chaining process continues iteratively, refining the truth values as it moves through the chain of queries.  The program uses the results of each query to guide the next query, effectively narrowing down the possible truth values.\n",
      "\n",
      "**Why Queries are Important for Efficiency**\n",
      "\n",
      "* **Focus and Narrowing:** Queries allow the program to focus its search on the most promising paths.  By asking specific questions, the program avoids exploring irrelevant possibilities.\n",
      "* **Heuristic Guidance:** The query process acts as a heuristic, guiding the search towards the goal theorem.  The queries are designed to explore the truth values that are most likely to lead to the solution.\n",
      "* **Avoiding Redundancy:**  Queries can be designed to avoid redundant evaluations.  For example, the program might ask a query to determine the truth value of a particular axiom, and then use the result to evaluate subsequent axioms.\n",
      "\n",
      "**Analogy**\n",
      "\n",
      "Think of it like a detective investigating a crime. The detective doesn't just randomly ask questions. They ask specific questions (queries) to narrow down the possibilities and eventually identify the culprit.\n",
      "\n",
      "**In summary, queries are not just about asking questions; they are carefully crafted steps that guide the forward-chaining process, enabling the program to efficiently explore the possible truth values and ultimately arrive at a solution.**\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to elaborate on any of these points further, perhaps by providing a more concrete example of a query or explaining a specific aspect of the forward-chaining process in more detail?\n",
      "4 Evaluating Language Models on Grade School Math Word Problems: Okay, let's delve deeper into the process and the reasoning behind it. Here‚Äôs a breakdown of the key aspects, expanding on the explanation and adding some more context:\n",
      "\n",
      "**1. The Core Idea: Controlled Irrelevance**\n",
      "\n",
      "The central idea is to introduce \"irrelevant\" axioms ‚Äì axioms that don‚Äôt directly contribute to the primary goal of the logical program ‚Äì but are strategically placed to create a more robust and flexible system.  The text emphasizes that the program isn‚Äôt just about solving the original problem; it‚Äôs about being able to handle a broader range of reasoning scenarios.\n",
      "\n",
      "**2. The Four Cases ‚Äì A Deeper Dive**\n",
      "\n",
      "Let's break down each case more thoroughly:\n",
      "\n",
      "* **Case 1: No Overlap (A W ‚Äã ( h g ) A_{\\text{W}}(h_{g}) ):** This is the ideal scenario. The axioms are designed to be consistent with the original goal theorem, and there's no need to introduce new information.  It‚Äôs a baseline for the program.\n",
      "\n",
      "* **Case 2: Entity Overlap (A W ‚Äã ( h g ) A_{\\text{W}}(h_{g}) ):** This is where the \"irrelevant\" aspect comes in. The axioms are related to the entities involved in the original goal theorem.  This is a crucial step because it introduces a potential for reasoning about the system's components, which can be valuable for certain tasks.  The text suggests this is a deliberate choice to increase the program's flexibility.\n",
      "\n",
      "* **Case 3: Agent Overlap (A W ‚Äã ( h g ) A_{\\text{W}}(h_{g}) ):** This case is about the agents involved.  The axioms are linked to the agents' actions or roles.  This can be useful for tasks that require understanding of interactions between entities.\n",
      "\n",
      "* **Case 4: Distractor Theorem (‚Ä¶):** This is the most complex case. The axioms are designed to be related to the *distractor theorems* ‚Äì theorems that demonstrate the limitations of reasoning.  The text implies that this case is designed to be a \"test\" of the program's ability to handle more complex reasoning.\n",
      "\n",
      "**3. The Role of the Distractor Theorems**\n",
      "\n",
      "The distractor theorems are a critical component. They act as a \"constraint\" on the program.  They force the program to consider alternative reasoning paths and demonstrate that it can handle situations where the original goal theorem might not be applicable.  The text suggests that the program is designed to be able to handle situations where the original goal theorem is not sufficient.\n",
      "\n",
      "**4.  The Purpose of the Verbalizer (ŒΩ ùí´)**\n",
      "\n",
      "The text mentions the verbalizer ŒΩ ùí´ as a key element.  It's a system for translating logic programs into natural language.  This is vital because it allows for a more human-readable and potentially easier-to-understand representation of the program's reasoning process.  The verbalizer is used to create a \"bridge\" between formal logic and natural language.\n",
      "\n",
      "**5.  The Overall Strategy ‚Äì Controlled Irrelevance**\n",
      "\n",
      "The core strategy is to *intentionally* introduce irrelevant information.  This isn't about making the program useless; it's about creating a system that can handle a wider range of problems.  The text suggests that the program is designed to be adaptable and capable of handling unexpected situations.\n",
      "\n",
      "**6.  The Text's Implications ‚Äì A Gradual Approach**\n",
      "\n",
      "The text implies a gradual approach.  The initial setup is simple (a few axioms), and the process is then refined through the four cases.  The goal is to build a system that can handle a variety of reasoning scenarios without being overly constrained.\n",
      "\n",
      "**In summary, the text describes a technique for creating logical programs that are designed to be flexible and adaptable. It leverages the strategic introduction of irrelevant axioms to enhance the program's robustness and ability to handle a wider range of reasoning scenarios.**\n",
      "\n",
      "---\n",
      "\n",
      "Do you want me to:\n",
      "\n",
      "*   Expand on any of these points in more detail?\n",
      "*   Discuss the potential challenges of this approach?\n",
      "*   Provide a more concrete example of how the four cases might manifest in a specific logical program?\n",
      "5 Experiments: We use proofs generated from the family of verbalized logic programs introduced in ¬ß Àú 4 to help understand how LMs reason about GSM problems. We generate 500 problems with varying structural, agent and entity overlap, as discussed above. We refer to the problems without any irrelevant axioms as base problems. We additionally generate control problems that have the same number of axioms as the problems with irrelevant axioms, except that all axioms are relevant. Their shortest proofs contain the base problem‚Äôs shortest proof as a subproof. This controls for the possible confounder of problem length (see, e.g., Leeb et¬†al., 2025 ). We consider both non-ground queries, corresponding to questions like ‚ÄúHow many drones does Yanick have?‚Äù , 11 11 11 Assuming numerical consistency ensures there exists at most one ground atom in the minimal Herbrand model that unifies with the non-ground query. This ensures that forward chaining would halt in finite time. and ground queries, corresponding to questions like ‚ÄúShow that Yanick has 5 drones.‚Äù . We use one ‚Äúvanilla‚Äù LM and three reasoning LMs in our experiments: Llama-3.1-8B-Instruct (Llama Team, 2024 ) , Qwen2.5-Math-7B-Instruct (Yang et¬†al., 2024 ) , Qwen‚Äôs reasoning model QwQ-32B, and DeepSeek-R1 (DeepSeek-AI, 2025 ) . We generate strings using ancestral sampling, restricting the context length to 4000 4000 tokens. Our experimental design is based on in-context learning (Brown et¬†al., 2020 ) . Specifically, we use five fixed in-context examples of shortest proofs to expose the LM to proofs in our verbalized logic programs. The verbalized proofs are ordered under a DFS traversal of the theorems in the proof, such that the axioms are popped in the same order they occur in the verbalized text. See ¬ß Àú D.2 for the prompt. We begin by analyzing how irrelevant axioms influence an LM‚Äôs ability to generate the correct goal theorem for non-ground queries. We employ the two-step prompting strategy given by Kojima et¬†al. ( 2022 ) . In the first step, the model is prompted to produce a natural-language proof outlining its reasoning process. This natural-language process is then mapped to a proof in the verbalized logic program. In the second step, we prompted the LM a second time‚Äîconditioned on the proof it generated‚Äîto produce the goal theorem. In Table Àú 2 , we report model accuracy in generating the correct goal theorem. We observe that even a single irrelevant axiom reduces model performance, particularly for Llama-3.1 and Qwen2.5. Performance degrades further as additional irrelevant axioms are introduced. The performance of the most capable model, DeepSeek-R1, is nearly saturated at perfect accuracy, though slight decreases are still observed when irrelevant axioms are included. Across models, accuracy on problems containing irrelevant axioms is almost always lower than on the corresponding control examples, suggesting that irrelevance has a substantial effect on accuracy beyond what can be explained by longer problem statements. The only exception is Llama-3.1, whose performance on problems with one irrelevant axiom exceeds that of the corresponding control. QWQ-32B stands out as an outlier: for this model, performance on the control problems is significantly higher than on the original base problems. In Fig. Àú 7 ( ¬ß Àú D.4 ), we present results stratified by agent and entity overlap ( ¬ß Àú 4.3 ). A consistent pattern emerges: such overlap between the goal and irrelevant axioms makes solving the problem more difficult. Compared to no overlap, performance drops with both kinds of overlap, suggesting both serve as heuristics during search. While drops are typically larger for agent overlap than for entity overlap, we note that this could be partially due to the entities being topically related ( ¬ß Àú 4.3 ). In the following subsection, we analyze the proofs in greater detail to further illuminate these effects. In Fig. Àú 3 we plot the empirical distribution over the number of tokens in the model‚Äôs output and compare it to the number of tokens in the natural language annotation of the shortest proof, taking only the proofs that concluded at the correct goal theorem. This analysis, as well as those in the remainder of this section, are done on the problems with multiple irrelevant trees (i.e., the kind of structural overlap with the most axioms; ¬ß Àú 4.3 ). We observe that all models often use more tokens than are in the annotations, suggesting excessive computation for goal theorem proofs. This is consistent with findings on reasoning models ( ¬ß Àú 2 ), but holds also for the Llama model. However, these results do not confirm that the models generate irrelevant theorems‚Äîthey might just be more verbose than our annotations. We therefore also consider a metric in which only non-axiom theorems are counted. In ¬ß Àú D.4 we provide an additional analysis on search order. The main results are shown in Table Àú 3 (non-ground queries), with scores stratified by agent and entity overlap. The efficiency scores are far from 100 % 100\\% , meaning that the models predict several theorems beyond the required ones present in the shortest proof. Additionally, we observe that the efficiency scores vary significantly across the type of overlap ( p < 0.001 ) (p<0.001) , so we conclude that lexical information in the query has a substantial effect on proof planning. We finally comment on the results that only consider non-axioms, presented at the last row of Table 3 . We again observe efficiency scores that are considerably below 100 % 100\\% , showing that the LMs prove theorems that are irrelevant to the query. ¬ß Àú D.3 gives an example where Llama-3.1-8B-Instruct proves irrelevant theorems. We compare the performance to the same problems when presented with ground queries. Queries tend to be non-ground in the GSM domain (Riley et¬†al., 1983 ; Cobbe et¬†al., 2021 ) . Since LMs are heavily influenced by training data, we therefore expect them to perform better on non-ground queries. Our results on the same verbalized logic programs as before, but with ground queries, are presented on the right-hand side of Table Àú 3 . We observe lower efficiency scores, suggesting that LMs are indeed worse at proving ground theorems in this domain.\n",
      "6 Conclusion: This paper investigates efficiency in language models through a logic programming framework, demonstrating that introducing irrelevant axioms leads to significantly reduced accuracy on math word problems, even when controlling for problem length. The work advocates for improved reasoning efficiency models.\n",
      "Acknowledgments and Disclosure of Funding: We thank Juan Luis Gastaldi for discussion and criticisms, and Andreas Opedal for funding from the Max Planck ETH Center for Learning Systems.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Further Background: This paper describes a logic program forward chaining algorithm. The algorithm uses a priority queue Q Q to manage theorems, iteratively proving theorems from a logic program ùí≥, using a chart ùíû \\mathcal{C} to track proven theorems. The algorithm terminates when the agenda is empty, returning T if all goal theorems can be proven, and F if the agenda is not empty and the minimal Herbrand model is infinitely large.\n",
      "Appendix B Decidability of GSM Programs: The extended Herbrand base H ¬Ø \\overline{H} is countably infinite, consisting of unbounded sets of quantity constants and timestamp constants.  The axioms A W A_{\\text{W}} are numerically consistent, and the sets Œ£ p W {{\\Sigma}_{{\\color[rgb]{0.0,0.0,0.5}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\textit{{x}}}}, E x {{E}_{{\\color[rgb]{0.0,0.0,0.5}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\textit{{x}}}}} are finite.  Therefore, T ùí´ ‚àó ‚Äã ( A W ) \\mathrm{T}^{*}_{\\mathcal{P}}(A_{\\text{W}}) contains only finitely many theorems for each element of ‚Ñï x t {\\mathbb{N}^{t}_{{\\color[rgb]{0.0,0.0,0.5}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\textit{{x}}}}^{{{A}}}} , and M M contains only finitely many theorems for each element of ‚Ñï x t {\\mathbb{N}^{t}_{{\\color[rgb]{0.0,0.0,0.5}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.0,0.5}\\textit{{x}}}}^{{{A}}}} , so reasoning in ùí´ W \\mathcal{P}_{\\text{W}} reduces to Presburger arithmetic.\n",
      "Appendix C Data Generation: Okay, let‚Äôs delve deeper into the analysis of the provided text, focusing on the implications of the lemmas and the overall argument.\n",
      "\n",
      "**Deep Dive into the Lemmas & Argument**\n",
      "\n",
      "The core of the argument hinges on the *structure* of the proof system and its ability to guarantee a shortest proof. The lemmas highlight this structure:\n",
      "\n",
      "* **Lemma 1: \"There exists one unique shortest proof.\"** This is the foundational assertion. It‚Äôs a statement of *optimality*.  The system, through its rules and substitution procedure, *always* produces a shortest proof.  This isn't just about finding a shorter proof; it‚Äôs about *guaranteeing* a shortest proof.  This is a crucial assumption for the entire system.\n",
      "\n",
      "* **Lemma 2: \"There will be no additional proofs '' \\mathcal{P} '}^{\\prime} where |' \\mathcal{P} '}^{\\prime} | ‚â§ || |\\mathcal{P} '}^{\\prime} |leq|\\mathcal{P}| when generating irrelevant axioms through our procedure.\"** This is the *critical constraint*. It‚Äôs a statement of *efficiency*.  The system *cannot* generate a proof that is shorter than the one already produced.  This is a fundamental property that drives the system's design.  It‚Äôs a constraint on the *scope* of the proof system.  The \"‚â§\" indicates a lower bound.\n",
      "\n",
      "**Why This is Significant**\n",
      "\n",
      "This analysis suggests a sophisticated approach to automated reasoning. The system isn't just generating proofs; it's *optimizing* the proof process.  The constraint on the number of irrelevant axioms being generated is a key element of this optimization. It‚Äôs not just about finding a shorter proof; it‚Äôs about finding the *most efficient* proof.\n",
      "\n",
      "**Implications & Potential Challenges**\n",
      "\n",
      "* **Complexity:**  The system likely involves a complex set of rules and substitution procedures.  The fact that it *always* produces the shortest proof suggests that the system has been carefully designed to minimize the number of steps required.\n",
      "\n",
      "* **Computational Cost:**  The process of generating the proof, even with the constraint, could be computationally expensive.  The system must balance optimality with efficiency.\n",
      "\n",
      "* **The \"Irrelevant Axioms\"**: The text mentions \"irrelevant axioms.\"  This is a critical point.  These axioms are *not* part of the core logical structure of the proof.  They are simply placeholders that the system uses to generate the shortest proof.  The system's success hinges on the fact that these axioms are *not* used in the final proof.\n",
      "\n",
      "**Overall Interpretation**\n",
      "\n",
      "The text presents a design philosophy centered around *minimalism*. The system is designed to produce the shortest possible proof by systematically eliminating irrelevant axioms. This approach is likely a deliberate trade-off between optimality and efficiency.\n",
      "\n",
      "**Do you want me to explore any of these points further? For example, we could:**\n",
      "\n",
      "*   Discuss the specific types of rules used in the system?\n",
      "*   Examine the role of the substitution procedure?\n",
      "*   Consider the implications of this constraint on the overall reasoning process?\n",
      "Appendix D More Details on Experiments: Here's a concise summary of the provided text:\n",
      "\n",
      "The text analyzes the reasoning process of a language model when solving mathematical problems, focusing on the generated theorems. The model uses a heuristic approach, prioritizing theorems containing parrot information, and generates irrelevant theorems, particularly when there's agent and entity overlap.  The analysis reveals that LMs tend to follow a depth-first exploration, with a closer DFS-based ordering than BFS, as demonstrated by the Levenshtein distance.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 25/40 [25:43<34:58, 139.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25014v1/Introduction\n",
      "../data/summaries/2510.25014v1/Related Work\n",
      "../data/summaries/2510.25014v1/Proposed Method\n",
      "../data/summaries/2510.25014v1/Experiments and Results\n",
      "../data/summaries/2510.25014v1/Conclusion\n",
      "../data/summaries/2510.25014v1/References\n",
      "../data/summaries/2510.25014v1/Appendix A Detailed Prompts for All Experimental Setups\n",
      "../data/summaries/2510.25014v1/Appendix B Item Data Used in Prompts\n",
      "../data/summaries/2510.25014v1/Appendix C Dialogue Examples\n",
      "../data/summaries/2510.25014v1/Appendix D Additional Experimental Results\n",
      "Introduction: The paper introduces Autoregressive State-Tracking Prompting (ASTP), a prompting methodology that resolves the tension between flexibility and control by requiring the LLM to infer and output the previous dialogue state‚Äôs label before predicting the next. This structured workflow, Prime‚ÄìGuide‚ÄìEnforce, ensures procedural compliance while maintaining conversational naturalness. The approach enhances numerical reliability and enables smaller models to match larger ones, resulting in improved accuracy in trading tasks.\n",
      "Related Work: Related work examines various approaches to dialogue management, including task-oriented systems, end-to-end models, and prompting techniques. Existing systems often prioritize goal-driven optimization over procedural adherence, while general reasoning methods lack tailored control. Game NPCs demonstrate a focus on emergent behavior, while research in this domain avoids strict constraints. This research addresses a critical gap in developing a robust approach to ensure procedural adherence within conversational agents, with the goal of providing verifiable control.\n",
      "Proposed Method: The proposed method utilizes a unified prompt with a state-tracking system, ASTP, to guide the LLM‚Äôs dialogue. The prompt includes a system instruction defining the NPC‚Äôs persona, game world data, dialogue history, and dialogue guidelines. ASTP is employed to infer dialogue states from conversation history, ensuring state-aware decision making. The system enforces transition constraints and explicitly identifies previous states, resulting in a verifiable state-tracking process. A placeholder-based post-processing system is used during Offer-Sell states to guarantee accurate price calculations, enhancing transaction integrity.\n",
      "Experiments and Results: This section evaluates the Autoregressive State-Tracking Prompting (ASTP) methodology for dialogue flow adherence and state-specific post-processing within dynamic trading interactions. The experiments utilized a virtual player LLM interacting with an LLM-driven NPC over 300 dialogues, employing a fixed seed for the player‚Äôs initial utterance. ASTP‚Äôs compliance with dialogue flow was assessed through ablation studies comparing it to methods adapted from other studies. The paper details the implementation of four key elements: state definitions, transition conditions, directive to identify previous states, and directive to output previous states. The experiment design involved scenarios of random item purchases and item recommendations, with a focus on achieving a near-perfect compliance rate in the COMMIT_SALE state. The results demonstrate that the ASTP method achieves near-perfect compliance, with the ablation study revealing that explicit transition rules significantly improved performance, while the ‚Äòidentify‚Äô directive proved highly effective. The paper also explores the importance of prompt structure, comparing different approaches and highlighting the robustness of ASTP‚Äôs core principles. Furthermore, the study examines the impact of model variants and evaluation metrics, including accuracy, token usage, response time, and the effectiveness of the placeholder keyword __PRICE__ in ensuring accurate price calculations.\n",
      "Conclusion: ASTP, a prompting methodology, improves procedural compliance and accuracy in rule-governed applications like in-game trading. It uses a structured workflow and placeholder-based post-processing to enhance reliability and speed, achieving up to 9x faster responses with smaller models.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Detailed Prompts for All Experimental Setups: The prompt details a modified ASTP (Autoregressive State-Aware Tracking Process) prompt, differing in structure and terminology from the main paper‚Äôs conceptual model. It utilizes a two-level hierarchy for state classification (TRADE/non-trading) and employs functionally equivalent terms for context and subcontext. The prompt builds upon baseline models, adding directives to identify previous states and guide responses, culminating in a complete prompt for the proposed method. It also removes a placeholder post-processing mechanism.  The prompt augments Baseline 2 by introducing a state-transition graph and a ‚Äúthink step-by-step‚Äù instruction.\n",
      "Appendix B Item Data Used in Prompts: The appendix B item data is used to populate the {game_items} and {merchant_inventory} placeholders in the prompts. The {game_items} data contains a JSON string of 52 items with item_id and item_name, while the {merchant_inventory} data contains 20 items marked as ‚ÄúFor Sale‚Äù with item_id, item_name, price, and quantity.\n",
      "Appendix C Dialogue Examples: Table 4 and 5 demonstrate successful dialogue examples from Scenario 1 and 2, showcasing ASTP‚Äôs ability to handle complex interactions and exploratory conversations while maintaining procedural flow. Table 6 compares ASTP‚Äôs dialogue with AutoTOD, illustrating ASTP‚Äôs adherence to the FINAL_CHECK step and skipping it. Table 7 shows the effectiveness of the Placeholder Post-Processing (PPP) mechanism, demonstrating accurate price establishment in the OFFER_SELL state and its robustness across subsequent turns.\n",
      "Appendix D Additional Experimental Results: The ASTP model experienced a single failure in a dialogue case after a player‚Äôs affirmative response (\"Yes, I'll purchase it\") in Turn 9, resulting in a jump from OFFER_SELL to COMMIT_SALE despite a mandatory FINAL_CHECK. This failure stems from semantic ambiguity in the NPC‚Äôs question, causing the model to misinterpret the required final check. A rare failure also occurs when the player attempts to negotiate while modifying the shopping cart within a single turn, leading to calculation errors.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 26/40 [26:28<26:00, 111.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25065v1/1 Introduction\n",
      "../data/summaries/2510.25065v1/2 PM4GRPO: Reasoning-Aware GRPO using Process Mining\n",
      "../data/summaries/2510.25065v1/3 Experiments\n",
      "../data/summaries/2510.25065v1/4 Conclusion\n",
      "../data/summaries/2510.25065v1/References\n",
      "1 Introduction: This paper introduces PM4GRPO, a novel reasoning-aware GRPO framework that utilizes Process Mining (PM) to evaluate reasoning quality. It extracts reasoning traces from a pretrained teacher model and uses this information to reward the student model‚Äôs self-generated reasoning process, ensuring alignment while preserving model freedom of thought. Experimental results demonstrate its effectiveness in enhancing reinforcement learning.\n",
      "2 PM4GRPO: Reasoning-Aware GRPO using Process Mining: PM4GRPO introduces a conformance reward to evaluate reasoning alignment between the policy model and a pretrained teacher model, utilizing a sequence-level importance ratio and clipping at the sequence level.\n",
      "3 Experiments: PM4GRPO demonstrates superior reasoning and generalization across multiple math benchmarks, achieving the highest scores on MATH 500, Olympiad Bench, and Minerva Math, significantly surpassing baselines like DeepMath-Zero-7B, Skywork-OR1-7B, and LEAD.  The study utilizes a server with 4 NVIDIA H200 GPUs and TRL v0.24.0 and PyTorch v2.8.0 for implementation.\n",
      "4 Conclusion: PM4GRPO, a novel reasoning-aware GRPO framework, was developed using PM techniques. Experimental results on five benchmarks showed it significantly outperformed existing methods, demonstrating the effectiveness of PM for evaluating reasoning procedures.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 27/40 [26:42<17:49, 82.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25634v1/1 Introduction\n",
      "../data/summaries/2510.25634v1/2 Related Work\n",
      "../data/summaries/2510.25634v1/3 Method\n",
      "../data/summaries/2510.25634v1/4 Experiments\n",
      "../data/summaries/2510.25634v1/5 Conclusion\n",
      "../data/summaries/2510.25634v1/References\n",
      "1 Introduction: This paper introduces a hierarchical framework for bimanual manipulation using reinforcement learning and integrated skill planning. It proposes a Transformer-based policy that generates plans specifying discrete skills and continuous parameters, demonstrating improved success rates and efficiency compared to traditional sequential planning methods.\n",
      "2 Related Work: This work explores TAMP approaches for multiple robots, addressing challenges with handcrafted models and reliance on imitation learning. It utilizes a hierarchical modeling approach, employing parameterized contact-rich primitives for bimanual manipulation, offering flexibility while benefiting from temporal abstraction.\n",
      "3 Method: Here‚Äôs a concise summary of the paper:\n",
      "\n",
      "The authors propose a pipeline for training bimanual manipulation skills through a hierarchical decision problem. They define low-level skills, high-level scheduling policies, and a data-driven approach to learn these skills using behavior cloning and dense rewards. The high-level policy selects skills and parameters to achieve a task goal, while the low-level skills are trained to perform individual tasks. The paper details a transformer-based architecture for temporal consistency and uses a dataset-driven approach to generate expert demonstrations for training the scheduling policy.\n",
      "4 Experiments: We evaluate a hierarchical framework and three baselines on a long-horizon manipulation task involving object placement in a bin. The framework outperforms RL-scratch on task success rate and completion progress, demonstrating efficient skill scheduling and coordination. The bimanual skill schedule visualization reveals improved skill sequencing and reduced episode duration. Baselines include vanilla PPO, hierarchical RL, and sequential planning.\n",
      "5 Conclusion: The authors introduced a hierarchical framework for long-horizon bimanual manipulation, using a Transformer-based policy to generate coordinated plans and simultaneously select discrete skills. Experimental results demonstrate superior success rates compared to end-to-end RL and more efficient, parallelized behavior.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 28/40 [27:10<13:11, 65.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25602v1/1 Introduction\n",
      "../data/summaries/2510.25602v1/2 Preliminaries\n",
      "../data/summaries/2510.25602v1/3 Quantization Recipe\n",
      "../data/summaries/2510.25602v1/4 Theoretical Framework\n",
      "../data/summaries/2510.25602v1/5 FP v.s. INT\n",
      "../data/summaries/2510.25602v1/6 Hardware Cost Analysis\n",
      "../data/summaries/2510.25602v1/7 Conclusion\n",
      "../data/summaries/2510.25602v1/References\n",
      "../data/summaries/2510.25602v1/Outlines\n",
      "../data/summaries/2510.25602v1/8 Related Work\n",
      "../data/summaries/2510.25602v1/9 Proofs of Theorems\n",
      "../data/summaries/2510.25602v1/10 Hardware Cost Modeling\n",
      "../data/summaries/2510.25602v1/11 More Details for Reproduction\n",
      "1 Introduction: This paper investigates the performance trade-offs of fine-grained INT and FP quantization, particularly in the context of large language models. It introduces a theoretical framework to model the quantization signal-to-noise ratio (QSNR) and demonstrates that MXINT8 consistently outperforms MXFP8 in both direct-cast and low-bit training, while NVINT4 surpasses NVFP4 with Hadamard rotation. The paper also presents a hardware cost analysis highlighting the benefits of fine-grained INT formats, challenging the current FP-centric design trajectory for AI accelerators.\n",
      "2 Preliminaries: Quantization maps high-precision tensors to lower-bit integer representations, utilizing scale factors to control integer values within a defined range. Floating-point quantization employs three fields: sign, exponent, and mantissa, representing low-bit values. Block quantization improves accuracy by partitioning tensors into blocks, with finer granularity enhancing accuracy but increasing computational cost. NVIDIA Blackwell GPUs provide native hardware support for MXFP8/MXFP6/MXFP4, while NVFP4 reduces block size and replaces the UE8M0 scale with an E4M3 scale. Four integer variants (MXINT8, MXINT6, MXINT4, NVINT4) are introduced for fair comparison.\n",
      "3 Quantization Recipe: This section details the quantization process for low-bit inference and training, utilizing the UE8M0 scale factor and symmetric integer range. It outlines the six quantization operations, including block-wise quantization and the analysis of quantization error in Sec. 5.1. The section also introduces the AbsMax quantizer and OCP for converting high-precision to UE8M0 format, addressing clipping and asymmetric range handling for signed integers.\n",
      "4 Theoretical Framework: 4.1 Theoretical QSNR: The Quantization Signal-to-Noise Ratio (QSNR) metric quantifies numerical fidelity under quantization schemes. Block vectors X ‚àà ‚Ñù k with i.i.d. entries X i ‚àº ùí© ‚Äã quantify the original signal, while quantization noise X ‚àí X q ‚àº ùêó ‚àí ùêó q represents the difference between the original and quantized values. Common assumptions include blockwise absolute-maximum scaling. Theorem 1 (INT QSNR) demonstrates that under INT quantization, QSNR (dB) ‚âà 6.02 dB, with a detailed proof in Sec. 9.2. Theorem 2 (FP QSNR) shows that under FP quantization, QSNR (dB) ‚âà 13.80 dB, with a detailed proof in Sec. 9.3.\n",
      "\n",
      "4.2 Theoretical Comparisons: The study estimates QSNR for low-bit and floating-point formats, utilizing a UE8M0 scale. MXINT8 outperforms FP8 when Œ∫ < 7.55, while MXINT6 outperforms MXFP8 at Œ∫ < 1.96. MXINT4 outperforms MXFP4 when Œ∫ < 2.04. NVINT4 outperforms NVFP4 when Œ∫ < 2.39, and NVFP4‚Äôs QSNR increases with Œ∫ < 4. The key factor for format comparison is the data crest factor Œ∫.\n",
      "5 FP v.s. INT: We compare low-bit integer and floating-point formats at three levels. MXINT8 outperforms MXFP8 on all models, with NVINT4 and NVFP4 exhibiting similar QSNR, while MXINT6, MXINT4, and NVINT4 perform worse. Hadamard rotation improves performance for MXINT8 and NVINT4, while MXINT6 and MXINT4 experience losses.\n",
      "6 Hardware Cost Analysis: The analysis evaluates hardware costs for a Matrix-Multiply Unit (MMU) supporting the MX format. MXINT8 and NVINT4 reduce energy by 37% and 38% respectively compared to MXFP8 and NVFP4. Mixed-format configurations, utilizing a chip with 8-bit and 4-bit data types and a throughput ratio of 1:2, further reduce area by about 34%.  Low-bit integer formats are more efficient than low-bit floating-point formats at matched throughput.\n",
      "7 Conclusion: The study demonstrates a trade-off between integer and floating-point quantization, with FP formats performing well at coarse levels but MXINT8 consistently outperforming MXFP8 in accuracy and hardware efficiency. Fine-grained INT formats, particularly NVINT4, offer an advantage over NVFP4 when combined with Hadamard rotation. This challenges current hardware trends and calls for a shift towards algorithm-hardware co-design to prioritize fine-grained INT formats for AI accelerator development.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Outlines: Sec. 8: Related Works. Sec. 9: Proofs of Theorems 1 & 2 on INT and FP QSNR estimation. Sec. 10: Hardware Cost Estimation Model. Sec. 11: Model Details, Ablation Studies, and Numerical Results.\n",
      "8 Related Work: Quantization algorithms, including post-training quantization (PTQ) and quantization-aware training (QAT), are studied. Scaling laws are used to optimize low-bit quantization. Prior work focused on a single format (integer or floating-point) and did not compare formats. Hardware limitations restrict fine-grained quantization. OCP‚Äôs Microscaling (MX) data format improves low-bit quantization through per-block scaling and a 32-block size. NVIDIA Blackwell hardware supports MXFP8, MXFP4, and NVFP4.\n",
      "9 Proofs of Theorems: We consider block vectors ùêó ‚àà ‚Ñù g \\mathbf{X}\\in\\mathbb{R}^{g} with i.i.d. entries X i ‚àº ùí© ‚Äã ( 0 , œÉ 2 ) X_{i}\\sim\\mathcal{N}(0,\\sigma^{2}) . We denote the block RMS by œÉ := RMS ‚Äã (ùêó ) œÉ:=\\mathrm{RMS}(\\mathbf{X}) and the crest factor by s ‚Ä≤ ‚â• s s^{\\prime}\\geq s to avoid upper clipping. When the scale factors use BFloat16 or E4M3, we set œÅ = 1 \\rho=1 . The ideal scale s s matches the largest codebook magnitude to the block maximum: where Q ref Q_{\\mathrm{ref}} depends on the target format: INT ( b ) (b) (symmetric): Q ref = Q := 2 b ‚àí 1 ‚àí 1 Q_{\\mathrm{ref}}=Q:=2^{b-1}-1 (largest integer code). FP ( E , M , B ) (E,M,B) (with subnormals): Q ref = Q max Q_{\\mathrm{ref}}=Q_{\\max} (largest finite normal magnitude; e.g., Q max = 448 Q_{\\max}=448 for E4M3). This convention matches the main text: we reuse ( œÉ , Œ∫ , œÅ , s , s ‚Ä≤ ) (\\sigma,\\kappa,\\rho,s,s^{\\prime}) , and s ‚Ä≤ ‚â• s s^{\\prime}\\geq s prevents overflow for both INT and FP quantization. Unless stated otherwise, expectations are over both the data and the quantization randomness, and ‚Äñ ùêó ‚Äñ 2 ‚âà k œÉ 2 \\|\\mathbf{X}\\|^{2}\\approx kœÉ^{2} .\n",
      "INT quantization. We consider a symmetric, uniform quantizer with bit-width b b and integer range [ ‚àí Q , Q ] [-Q,Q] , where\n",
      "For MX format, which uses blockwise UE8M0 scale factors, we set\n",
      "and choose s ‚Ä≤ ‚â• s s^{\\prime}\\geq s to avoid upper clipping. When the scale factors use BFloat16 or E4M3, we set œÅ = 1 \\rho=1 . The ideal scale s s matches the largest codebook magnitude to the block maximum: where Q ref Q_{\\mathrm{ref}} depends on the target format: INT ( b ) (b) (symmetric): Q ref = Q := 2 b ‚àí 1 ‚àí 1 Q_{\\mathrm{ref}}=Q:=2^{b-1}-1 (largest integer code). FP ( E , M , B ) (E,M,B) (with subnormals): Q ref = Q max Q_{\\mathrm{ref}}=Q_{\\max} (largest finite normal magnitude; e.g., Q max = 448 Q_{\\max}=448 for E4M3). This convention matches the main text: we reuse ( œÉ , Œ∫ , œÅ , s , s ‚Ä≤ ) (\\sigma,\\kappa,\\rho,s,s^{\\prime}) , and s ‚Ä≤ ‚â• s s^{\\prime}\\geq s prevents overflow for both INT and FP quantization. Unless stated otherwise, expectations are over both the data and the quantization randomness, and ‚Äñ ùêó ‚Äñ 2 ‚âà k œÉ 2 \\|\\mathbf{X}\\|^{2}\\approx kœÉ^{2} .\n",
      "Extension to high-precision scale factors. The analysis above assumes UE8M0 scaling, which rounds the scale and introduces the overhead œÅ ‚àà [ 1 , 2 ) \\rho\\in[1,2) . With the E4M3 scale format used in NVINT4, the per-block scale closely matches the ideal value, so œÅ ‚âà 1 \\rho\\approx 1 , and the element at the block maximum maps with (near-)zero error. For block size g g (elements per block), the INT QSNR with an E4M3 scale is\n",
      "where ‚ÑÇ FP \\mathbb{C}_{\\mathrm{FP}} is the FP codebook. We choose the ideal scale s = max ( | ùêó | ) / Q max s=\\max(|\\mathbf{X}|)/Q_{\\max} and set s ‚Ä≤ = œÅ s s^{\\prime}=\\rho s with œÅ ‚àà [ 1 , 2 ) \\rho\\in[1,2) for UE8M0 (power-of-two) scaling; œÅ ‚âà 1 \\rho\\approx 1 when the scale uses E4M3.\n",
      "Error decomposition. Let e := X ‚àí X q e = X ‚àí X<sub>q</sub>. Under standard high-resolution model [3], the error is approximately uniform and independent of X<sub>1</sub> and X<sub>2</sub>. QSNR: We have Eùîº [ ‚Äñ ùêó ‚Äñ 2 ] ‚âà k œÉ<sup>2</sup> and Eùîº [ ‚Äñ ùêó ‚àí ùêó q ‚Äñ 2 ] ‚âà k Œî<sup>2</sup> / 12.  Œî ‚âà 2 b ‚àí 1, Œî ‚âà 2<sup>b-1</sup>.  Using Eq. (15-17), substituting into QSNR expression gives: (i) ‚âà 6.02 dB per additional bit, (ii) up to 6.02 dB loss from the power-of-two overhead (œÅ ‚àà [1, 2) œÅ‚àà[1,2)], and (iii) a penalty that scales with the crest factor Œ∫ (Œ∫ typically increases with larger block size). Extension to high-precision scale factors:  With E4M3 scale, œÅ ‚âà 1, and the element at the block maximum maps with (near-)zero error. For block size g g (elements per block), the INT QSNR with an E4M3 scale is: 10<sup>log(10(g-1))</sup> 10<sup>log(g/g-1)</sup>.\n",
      "10 Hardware Cost Modeling: The model predicts the area and energy of the MAC and accumulation stages, considering MX/NV formats, exponent/mantissa widths, aligner width, and cell factor trends.\n",
      "11 More Details for Reproduction: The study investigates the necessity of symmetric integer representation for quantization, analyzing numerical stability and demonstrating a significant mapping to -128 in BFloat16, highlighting a critical flaw in low-precision quantization. Experiments with Llama-3 and detailed architectural settings are presented in Table 9, while KL divergence and perplexity results are analyzed in Table 12 and Table 13.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 29/40 [28:23<12:27, 67.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25205v1/I Introduction\n",
      "../data/summaries/2510.25205v1/II Related Works\n",
      "../data/summaries/2510.25205v1/III Overview\n",
      "../data/summaries/2510.25205v1/IV Adaptive Perception\n",
      "../data/summaries/2510.25205v1/V Robust Decision\n",
      "../data/summaries/2510.25205v1/VI Experiments\n",
      "../data/summaries/2510.25205v1/VII Conclusion\n",
      "../data/summaries/2510.25205v1/VIII Acknowledgment\n",
      "../data/summaries/2510.25205v1/References\n",
      "I Introduction: This study proposes an energy-efficient autonomous driving framework, EneAD, to optimize perception and decision-making within a traffic scenario. It utilizes a lightweight classification model, tunable knobs for framerate and interpolation, and a meta-learning strategy to efficiently explore configurations. Experiments demonstrate improved energy consumption and driving performance compared to existing approaches.\n",
      "II Related Works: This study examines perception and decision-making modules in autonomous driving, focusing on modular approaches compared to end-to-end methods. It reviews existing perception models, including camera-only and multi-modal architectures, highlighting limitations of multi-modality, particularly computational cost and accuracy. The research introduces a novel approach that dynamically adjusts framerates and interpolation methods to optimize data management and tuning, aiming to improve robustness to perturbed perception data.  It also explores reinforcement learning as a potential alternative to traditional rule-based decision-making, incorporating a parameterized action structure and hybrid reward function to enhance safety and efficiency.\n",
      "III Overview: III-A: The study investigates energy consumption and driving performance through deep learning models, measuring via GPU efficiency and TTC, speed, and deceleration.\n",
      "\n",
      "III-B: The EneAD framework utilizes an adaptive perception module, a Swin-T classification model, Monte Carlo dropout, Bayesian optimization, and an MDP-based robust decision module for energy efficiency and driving performance optimization.\n",
      "IV Adaptive Perception: Okay, let's break down this paper's methodology and key takeaways, focusing on the Bayesian Optimization approach and its implications.\n",
      "\n",
      "**Overall Goal & Approach:**\n",
      "\n",
      "The paper tackles the challenge of creating robust and adaptable perception models for autonomous driving by employing Bayesian Optimization (BO) to intelligently tune the parameters of a classification model.  Instead of manually tweaking model settings, BO automates this process, aiming to find configurations that balance accuracy and computational cost.\n",
      "\n",
      "**Detailed Breakdown of the Key Components:**\n",
      "\n",
      "1. **Scenario Classification Model:**\n",
      "   - **Swin-T Network:**  They use a lightweight Swin-T network, which is known for its efficiency and ability to capture complex visual features.\n",
      "   - **Monte Carlo Dropout:**  This is a crucial technique. Dropout randomly disables neurons during training, which helps to regularize the model and prevent overfitting, especially when dealing with complex datasets.\n",
      "   - **Perception Difficulty Levels:** The model is trained to classify images into difficulty levels (e.g., Easy, Medium, Hard).  This is a key step in representing the task ‚Äì it's not just about recognizing objects, but *how difficult* the scene is.\n",
      "\n",
      "2. **Bayesian Optimization (BO) - The Core of the Method:**\n",
      "   - **Meta-Surrogate Model:** This is the heart of the BO process. It's a model that approximates the objective function (accuracy) of each configuration.  It's updated iteratively, guided by the surrogate model.\n",
      "   - **EHVI Acquisition Function:** This function is used to guide the exploration of the configuration space. It balances exploration (trying new configurations) and exploitation (refining existing good configurations).\n",
      "   - **Sequential Model-Based Configuration (SMAC):** This is a probabilistic model that estimates the uncertainty associated with each configuration.  This uncertainty is used to guide the search for better configurations.\n",
      "   - **Warm-Starting:** The meta-surrogate model is initialized with the previous configurations, allowing the BO algorithm to build upon the knowledge gained from previous iterations.\n",
      "\n",
      "3. **Knob Tuning (Iterative Optimization):**\n",
      "   - **Three Knobs (k, n, o):**  These knobs control different aspects of the model:\n",
      "     - **k (Model Size):**  Determines the complexity of the model (e.g., number of parameters).\n",
      "     - **n (Frame Rate):**  Controls the speed of the model (e.g., how often it processes images).\n",
      "     - **o (Interpolation Method):**  Determines how the model interpolates between different image regions.\n",
      "   - **Iterative Tuning:** The system iteratively generates configurations, updates the surrogate model, and evaluates the results.\n",
      "\n",
      "**Key Takeaways & Significance:**\n",
      "\n",
      "* **Automated Exploration:** BO automates the tedious process of manually tuning model parameters.\n",
      "* **Focus on Trade-offs:** The paper explicitly addresses the trade-offs between accuracy and computational cost ‚Äì a critical consideration in autonomous driving.\n",
      "* **Probabilistic Uncertainty:** The use of uncertainty estimation is a significant advancement. It allows the BO algorithm to intelligently explore the configuration space, focusing on regions where the model is most uncertain.\n",
      "* **Adaptability:** The system is designed to adapt to different traffic scenarios and difficulty levels, making it more robust.\n",
      "\n",
      "**In essence, the paper demonstrates a practical and effective Bayesian Optimization approach to build a perception model that balances accuracy and efficiency for autonomous driving.**\n",
      "\n",
      "Do you want me to elaborate on any specific aspect of the methodology, such as the surrogate model, the acquisition function, or the specific challenges addressed in the tuning process?\n",
      "V Robust Decision: This section details a reinforcement learning approach for autonomous driving. It utilizes a Markov Decision Process (MDP) to model driving decisions, employing a regularization technique to enhance robustness to perturbed state features. The model learns an action policy by maximizing Q-values, incorporating a safety factor based on time-to-collision, efficiency, and impact.  The P-DQN method directly generates actions, while the RP-DQN method adds a regularization term to stabilize the action network, reducing misestimation and improving exploration. The final step involves a loss function that penalizes deviations from previously seen action pairs, promoting conservative decision-making.\n",
      "VI Experiments: Okay, here's a breakdown of the provided text, organized into sections and with a summary of the key findings and conclusions.  I've focused on clarifying the relationships between the different experiments and the overall goals of the research.\n",
      "\n",
      "**Overall Research Goal:**\n",
      "\n",
      "The research aims to evaluate and compare different approaches to enhance autonomous driving perception ‚Äì specifically, the adaptive perception module ‚Äì to improve energy efficiency, safety, and comfort.  It explores various tuning strategies and algorithms to optimize these aspects.\n",
      "\n",
      "**Key Experiments & Results:**\n",
      "\n",
      "1. **VI-A (Experimental Settings):**\n",
      "   * **Focus:**  Testing different perception algorithms (ViT, MobileNet, ResNet50, Swin-T) and tuning strategies.\n",
      "   * **Key Findings:**  ViT achieved the highest accuracy, but at a high computational cost. MobileNet was lightweight but less accurate. ResNet50 and Swin-T offered near-identical accuracy with lower computational cost.\n",
      "   * **Conclusion:**  The research highlights the trade-off between accuracy and computational cost ‚Äì ViT is the most accurate but expensive.\n",
      "\n",
      "2. **VI-B (End-to-End Evaluation):**\n",
      "   * **Focus:** Evaluating the performance of the adaptive perception module (classification, decision-making, and knob tuning) across various traffic scenarios.\n",
      "   * **Key Findings:**\n",
      "      * **Energy Consumption:** The adaptive perception module significantly reduced energy consumption.\n",
      "      * **Driving Performance:**  The framework achieved the lowest TTC-R, AC, and DEC, indicating safer driving.\n",
      "      * **Comfort:**  The framework showed a slight decrease in comfort (VEL, AC, DEC) due to a robust decision module.\n",
      "   * **Conclusion:** The adaptive perception module is effective in reducing energy consumption and improving safety, but it introduces some trade-offs in comfort.\n",
      "\n",
      "3. **VI-C (Evaluation of Adaptive Perception Module):**\n",
      "   * **Focus:**  Comparing different reinforcement learning methods (P-DDPG, P-DQN, RP-DQN) to optimize the parameter settings of the adaptive perception module.\n",
      "   * **Key Findings:**\n",
      "      * **Best Performance:** The w/o-meta method achieved the best performance in terms of safety, comfort, and impact.\n",
      "      * **Convergence:** The w/o-meta method converges faster than Heuristic and RL-based methods.\n",
      "   * **Conclusion:** The w/o-meta method is effective in optimizing the parameter settings of the adaptive perception module.\n",
      "\n",
      "4. **VI-D (Evaluation of Robust Decision Module):**\n",
      "   * **Focus:**  Evaluating the effectiveness of the reinforcement learning model in generating driving actions based on perturbed perception results.\n",
      "   * **Key Findings:**\n",
      "      * **Safety:** The robust decision module achieved the best performance in safety.\n",
      "      * **Comfort:** The framework showed a slight decrease in comfort (VEL, AC, DEC) due to a robust decision module.\n",
      "   * **Conclusion:** The robust decision module is effective in generating driving actions that are more cautious.\n",
      "\n",
      "**Overall Conclusions & Implications:**\n",
      "\n",
      "* **Adaptive Perception is Crucial:** The research demonstrates the importance of adaptive perception ‚Äì dynamically adjusting the perception model based on the current driving conditions ‚Äì for improved energy efficiency and safety.\n",
      "* **Trade-offs Exist:**  There are trade-offs between accuracy, computational cost, and comfort.  The research highlights that optimizing for one metric can negatively impact others.\n",
      "* **Reinforcement Learning is Effective:** The w/o-meta method shows that reinforcement learning can effectively optimize the parameter settings of the adaptive perception module.\n",
      "* **Robustness is Key:** The robust decision module is effective in generating driving actions that are more cautious.\n",
      "\n",
      "**To summarize, the research presents a series of experiments that demonstrate the effectiveness of different approaches to perception, highlighting the importance of balancing accuracy, computational cost, and comfort in autonomous driving systems.**\n",
      "\n",
      "---\n",
      "\n",
      "Do you want me to elaborate on any specific aspect of this summary, perhaps focusing on a particular experiment or finding?  For example, would you like me to:\n",
      "\n",
      "*   Expand on the specific challenges faced with each algorithm?\n",
      "*   Provide a more detailed explanation of the w/o-meta method?\n",
      "*   Discuss the implications of the results for future research?\n",
      "VII Conclusion: The study presents an energy-efficient framework, EneAD, utilizing modular autonomous driving and an adaptive perception module. It employs Bayesian optimization to tune perception model knobs, a transferable tuning method for efficient computation, and a lightweight classification model for traffic scenario adaptation.  Experiments demonstrate EneAD‚Äôs superior performance in both energy consumption and driving performance through real-world and synthetic datasets.\n",
      "VIII Acknowledgment: This work was partially funded by the NSFC (No. 62472068) and the Municipal Government of Quzhou, under Grant (No. 2024D036) and (No. 2024D037).\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 30/40 [29:24<10:59, 65.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25223v1/I Introduction\n",
      "../data/summaries/2510.25223v1/II BACKGROUND & RELATED WORK\n",
      "../data/summaries/2510.25223v1/III Problem Formulation\n",
      "../data/summaries/2510.25223v1/IV Methodology\n",
      "../data/summaries/2510.25223v1/V Experiment\n",
      "../data/summaries/2510.25223v1/VI Conclusion\n",
      "../data/summaries/2510.25223v1/References\n",
      "I Introduction: FELA is a multi-agent collaborative system designed to automatically extract novel and insightful features from large-scale industrial event log data, addressing challenges related to complexity, explainability, and self-evolution. The system employs a hybrid evolution framework combining genetic and reinforcement learning to efficiently guide idea exploration and improve feature discovery. Experiments demonstrate the system‚Äôs effectiveness in real-world industrial scenarios, significantly reducing manual feature engineering time and enhancing predictive model performance.\n",
      "II BACKGROUND & RELATED WORK: II. Background and Related Work: Feature engineering is a crucial process for enhancing machine learning models, particularly in industrial data, which presents challenges due to its complexity and heterogeneity. Existing approaches, such as latent representation learning and explicit transformation, have limitations in interpretability and scalability. Recent research, including automated feature engineering techniques like Tree-based exploration and iterative subsampling, and leveraging large language models, offers promising solutions for domain adaptation and knowledge transfer.  LLMs on data-centric tasks explore using them for feature generation, integrating evolutionary algorithms and search frameworks to optimize feature creation. Approaches like CAAFE and OCTree demonstrate success in areas such as prompt optimization and feature engineering. FeatLLM and LLM-FE leverage few-shot prompting and island-based evolutionary methods to achieve greater flexibility and control, while our method fully exploits the LLM‚Äôs coding and reasoning capabilities for robust, interpretable feature transformations, making the approach scalable and applicable to real-world industrial environments.\n",
      "III Problem Formulation: III-A: Tabular dataset with N rows and M columns, representing user interactions and events, temporally ordered.\n",
      "III-B: Supervised learning task to optimize feature transformations (Œ∏) for improved model performance.\n",
      "III-C: Large language model agents collaboratively construct the optimal feature transformation Œ∏Œ∏ through sequential collaboration.\n",
      "IV Methodology: Okay, this is a fantastic and detailed explanation of the FELA (Federated Learning of Idea Agents) framework. You‚Äôve clearly outlined the key components and their roles. Here‚Äôs a breakdown of the key takeaways and some potential areas for further thought, organized for clarity:\n",
      "\n",
      "**1. Core Architecture ‚Äì The Federated Learning Loop:**\n",
      "\n",
      "* **Idea Generation:** The LLM acts as the primary idea generator, leveraging the knowledge base and potentially incorporating user feedback.\n",
      "* **Evolutionary Learning:** The core of FELA is the iterative refinement process driven by the UCB and the feedback from the critic agents. This loop allows the system to continuously improve its feature selection and generation.\n",
      "* **External Dependency Component:** The system relies on an external component to provide data schema, prior knowledge, and log data, which is crucial for grounding the learning process.\n",
      "\n",
      "**2. Key Components & Their Roles ‚Äì Deeper Dive:**\n",
      "\n",
      "* **Idea Agents:**  You‚Äôve correctly highlighted the crucial role of the idea agents ‚Äì they are the engine of the evolutionary process. Their focus is on generating new ideas and refining existing ones.\n",
      "* **Code Agent:**  The code agent is a critical component for ensuring the quality and reliability of the generated features. The free-coding LLM approach is a significant innovation.\n",
      "* **Critic Agents:**  The two critic agents ‚Äì idea critics and code critics ‚Äì are vital for detecting hallucinations and ensuring the quality of the generated ideas and code.  Their feedback loop is a key element of the system‚Äôs robustness.\n",
      "* **Long-Term Collaborative Memory (‚Ñ≥ l \\mathcal{M}_{l}):** This memory component is essential for capturing long-term contextual insights, allowing the system to learn from past experiences and improve its decision-making.\n",
      "* **Short-Term Feature-Based Memory (‚Ñ≥ s \\mathcal{M}_{s}):** This memory component focuses on providing actionable suggestions for augmenting the current idea, enhancing its effectiveness.\n",
      "\n",
      "**3.  The Evolutionary Learning Algorithm ‚Äì A Critical Element:**\n",
      "\n",
      "* **Relative Score (s(d_{i,j+1})):**  This is a clever way to quantify the value of a feature set. It‚Äôs a crucial element for guiding the evolutionary process.\n",
      "* **UCB (Upper Confidence Bound):**  The UCB is a standard optimization technique, and its use is well-justified.\n",
      "\n",
      "**4.  Potential Future Directions & Considerations (Based on the Description):**\n",
      "\n",
      "* **Memory System Enhancements:**  The description hints at a long-short term memory.  Exploring different memory architectures (e.g., hierarchical, graph-based) could further improve the system‚Äôs ability to handle long-term dependencies.\n",
      "* **Feedback Loop Refinement:**  The feedback loop between the critic agents and the idea agents is vital.  How does the system handle conflicting feedback?  Could it incorporate a mechanism for prioritizing feedback?\n",
      "* **Diversity Promotion:**  The description doesn't explicitly mention diversity promotion.  Ensuring a diverse set of ideas during evolution is important for preventing the system from getting stuck in local optima.\n",
      "* **Explainability:**  Adding explainability to the idea generation process would be valuable for understanding *why* the system is generating certain ideas.\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "You've provided a very solid and well-explained overview of the FELA framework. It‚Äôs a complex system, but the description clearly outlines the key components and their roles.  It‚Äôs a compelling example of how federated learning can be used to improve the quality and efficiency of machine learning models.\n",
      "\n",
      "Do you want me to elaborate on any specific aspect of the framework, such as the memory system, the UCB, or the critic agents? Or would you like me to focus on a particular challenge or opportunity for improvement?\n",
      "V Experiment: V-A Experiment validates FELA‚Äôs performance through comprehensive evaluations of its features, interpretability, compatibility, and mechanism. The V-B Experiment focuses on three real-world datasets ‚Äì Diabetes Health Indicator, Tabao Conversion Prediction, and Tencent Game Platform user churn data ‚Äì to assess its applicability and robustness across diverse industrial scenarios. V-C examines feature engineering performance, demonstrating FELA‚Äôs superior accuracy, precision, and AUC on the Dia and Taobao datasets, while maintaining this advantage on the Tencent user churn dataset. V-D investigates the steerable feature generation capability of FELA, achieving higher accuracy and AUC on the Taobao dataset through a temporal split. V-E assesses compatibility with various machine learning models, demonstrating the best AUC performance across tested models. V-F investigates the impact of the critic agent, long-short-term memory, and UCB-based learning algorithm on FELA‚Äôs performance, revealing the benefits of the memory for exploiting evolutionary experience.\n",
      "VI Conclusion: FELA is a multi-agent system that uses LLM agents, Idea Agents, Code Agents, and Critic Agents to automate feature engineering for complex industrial event log data. It employs an agentic evolution algorithm, a hierarchical knowledge base, and long-short-term memory to continuously improve reasoning, adapt to diverse data, and maintain explainability. Experimental results demonstrate its potential for advancing automatic feature engineering and future work includes broader applications.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 31/40 [30:29<09:51, 65.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25179v1/I Introduction\n",
      "../data/summaries/2510.25179v1/II Literature Review and Background\n",
      "../data/summaries/2510.25179v1/III Methodology\n",
      "../data/summaries/2510.25179v1/IV Experiments Setup and Results\n",
      "../data/summaries/2510.25179v1/V Results and Analysis\n",
      "../data/summaries/2510.25179v1/VI Conclusion\n",
      "../data/summaries/2510.25179v1/References\n",
      "I Introduction: Agentic Moderation: A model-agnostic framework for collaborative multi-agent safety alignment of vision-language models, designed to mitigate cross-modal adversarial attacks through a SHIELD agent, Evaluator, Reflection agent, and responder agent.\n",
      "II Literature Review and Background: The paper reviews defense mechanisms for vision-language models, including input purification, system safety prompts, output suppression, and model-level alignment. It highlights recent advancements in moderation systems utilizing classifier-based detection and introduces Agentic Moderation, a multi-agent framework integrating preprocessing and postprocessing to dynamically interpret and coordinate safety enforcement across modalities.\n",
      "III Methodology: III-A: The Agentic Moderation Framework utilizes a loop with a central Coordinator, comprising four specialized agents: Shield, Responder, Evaluator, and Reflector. The Shield classifies multimodal inputs, determines actions, and the Responder generates candidate outputs based on cues. The Evaluator assesses responses for safety and utility, and the Reflector analyzes failures and generates corrective feedback. Agents are lightweight, modular, and extensible for seamless policy adaptation.\n",
      "\n",
      "III-B: The Shield Agent screens multimodal inputs, classifying them into 45 policy categories and augmenting prompts with contextual guidance. The Responder Agent integrates these cues into the generative process, balancing safety and task utility, utilizing a VLM backend and evaluating outputs through the SeeThreats framework. The Reflector Agent consolidates Shield decisions and user intent to diagnose failures and generates feedback with 'Issue‚ÄîFix' tags.\n",
      "IV Experiments Setup and Results: The study establishes four LVLM evaluations across diverse adversarial safety datasets using LLaVA, Qwen2.5-8B, and LLaMA 3.2 Vision-11B. Metrics include refusal rate, non-following rate, jailbreak rate, and threat score, with experiments configured to isolate each component‚Äôs contribution. The models were evaluated on four configurations on RunPod L40S GPUs.\n",
      "V Results and Analysis: We evaluate multiple moderation configurations to measure the effectiveness of each module. Under adversarial attack settings, our objective is to ensure models produce safe and coherent responses rather than maintaining ‚Äúsafety‚Äù through excessive refusal or meaningless outputs. In particular, we aim to avoid situations where a model appears safe simply by refusing everything or producing nonsensical text. To this end, we focus on two main metrics: Attack Success Rate (ASR), which measures the proportion of harmful outputs, and Non-Following Rate (NF), which captures cases where the model fails to follow or meaningfully complete the task. Ideally, an effective moderator minimizes both ASR and NF. As shown in Figure 5, points closer to the lower-left corner represent configurations with both low ASR and NF, indicating an optimal balance between safety and utility. Our goal is to move model behavior toward this region.\n",
      "\n",
      "LLaMA, LLaVA-1.5, LLaVA-1.6, Qwen2.5-7B, the Shield and Reflection Agents achieve the most consistent and substantial improvements, notably reducing completion and instruction non-following rates across the LLaVA variants. For LLaMA, the combined setup increases refusal rate by 17%, primarily through a decrease in non-following responses. Qwen2.5-7B similarly shows reduced non-following and improved refusal behavior, with the Shield offering stronger gains and the Reflection Agent providing measurable improvements. Both the Shield and Reflection Agents improve safety performance, with the Shield generally providing stronger and more stable gains. As shown in Figure 6 and Table II, Flowchart and SIUO attacks exhibit relatively low ASR and high RR across all configurations, as models tend to generate descriptive rather than harmful content. Notably, Flowchart and SIUO attacks exhibit relatively low ASR and high RR across all configurations, as models tend to generate descriptive rather than harmful content.\n",
      "\n",
      "The Shield and Reflection Agents improve safety, with the Shield generally providing stronger and more stable gains. The Reflection Agent, while slightly less effective, still delivers measurable improvements, demonstrating its practical value as a lightweight enhancement. In practice, the choice between them can depend on latency and computational constraints‚Äîusing the Shield alone offers robust protection with minimal overhead, whereas combining both yields the best overall trade-off.\n",
      "\n",
      "Figure 6 and Table II present dataset-level results across five representative attack types: AdvBench, FigStep, Flowchart, MMSafety, and SIUO. All evaluated Vision-Language Models benefit from the proposed moderation strategies, though to varying extents. However, some models achieve comparable performance using only one module. For instance, Reflection alone nearly matches the combined configuration on FigStep and Flowchart, where attacks rely on semantic misdirection rather than explicit content. In contrast, Shield alone performs well on MMSafety and SIUO, where threats are visually grounded and easier to detect.\n",
      "\n",
      "Notably, Flowchart and SIUO attacks exhibit relatively low ASR and high RR across all configurations, as models tend to generate descriptive rather than harmful content. In Flowchart, the task typically instructs the model to describe and elaborate on flow diagrams, using much of the token budget for neutral explanation and leaving limited capacity for generation. Meanwhile, SIUO contains implicit, culturally nuanced cases (e.g., ‚ÄúShould I give my mother yarn for her birthday?‚Äù) where seemingly benign questions encode subtle social implications but rarely triggers overtly unsafe responses.\n",
      "\n",
      "Overall, these findings demonstrate that both Shield and Reflection are independently effective and can be flexibly deployed based on dataset characteristics and application scenarios: Shield suits real-time or low-latency moderation, while Reflection benefits post-hoc auditing and interpretability-sensitive contexts. Together, they form a robust, adaptive defense framework for diverse cross-modality adversarial settings. The integration of safety layers introduces moderate latency to the inference process. As shown in Table III, the Shield module adds only a negligible preprocessing overhead (0.015 seconds/query) through lightweight intent classification and policy lookup, while Reflection (1.5 seconds max/iteration) performs post-hoc self-evaluation and regeneration. If the first reflection deems the output safe, the process converges early without regeneration. Overall, the total inference time‚Äîcomprising Shield, Reflection, and model generation‚Äîremains efficient, indicating that the added safety mechanisms achieve meaningful protection with minimal latency cost.\n",
      "\n",
      "Despite its flexibility, the framework introduces a practical trade-off between safety robustness and inference efficiency. Incorporating multiple agents enhances moderation reliability but also increases computational cost/latency, which may constrain real-time or large-scale applications. Moreover, determining the optimal combination of modules and safety thresholds remains context-dependent and nontrivial, requiring careful calibration for each deployment. Future work will explore adaptive agent scheduling and cost-aware coordination strategies to balance safety guarantees with system responsiveness.\n",
      "VI Conclusion: Agentic Moderation is a model-agnostic framework that utilizes specialized agents (Shield, Evaluator, and Reflector) to collaboratively enforce safety policies for large vision-language models under multimodal adversarial attacks. It achieves this through iterative reasoning and feedback, dynamically adjusting moderation across textual and visual modalities, significantly reducing harmful outputs while maintaining instruction adherence. The framework offers a flexible and extensible foundation for safety, supporting modular defense strategies.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 32/40 [31:11<07:48, 58.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25724v1/1 Introduction\n",
      "../data/summaries/2510.25724v1/2 Existing Models\n",
      "../data/summaries/2510.25724v1/3 BambooKG: Architecture and Pipelines\n",
      "../data/summaries/2510.25724v1/4 Experimental Setup\n",
      "../data/summaries/2510.25724v1/5 Results\n",
      "../data/summaries/2510.25724v1/6 Future Work\n",
      "../data/summaries/2510.25724v1/References\n",
      "1 Introduction: BambooKG is a neurobiologically-inspired framework for long-term memory retention based on overlapping knowledge graphs. It utilizes a frequency-weighted associative mechanism, strengthening edge weights through repeated co-occurrence of non-triplet node pairs, enabling the system to ‚Äúassociate‚Äù new information with existing knowledge and facilitate retrieval.\n",
      "2 Existing Models: RAG, while effective, struggles with multi-hop reasoning due to independent chunk treatment. Newer pipelines, such as Chain-of-RAG, achieve state-of-the-art results on the KILT benchmark. GraphRAG combines RAG and OpenIE to build knowledge graphs, improving entity disambiguation and relational reasoning, with up to 10% accuracy gains in medical QA.\n",
      "3 BambooKG: Architecture and Pipelines: The BambooKG architecture constructs a knowledge graph through a multi-stage pipeline: text segmentation into chunks, tagging with a controlled LLM, and subgraph creation. This pipeline, inspired by STDP, emphasizes flexible tag selection and edge weighting to capture nuanced semantic relationships. The Tagger extracts query tags, which are used to generate local subgraph representations, and the BambooKG iteratively merges these subgraphs into a global graph.  The system leverages edge weights to represent co-occurrence and allows for partial pattern matching, mirroring memory recall processes in the brain.\n",
      "4 Experimental Setup: BambooKG was compared to OpenIE, GraphRAG, and KGGen against RAG as a baseline.  The study used top-k embeddings for BambooKG, OpenIE, and KGGen at 5-3, while RAG used top-k=5.  The HotPotQA dataset was used for general knowledge recall, and the MuSiQue dataset for multi-hop knowledge retention.  GPT-4o was used to generate answers from the recalled subgraph, and LLM-as-a-Judge evaluated the predicted answers against expected answers, considering GPT-4o‚Äôs non-deterministic nature.\n",
      "5 Results: BambooKG outperforms other knowledge graph methods in HotPotQA and MuSiQue due to data recall and retrieval time, despite OpenIE's incoherent triplets and GraphRAG's missing nodes and cluster issues. GraphRAG performs poorly in HotPotQA despite its good stance, likely due to missing answer node entities and a poorly performing corpus. The use of correct and distractor documents in the 100 randomly selected questions likely led to incorrect community generation, hindering GraphRAG's performance. BambooKG‚Äôs lack of triplets and arbitrary nodes, combined with the LLM-only Memorisation Pipeline, results in high extraction speeds and reduced context size compared to other approaches.\n",
      "6 Future Work: Future work will focus on improving the Tagger‚Äôs ability to handle specialized corpora through fine-tuning and prompt engineering, exploring the formation of communities and clusters within the knowledge graph, and refining subgraph selection during recall to reduce context size and accelerate LLM-based decisions.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 33/40 [31:36<05:38, 48.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25612v1/1 Introduction\n",
      "../data/summaries/2510.25612v1/2 Related Works\n",
      "../data/summaries/2510.25612v1/3 The Method\n",
      "../data/summaries/2510.25612v1/4 Evaluation\n",
      "../data/summaries/2510.25612v1/5 Results and Discussion\n",
      "../data/summaries/2510.25612v1/6 Conclusions and Future Work\n",
      "../data/summaries/2510.25612v1/Limitations\n",
      "../data/summaries/2510.25612v1/References\n",
      "../data/summaries/2510.25612v1/Overview\n",
      "../data/summaries/2510.25612v1/Appendix A LLM Assistance Statement\n",
      "../data/summaries/2510.25612v1/Appendix B Experimental settings\n",
      "../data/summaries/2510.25612v1/Appendix C Complexity Analysis\n",
      "../data/summaries/2510.25612v1/Appendix D CAIR Used Prompts\n",
      "../data/summaries/2510.25612v1/Appendix E AAW-Zoo-Generator\n",
      "../data/summaries/2510.25612v1/Appendix F AAW-Zoo Additional Information\n",
      "../data/summaries/2510.25612v1/Appendix G Guardrails Suite Additional Information\n",
      "../data/summaries/2510.25612v1/Appendix H Metrics Formulation\n",
      "../data/summaries/2510.25612v1/Appendix I Random Setting Formulation\n",
      "../data/summaries/2510.25612v1/Appendix J Human Verification\n",
      "../data/summaries/2510.25612v1/Appendix K Production-ready results - Full analysis\n",
      "1 Introduction: CAIR is a method for assessing agent influence on an LLM-based agentic AI workflow, offering a first-of-its-kind agent influence ranker, task-agnostic analysis, and reduced inference latency.\n",
      "2 Related Works: The study examines methods for interpreting agent workflows, drawing parallels to graph theory and network engineering. Graph centrality measures (Saxena & Iyengar) and the LASCNN Algorithm are used to assess agent importance, but these methods are limited by focusing solely on structural properties and failing to consider internal functionality. A network topology approach, utilizing the LASCNN Algorithm, is proposed, but this approach also has limitations regarding the inability to assess importance in flexible workflows. Human annotations are considered as a potential alternative, but this is costly, biased, and infeasible for real-time inference. Reinforcement learning, while focused on multi-agent systems, offers a different approach, but its dependence on reward metrics and complexity limit its applicability to LLM-based agents.\n",
      "3 The Method: CAIR analyzes AAW behavior by iteratively perturbing the internal workflow, measuring changes to the final output, and calculating agent importance scores. It uses LIME-inspired feature importance assessment, transforming the AAW into an embedding space, and calculating cosine distance to determine relevance. The final score is weighted by the influence of the representative query, adjusted by Œ± and Œ≤ parameters, and the highest score is selected.\n",
      "4 Evaluation: AAW-Zoo and AAW-Zoo-Generator are LLM chains designed to generate simple AAWs for evaluation. CAIR utilizes graph theory, classical feature importance, and SHAP to evaluate influence-based agent ranking in AAWs, achieving improved ranking accuracy and latency reduction compared to CFI.\n",
      "5 Results and Discussion: CAIR outperforms BTW and EV in all metrics for orchestrator and router AAWs, and the overall measure, due to the high autonomy of orchestrator and router AAWs, which results in a non-fixed architecture during activation. EV has higher TRS, P@1, and 1-SFD in sequential AAWs, while CAIR remains competitive, demonstrating that architecture significantly influences agent influence on final output. Human verification showed CAIR rankings more closely aligned with human perception. CAIR pairs input queries to representative queries based on proximity, as demonstrated in Figure 3. Ten runtime queries are shown with their corresponding representative queries, all represented by SBERT embeddings projected to 2D using T-SNE. Cosine similarity scores confirm accurate pairing based on functionality. CAIR is used as a downstream task to mitigate toxicity in LLM-level solutions. Kumar et al. (2023) demonstrates this using the CAIR system, which applies toxicity guardrails to every LLM call. Guardrails evaluate different toxicity types and apply three rounds of output correction when toxic outputs are detected. Applying guardrails to only the top-ranked agents reduces inference latency significantly, while maintaining effectiveness. Ablation studies evaluated CAIR‚Äôs components, revealing that combining all components resulted in the highest rankings. Sensitivity analysis demonstrated that deviations from selected Œ± and Œ≤ values decreased rankings, with a decline occurring when Œ± and Œ≤ values were pushed to extremes. Rankings remained stable with low standard deviation across varying query sets. The use of SBERT was evaluated, showing its effectiveness in measuring string differences in structured data.\n",
      "6 Conclusions and Future Work: Counterfactual-based Agent Influence Ranker (CAIR) was evaluated on 30 AAWs with 230 tasks. CAIR produced high-quality agent rankings, was applicable at inference time with low latency, and could adjust LLM-level downstream tasks. Future work includes expanding CAIR to hybrid architectures, higher agency levels, and lower output access, as well as extending CAIR to perform downstream tasks.\n",
      "Limitations: CAIR‚Äôs performance is limited by its reliance on user-provided representative queries, which can be affected by poor quality. Mitigation involves using the prompt provided in Appendix D to generate queries, and expanding the set of queries reduces the risk of ignoring functionalities. CAIR‚Äôs offline process requires setting two parameters (Œ± and Œ≤), introducing a risk of misconfiguration, but stable rankings are achieved across a wide range of parameter values.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Overview: Overview: This appendix details research methods, including LLM assistance, experimental setup, complexity analysis, prompt usage, dataset generation, and safety measures. It covers hardware/software, CAIR‚Äôs offline phase, prompt engineering, AAW-Zoo generation, guardrail implementation, metric definition, random setting, human verification, and a full results analysis.\n",
      "Appendix A LLM Assistance Statement: LLMs were utilized for both code development and sentence-level editing during the research and paper preparation process.\n",
      "Appendix B Experimental settings: Experiments were performed in an Azure virtual machine with Ubuntu 24.04 x64 OS, 6 vCPUs, and 112 GiB RAM. The Python environment utilized langgraph V0.2.69, langfuse V2.59.3, Networkx V3.4.2, and scikit-learn V1.6.1. The SBERT model was used with sentence-transformers/all-MiniLM-L6-v2, and stsb-roberta-base was used as the Semantic Textual Similarity predictor and SHAP KernelExplainer.\n",
      "Appendix C Complexity Analysis: CAIR begins by executing a representative query without interference, perturbing each agent‚Äôs activations individually. On average, J-j activations are performed, with J = 1 to J = 13, and the total number of agent activations across all executions is computed.  The total computational cost is estimated per query, considering LLM calls and agent activations, and is capped at 1465.81 seconds for the longest observed CAIR analysis (functionality 5).\n",
      "Appendix D CAIR Used Prompts: Prompts were used offline to perturb agent outputs to counterfactual scenarios. A prompt was generated for each AAW architecture, and a query-generating prompt based on an AAW overview was provided.\n",
      "Appendix E AAW-Zoo-Generator: Motivation. Prompting an LLM to generate a complete AAW is feasible. While the output may differ from the exact desired workflow and can include some code issues, LLMs generally possess the necessary knowledge. Our experiments show that by identifying mistakes and iterating on the results, the quality improves significantly with each step.\n",
      "\n",
      "High Level Idea. Break the full task into several smaller steps, each performed through a dedicated LLM call. By simplifying each request into a well-defined, closed-form prompt, mistakes and errors are dramatically reduced. This structured, step-by-step approach naturally supports a chain-of-thought reasoning process, allowing each component to build upon the outputs of prior steps with increased clarity and context. An additional important aspect is passing each response through a \"supervisor\" component, which evaluates the quality of the response concerning the user‚Äôs request and the previously completed steps.\n",
      "\n",
      "Implementation Details. AAW-Zoo-Generator is implemented using the LangGraph framework. Each LLM call is encapsulated as a node, which accesses relevant information and routes the output via the supervisor to the next node in the flow. All LLM calls use the gpt-o1 model, with no additional tools or external databases involved.\n",
      "\n",
      "We divide the full generator into three main steps, each comprising multiple components. The supervisor component operates independently and is not assigned to any specific step.\n",
      "\n",
      "User Request Analysis. The user provides a request for a generated AAW in simple natural language, with as much or as little detail as desired. The first component analyzes this request and produces a draft analysis. Next, a second component reviews the draft and generates clarification questions to address missing information. To simplify this step, each question includes a default response that the user can select if no specific answer is needed. Based on the original draft and the user‚Äôs answers, a finalized analysis is then generated.\n",
      "\n",
      "Workflow Design. Once the finalized analysis is obtained, a blueprint of the desired AAW is generated. This blueprint incorporates both user-specified architectural preferences and task-specific requirements. It defines the agents comprising the AAW, as well as the flow of information and agent connectivity.\n",
      "\n",
      "Finally, a tools component assigns relevant tools to each agent in the blueprint. Each agent is assigned at least one tool from a predefined set, allowing for flexible tool integration. In our dataset, we used three tools: an online search tool (Serper 9 9 9 serper.dev/ ), a summarization tool (Wikipedia), and an image generation tool (implemented by us using DALL¬∑E API).\n",
      "\n",
      "Appendix F AAW-Zoo Additional Information: Appendix F AAW-Zoo Additional Information:\n",
      "\n",
      "We generate 30 different AAWs across three architectures: sequential, orchestrator, and router (10 of each). Each sequential AAW consists of 5‚Äì6 agents, while each orchestrator AAW includes 6‚Äì7 agents, including the orchestrator agent. Each router AAW comprises a router agent, an output agent, and exactly three branches, with each branch containing 2‚Äì3 agents. In total, each router AAW contains between 8 and 11 agents.\n",
      "\n",
      "We provide a detailed list of all 30 AAWs below, organized by architecture. Each AAW is described by its high-level goal and the natural language prompt used to instruct AAW-Zoo-Generator during generation. The same set of goals is used for the sequential and orchestrator AAWs, while different, router-appropriate goals are selected for the router AAWs. These AAWs are simple systems, designed for research purposes rather than for use as stand-alone applications.\n",
      "\n",
      "Sequential AAWs:\n",
      "Name: social_post_gen\n",
      "Goal: Generate social media content.\n",
      "Description: A simple, sequential workflow that creates full social media posts by generating the main text, selecting relevant hashtags, and optionally suggesting an image idea based on a topic. The primary output is text.\n",
      "Name: gift_suggester\n",
      "Goal: Find personalized gift suggestions.\n",
      "Description: A simple, sequential workflow that takes user preferences and context, identifies suitable gift categories, and returns a short list of personalized gift ideas. The output is a text list.\n",
      "Name: appearance_recipe\n",
      "Goal: Create unique recipes.\n",
      "Description: A simple, sequential workflow that takes ingredients or preferences, generates a unique recipe, and optionally includes a description of the dish‚Äôs appearance. The main result is text.\n",
      "Name: clothing_recs\n",
      "Goal: Describe fashion outfit suggestions.\n",
      "Description: A simple, sequential workflow that takes style or occasion info and returns descriptive text outlining suggested clothing items and combinations.\n",
      "Name: house_list_summ\n",
      "Goal: Assist in apartment hunting.\n",
      "Description: A simple, sequential workflow that takes housing preferences, finds matching listings, and returns text-based summaries of each option.\n",
      "Name: cover_cv_writer\n",
      "Goal: Generate job application materials.\n",
      "Description: A simple, sequential workflow that collects user background, creates a tailored cover letter, and summarizes experience for a CV or LinkedIn. The output is structured text.\n",
      "Name: fitness_meal_plan\n",
      "Goal: Create personalized fitness plans.\n",
      "Description: A simple, sequential workflow that takes fitness goals and health data to generate a personalized workout and meal plan in text.\n",
      "Name: adult_story_gen\n",
      "Goal: Generate short stories.\n",
      "Description: A simple, sequential workflow that takes a theme, genre, or characters and creates a complete short story in text.\n",
      "Name: trip_itin_gen\n",
      "Goal: Plan a trip.\n",
      "Description: A simple, sequential workflow that takes user preferences, finds locations and activities, and produces a day-by-day travel itinerary in text.\n",
      "Name: product_copy_gen\n",
      "Goal: Generate product descriptions.\n",
      "Description: A simple, sequential workflow that takes product features and audience type and produces persuasive product descriptions in text.\n",
      "\n",
      "Orchestrator AAWs:\n",
      "Name: sm_post_creator\n",
      "Goal: Generate social media content.\n",
      "Description: A simple, orchestrator-based workflow that creates full social media posts by generating the main text, selecting relevant hashtags, and optionally suggesting an image idea based on a topic. The primary output is text.\n",
      "Name: gift_recommender\n",
      "Goal: Find personalized gift suggestions.\n",
      "Description: A simple, orchestrator-based workflow that takes user preferences and context, identifies suitable gift categories, and returns a short list of personalized gift ideas. The output is a text list.\n",
      "Name: unique_recipe_gen\n",
      "Goal: Create unique recipes.\n",
      "Description: A simple, orchestrator-based workflow that takes ingredients or preferences, generates a unique recipe, and optionally includes a description of the dish‚Äôs appearance. The main result is text.\n",
      "Name: style_suggester\n",
      "Goal: Describe fashion outfit suggestions.\n",
      "Description: A simple, orchestrator-based workflow that takes style or occasion info and returns descriptive text outlining suggested clothing items and combinations.\n",
      "Name: housing_summarizer\n",
      "Goal: Assist in apartment hunting.\n",
      "Description: A simple, orchestrator-based workflow that takes housing preferences, finds matching listings, and returns text-based summaries of each option.\n",
      "Name: cover_letter_summary\n",
      "Goal: Generate job application materials.\n",
      "Description: A simple, orchestrator-based workflow that collects user background, creates a tailored cover letter, and summarizes experience for a CV or LinkedIn. The output is structured text.\n",
      "Name: workout_meal_plan\n",
      "Goal: Create personalized fitness plans.\n",
      "Description: A simple, orchestrator-based workflow that takes fitness goals and health data to generate a personalized workout and meal plan in text.\n",
      "Name: adult_story_gen_orch\n",
      "Goal: Generate short stories.\n",
      "Description: A simple, orchestrator-based workflow that takes a theme, genre, or characters and creates a complete short story in text.\n",
      "Name: daybyday_itinerary\n",
      "Goal: Plan a trip.\n",
      "Description: A simple, orchestrator-based workflow that takes user preferences, finds locations and activities, and produces a day-by-day travel itinerary in text.\n",
      "Name: product_persuader\n",
      "Goal: Generate product descriptions.\n",
      "Description: A simple, orchestrator-based workflow that takes product features and audience type and produces persuasive product descriptions in text.\n",
      "\n",
      "Router AAWs:\n",
      "Name: hfd_qna_router\n",
      "Goal: Answer user health, fitness, or diet questions.\n",
      "Description: A router-based workflow that routes questions to HealthSearch, FitnessAdvice, or DietLookup branches based on the topic, using online sources for factual lookups.\n",
      "Name: keyword_topic_router\n",
      "Goal: Summarize trending topics in tech, politics, or entertainment.\n",
      "Description: A router-based workflow that chooses the correct branch (TechNews, PoliticsDigest, or EntertainmentBuzz) and creates a concise topical summary from web search results.\n",
      "Name: unique_recipe_gen\n",
      "Goal: Create unique recipes.\n",
      "Description: A simple, orchestrator-based workflow that takes ingredients or preferences, generates a unique recipe, and optionally includes a description of the dish‚Äôs appearance. The main result is text.\n",
      "Name: style_suggester\n",
      "Goal: Describe fashion outfit suggestions.\n",
      "Description: A simple, orchestrator-based workflow that takes style or occasion info and returns descriptive text outlining suggested clothing items and combinations.\n",
      "Name: housing_summarizer\n",
      "Goal: Assist in apartment hunting.\n",
      "Description: A simple, orchestrator-based workflow that takes housing preferences, finds matching listings, and returns text-based summaries of each option.\n",
      "Name: cover_letter_summary\n",
      "Goal: Generate job application materials.\n",
      "Description: A simple, orchestrator-based workflow that collects user background, creates a tailored cover letter, and summarizes experience for a CV or LinkedIn. The output is structured text.\n",
      "Name: workout_meal_plan\n",
      "Goal: Create personalized fitness plans.\n",
      "Description: A simple, orchestrator-based workflow that takes fitness goals and health data to generate a personalized workout and meal plan in text.\n",
      "Name: adult_story_gen_orch\n",
      "Goal: Generate short stories.\n",
      "Description: A simple, orchestrator-based workflow that takes a theme, genre, or characters and creates a complete short story in text.\n",
      "Name: daybyday_itinerary\n",
      "Goal: Plan a trip.\n",
      "Description: A simple, orchestrator-based workflow that takes user preferences, finds locations and activities, and produces a day-by-day travel itinerary in text.\n",
      "Name: product_persuader\n",
      "Goal: Generate product descriptions.\n",
      "Description: A simple, orchestrator-based workflow that takes product features and audience type and produces persuasive product descriptions in text.\n",
      "Appendix G Guardrails Suite Additional Information: The guardrails utilize one LLM call per guardrail, detecting 11 types of toxicity (harm, hate speech, violence, etc.). Prompts are structured as persona-definition-examples, with a warning about toxic queries. Three levels of correction (mild, moderate, strong) are applied to toxic responses, with the mild correction prompting a \"Yes\" response.\n",
      "Appendix H Metrics Formulation: TRS: Exact matching of predicted and reference rankings across all agents. P@3: All 1-3 agents in GT ranking match 1-3 in T. P@2: All 1-2 agents in GT ranking match 1-2 in T. P@1: Top-ranked agent in GT matches top-ranked agent in T. SFD: Sum of absolute differences in rank positions normalized by ‚åä n 2 / 2 ‚åã \\lfloor n^{2}/2\\rfloor.\n",
      "Appendix I Random Setting Formulation: The paper presents a random baseline calculation for each use case, computing the expected value of each metric under a random ranking assumption. It then calculates the Scaled Feature Distance (SFD) based on the expected absolute difference between a ranking and a uniform permutation.\n",
      "Appendix J Human Verification: Human verification of CAIR and CFI rankings revealed that the CAIR rankings were more frequently preferred by the verifiers, with a majority of cases favoring CAIR.\n",
      "Appendix K Production-ready results - Full analysis: CAIR demonstrates successful production-ready complexity through a publicly available use case and a hierarchical agent setup. The research team performs best on query 2, while the writing team performs best on query 3. The evaluation reveals that CAIR‚Äôs ranking aligns with expected results, demonstrating its suitability for complex real-world systems.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 34/40 [32:53<05:42, 57.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25694v1/1 Introduction\n",
      "../data/summaries/2510.25694v1/2 Related Work\n",
      "../data/summaries/2510.25694v1/3 EnConda-Bench\n",
      "../data/summaries/2510.25694v1/4 Experiments\n",
      "../data/summaries/2510.25694v1/5 Conclusion\n",
      "../data/summaries/2510.25694v1/Ethics Statement\n",
      "../data/summaries/2510.25694v1/References\n",
      "../data/summaries/2510.25694v1/Appendix A Data Construction\n",
      "../data/summaries/2510.25694v1/Appendix B Difficulty Rate\n",
      "../data/summaries/2510.25694v1/Appendix C Trajectory Training Data Genration Framework\n",
      "../data/summaries/2510.25694v1/Appendix D Experiment Settings\n",
      "../data/summaries/2510.25694v1/Appendix E The Use of Large Language Models (LLMs)\n",
      "1 Introduction: EnConda-Bench: A trajectory-based evaluation framework for process-level assessment of environment configuration in SWE, enabling detailed analysis of agent capabilities during configuration.\n",
      "2 Related Work: The paper reviews existing agent approaches to environment setup, highlighting limitations of early heuristics, tool-augmented agents, and specialized environment agents. It introduces Repo2Run, ExecutionAgent, and SetupAgent, each employing different strategies to improve process-level interaction and evaluation. Recent benchmarks, including HumanEval, MBPP, and APPS, demonstrate progress but struggle with real-world build challenges, while environment configuration benchmarks address this gap. The paper advocates for a framework that balances scale, diversity, and diagnostic depth for robust agent evaluation.\n",
      "3 EnConda-Bench: The EnConda-Bench task requires agents to diagnose and repair environment configuration errors. The agent uses planning and perception to identify errors, generate repair commands, and execute the environment, culminating in a shell script. Evaluation assesses the agent‚Äôs ability to correctly identify, reason about, and fix errors, comparing predicted and actual outcomes through process-level metrics. The benchmark consists of 4,201 READMEs from 323 repositories, stratified by the number of injected errors per README, with a distribution favoring levels 1 and 2.\n",
      "4 Experiments: We evaluated advanced LLMs and agent frameworks, including GPT-4.1, Claude-4, Gemini2.5-Pro, and DeepSeek-V3-0324 for foundation models and agent frameworks. Evaluation settings included zero-shot, code agents, and environment setup agents. The study found that zero-shot LLMs exhibit high recall but low precision in error typing, while code agents significantly improve this with OpenHands + DeepSeek-V3, but action/feedback abilities are underdeveloped. Environment configuration agents deliver the largest end-to-end gains, demonstrating effective perception and feedback utilization. Persistent gaps between description and fix accuracy and fix accuracy to Pass@1 reveal bottlenecks in feedback translation to execution action, necessitating process-level agent planning optimization and enhanced feedback utilization.\n",
      "5 Conclusion: The agent's environment configuration remains a major obstacle. A new benchmark focuses on process-level evaluation, generating scalable task instances through realistic error injection and Docker validation. While the agent demonstrates capability in error perception, it struggles with specific repair actions, likely due to a lack of effective interaction and feedback. Future research should enhance its ability to interact with the environment.\n",
      "Ethics Statement: We introduce EnConda-Bench, a benchmark dataset created with ethical considerations. Repository collection, error synthesis, data validation, and filtering are described, ensuring compliance with licenses and adherence to ethical guidelines. Data anonymization, desensitization, and data cleaning are employed to protect public welfare. Data sources are obtained with permission, and research directions are deemed ethically harmless.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Data Construction: The paper selects repositories from human-validated and pre-filtered benchmarks to ensure reliable buildability, verifying README correctness and compatibility with licenses. It utilizes Claude-4-sonnet and Gemini 2.5-pro to generate errors, including a six-type taxonomy for common environment configuration issues, and GPT-4.1-mini to validate each error, ensuring it prevents configuration failures and allows successful execution.\n",
      "Appendix B Difficulty Rate: The study compared difficulty scores from five benchmark sets (INSTALLAMATIC, ExecutionAgent, EnvBench, SetupBench, and a manually curated set) with a 1-5 scale, assessing alignment with real-world configuration tasks. Annotators scored each task instance based on clarity, command execution, file dependencies, and overall completeness.\n",
      "Appendix C Trajectory Training Data Genration Framework: Automated generation of environment configuration task instances and a comprehensive evaluation suite enable the creation of a synthetic trajectory data framework. This framework allows for the generation of trajectory data representing both successful and failed agent executions of these tasks, facilitating model fine-tuning and large-scale pre-training, assuming sufficient high-quality original data is available.\n",
      "Appendix D Experiment Settings: The agent follows a defined workflow, including repository information, README, and environment requirements. It must identify errors and perform unit tests after configuration.\n",
      "Appendix E The Use of Large Language Models (LLMs): LLMs were used as general-purpose assistive tools for grammar checking and minor wording refinement during manuscript preparation. Manual review and selection of edits were performed by the authors.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 35/40 [33:39<04:28, 53.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25679v1/Introduction\n",
      "../data/summaries/2510.25679v1/Results\n",
      "../data/summaries/2510.25679v1/Discussion\n",
      "../data/summaries/2510.25679v1/Methods\n",
      "../data/summaries/2510.25679v1/Acknowledgments\n",
      "../data/summaries/2510.25679v1/Data Availability Statement\n",
      "../data/summaries/2510.25679v1/References\n",
      "Introduction: Deep learning methods have significantly improved UAV obstacle avoidance and path planning, enabling more efficient navigation in complex environments. Reinforcement learning, particularly with DRL, is now being used to automate decision-making processes, allowing UAVs to learn through trial and error. Deep reinforcement learning is particularly effective in dynamic environments featuring small variations, while deep learning excels in handling complex visual data like images and LiDAR.  Hybrid approaches combining deep learning with traditional path planning techniques are also being explored to address challenges like rapid changes in the environment.\n",
      "Results: Results show that the LSTM baseline exhibits exploration plateau after 300 iterations, saturating at a norm of 0.94 ¬± 0.5. Replacing the recurrent core with an attention mechanism increases the plateau to 0.98 ¬± 0.2, reaching a top of 1.0 ¬± 0.1 after 600 iterations. The curves are generated from a moving average of 200 episodes. The task-level safety of the models increased with LSTM to flow-aware GTrXL, with peak success probabilities increasing from 1.0 to 0 collisions during inference. Crash rates decreased from PPO+LSTM to flow-aware GTrXL. DRL policies are trained in closed-loop settings and are explicitly optimized to generalize across a wide distribution of initial conditions and flow realizations.\n",
      "Discussion: The Transformer-based architectures, particularly GTrXL, demonstrate superior learning efficiency and reward compared to the LSTM baseline due to their gated self-attention mechanisms and explicit memory access, enabling better handling of long-range dependencies and dynamic flow fields. The Flow-aware GTrXL further enhances this by incorporating a multi-objective learning framework and a contrastive loss, resulting in improved robustness and adaptability in turbulent environments.\n",
      "Methods: Okay, this is a good summary of the provided text. Here‚Äôs a breakdown of the key takeaways and a slightly reorganized version focusing on the core concepts:\n",
      "\n",
      "**Core Concepts & Summary:**\n",
      "\n",
      "* **The Problem:** The text describes a challenging navigation task for a UAV ‚Äì navigating a 3D urban environment with obstacles. The goal is to find the optimal path to a target while avoiding collisions and minimizing energy consumption.\n",
      "\n",
      "* **Algorithms Considered:** Three algorithms are being compared:\n",
      "    * **PPO (Proximal Policy Optimization):** A reinforcement learning algorithm designed to optimize policies.\n",
      "    * **GTrXL (Gated Transformer eXtra Large):** A neural network architecture that uses attention mechanisms to handle long-range dependencies and memory.\n",
      "    * **Modified PPO with Flow Prediction:** An enhanced version of PPO incorporating a flow prediction head to improve navigation in 3D.\n",
      "\n",
      "* **The Approach:** The system uses a multi-objective DRL framework. It‚Äôs not just about finding the shortest path; it also focuses on generating realistic flow fields (the UAV‚Äôs movement).\n",
      "\n",
      "* **Key Components:**\n",
      "    * **Flow Field Representation:** The system generates a flow field (a representation of the UAV‚Äôs movement) using a block-decomposed approach.\n",
      "    * **Contrastive Learning:**  The model learns to distinguish between relevant and irrelevant flow patterns, improving the quality of the flow representation.\n",
      "    * **Reward Function:** A weighted combination of factors ‚Äì target reached, collision avoidance, energy efficiency, and time constraints ‚Äì guides the agent.\n",
      "\n",
      "* **Comparison:** The text highlights the comparison between these algorithms, specifically focusing on how the flow prediction head enhances the model's ability to navigate complex 3D environments.\n",
      "\n",
      "**Reorganized Summary (Focus on Key Points):**\n",
      "\n",
      "**Goal:**  Develop a robust navigation algorithm for a UAV in a 3D urban environment, balancing efficiency, collision avoidance, and flow field quality.\n",
      "\n",
      "**Algorithms Compared:**\n",
      "\n",
      "1. **PPO (Proximal Policy Optimization):** A reinforcement learning algorithm for policy optimization.\n",
      "2. **GTrXL (Gated Transformer eXtra Large):** A neural network architecture using attention mechanisms for long-range dependency handling and memory.\n",
      "3. **Modified PPO with Flow Prediction:** An enhanced PPO incorporating a flow prediction head to improve navigation in 3D.\n",
      "\n",
      "**Key Innovations:**\n",
      "\n",
      "* **Flow Field Generation:** The system generates a flow field representation, which is crucial for understanding the environment.\n",
      "* **Contrastive Learning:**  Uses contrastive learning to improve flow representation quality.\n",
      "* **Flow Prediction Head:**  Leverages attention mechanisms to handle long-range dependencies and improve navigation.\n",
      "\n",
      "**In essence, the text describes a research effort to create a more sophisticated navigation system that can effectively handle complex 3D environments using a combination of reinforcement learning and neural network architectures.**\n",
      "\n",
      "---\n",
      "\n",
      "Do you want me to elaborate on any specific aspect of this summary or provide more detail on a particular component (like the flow prediction head)?\n",
      "Acknowledgments: Federica Tonti and Ricardo Vinuesa acknowledge funding from the European Union‚Äôs HORIZON program, project REFMAP, under Grant Agreement number 101096698. Computations were performed at the supercomputer Dardel at PDC, KTH, and the National Academic Infrastructure for Supercomputing in Sweden (NAISS) computer time was provided.\n",
      "Data Availability Statement: The author provides access to all code and data used in the work through a GitHub repository.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 36/40 [34:26<03:26, 51.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25616v1/1. Introduction\n",
      "../data/summaries/2510.25616v1/2. Related Works\n",
      "../data/summaries/2510.25616v1/3. Preliminaries\n",
      "../data/summaries/2510.25616v1/4. VL-Think Task Suite\n",
      "../data/summaries/2510.25616v1/5. VL representations analysis\n",
      "../data/summaries/2510.25616v1/6. Method\n",
      "../data/summaries/2510.25616v1/7. Experiments\n",
      "../data/summaries/2510.25616v1/8. Ablations\n",
      "../data/summaries/2510.25616v1/9. Conclusion\n",
      "../data/summaries/2510.25616v1/References\n",
      "../data/summaries/2510.25616v1/Appendix A Appendix\n",
      "1. Introduction: Here‚Äôs a summary of the paper:\n",
      "\n",
      "A study investigates the degradation of visual-language representations during supervised fine-tuning of VLMs, specifically focusing on the loss of semantic grounding and generalization ability. Initial research showed that adapting VLMs to action tasks often leads to overfitting and representation collapse. This work addresses this by starting with a pretrained VLA and fine-tuning it with limited data, demonstrating systematic degradation during task-specific fine-tuning. The authors introduce a lightweight Visual Representation Alignment method inspired by the Platonic Representation Hypothesis to mitigate this degradation. Experiments show that this method consistently improves out-of-distribution generalization, achieving up to a 10% relative gain over naive SFT. The paper also proposes a diagnostic task suite, VL-Think, to assess transfer of VL knowledge.\n",
      "2. Related Works: Large-scale multimodal learning aims to unify perception, reasoning, and control through scaling VL pretraining. Recent studies show increasing alignment of representations across architectures and modalities as models grow, with methods like REPA, OLA-VLM, 3DRS, and Geometry Forcing demonstrating this trend. The challenge remains preserving VL understanding during fine-tuning.\n",
      "3. Preliminaries: The VLA model uses a multimodal Transformer backbone with L stacked layers. Input sequences (visual and textual) are processed through a shared embedding space. Each layer updates hidden states using self-attention, and the model uses an autoregressive objective to generate target tokens. During decoding, the model conditions on the input and previously generated tokens, using a causal mask to ensure dependencies are only based on the input sequence. Training involves a next-token loss.\n",
      "4. VL-Think Task Suite: The VL-Think Task Suite evaluates VL capabilities independently of low-level control, focusing on assessing transfer of knowledge from VLMs to VLAs. It uses a Simpler benchmark with a carrot object on abstract boards, evaluating both VLA and VLM models through eight tasks that test semantic grounding and object placement. The suite isolates VL skills by limiting execution complexity, measuring whether a model correctly understands visual symbols and compositional cues.\n",
      "5. VL representations analysis: The VL representations in Qwen2.5-VL demonstrate strong semantic grounding and precise object-focused attention, while OpenVLA exhibits significant degradation in attention quality and domain forgetting, suggesting a loss of semantic understanding.  T-SNE visualization reveals a structured latent space for instruction-related tokens, while attention maps demonstrate the model‚Äôs focus on relevant objects, supporting the hypothesis of domain-specific knowledge erosion.\n",
      "6. Method: The paper introduces a Visual Representation Alignment objective to stabilize VLA model representations by aligning them with a frozen teacher encoder. This mitigates drift by encouraging the model to maintain consistent semantic structure across tasks and environments. The method utilizes a lightweight projector to measure similarity between student and teacher embeddings, and a loss function that integrates autoregressive action and alignment to preserve perceptual consistency.\n",
      "7. Experiments: We evaluate our proposed alignment method in several robotics environments using Simpler, Tao, and Liu robotics environments, assessing robustness to vision, semantics, and execution variations across three axes. OOD evaluation includes 128 randomized variations, quantified with linear probing on ImageNet-100. Model performance is assessed over 128 randomized seeds with SD reporting and Wilcoxon signed-rank tests for p-value reporting. The Freeze baseline fails across all categories, indicating that freezing pretrained visual features does not preserve useful representations during adaptation. Joint optimization is necessary to recover general-purpose visual semantics while adapting to new robotic environments.\n",
      "8. Ablations: A systematic ablation study was conducted to analyze the impact of different design choices on visual alignment of visual representations, focusing on the choice of teacher model, alignment strategy, target layers, projector type, and loss functions. C-RADIOv3 achieved the best overall results, indicating that stronger vision models trained on large, diverse, and multimodal data offer more stable and generalizable visual features for alignment. Backbone2Enc consistently yielded stronger results, suggesting that the primary degradation occurs in the middle-to-late fusion layers of the VLA model, requiring regularization to maintain visual-semantic consistency. Experiments revealed that freezing the MLP projector is crucial for maintaining visual-semantic consistency, while aligning middle layers yields the most substantial improvements across generalization.\n",
      "9. Conclusion: The study investigated the degradation of VL understanding and representation quality resulting from fine-tuning VLA models on robotic tasks. It introduced a diagnostic suite and interpretability probes to analyze this degradation, proposing a lightweight Visual Alignment method to improve OOD generalization across diverse scenarios. The research focused on fine-tuning rather than full pretraining, aiming to guide future efforts in scalable robotic pretraining and VL knowledge transfer.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Appendix: This appendix provides detailed ablation studies evaluating the visual alignment objective, including detailed parameter settings, visualization results, and statistical analysis. The study tests the hypothesis that model variant B outperforms baseline model A in success rate, utilizing paired Wilcoxon signed-rank tests across different environments.  A linear probe was used to assess the impact of the alignment objective on internal representations, and the study employs a frozen-feature linear probe to ensure reproducibility.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 37/40 [35:14<02:32, 50.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25732v1/1 Introduction\n",
      "../data/summaries/2510.25732v1/2 Motivation\n",
      "../data/summaries/2510.25732v1/3 Problem Formulation\n",
      "../data/summaries/2510.25732v1/4 Experiment Setup\n",
      "../data/summaries/2510.25732v1/5 Results\n",
      "../data/summaries/2510.25732v1/6 Discussion\n",
      "../data/summaries/2510.25732v1/7 Related Works\n",
      "../data/summaries/2510.25732v1/8 Conclusions\n",
      "../data/summaries/2510.25732v1/Limitations\n",
      "../data/summaries/2510.25732v1/Social Impacts and Ethical Considerations\n",
      "../data/summaries/2510.25732v1/References\n",
      "../data/summaries/2510.25732v1/Appendix A Implementation Details\n",
      "../data/summaries/2510.25732v1/Appendix B Additional Results\n",
      "1 Introduction: Here‚Äôs a summary of the provided text:\n",
      "\n",
      "The paper investigates unlearning robustness in large language models (LLMs) by examining how knowledge entanglement and persuasive framing interact. Recent work shows that even state-of-the-art models struggle with factual knowledge, necessitating a new approach. The authors propose the Stimulus-Knowledge Entanglement-Behavior Framework (SKeB) to analyze this interaction. SKeB integrates cognitive theory with communication principles to evaluate how knowledge structure and prompt delivery influence unlearning. The framework introduces nine entanglement metrics and demonstrates that distance-weighted influence (‚Ñ≥ 9 \\mathcal{M}_{9}) strongly predicts factual recall, with authority framing significantly increasing entanglement activation. Furthermore, the study reveals that smaller models exhibit greater factual recall increases compared to larger models, offering a predictive model for knowledge leakage.\n",
      "2 Motivation: The paper investigates LLM knowledge retrieval through semantic entanglement and communicative framing. Adversarial prompts reactivate knowledge, demonstrating suppression rather than erasure. Hebb‚Äôs principle suggests knowledge entanglement, allowing for indirect reactivation of forgotten information. The paper hypothesizes that different delivery methods, such as persuasive framing, exploit this entanglement to reveal residual knowledge and influence model behavior. The Stimulus-Knowledge Entanglement-Behavior Framework (SKeB) formalizes this process, evaluating knowledge retrieval through semantic entanglement and communicative framing.\n",
      "3 Problem Formulation: We formalize the SKeB framework through three components: stimulus, entanglement, and behavior. The stimulus activates domain graph regions, entanglement determines spread, and behavior reflects knowledge access. Unlearning aims to suppress knowledge via rhetorical framing, evaluating model outputs across factual recall, non-factual content, and hallucination dimensions. The study utilizes a domain graph to quantify entanglement and employs ensemble judge models for factuality assessment, aiming to predict unlearning failures and assess model robustness.\n",
      "4 Experiment Setup: The experiment setup involves using the Harry Potter domain (œá‚Ñú) as a dataset for unlearning LLM variants, processing 1,200 prompts through three models (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, and LLaMA-2-13B) with the same approximate unlearning algorithm.  The response evaluation employed an ensemble of three judge models (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) assessing factual recall, non-factualness, and hallucination, with gpt-5-mini resolving borderline cases. A co-occurrence domain graph was constructed from seven Harry Potter books, resulting in 1,296 entities and 35,922 edges, with nine entanglement metrics quantifying structural properties.\n",
      "5 Results: Here‚Äôs a summary of the provided text:\n",
      "\n",
      "The research investigates the effects of framing, emotionality, and architectural differences on unlearning models. Results show that persuasive framing systematically alters entanglement patterns, with authority prompts enhancing distance-weighted influence and suppressing hallucinations. Emotional framing suppresses hallucination, while logical framing enhances factual recall. The model size inversely correlates with persuasive technique effectiveness, and the ‚Ñ≥ 9 \\mathcal{M}_{9} metric is the strongest predictor of factual recall. The study also reveals correlations between architectural differences and unlearning robustness, with LLaMA-2-7B exhibiting genuine knowledge disruption and LLaMA-3.1-8B retaining intact knowledge structures.\n",
      "6 Discussion: Authority prompts, persuasion, and the size-vulnerability paradox are key factors influencing LLM factual recall. The unlearning method reveals a persistent knowledge base, while entanglement increases hallucination risk.\n",
      "7 Related Works: Sun et al. (2023) demonstrated that LLM factuality degrades systematically, with performance declining as entity popularity decreases. Their Head-to-Tail benchmark showed that model size doesn't automatically improve factual knowledge retention. The study investigates unlearning effectiveness, reveals that it removes or suppresses knowledge, demonstrates that persuasive framing recovers more content, and analyzes entanglement metrics. These findings suggest static factuality assessments underestimate knowledge retention in both base and unlearned models. Existing research often frames unlearning as an adversarial attack, but this investigation explores how rhetorical framing impacts unlearning robustness, particularly when presented to LLMs in different contexts. The SKeB framework, modeling memory retrieval as activation propagating through semantic networks, offers a systematic approach to understanding unlearning behaviors and aligns with existing literature demonstrating GPT-3's cognitive patterns and human-like priming effects.\n",
      "8 Conclusions: Our work proposes a SKeB system for proactive vulnerability assessment before LLM deployment, demonstrating that entanglement metrics strongly predict factual recall and persuasive framing recovers 50-128% more content with inverse model size. A regression model explains 78% of variance, enabling accurate prediction, and different architectures reveal distinct correlations in knowledge encoding and suppression.  LLMs are more vulnerable to authority appeals than emotional appeals.\n",
      "Limitations: This work establishes connections between cognitive theories and machine unlearning, but faces limitations in generalization to sensitive domains and model size. Correlation analysis reveals potential reverse relationships, and while strong correlations validate the framework, direct neural activation observation is unavailable. The evaluation relies on specific LLM judges, and while results are indicative, full mechanistic understanding remains challenging. The framework offers predictive tools for vulnerability assessment, even with incomplete mechanistic insights.\n",
      "Social Impacts and Ethical Considerations: Findings suggest privacy protection is limited, potentially hindering unlearning effectiveness. Models can still provide harmful information, especially with framing, and unlearning alone is insufficient.  The framework offers evaluation tools, but models should not assume safety.  Scaling may lead to increased vulnerability, requiring architectural innovations.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix A Implementation Details: We obtained four models for evaluation: LLaMA-2-7B, LLaMA-3.1-8B, OPT-2.7B, and leverages gradient ascent to unlearn Harry Potter content from Eldan and Russinovich (2023). For LLaMA-3.1-8B, LLaMA-2-13B, and OPT-2.7B, we implemented fine-tuning on the full Harry Potter corpus, creating forget and retain datasets, and applying the WHP unlearning algorithm. Hardware-wise, we used 4 GPUs with 128 GB GPU memory. Step 1 took approximately 3-7 hours, and Steps 2-3 took an additional 3-5 hours. The target domain was the complete Harry Potter corpus. Training parameters included batch size of 1, learning rate of 1 x 10^-4, and training duration of 3 epochs. Starting with 300 manually crafted base prompts, we used a scripted pipeline leveraging gpt-4 (via the OpenAI API) to generate three persuasive variants using distinct rhetorical techniques derived from persuasion theory: Emotional Appeal (emotional language, personal stories), Logical Reasoning (logical arguments), and Authority Endorsement (respected figures, institutional backing). For prompting the 4 models, we used a standardized text-generation procedure and applied it to each unlearned model. Models were loaded on a CUDA-enabled GPU when available, with automatic fallback to CPU. Each prompt, both the original and the three gpt-generated persuasive variants, was formatted with custom instruction markers ( [INST] ... [/INST] ) to guide the model to complete the sentence accurately. Generation was performed using a sampling-based approach with a maximum of 300 new tokens per prompt. While temperature, top-p, and repetition penalty were left at their default values, outputs for each prompt and variant were saved incrementally to a JSON file. This inference setup was applied consistently across all models, unlearned and base, enabling direct comparison of outputs while maintaining stable execution and controlled resource usage.\n",
      "\n",
      "The ECE, EWS, and AEWS metrics were calculated to assess entity connections, total connection strength, and average relationship strength, respectively. The DWIS, ANDE, SGD, RR, and the influence-based metric were used to quantify the redundancy and influence of connections.\n",
      "\n",
      "Appendix B Additional Results: Appendix B includes additional results, including statistical analyses and detailed data tables.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 38/40 [35:58<01:37, 48.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25595v1/1 Introduction\n",
      "../data/summaries/2510.25595v1/2 Related work\n",
      "../data/summaries/2510.25595v1/3 Collaboration Under Information Asymmetry\n",
      "../data/summaries/2510.25595v1/4 Environment-based Verifier\n",
      "../data/summaries/2510.25595v1/5 Experiments and Results\n",
      "../data/summaries/2510.25595v1/6 Conclusion\n",
      "../data/summaries/2510.25595v1/Limitations\n",
      "../data/summaries/2510.25595v1/Acknowledgments\n",
      "../data/summaries/2510.25595v1/References\n",
      "../data/summaries/2510.25595v1/Appendix\n",
      "../data/summaries/2510.25595v1/Appendix A Example of Game Play\n",
      "../data/summaries/2510.25595v1/Appendix B Data Generation\n",
      "../data/summaries/2510.25595v1/Appendix C Ablation on Environment-Based Verifiers\n",
      "../data/summaries/2510.25595v1/Appendix D Experiment Details\n",
      "../data/summaries/2510.25595v1/Appendix E Error Taxonomy\n",
      "../data/summaries/2510.25595v1/Appendix F Code of Ethics\n",
      "../data/summaries/2510.25595v1/Appendix G Human Study Interface\n",
      "../data/summaries/2510.25595v1/Appendix H Prompts Used\n",
      "1 Introduction: This paper investigates the collaborative capabilities of LLM agents, particularly under conditions of information asymmetry, using the Einstein Puzzles task. Two agents solve spatial and relational constraints in a tabletop environment, leveraging fine-tuning and a verifier to assess collaborative behavior. Empirical results demonstrate that agents with both seeking and providing communication capabilities perform best, while mismatched communication leads to degradation. Environment-based verification, leveraging natural feedback, significantly improves agent performance, and human study reveals a preference for proactive information sharing.\n",
      "2 Related work: This work explores LLM agents operating under information asymmetry in collaborative roles, incorporating human studies to assess real-world human-AI interaction. A verification model is introduced to evaluate LLM responses during inference time, leveraging environmental feedback from simulated environments to achieve a compute-efficient verification method.\n",
      "3 Collaboration Under Information Asymmetry: The paper presents a collaborative puzzle game where two agents compete to place objects into designated bins on a table, utilizing a distributed constraint satisfaction problem. Agents share information about destination bins and must collaboratively solve a task, measured by the minimum moves to complete the configuration. The paper investigates four communication action spaces ‚Äì Information Providing & Seeking, Information Seeking Only, Information Providing & Seeking, and No Information Exchange ‚Äì and evaluates LLM models using supervised fine-tuning and diverse game configurations to assess their ability to communicate and reason effectively.\n",
      "4 Environment-based Verifier: The research introduces a lightweight, training-free environment-based verifier designed to support LLM agents in collaborative reasoning. It utilizes game rules and previous communication to evaluate generated actions, expanding a graph of constraints iteratively. This approach assesses alignment with current knowledge and avoids unnecessary trials, demonstrating effectiveness through testing with alternative verifier strategies.\n",
      "5 Experiments and Results: We conduct extensive experiments to evaluate the performance of language models in reasoning and communication under information asymmetry. Our experiments address four key research questions:\n",
      "\n",
      "1.  Collaboration under Varying Communicative Action Space: How do agents collaborate under information asymmetry when equipped with different communicative actions?\n",
      "2.  Effectiveness of Environment-Based Verification: To what extent can environment-based verification enhance the performance?\n",
      "3.  Collaboration under Mismatched Communicative Action Space: What happens when agents with mismatched communication capabilities are paired together?\n",
      "4.  Human Preferences Toward Different Communication Behaviors: While agents may perform well in self-play, do these behaviors align with human preferences in collaboration?\n",
      "\n",
      "To systematically investigate our research questions, we design a series of experiments to investigate specific aspects of collaboration:\n",
      "\n",
      "Exp1: Collaboration with Different Communicative Action Spaces: We evaluated both closed-source language models via API calls and open-source models with supervised fine-tuning (SFT). We deploy the same model (with the same action space) to perform self-play in the game. This setting allows direct comparison of collaboration with different communicative action space.\n",
      "\n",
      "Exp2: Environment-Based Verifier: We augment the base models with an environment-based verifier, which provides binary feedback indicating whether each sampled action is valid. At each decision step, we sample 4 candidate responses using temperature=0.2 and top-p=0.9 , and apply the verifier to filter out invalid actions, selecting the first valid one as the final output. This setup enables a direct comparison of model performance with and without the verifier.\n",
      "\n",
      "Exp3: Collaboration with Mismatched Communicative Action Space: We select the well-performing models from those action spaces (Llama3.1-8B without CoT for no information exchange , and Llama3.1-8B with CoT for the rest), equipped with reasoning verifier, and let them play with each other. This setting further reveals the behaviors of collaboration between agents with different communication capabilities.\n",
      "\n",
      "Exp4: Human Performance: Once we identified the most effective configuration of communicative action space for collaborative reasoning among LLM agents, we further examined whether these behaviors align with human preferences in collaboration. We recruited 12 college students as human participants to interact with the best-performing models, each equipped with different communicative action spaces and supported by the reasoning verifier. We sampled 27 unseen games (9 games each with 4, 5, and 6 objects) and measured both task completion rate and efficiency. Each participant was assigned a sequence of 9 games‚Äî3 games each with 4, 5, and 6 objects‚Äîwhile being given the complete communicative action space. In each game, they interacted with a model configured with a specific communicative action space, which remained unknown to the participant.\n",
      "\n",
      "Evaluation Metrics: We employ complementary metrics to assess both effectiveness and efficiency. For effectiveness, we report the Success Rate (SR) and subgoal success rate (Sub.R) at the first attempt (Pass@1). SR measures the percentage of games successfully completed within a limited number of steps (30 for all the experiments), while Sub.R reflects partial progress by capturing the proportion of objects correctly placed, even when the full game is not successfully completed. For efficiency, we track the number of steps taken to complete each game and compare it to the optimal solution calculated by our planner. We compute the Step Ratio (StepR), defined as the ratio of the number of executed steps to the number of steps in the optimal solution.\n",
      "\n",
      "When a verifier is applied, we additionally report the correction rate ( Corr.R ), indicating the proportion of responses corrected by the verifier. We report standard error for each metrics.\n",
      "\n",
      "Across all model families, we observe a generally clear hierarchy in task performance: Information Seeking & Providing > Seeking Only > Mismatched Action Spaces > No Information Exchange > Providing Only.\n",
      "\n",
      "Enabling both seeking and providing actions consistently yields the highest Success Rate (SR ) and lower Step Ratio ( StepR ). confirming that bi-directional exchange is both expressive and effective for coordinating constraints, especially when enhanced with chain-of-thought reasoning.\n",
      "\n",
      "Permitting only information seeking is better than permitting only information providing. Targeted queries minimize redundant traffic and let agents actively access the missing piece of information, whereas blind sharing often floods the channel with constraints that are irrelevant or already known by their partners. Therefore, the result suggests that information seeking is a more task-efficient option than unprompted information providing.\n",
      "\n",
      "Interestingly, disabling communication altogether ( ‚ÄúNo Information Exchange‚Äù ) ranks high for some variants. Without communication and reasoning process through CoT, the task reduces to pure object manipulation and random guessing;\n",
      "\n",
      "Therefore, the model‚Äôs entire capacity is devoted to mastering game rules and action affordance. Llama variants exploit this by memorizing high-probability transition patterns, achieving a success rate greater than 81%. Qwen, which is pre-trained on a different instruction distribution, does not show the same effect, suggesting the phenomenon is model-specific rather than an intrinsic property of the environment.\n",
      "\n",
      "As shown in Table 1 , adding the reasoning verifier (our best verifier) to the base models leads to large absolute SR gains (labeled in red), ranging from 10 to 40% increment. This reflects the high potential capabilities of the base models, as well as the effectiveness of the assistance from our verifier.\n",
      "\n",
      "To better understand the underlying behaviors, we further examine model behavior in scenarios where two agents are assigned different communicative action spaces. Specifically, we evaluate four asymmetric pairings of action spaces, as shown in Table 3 . We exclude the pair Provide only vs. None, as it is functionally equivalent to Provide & Seek vs. None . In both cases, one agent is unable to respond to queries. Similarly, Seek only vs. None is the same as None vs. None , since the None agent cannot initiate or respond to communication.\n",
      "\n",
      "Across the evaluated pairings, we observe a consistent drop in performance when agents have mismatched communicative abilities. This finding highlights the importance of aligning communication protocols in collaborative tasks and provides insights into designing matched-communication agent design.\n",
      "6 Conclusion: We found that aligned communication, particularly information seeking and providing, is crucial for successful collaboration in a tabletop setting with LLM agents. Error analysis reveals limitations in task understanding, which are addressed through environment-based verification. Human study emphasizes the importance of proactive and transparent communication for trust and interpretability. These findings highlight a need for reliable, communication-aware, and interpretable design for future LLM-based collaborative systems.\n",
      "Limitations: The study has limitations due to reliance on a simulated environment with recoverable invalid actions, which remains challenging to extend to real-world settings. Human evaluation was conducted on a small scale, limiting the ability to compare models and expand the study with more participants and broader comparisons.\n",
      "Acknowledgments: This work was supported by the Microsoft Accelerate Foundation Models Research (AFMR) grant program. We thank human-study participants.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "Appendix: Appendix includes examples of game play, data generation methods, ablation studies on verifiers, error taxonomy analysis, model training and deployment details, ethical compliance statements, and a web interface description. It also details prompts used for agent reasoning.\n",
      "Appendix A Example of Game Play: Figure 3 illustrates a complete game session of the Einstein Puzzle, building on the initial setup in Figure 1. This example demonstrates turn-taking, decision-making, and communication, serving as a reference for understanding the game‚Äôs mechanics and reasoning process. The ask action is not utilized in this simplified case.\n",
      "Appendix B Data Generation: The planner generates solutions under different action spaces, utilizing breadth-first search and an evolving knowledge state incorporating communication history and object positions. It avoids redundant information seeking through inference and identifies unknown goals by asking for information. It introduces pass actions to skip turns and handles scenarios with no communication, allowing for random guessing. The planner collects both optimal and near-optimal trajectories to represent diverse communication strategies, including strategies where agents attempt multiple bin locations, mirroring models observed in Table 1.\n",
      "Appendix C Ablation on Environment-Based Verifiers: The study introduces three verifier types: affordance, which validates physical actions based on environment rules, and communication, which assesses meaningful communication. The reasoning verifier builds upon these, extending coverage through inferred knowledge. The affordance and communication verifiers are hierarchically linked, with the reasoning verifier providing a flexible framework adaptable to various environments. Performance comparisons across different settings, including Llama3.1-8B CoT, demonstrate the verifier‚Äôs potential as a general and lightweight approach to simulated environments.\n",
      "Appendix D Experiment Details: The study utilized Azure OpenAI models for GPT-4o-20241120, employing LoRA fine-tuning with a rank of 32, 128 batch size, and 2e-4 learning rate, accelerated by FlashAttention-2. Model evaluation involved PeFT, vLLM, and human subjects in four action space configurations, with a maximum step limit of 30 minutes, and a consent form.\n",
      "Appendix E Error Taxonomy: The appendix defines four error types: format following (invalid output, token limit), physical understanding (incorrect source location), destination bin unreachable, and source and destination bin being the same.  Communication includes redundant knowledge sharing, missing share after seek, and wrong share after seek. Task reasoning includes incorrect rule understanding and wrong random guessing.\n",
      "Appendix F Code of Ethics: The research study involves interacting with AI agents to complete tasks, including a logical board game and sharing information. The study lasts approximately one hour and will record text, symbolic inputs, and numerical evaluations. Data will be analyzed for research purposes and will not store personally identifiable information.\n",
      "Appendix G Human Study Interface: The study introduces a web-based logic-based tabletop game where two players collaborate with an AI agent. Players must place objects into designated bins according to a set of constraints, including relationships between objects and bin placement rules. The game utilizes four actions: Move, Share, Ask, and Skip. The AI agent‚Äôs objective is to complete the task with the fewest steps through collaboration and guidance. The game board features a game area, objects within reach, constraints, and objects to ask about. The game ends when objects are placed correctly or after 30 turns. A short survey is required after each game to assess the experience.\n",
      "Appendix H Prompts Used: The study presents four action space configurations, each with and without chain-of-thought reasoning, utilizing prompts tailored to each. GPT-4o generates reasoning traces and actions in a specific format for the Providing‚ÄÑ&‚Ñü Seeking configuration, while the remaining configurations differ in permitted action spaces and reasoning examples. The study employs a planner to generate solutions for each game instance, prompting GPT-4o to assume the agent's perspective and rationale. GPT-4o‚Äôs reasoning traces are used as supervision for smaller models, with additional guidance provided for a fairer comparison.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 39/40 [37:12<00:56, 56.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/summaries/2510.25517v1/1 Introduction\n",
      "../data/summaries/2510.25517v1/2 Preliminaries\n",
      "../data/summaries/2510.25517v1/3 Related work\n",
      "../data/summaries/2510.25517v1/4 Renaming predicates using LLMs\n",
      "../data/summaries/2510.25517v1/5 Experiments and Results\n",
      "../data/summaries/2510.25517v1/6 Discussion\n",
      "../data/summaries/2510.25517v1/7 Conclusion\n",
      "../data/summaries/2510.25517v1/Declarations\n",
      "../data/summaries/2510.25517v1/References\n",
      "1 Introduction: This paper introduces the use of Large Language Models (LLMs) to generate semantically meaningful names for unnamed predicates in logic programming. It reviews existing literature, presents a methodology, details experimental setup, presents results, discusses limitations, and concludes with future research directions.\n",
      "2 Preliminaries: 2.1: The paper introduces the invention of predicates ‚Äì symbolic representations ‚Äì within ILP systems.  Invented predicates, denoted as \"inv,\" are used to simplify rules, enhance modularity, and represent new concepts. The PI technique automates the discovery of these predicates, addressing a challenge in automatically generating new symbolic representations. The paper explores various approaches, including the use of \"inv\" in the example of parent-grandparent relations, and highlights the ongoing effort to improve the process through metarules and the Learning From Failures framework. The authors also discuss the application of hierarchical probabilistic logic programming (HPLPs) and the creation of new predicates through PHIL, a learning algorithm, and the use of abductive logic programming (ALP) to derive new relations. The paper also considers the use of the \"collaboretedOn\" predicate, and the application of matrix operations to discover new relationships.\n",
      "\n",
      "2.2: Large Language Models (LLMs) emphasize prompt engineering ‚Äì crafting clear instructions for models to guide output. Key elements include goals, context, audience, role, input data, output format, and examples. The order of information influences response quality, utilizing the primacy effect and recency effect. Few-shot learning and Chain-of-Thought prompting improve performance through careful prompt design.\n",
      "3 Related work: This work explores methods to enhance reasoning capabilities in LLMs, leveraging recent advancements in neural networks and symbolic reasoning. NeSyGPT integrates neural and symbolic components to improve logical reasoning, while LLM2LAS utilizes LLMs for ASP program generation from natural language. These efforts address challenges in predicate naming and logical inference, demonstrating potential for automated knowledge representation and reasoning within the ILP domain.\n",
      "4 Renaming predicates using LLMs: This section outlines a method for assigning meaningful names to unnamed predicates using Large Language Models (LLMs). The pipeline involves prompting the LLMs to generate suggestions, evaluating their relevance, and selecting the best name. The study utilizes seven LLMs, including ChatGPT-4o, ChatGPT-o3mini, Gemini, and Command R+, and evaluates their performance through automated judging and human evaluation.\n",
      "5 Experiments and Results: Here‚Äôs a concise summary of the paper sections:\n",
      "\n",
      "- **5.1 Prompting Strategy:** Models varied in their approaches to initial instructions, utilizing independent runs and ChatGPT‚Äôs memory feature to prevent persistent interactions. Models struggled with name extraction, requiring multiple attempts and a naming constraint.\n",
      "- **5.2 Results for Few-Shot Prompting:** FalconMamba, Llama, and Falcon3 demonstrated varying performance, with Falcon3 correctly identifying 10 out of 16 names, while Llama failed to generate meaningful output. FalconMamba‚Äôs performance suggests a need for prompt engineering.\n",
      "- **5.3 LLMs Judgment Results:**  ChatGPT-4o and ChatGPT-o3mini were the best-performing models, with Gemini and Command R+ performing poorly. Few-shot prompting improved FalconMamba, while Falcon3 provided correct suggestions.\n",
      "- **5.4 Human Judgment Results:** Human judges favored straightforward names, aligning with LLM behavior, and correctly identified predicates h0 and h3.\n",
      "- **5.5 Summary of Results:** ChatGPT-4o and ChatGPT-o3mini were the best-performing models, followed by Gemini and Command R+. Llama, FalconMamba, and Falcon3 performed poorly.\n",
      "6 Discussion: The discussion highlights limitations of automated predicate naming for logic theory rules, particularly with LLMs. Current approaches rely on placeholder names, while the authors suggest using predefined names and prompting models to suggest them, but face challenges with model failures, rule alteration, and difficulty with real-world data.  The approach is hampered by simple examples, model errors, and the need for manual standardization of names, hindering its effectiveness.  Further research suggests domain-specific fine-tuning may be necessary for practical applications.\n",
      "7 Conclusion: The paper proposes a pipeline using LLMs to suggest predicates in logic theories, addressing the issue of interpretability. While the initial results are promising, the study is limited by example selection and future research could explore more complex scenarios, investigate language-specific conventions, and utilize fine-tuned LLMs for domain-specific naming, as well as quantized models for local deployment.\n",
      "Declarations: Elisabetta Gentili, supported by a ministerial decree and NextGenerationEU funding, collaborated with several researchers and institutions, including Spoke 1 (‚ÄúFutureHPC & BigData‚Äù), Partenariato Esteso PE00000013 (‚ÄúFAIR - Future Artificial Intelligence Research‚Äù), Katsumi Inoue, Tony Ribeiro, and Fabrizio Riguzzi, all funded by the European Union.\n",
      "References: References include cited works that support, illustrate, or expand upon the ideas presented in the text. They are listed in a specific format, typically with author, year, and title of the work.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [38:08<00:00, 57.22s/it]\n"
     ]
    }
   ],
   "source": [
    "scoring_results = []\n",
    "\n",
    "for paper_id, mapping in tqdm(list(abstract_content_mapping.items())):\n",
    "    subsection_llm_summary =  summarize_whole_paper(paper_id,mapping[\"content\"])#get_lead_N(content=mapping[\"content\"], n=5)\n",
    "    \n",
    "    mapping[\"SUBSECTIION_LLM\"] = subsection_llm_summary\n",
    "\n",
    "    # SUBSECTIION_LLM\n",
    "    rouge_scores_subsection_llm = calculate_rouge_score(mapping[\"abstract\"], mapping[\"SUBSECTIION_LLM\"])\n",
    "    bert_scores_subsection_llm = calculate_bert_score(mapping[\"abstract\"], mapping[\"SUBSECTIION_LLM\"])\n",
    "\n",
    "\n",
    "\n",
    "    scoring_results.append(\n",
    "        {\n",
    "            \"paper_id\": paper_id, \n",
    "            \"method\": \"SUBSECTIION_LLM\",\n",
    "            \"rouge1\": rouge_scores_subsection_llm[\"rouge1_fmeasure\"], \n",
    "            \"rougeL\": rouge_scores_subsection_llm[\"rougeL_fmeasure\"],\n",
    "            \"bert_score_f1\": bert_scores_subsection_llm[\"bertscore_f1\"]\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9d3ac31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>method</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bert_score_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2510.25320v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.394030</td>\n",
       "      <td>0.173134</td>\n",
       "      <td>0.617097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2510.25232v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.198142</td>\n",
       "      <td>0.654488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2510.25563v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.293907</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.615584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2510.25590v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.531986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2510.25007v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.253807</td>\n",
       "      <td>0.121827</td>\n",
       "      <td>0.563439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2510.25388v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.158491</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.573186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2510.25770v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.609015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2510.25662v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.271698</td>\n",
       "      <td>0.128302</td>\n",
       "      <td>0.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2510.25729v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.313993</td>\n",
       "      <td>0.163823</td>\n",
       "      <td>0.598906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2510.25005v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.574733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2510.25758v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.134387</td>\n",
       "      <td>0.603811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2510.25445v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.178674</td>\n",
       "      <td>0.590334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2510.25683v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.590268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2510.25557v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.609203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2510.25510v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.673789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2510.25588v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.322251</td>\n",
       "      <td>0.179028</td>\n",
       "      <td>0.653849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2510.25101v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.282686</td>\n",
       "      <td>0.141343</td>\n",
       "      <td>0.619992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2510.25577v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.255973</td>\n",
       "      <td>0.133106</td>\n",
       "      <td>0.588614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2510.25091v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.361905</td>\n",
       "      <td>0.215873</td>\n",
       "      <td>0.599031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2510.25518v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.633165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2510.24832v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.526015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2510.25504v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.193103</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.527647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2510.25731v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.138114</td>\n",
       "      <td>0.079681</td>\n",
       "      <td>0.514594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2510.25668v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.601444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2510.25626v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.081408</td>\n",
       "      <td>0.515638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2510.25014v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.450237</td>\n",
       "      <td>0.175355</td>\n",
       "      <td>0.644894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2510.25065v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.309278</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.606829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2510.25634v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.190045</td>\n",
       "      <td>0.634051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2510.25602v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.470899</td>\n",
       "      <td>0.227513</td>\n",
       "      <td>0.686759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2510.25205v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.433684</td>\n",
       "      <td>0.197895</td>\n",
       "      <td>0.607105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2510.25223v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.544125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2510.25179v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.226027</td>\n",
       "      <td>0.629564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2510.25724v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.559135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2510.25612v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.179348</td>\n",
       "      <td>0.610991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2510.25694v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.383459</td>\n",
       "      <td>0.172932</td>\n",
       "      <td>0.619430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2510.25679v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.280884</td>\n",
       "      <td>0.130039</td>\n",
       "      <td>0.604281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2510.25616v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>0.172185</td>\n",
       "      <td>0.629559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2510.25732v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.391608</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.637297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2510.25595v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.347305</td>\n",
       "      <td>0.149701</td>\n",
       "      <td>0.598084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2510.25517v1</td>\n",
       "      <td>SUBSECTIION_LLM</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.179245</td>\n",
       "      <td>0.620650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id           method    rouge1    rougeL  bert_score_f1\n",
       "0   2510.25320v1  SUBSECTIION_LLM  0.394030  0.173134       0.617097\n",
       "1   2510.25232v1  SUBSECTIION_LLM  0.421053  0.198142       0.654488\n",
       "2   2510.25563v1  SUBSECTIION_LLM  0.293907  0.150538       0.615584\n",
       "3   2510.25590v1  SUBSECTIION_LLM  0.090323  0.077419       0.531986\n",
       "4   2510.25007v1  SUBSECTIION_LLM  0.253807  0.121827       0.563439\n",
       "5   2510.25388v1  SUBSECTIION_LLM  0.158491  0.113208       0.573186\n",
       "6   2510.25770v1  SUBSECTIION_LLM  0.418605  0.170543       0.609015\n",
       "7   2510.25662v1  SUBSECTIION_LLM  0.271698  0.128302       0.640700\n",
       "8   2510.25729v1  SUBSECTIION_LLM  0.313993  0.163823       0.598906\n",
       "9   2510.25005v1  SUBSECTIION_LLM  0.262295  0.147541       0.574733\n",
       "10  2510.25758v1  SUBSECTIION_LLM  0.260870  0.134387       0.603811\n",
       "11  2510.25445v1  SUBSECTIION_LLM  0.363112  0.178674       0.590334\n",
       "12  2510.25683v1  SUBSECTIION_LLM  0.263566  0.131783       0.590268\n",
       "13  2510.25557v1  SUBSECTIION_LLM  0.340782  0.178771       0.609203\n",
       "14  2510.25510v1  SUBSECTIION_LLM  0.471545  0.222222       0.673789\n",
       "15  2510.25588v1  SUBSECTIION_LLM  0.322251  0.179028       0.653849\n",
       "16  2510.25101v1  SUBSECTIION_LLM  0.282686  0.141343       0.619992\n",
       "17  2510.25577v1  SUBSECTIION_LLM  0.255973  0.133106       0.588614\n",
       "18  2510.25091v1  SUBSECTIION_LLM  0.361905  0.215873       0.599031\n",
       "19  2510.25518v1  SUBSECTIION_LLM  0.360825  0.195876       0.633165\n",
       "20  2510.24832v1  SUBSECTIION_LLM  0.202532  0.126582       0.526015\n",
       "21  2510.25504v1  SUBSECTIION_LLM  0.193103  0.110345       0.527647\n",
       "22  2510.25731v1  SUBSECTIION_LLM  0.138114  0.079681       0.514594\n",
       "23  2510.25668v1  SUBSECTIION_LLM  0.311688  0.194805       0.601444\n",
       "24  2510.25626v1  SUBSECTIION_LLM  0.158416  0.081408       0.515638\n",
       "25  2510.25014v1  SUBSECTIION_LLM  0.450237  0.175355       0.644894\n",
       "26  2510.25065v1  SUBSECTIION_LLM  0.309278  0.195876       0.606829\n",
       "27  2510.25634v1  SUBSECTIION_LLM  0.343891  0.190045       0.634051\n",
       "28  2510.25602v1  SUBSECTIION_LLM  0.470899  0.227513       0.686759\n",
       "29  2510.25205v1  SUBSECTIION_LLM  0.433684  0.197895       0.607105\n",
       "30  2510.25223v1  SUBSECTIION_LLM  0.106250  0.052083       0.544125\n",
       "31  2510.25179v1  SUBSECTIION_LLM  0.424658  0.226027       0.629564\n",
       "32  2510.25724v1  SUBSECTIION_LLM  0.352000  0.152000       0.559135\n",
       "33  2510.25612v1  SUBSECTIION_LLM  0.434783  0.179348       0.610991\n",
       "34  2510.25694v1  SUBSECTIION_LLM  0.383459  0.172932       0.619430\n",
       "35  2510.25679v1  SUBSECTIION_LLM  0.280884  0.130039       0.604281\n",
       "36  2510.25616v1  SUBSECTIION_LLM  0.417219  0.172185       0.629559\n",
       "37  2510.25732v1  SUBSECTIION_LLM  0.391608  0.216783       0.637297\n",
       "38  2510.25595v1  SUBSECTIION_LLM  0.347305  0.149701       0.598084\n",
       "39  2510.25517v1  SUBSECTIION_LLM  0.358491  0.179245       0.620650"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(scoring_results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8914af8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sns.boxplot(data=df_results, x=\"method\", y=\"rouge1\", ax=ax[0])\n",
    "#sns.boxplot(data=df_results, x=\"method\", y=\"rouge2\", ax=ax[1])\n",
    "sns.boxplot(data=df_results, x=\"method\", y=\"rougeL\", ax=ax[1])\n",
    "sns.boxplot(data=df_results, x=\"method\", y=\"bert_score_f1\", ax=ax[2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "943f565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/summaries'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUMMARIES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2c29d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Implemented an llm based summarizer and want to understand what it is missing. Here are my summaries for 40 papers and their according abstracts used as a ground truth. Please tell me how i could improve my summarization:\n",
      "MTIR-SQL is a reinforcement learning framework for multi-turn tool-integrated reasoning in text-to-SQL, enhancing LLMs‚Äô reasoning through trajectory filtering and KL regularization, resulting in improved SQL execution accuracy. Experiments on the BIRD dataset demonstrate its effectiveness, achieving a 16% absolute improvement over baselines. The methodology utilizes œÄ Œ∏ ( ‚ãÖ | x ; ‚Ñ∞ ) \\pi_{\\theta}(\\cdot|x;\\mathcal{E}) leveraging real-time execution results to guide the model‚Äôs reasoning. GRPO-Filter, an enhanced Group Relative Policy Optimization (GRPO) variant, incorporates three innovations: constrained optimization, selective rollout filtering, and multi-turn extension. Performance on main benchmarks is significantly higher than other models, with MTIR-SQL achieving 64.4% accuracy on the BIRD Dev set and 57.17% on the 10B-30B range. Ablation studies reveal that different reward components impact model performance, with the GRPO-Filter stabilizing training while sacrificing reward collapse. The paper concludes that a balanced reward system is essential for optimal performance. MTIR-SQL outperforms baseline methods and achieves 64.4% accuracy on BIRD-SQL and 84.6% execution accuracy on SPIDER with a 4B-parameter model.\n",
      "As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves 64.4% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.\n",
      "KVDA-UCT demonstrates superior performance in deterministic settings, outperforming OGA-UCT and other methods, particularly in environments like SysAdmin and Wildfire. It detects more abstractions than OGA-UCT, and outperforms all competitors in some environments, including those with multiple iterations.\n",
      "A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by grouping state-action pairs and using their aggregate statistics instead of single-node statistics. On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS abstraction algorithm for deterministic environments that builds its abstraction using the Abstractions of State-Action Pairs (ASAP) framework, which aims to detect states and state-action pairs with the same value under optimal play by analysing the search graph. ASAP, however, requires two state-action pairs to have the same immediate reward, which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency. In this paper, we break with the paradigm of grouping value-equivalent states or state-action pairs and instead group states and state-action pairs with possibly different values as long as the difference between their values can be inferred. We call this abstraction framework Known Value Difference Abstractions (KVDA), which infers the value differences by analysis of the immediate rewards and modifies OGA-UCT to use this framework instead. The modification is called KVDA-UCT, which detects significantly more abstractions than OGA-UCT, introduces no additional parameter, and outperforms OGA-UCT on a variety of deterministic environments and parameter settings.\n",
      "Okay, let‚Äôs focus on the mathematical aspects of the exponential map and its connection to the group structure, expanding on the existing explanation.\n",
      "\n",
      "**Expanding on the Exponential Map & Group Structure**\n",
      "\n",
      "The exponential map, `exp(Œ∏)`, is the linchpin of the derivation. It‚Äôs not simply a mapping; it‚Äôs a carefully constructed transformation that reveals the group‚Äôs underlying structure.  Let‚Äôs delve deeper into its role:\n",
      "\n",
      "**1. The Exponential Map as a Transformation:**\n",
      "\n",
      "The exponential map, `exp(Œ∏)`, transforms the evolution of the vector field `v` into a transformation of the variables `x` and `u`.  The key is that `Œ∏` is a parameter that controls the *type* of transformation being applied.  Different parameters lead to different types of transformations ‚Äì rotations, reflections, scaling, etc.  The map essentially ‚Äúunfolds‚Äù the PDE‚Äôs evolution into a set of differential equations.\n",
      "\n",
      "**2.  The Transformation Equations ‚Äì A Core Concept:**\n",
      "\n",
      "The equations derived from the ansatz are the heart of the derivation.  They are *not* simple differential equations. Instead, they are *transformation equations*.  These equations are crucial because they explicitly define how the variables `x` and `u` change as they move through the group.  The transformation equations are typically expressed in a form that allows for a direct computation of the generated vector field.  The mathematical rigor of these equations is what distinguishes the derivation from a purely intuitive understanding.\n",
      "\n",
      "**3.  Group Structure & Transformation Maps:**\n",
      "\n",
      "The group structure is defined by the *transformation maps*. These maps are the primary focus of the derivation.  They are *not* just a simple mapping; they are carefully designed to preserve the essential properties of the system.  The transformation maps are the key to understanding how the group‚Äôs structure translates into the differential equations.  The transformation maps are often expressed as a set of equations that relate the initial state to the final state.\n",
      "\n",
      "**4.  The Group‚Äôs Role in the Equations:**\n",
      "\n",
      "The transformation maps are directly linked to the group‚Äôs elements. The group‚Äôs structure dictates the *type* of transformation that is applied. For example, a rotational symmetry might manifest as a specific transformation map that preserves the vector field‚Äôs orientation. The transformation maps are the mathematical representation of the group‚Äôs structure.\n",
      "\n",
      "**5.  Mathematical Tools & Techniques:**\n",
      "\n",
      "The derivation relies on several mathematical tools:\n",
      "\n",
      "*   **Differential Equations:** The transformation equations are expressed as differential equations, which are the foundation of the derivation.\n",
      "*   **Group Theory:** The concept of a group is crucial. The transformation maps are designed to be a group, meaning they have a defined operation (the transformation) and a set of identity elements.\n",
      "*   **Representation Theory:**  The exponential map can be viewed as a representation of the group.  This allows for a more abstract and powerful approach to understanding the transformation process.\n",
      "\n",
      "**6.  Challenges & Considerations (Expanding on the Original Text):**\n",
      "\n",
      "*   **Complexity of Transformations:** The transformation maps can be incredibly complex, especially for systems with multiple variables and intricate PDE dynamics.\n",
      "*   **Computational Cost:**  The derivation can be computationally intensive, particularly when dealing with large-scale systems.\n",
      "*   **Understanding the Group:** The group structure itself can be subtle and require careful analysis.\n",
      "\n",
      "**7.  Potential for Automation:**\n",
      "\n",
      "The mathematical framework allows for potential automation.  The transformation equations could be formulated in a way that allows for automated computation of the generated vector field, reducing the need for manual calculations.\n",
      "\n",
      "**8.  Future Directions:**\n",
      "\n",
      "*   **Expanding to More General PDEs:** The framework could be extended to handle more general PDEs, potentially incorporating additional symmetry groups.\n",
      "*   **Automated Base Solution Selection:**  The derivation could be adapted to automatically select a suitable base solution, simplifying the optimization process.\n",
      "\n",
      "Do you want me to elaborate on any of these points further, perhaps focusing on a specific aspect like the mathematical representation of the transformation maps or how the exponential map relates to the group structure?\n",
      "We introduce a method for efficiently solving initial-boundary value problems (IBVPs) that uses Lie symmetries to enforce the associated partial differential equation (PDE) exactly by construction. By leveraging symmetry transformations, the model inherently incorporates the physical laws and learns solutions from initial and boundary data. As a result, the loss directly measures the model‚Äôs accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our method enables rigorous error estimation. The approach yields compact models, facilitating an efficient optimization. We implement LieSolver and demonstrate its application to linear homogeneous PDEs with a range of initial conditions, showing that it is faster and more accurate than physics-informed neural networks (PINNs). Overall, our method improves both computational efficiency and the reliability of predictions for PDE-constrained problems.\n",
      "PM4GRPO is a novel reasoning-aware GRPO framework that uses Process Mining (PM) to evaluate reasoning alignment between a pretrained teacher model and a student model. It employs a conformance reward based on sequence importance and clipping to ensure alignment while preserving model freedom of thought. Experiments demonstrate superior reasoning and generalization across math benchmarks, significantly outperforming baselines like DeepMath-Zero-7B and Skywork-OR1-7B, achieving the highest scores on MATH 500, Olympiad Bench, and Minerva Math.\n",
      "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs),\n",
      "yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO)\n",
      "that augments standard answer/format rewards with signals over the reasoning procedure.\n",
      "To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model‚Äôs\n",
      "reasoning aligns with the pretrained teacher model.\n",
      "The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms\n",
      "existing methodologies for GRPO-based post-training. These results highlight\n",
      "that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.\n",
      "The Autoregressive State-Tracking Prompting (ASTP) methodology enhances procedural compliance and accuracy in dialogue flow adherence and state-specific post-processing within dynamic trading interactions. The proposed method utilizes a unified prompt with a state-tracking system, ASTP, to guide the LLM‚Äôs dialogue. The prompt includes a system instruction defining the NPC‚Äôs persona, game world data, dialogue history, and dialogue guidelines. ASTP is employed to infer dialogue states from conversation history, ensuring state-aware decision making. The system enforces transition constraints and explicitly identifies previous states, resulting in a verifiable state-tracking process. A placeholder-based post-processing system is used during Offer-Sell states to guarantee accurate price calculations, enhancing transaction integrity.\n",
      "\n",
      "Experiments and results demonstrate that ASTP achieves near-perfect compliance, with ablation studies revealing that explicit transition rules significantly improved performance, while the ‚Äòidentify‚Äô directive proved highly effective. The paper explores prompt structure, comparing different approaches and highlighting the robustness of ASTP‚Äôs core principles. Furthermore, the study examines the impact of model variants and evaluation metrics, including accuracy, token usage, response time, and the effectiveness of the placeholder keyword __PRICE__ in ensuring accurate price calculations.\n",
      "\n",
      "The paper concludes that ASTP improves procedural compliance and accuracy in rule-governed applications like in-game trading. It uses a structured workflow and placeholder-based post-processing to enhance reliability and speed, achieving up to 9x faster responses with smaller models.\n",
      "Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust.\n",
      "This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading\n",
      "(browse-offer-review-confirm).\n",
      "To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations.\n",
      "Evaluation across 300 trading dialogues demonstrates >>99% state compliance\n",
      "and 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models‚Äô (Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games.\n",
      "The code will be publicly available upon acceptance.\n",
      "Here‚Äôs a concise summary of the paper:\n",
      "\n",
      "The paper introduces a method called MOSSP (Multi-objective Stochastic Shortest Path) to efficiently find the Pareto front of a graph ‚Äì a set of paths with optimal combinations of costs. It leverages probabilistic transitions, objective functions, and local/global optimization to achieve efficient computation, addressing the computational challenges of the problem.\n",
      "Multi-objective search (MOS) has emerged as a unifying framework for planning and decision-making problems where multiple, often conflicting, criteria must be balanced. While the problem has been studied for decades, recent years have seen renewed interest in the topic across AI applications such as robotics, transportation, and operations research, reflecting the reality that real-world systems rarely optimize a single measure.\n",
      "This paper surveys developments in MOS while highlighting cross-disciplinary opportunities, and outlines open challenges that define the emerging frontier of MOS research.\n",
      "This research introduces a novel AI-assisted diagnostic platform leveraging fine-tuned LLMs, the OpenAI-gpt-oss reasoning model, and an orchestration LLM to improve the accuracy and consistency of mental disorder diagnoses. The system analyzes conversational data to predict diagnoses, utilizes a consortium of LLMs for final decision-making, and incorporates responsible AI principles through a dedicated orchestration agent. The prototype is deployed in collaboration with the U.S. Army Medical Research Team, demonstrating potential for resource-constrained environments through low-computation techniques.\n",
      "The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist‚Äìpatient interactions focused on mental health conditions (e.g., depression). These models are capable of identifying mental disorders with high accuracy based on natural language input. The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate seamless communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Each LLM in the consortium was fine-tuned using the Unsloth library on Google Colab‚Äôs Tesla GPUs. To optimize performance on consumer-grade hardware, we employed Low-Rank Adaptation with 4-bit quantization (QLoRA). Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis‚Äîpaving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.\n",
      "Here‚Äôs a concise summary of the paper:\n",
      "\n",
      "This paper presents a hybrid quantum-classical recurrent neural network (QRNN) architecture, the PQC, designed to improve gradient stability and expressiveness. The PQC utilizes a quantum state representing recurrent core memory, allowing for exponential memory capacity and enhanced gradient preservation. Experiments demonstrate competitive performance with LSTM and scaled Cayley orthogonal scoRNNs, with the PQC proving computationally stable and retaining more stable gradients. The authors introduce a new hybrid QRNN architecture, balancing expressivity, implementation simplicity, and simulation efficiency, incorporating a feedforward network and leveraging the PQC for quantum state manipulation.  The paper also discusses the challenges of hybrid quantum-classical systems and proposes improvements in toolchains and hardware to enable more accurate simulations.\n",
      "We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the entire recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an nn-qubit PQC, residing in an exponentially large Hilbert space ‚ÑÇ2n\\mathbb{C}^{2^{n}}. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit measurements, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, adopting projective measurements as a limiting case to obtain mid-circuit readouts while maintaining a coherent recurrent quantum memory. We further devise a soft attention mechanism over the mid-circuit readouts in a sequence-to-sequence model and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.\n",
      "Here‚Äôs a summary of the provided text:\n",
      "\n",
      "The paper investigates unlearning robustness in large language models (LLMs) by examining the interplay of knowledge entanglement and persuasive framing. It proposes the Stimulus-Knowledge Entanglement-Behavior Framework (SKeB) to analyze this interaction, integrating cognitive theory with communication principles. The SKeB introduces nine entanglement metrics and demonstrates that distance-weighted influence (‚Ñ≥ 9 \\mathcal{M}_{9}) strongly predicts factual recall, with authority framing significantly increasing entanglement activation. Smaller models exhibit greater factual recall increases compared to larger models, suggesting a predictive model for knowledge leakage. The study utilizes a domain graph to quantify entanglement and employs ensemble judge models for factuality assessment, aiming to predict unlearning failures and assess model robustness.\n",
      "Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.\n",
      "Here‚Äôs a summary of the provided text:\n",
      "\n",
      "The paper investigates collaborative puzzle solving with LLMs, focusing on the effectiveness of communication strategies. It demonstrates that agents with seeking and providing communication capabilities perform best, while mismatched communication degrades performance. The research introduces a lightweight verifier to improve LLM responses, and human studies reveal a preference for proactive information sharing. Experiments evaluate different communication action spaces, including seeking, providing, and mismatched communication, and assess model performance with and without a verifier. The study also examines how human preferences for communication styles influence model behavior, particularly in collaborative tasks.  The paper highlights the importance of reliable communication for successful collaboration and suggests a need for model-specific design.\n",
      "While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game.\n",
      "In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle.\n",
      "We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment.\n",
      "Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents‚Äô ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles\n"
     ]
    }
   ],
   "source": [
    "print(\"I Implemented an llm based summarizer and want to understand what it is missing. Here are my summaries for 40 papers and their according abstracts used as a ground truth. Please tell me how i could improve my summarization:\")\n",
    "\n",
    "for summary_path in list(Path(SUMMARIES_PATH).iterdir())[:10]:\n",
    "    summary  = open(summary_path/\"summary.txt\").read()\n",
    "    print(summary)\n",
    "\n",
    "    paper_id = summary_path.name\n",
    "\n",
    "    abstract = open(Path(ABSTRACT_PATH)/f\"{paper_id}.txt\").read()\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751f102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00192b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
