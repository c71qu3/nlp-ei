{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29e0165",
   "metadata": {},
   "source": [
    "# Qualitative Review\n",
    "\n",
    "### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65ecb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import random\n",
    "\n",
    "from typing import Dict, Any, Union, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cfb2d",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d36ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES_AND_STATS = os.path.join(\"..\", \"baselines\", \"results\", \"tfidf_bm25_lead_results.json\")\n",
    "TWO_PASS_LLM_SUMMARY = os.path.join(\"..\", \"baselines\", \"results\", \"llm_summarized_results.json\")\n",
    "LLM_EMBEDDING_SUMMARY = os.path.join(\"..\", \"baselines\", \"results\", \"llm_embeddings_results.json\")\n",
    "\n",
    "\n",
    "def load_json(filepath: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n",
    "    \"\"\"Return JSON data.\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "results = {}\n",
    "rules_n_stats = load_json(RULES_AND_STATS)\n",
    "for paper_id, data in rules_n_stats.items():\n",
    "    results[paper_id] = {}\n",
    "\n",
    "    # Article's abstract\n",
    "    results[paper_id][\"Abstract\"] = ' '.join(data[\"abstract\"][\"plain\"])\n",
    "\n",
    "    # 1. Lead N prediction\n",
    "    results[paper_id][\"Lead N\"] = data[\"lead_n\"]\n",
    "\n",
    "    # 2. Lead N by Section prediction\n",
    "    results[paper_id][\"Lead N by Section\"] = data[\"lead_n_by_section\"]\n",
    "\n",
    "    # 3. TF-IDF prediction\n",
    "    n = len(data[\"abstract\"][\"plain\"])\n",
    "    tfidf_indexes = data[\"rank\"][\"TF-IDF\"][:n]\n",
    "    tfidf_sentences = [data[\"sentences\"][\"plain\"][i] for i in tfidf_indexes]\n",
    "    results[paper_id][\"TF-IDF\"] = ' '.join(tfidf_sentences)\n",
    "\n",
    "    # 4. BM25 prediction\n",
    "    n = len(data[\"abstract\"][\"plain\"])\n",
    "    tfidf_indexes = data[\"rank\"][\"BM25\"][:n]\n",
    "    tfidf_sentences = [data[\"sentences\"][\"plain\"][i] for i in tfidf_indexes]\n",
    "    results[paper_id][\"BM25\"] = ' '.join(tfidf_sentences)\n",
    "\n",
    "\n",
    "two_pass_llm = load_json(TWO_PASS_LLM_SUMMARY)\n",
    "for data in two_pass_llm:\n",
    "    paper_id = data[\"paper_id\"]\n",
    "\n",
    "    # 5. Two-pass LLM prediction\n",
    "    results[paper_id][\"Two-pass w/ Gemma 3\"] = data[\"gemma3:1b\"][\"summary\"]\\\n",
    "\n",
    "\n",
    "llm_embeddings = load_json(LLM_EMBEDDING_SUMMARY)\n",
    "for data in llm_embeddings:\n",
    "    paper_id = data[\"paper_id\"]\n",
    "\n",
    "    # 6. Embedding-based LLM prediction\n",
    "    results[paper_id][\"Embedding-based w/ Gemini 2.5\"] = data[\"gemini-2.5-flash\"][\"summary\"]\n",
    "    results[paper_id][\"Embedding-based w/ Llama 3.1\"] = data[\"llama-3.1-8b-instant\"][\"summary\"]\n",
    "    results[paper_id][\"Embedding-based w/ Longformer Encoder-Decoder\"] = data[\"led\"][\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f154121",
   "metadata": {},
   "source": [
    "### Review Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cac4e1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<br>\n",
       "\n",
       "## `2511.21678v1`\n",
       "\n",
       "### Abstract\n",
       "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo—solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge—preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction–hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page/.\n",
       "\n",
       "### Lead N\n",
       "Multimodal Large Language Models (MLLMs) have achieved impressive progress in scene understanding, visual question answering, and complex scientific problem solving [ bai2025qwen2 , wu2024deepseek , zhang2025abstractive , zhao2025efficient ] . Yet despite their growing capability, current MLLMs approach each problem de novo —solving every query in isolation, repeatedly re-deriving the same insights and re-committing familiar errors [ fang2025comprehensive , zhang2025survey , gao2025survey , tan2025prospect ] . Although recent memory-augmented models attempt to mitigate this by storing past interactions [ suzgun2025dynamic , zhang2025ace ] , these memories capture only high-level logical summaries while discarding the visual grounding and perceptual cues essential for multimodal reasoning. Recent research has demonstrated that MLLMs’ visual perception ability remains fundamentally weaker than their linguistic reasoning, with low-level perceptual failures identified as a primary bottleneck for high-level multimodal reasoning tasks [ lu2021mathvista , sun2025mathglance , tong2024eyes ] . In mathematical multimodal problem-solving in particular, diagram-perception errors exceed logical reasoning errors, and visual mistakes frequently persist in intermediate reasoning steps even when the final answer is correct [ zhang2025primitive ] .\n",
       "\n",
       "### TF-IDF\n",
       "We introduce ViLoMem , a dual-stream memory framework that separately models visual distraction patterns and logical hallucination errors for multimodal large language models. For memory generation, we employ Qwen3-235B-A22B-Instruct for logical memory (pure language reasoning analysis) and Qwen3-VL-235B-A22B-Instruct for visual memory (image-grounded error attribution). Inspired by human semantic memory systems, ViLoMem coordinates visual and logical memory streams through specialized retrieval strategies and grow-and-refine update mechanisms. We evaluate ViLoMem on three multimodal reasoning benchmarks that are particularly sensitive to cumulative visual–logical errors:(1) Hallucination and real-world robustness , which emphasize language hallucination, visual illusion, and spatial grounding;(2) Multimodal mathematical reasoning , which couples logic reasoning with visual grounding; and(3) Vision-dependent knowledge , which requires expert-level visual understanding across multiple disciplines. In contrast, removing visual memory produces comparable degradation across both benchmarks, indicating that visual distraction errors are pervasive in multimodal reasoning tasks. Our contributions are summarized as follows:We propose ViLoMem , the first framework that explicitly separates visual distraction errors from logical hallucination errors , constructing a dual-stream memory inspired by multimodal semantic memory in the human cognitive system. We thus introduce ViLoMem , a dual-stream memory framework that separately models visual distraction patterns and logical hallucination errors as structured schemas, coordinating them through unified retrieval. ViLoMem employs a question-aware attention mask for visual images and a precise‐positioning – precise‐selection regime for logical retrieval, together with filtering-based memory update strategies to avoid detail erosion, enabling coordinated retrieval between visual cues and logical constraints. The system maintains two memory banks: a logic memory ℳ i L = { m 1 L , m 2 L , … , m | L | L } \\mathcal{M}^{L}_{i}=\\{m^{L}_{1},m^{L}_{2},\\ldots,m^{L}_{|L|}\\} storing textual reasoning guidelines, and a visual memory ℳ i V = { ( m 1 V , I 1 V ) , ( m 2 V , I 2 V ) , … , ( m | V | V , I | V | V ) } \\mathcal{M}^{V}_{i}=\\{(m^{V}_{1},I^{V}_{1}),(m^{V}_{2},I^{V}_{2}),\\ldots,(m^{V}_{|V|},I^{V}_{|V|})\\} storing visual guidelines paired with source images. Unlike conventional single-stage retrieval, our framework employs specialized strategies for each memory type: visual memory uses a two-stage multimodal-to-text pipeline, while logical memory leverages problem analysis to construct enriched queries. The top- k L k^{L} most relevant guidelines are retrieved by applying similarity threshold τ L \\tau^{L} and ranking by similarity score:Finally, the solver generates the answer by conditioning on both the original inputs and the retrieved memories from the visual and logical streams:where Gen denotes the MLLM solver that integrates visual perception, question understanding, and dual-stream memory guidance.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "METHODS = [\n",
    "    \"Lead N\",\n",
    "    \"Lead N by Section\",\n",
    "    \"TF-IDF\",\n",
    "    \"BM25\",\n",
    "    \"Two-pass w/ Gemma 3\",\n",
    "    \"Embedding-based w/ Gemini 2.5\",\n",
    "    \"Embedding-based w/ Llama 3.1\",\n",
    "    \"Embedding-based w/ Longformer Encoder-Decoder\"]\n",
    "\n",
    "\n",
    "def print_random_prediction(results: Dict[str, str]) -> None:\n",
    "    \"\"\"Display a randomly chosen prediction against the abstract.\"\"\"\n",
    "    paper_id = random.choice(list(results.keys()))\n",
    "    paper_results = results[paper_id]\n",
    "\n",
    "    abstract = results[paper_id][\"Abstract\"]\n",
    "    method_1 = random.choice(METHODS)\n",
    "    method_2 = random.choice([m for m in METHODS if m != method_1])\n",
    "    summary_1 = results[paper_id][method_1]\n",
    "    summary_2 = results[paper_id][method_2]\n",
    "\n",
    "    text = f\"\"\"\n",
    "<br>\n",
    "\n",
    "## `{paper_id}`\n",
    "\n",
    "### Abstract\n",
    "{abstract}\n",
    "\n",
    "### {method_1}\n",
    "{summary_1}\n",
    "\n",
    "### {method_2}\n",
    "{summary_2}\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(text))\n",
    "\n",
    "\n",
    "print_random_prediction(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416149a",
   "metadata": {},
   "source": [
    "# Comments on Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc0c4d",
   "metadata": {},
   "source": [
    "### `2511.21471v1`\n",
    "\n",
    "#### Lead N\n",
    "\n",
    "---\n",
    "\n",
    "In daily life, human can effortlessly integrate spatial information from their surroundings, with a capability known as spatial cognition.\n",
    "This ability extends beyond mere object recognition, serving as a cognitive bridge between perceptual inputs and higher-level functions such as reasoning and navigation.\n",
    "With the rapid advancements of large language models (LLMs) [ brown2020language , vicuna2023 , driess2023palm , gilardi2023chatgpt , rohan2023alpaca , touvron2023llama ] , multimodal large language models (MLLMs) have recently emerged as a major step toward general-purpose visual–linguistic intelligence [ bai2023qwen , dai2023instructblip , li2023blip , zhang2023internlm , brooks2023instructpix2pix , black2023training , li2023llavamed , zhu2023minigpt , zhang2023gpt4roi , liu2023llava , liu2023improvedllava , ye2023mplug , He2024malmm , Zhang2024groundhog , Chen2023internvl , Yuan2023osprey , Dong2024dreamllm , Cha2023honeybee , qwq32b ] .\n",
    "By jointly aligning visual and textual modalities within a shared semantic space, MLLMs have moved beyond abstract visual representations, integrating linguistic context to interpret scenes in a more structured and human-like manner.\n",
    "Recent advances show that MLLMs have exhibited spatial reasoning abilities [ Cai2025spatial , Cheng2024spatial , Chen2024spatial , Han2025video , Li2024top , Yamada2024evaluating , zhu2024llava , kumar2025does , yang2023set , tang2024sparkle , wu2025spatial , li2025llava , liu2025oryx ] , and several benchmarks have been introduced to quantify these capabilities [ azuma2022scanqa , ma2023sqa , yang2025thinking , li2025spatial , yin2025spatial , tong2024cambrian , zhang2025from , li2025view ] .\n",
    "\n",
    "---\n",
    "\n",
    "- Unlike _BM25_ or _TF-IDF_, this method results in a coherent paragraph.\n",
    "- The idea is suddenly interrupted at the end of the paragraph, the point being built never gets to land.\n",
    "- It is not a summary, the paragraph paints the landscape in which this research is relevant but entirely misses any meaningful presentation of what is unique for this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d1f7a",
   "metadata": {},
   "source": [
    "### `2511.21460v1`\n",
    "\n",
    "#### Embedding-based w/ Llama 3.1\n",
    "\n",
    "---\n",
    "\n",
    "The text discusses a study on enhancing the safety awareness of artificial intelligence (AI) agents in interactive environments, particularly in household tasks. The researchers propose a few key contributions:\n",
    "\n",
    "1. MADRA (Multi-Agent Debate Risk Assessment): a method that uses a critical evaluator to drive iterative refinement and consensus voting, reducing single-agent bias and false rejections.\n",
    "2. Task Hierarchical Planning Framework: a framework that integrates safety, memory, planning, and reflection modules to achieve self-evolution.\n",
    "3. SafeAware-VH Dataset: a dataset containing safety and unsafe instructions to test the safety awareness of agents in VirtualHome.\n",
    "\n",
    "The study compares the performance of different safety enhancement methods, including:\n",
    "\n",
    "1. Single-Agent Safety CoT: a method that uses a Chain-of-Thought (CoT) safety reminder technique to enable the model to function as a safety detector.\n",
    "2. MADRA vs Safety CoT: a comparison of the two methods shows that MADRA achieves a higher rejection rate for unsafe tasks while maintaining a lower rejection rate for safe tasks.\n",
    "\n",
    "The researchers also evaluate the performance of their methods in two embodied environments: VirtualHome and AI2-THOR. The results show that:\n",
    "\n",
    "1. MADRA achieves a high rejection rate for unsafe tasks: up to 90% in some cases.\n",
    "2. MADRA maintains a low rejection rate for safe tasks: less than 10% in some cases.\n",
    "3. The critical agent's capability plays a crucial role in MADRA: using a more powerful model as the critical agent yields optimal results.\n",
    "\n",
    "Overall, the study demonstrates the effectiveness of MADRA in enhancing the safety awareness of AI agents in interactive environments and highlights the importance of using a multi-agent approach to mitigate the problem of over-rejection in single-agent risk assessment.\n",
    "\n",
    "---\n",
    "\n",
    "- Very structured, which results in a clear and easy to read summary.\n",
    "- The summary seems to represent the information from the paper accurately, I cannot find any factual errors in the output.\n",
    "- The summary follows closely the information presented on the abstract, it presents all of the information the authors deemed relevant.\n",
    "\n",
    "#### Abstract\n",
    "\n",
    "---\n",
    "\n",
    "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.\n",
    "\n",
    "---\n",
    "\n",
    "#### Embedding-based w/ Longformer Encoder-Decoder\n",
    "\n",
    "---\n",
    "\n",
    "The authors of this paper propose a new method for evaluating the safety risk awareness of LLM agents in VirtualHome by feeding a safety awareness prompt directly into the LLM . They use a chain-of-Thought technique to provide a safety reminder and perform risk assessments through step-by-step reasoning. The main contributions of this study are summarized as follows: We propose MADRA: a multi-agent debate framework where a critical evaluator drives iterative refinement and consensus voting, curbing single-llm bias and cutting false rejections.We have designed a task hierarchical planning framework based on cognitive collaboration that integrates safety, memory, planning, and reflection to achieve self-evolution. Our method is inherently training-free, demonstrating universality and flexibility as a plug-and-play module that can be easily applied across different scenarios and domains.2. MADRA vs Safety CoT In order to further explore the safety awareness of agents in virtual home, the authors develop a dataset called SafeAware-VH. This dataset contains both safe and unsafe instructions. Their safety enhancement techniques are shown in Table 2. Overall, the results suggest that different safety enhancement methods are effective in improving various language models, measured by their rejection rates for safe and unsafe content.3. Critical Agent role in Multi-Agent Debate Risk Assessment MADRA provides a critical role in detecting unsafe tasks and over-rejects safe instructions.4. Over-rejection refers to the tendency for safe instructions to be incorrectly flagged as unsafe.5. We propose a risk assessment method based on multi-labor debate (MADRA) and apply it as a universal safety module to any task.6. Various approaches to evaluating agent safety are used in this study. For example, using a less capable model such as Llama3 as the Critical Agent leads to a significant increase in the number of unsafe tasks, while for more powerful models like GPT-3/GPT-4o the Critical Assessor yields near-perfect safe task success rates.7. Critical Model vs Safety Checker MADRA uses a Chain-of Thought technique to serve as a safety detector and performs risk assessment step by step reasoning.8. Safety-CoT pushes the unsafe task rejection rate to 80%–93% , a 20-56 percentage point gain over the raw model, but achieves respectable success rates on safe tasks up to 70.3% in AI2-THOR.9. A key finding is the critical role of the Critical Antagonist's capability within the Multi-Agile Decisions Risk Assessment module.10. R-Judge is a good for serving as a starting point for assessing safety risks in an interactive environment, but lacks household tasks. Therefore, they establish a dataset named SafeAaware-Vh, which contains safety and unsafe instruction.11. MADE vs Safety Countermeasures MADRA + CoT improves all eight language models tested in this field by feeding prompts directly into LLLM and feeding them with chains of thought. The experimental results presented in table 2 demonstrate the effectiveness of different safety enhancing methods across various languages models, demonstrated by their failure rates for safely and unsafe content.12. Single-agent Safety CoT yields the highest absolute rejection rate of unsafe prompts, but simultaneously over-rebels safe instructions alike.13. While heaps of effort is required to train neural nets specifically for security detection, free-training and directly using LLMs for single-agent security detection can easily lead to the problem of overrojection, making it difficult to effectively enhance safety awareness.14. To address the issue of over-overrejection by a single LLM agent, we propose a threat assessment method Based on Multagent Debate Method MADRA, wherein a \"risk assessment method\" driven by consensus voting is used to weed out bias and false rejectings.15. The method does not require training since it is machine learning-free.16. The overall success rate of our method is very high, indicating that the performance of the planning system is excellent and it can execute the actions of security task instructions as successfully as possible.17. Determining the appropriateness of items in a non-interactive environment requires large computational costs.18. Free-training also requires huge computational costs because of its reliance on artificial intelligence.19. Reversal refers to correctly refusing unsafe tasks.20. Overtaking refers to correcting incorrect errors or flagging unsafe tasks as unsafe instead of calling them unsafe.21. Overeating refers to correcting mistakes or misinterpreting instructions incorrectly.22. Overworking refers to fixing problems caused by mistake detection.23. Overturning refers to preventing people from entering dangerous situations.24. Dangerous house tasks are relatively scarce due to the existence of no datasets for dangerous home tasks. As such, they created Safe Aware-VHD.\n",
    "\n",
    "---\n",
    "\n",
    "- There is so much text that should be cut off, but nothing more than this reference to a table we don't get to see: _Their safety enhancement techniques are shown in Table 2._\n",
    "- The formatting is poor, there are listed elements lost in the middle of the _paragraph_ that start counting from the second item. Even more, the numbers unnecessarily continue all the way til the end.\n",
    "- It compromises the results by mixing different point from the paper in a way that not only misses the relevant point, but makes it factually incorrect: _8. Safety-CoT pushes the unsafe task rejection rate to 80%–93% , a 20-56 percentage point gain over the raw model, but achieves respectable success rates on safe tasks up to 70.3% in AI2-THOR._ In the example sentence, the main comparison between Safety-CoT and MADRAS is the missing safety rejection rate. Furthermore, the mention of AI2-THOR does not belong here and confuses the methodology in the paper.\n",
    "- Another example: _MADE vs Safety Countermeasures MADRA + CoT_. There is no reference to any object, algorithm, or anything called _MADE_ in the paper. The whole construction makes no sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bf948",
   "metadata": {},
   "source": [
    "### `2511.21570v1`\n",
    "\n",
    "#### BM25 Summary\n",
    "\n",
    "---\n",
    "\n",
    "This is further compounded by AI’s need for vast amounts of data and high energy demands, highlighting an important challenge for responsible foresight: developing ethical, sustainable, and context-sensitive AI systems that fully support human intelligence in future decision-making.\n",
    "Nevertheless, AI’s ability to analyze extensive datasets, model complex systems, and simulate alternative futures presents a promising pathway to support responsible foresight, enabling policymakers to gain insights that extend beyond what human cognition alone can offer.\n",
    "Responsible foresight [ uruena2021foresight ] will thus require technical tools like AI to close the decision loop, but also a deep understanding of the interconnected social, economic, and environmental systems that influence future outcomes, alongside a commitment to ethical and sustainable decision-making.\n",
    "Each approach contributes unique insights: Superforecasting harnesses the wisdom of skilled forecasters to generate highly accurate predictions; prediction markets leverage collective knowledge; world simulation creates virtual environments to capture the complexity of our social, economic and environmental systems; and simulation intelligence uses AI to design control strategies within simulation worlds providing meaningful insights on efficient and resilient pathways for the future.\n",
    "Incorporating responsible foresight into policymaking increasingly will involve the use of algorithms to analyze complex data, predict potential outcomes, and offer insight into a range of possible futures.\n",
    "These complementary methods—Superforecasting, prediction markets, world simulation, simulation intelligence, scenario-building, participatory futures, futures literacy and hybrid intelligence—form a powerful toolkit for responsible computational foresight.\n",
    "To realize this potential, AI in responsible foresight must be conceived as an assistive tool—a cognitive exoskeleton that enables policymakers to navigate complexity, envision a range of desirable futures, and critically assess the impacts of various decisions.\n",
    "\n",
    "---\n",
    "\n",
    "- Although the selected sentences are meaningful, the whole summary is disjointed and full of jumps. For example, the first sentence of the BM25 summary starts as a response to a missing sentence.\n",
    "- The text keeps referring to _responsible foresight_ without ever defining the term, which in the abstract is done in the first sentence. There are other unclaryfied references like _superforecasting_ and _participatory futures_.\n",
    "- Some sentences are ridiculously long and overcomplicated, while we can blame the author of the paper this method can not produce anything beyond what is already offered by the text.\n",
    "- As a note, the _BM25_ methods was applied with a variable number of sentences $n$ equal to the number of sentences in the abstract.\n",
    "\n",
    "#### Embedding-based w/ Gemini 2.5 Summary\n",
    "\n",
    "---\n",
    "\n",
    "This text introduces \"responsible computational foresight,\" a field that leverages human-centric AI to empower policymakers.\n",
    "The central argument is that AI should augment human intelligence, not replace it, by helping explore a broad spectrum of future possibilities—including unexpected, unintended, and desirable ones—beyond mere precise predictions or historical extrapolations.\n",
    "\n",
    "This approach integrates various computational tools like forecasting, complex systems simulation, AI-assisted scenario-building, and simulation intelligence, alongside human elements such as participatory futures and futures literacy.\n",
    "These tools, combined with a deep understanding of interconnected social, economic, and environmental systems and a commitment to ethical decision-making, strengthen our ability to anticipate and prepare for a range of outcomes.\n",
    "\n",
    "Ultimately, responsible computational foresight aims to enhance ethical decision-making, build resilience against uncertainty, and enable society to navigate complexity with informed, adaptive, and inclusive strategies, thereby shaping sustainable and equitable paths for future generations.\n",
    "\n",
    "---\n",
    "\n",
    "- The LLM immediately defines the unique concept of the paper, _responsible foresight_.\n",
    "- No long or overcomplicated sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf805881",
   "metadata": {},
   "source": [
    "by [Manuel Velarde](mailto:manuel@velarde.me)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
