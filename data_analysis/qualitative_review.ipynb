{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29e0165",
   "metadata": {},
   "source": [
    "# Qualitative Review\n",
    "\n",
    "### Findings\n",
    "\n",
    "| Method | Advantages | Issues | Comments |\n",
    "|--------|------------|--------|----------|\n",
    "| _Lead N_ | Results in a coherent paragraph. Facts are not changed by the model. | Not a summary for scientific papers. Misses the point of the paper. |  |\n",
    "| _Lead N by Section_ | Reflects structure of document. Sentences have some continuity. Facts are not changed by the model. | Does not summarize the papers. Similar to _Lead N_, mostly misses the point of the papers. |  |\n",
    "| _TF-IDF_ | Sometimes captures relevant information. Facts are not changed by the model. | Often weird or extremely long sentences that are irrelevant get the highest scores. Disjointed paragraph, even when sentences belong together a sensible order is not guarranteed. |  |\n",
    "| _BM25_ | Captures mostly relevant information. Facts are not changed by the model. | Results in disjointed paragraph. |  |\n",
    "| _Two-pass w/ Gemma 3_ | Less verbose than other LLM-based models. | Facts can be changed by the model. Often returns an over-simplified summary that misses much of why or how the research is done. The summaries include excessive structural or sectional detail (introductions or background explanations) while overlooking the core contributions explicitly stated in the abstract.|  |\n",
    "| _Embedding-based w/ Gemini 2.5_ | Produces a very clear human-understandable structure. | Facts can be changed by the model. | Tends to organize information in lists. |\n",
    "| _Embedding-based w/ Llama 3.1_ |  | Facts can be changed by the model. |  |\n",
    "| _Embedding-based w/ Longformer Encoder-Decoder_ | The most verbose model. | Facts are often changed by the model. | Outputs walls of text. |\n",
    "\n",
    "### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65ecb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import random\n",
    "\n",
    "from typing import Dict, Any, Union, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cfb2d",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d36ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES_AND_STATS = os.path.join(\"..\", \"baselines\", \"results\", \"tfidf_bm25_lead_results.json\")\n",
    "TWO_PASS_LLM_SUMMARY = os.path.join(\"..\", \"baselines\", \"results\", \"llm_summarized_results.json\")\n",
    "LLM_EMBEDDING_SUMMARY = os.path.join(\"..\", \"baselines\", \"results\", \"llm_embeddings_results.json\")\n",
    "\n",
    "\n",
    "def load_json(filepath: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n",
    "    \"\"\"Return JSON data.\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "results = {}\n",
    "rules_n_stats = load_json(RULES_AND_STATS)\n",
    "for paper_id, data in rules_n_stats.items():\n",
    "    results[paper_id] = {}\n",
    "\n",
    "    # Article's abstract\n",
    "    results[paper_id][\"Abstract\"] = ' '.join(data[\"abstract\"][\"plain\"])\n",
    "\n",
    "    # 1. Lead N prediction\n",
    "    results[paper_id][\"Lead N\"] = data[\"lead_n\"]\n",
    "\n",
    "    # 2. Lead N by Section prediction\n",
    "    results[paper_id][\"Lead N by Section\"] = data[\"lead_n_by_section\"]\n",
    "\n",
    "    # 3. TF-IDF prediction\n",
    "    n = len(data[\"abstract\"][\"plain\"])\n",
    "    tfidf_indexes = data[\"rank\"][\"TF-IDF\"][:n]\n",
    "    tfidf_sentences = [data[\"sentences\"][\"plain\"][i] for i in tfidf_indexes]\n",
    "    results[paper_id][\"TF-IDF\"] = ' '.join(tfidf_sentences)\n",
    "\n",
    "    # 4. BM25 prediction\n",
    "    n = len(data[\"abstract\"][\"plain\"])\n",
    "    tfidf_indexes = data[\"rank\"][\"BM25\"][:n]\n",
    "    tfidf_sentences = [data[\"sentences\"][\"plain\"][i] for i in tfidf_indexes]\n",
    "    results[paper_id][\"BM25\"] = ' '.join(tfidf_sentences)\n",
    "\n",
    "\n",
    "two_pass_llm = load_json(TWO_PASS_LLM_SUMMARY)\n",
    "for data in two_pass_llm:\n",
    "    paper_id = data[\"paper_id\"]\n",
    "\n",
    "    # 5. Two-pass LLM prediction\n",
    "    results[paper_id][\"Two-pass w/ Gemma 3\"] = data[\"gemma3:1b\"][\"summary\"]\\\n",
    "\n",
    "\n",
    "llm_embeddings = load_json(LLM_EMBEDDING_SUMMARY)\n",
    "for data in llm_embeddings:\n",
    "    paper_id = data[\"paper_id\"]\n",
    "\n",
    "    # 6. Embedding-based LLM prediction\n",
    "    results[paper_id][\"Embedding-based w/ Gemini 2.5\"] = data[\"gemini-2.5-flash\"][\"summary\"]\n",
    "    results[paper_id][\"Embedding-based w/ Llama 3.1\"] = data[\"llama-3.1-8b-instant\"][\"summary\"]\n",
    "    results[paper_id][\"Embedding-based w/ Longformer Encoder-Decoder\"] = data[\"led\"][\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f154121",
   "metadata": {},
   "source": [
    "### Review Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac4e1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<br>\n",
       "\n",
       "## [2511.21471v1](https://arxiv.org/pdf/2511.21471v1)\n",
       "\n",
       "### Abstract\n",
       "Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability–oriented metric that reliably assesses a model’s overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems. Our codes are available at https://github.com/XPR2004/SpatialBench.\n",
       "\n",
       "### Lead N\n",
       "In daily life, human can effortlessly integrate spatial information from their surroundings, with a capability known as spatial cognition. This ability extends beyond mere object recognition, serving as a cognitive bridge between perceptual inputs and higher-level functions such as reasoning and navigation. With the rapid advancements of large language models (LLMs) [ brown2020language , vicuna2023 , driess2023palm , gilardi2023chatgpt , rohan2023alpaca , touvron2023llama ] , multimodal large language models (MLLMs) have recently emerged as a major step toward general-purpose visual–linguistic intelligence [ bai2023qwen , dai2023instructblip , li2023blip , zhang2023internlm , brooks2023instructpix2pix , black2023training , li2023llavamed , zhu2023minigpt , zhang2023gpt4roi , liu2023llava , liu2023improvedllava , ye2023mplug , He2024malmm , Zhang2024groundhog , Chen2023internvl , Yuan2023osprey , Dong2024dreamllm , Cha2023honeybee , qwq32b ] . By jointly aligning visual and textual modalities within a shared semantic space, MLLMs have moved beyond abstract visual representations, integrating linguistic context to interpret scenes in a more structured and human-like manner. Recent advances show that MLLMs have exhibited spatial reasoning abilities [ Cai2025spatial , Cheng2024spatial , Chen2024spatial , Han2025video , Li2024top , Yamada2024evaluating , zhu2024llava , kumar2025does , yang2023set , tang2024sparkle , wu2025spatial , li2025llava , liu2025oryx ] , and several benchmarks have been introduced to quantify these capabilities [ azuma2022scanqa , ma2023sqa , yang2025thinking , li2025spatial , yin2025spatial , tong2024cambrian , zhang2025from , li2025view ] .\n",
       "\n",
       "### Embedding-based w/ Llama 3.1\n",
       "The text discusses the development of a new benchmark called SpatialBench for evaluating the spatial intelligence of Multimodal Learning Models (MLLMs). SpatialBench is designed to assess MLLMs' ability to understand spatial relationships and navigate through environments. The benchmark consists of 15 question types, each mapped to one of five cognitive levels:\n",
       "\n",
       "1. Observation (L1): basic object recognition and distance calculation\n",
       "2. Topology and relation (L2): understanding relative positions and directions\n",
       "3. Symbolic reasoning (L3): multi-hop spatial reasoning and landmark-based localization\n",
       "4. Causality (L4): understanding spatial cause-and-effect relationships\n",
       "5. Planning (L5): route planning and visual-based commands\n",
       "\n",
       "The SpatialBench dataset includes 1,347 question-answer pairs sourced from 50 videos captured from an egocentric perspective, covering various indoor and outdoor settings. The authors conducted experiments using state-of-the-art MLLMs and found that while they excel in perceptual and relational reasoning tasks, they struggle with high-level tasks such as symbolic reasoning, causality, and planning.\n",
       "\n",
       "The contributions of this work are:\n",
       "\n",
       "1. A comprehensive and cognitively grounded framework for assessing spatial intelligence in MLLMs\n",
       "2. The development of SpatialBench, a large-scale benchmark for evaluating MLLMs' spatial cognition\n",
       "3. A high-level ability-driven evaluation metric for assessing spatial cognition in MLLMs\n",
       "\n",
       "Overall, SpatialBench provides a standardized and scalable evaluation framework for MLLMs' spatial intelligence, enabling researchers to better understand their strengths and limitations in navigating and understanding complex environments.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "METHODS = [\n",
    "    \"Lead N\",\n",
    "    \"Lead N by Section\",\n",
    "    \"TF-IDF\",\n",
    "    \"BM25\",\n",
    "    \"Two-pass w/ Gemma 3\",\n",
    "    \"Embedding-based w/ Gemini 2.5\",\n",
    "    \"Embedding-based w/ Llama 3.1\",\n",
    "    \"Embedding-based w/ Longformer Encoder-Decoder\"]\n",
    "\n",
    "\n",
    "def print_random_prediction(results: Dict[str, str]) -> None:\n",
    "    \"\"\"Display a randomly chosen prediction against the abstract.\"\"\"\n",
    "    paper_id = random.choice(list(results.keys()))\n",
    "    paper_results = results[paper_id]\n",
    "\n",
    "    abstract = results[paper_id][\"Abstract\"]\n",
    "    method_1 = random.choice(METHODS)\n",
    "    method_2 = random.choice([m for m in METHODS if m != method_1])\n",
    "    summary_1 = results[paper_id][method_1]\n",
    "    summary_2 = results[paper_id][method_2]\n",
    "\n",
    "    text = f\"\"\"\n",
    "<br>\n",
    "\n",
    "## [{paper_id}](https://arxiv.org/pdf/{paper_id})\n",
    "\n",
    "### Abstract\n",
    "{abstract}\n",
    "\n",
    "### {method_1}\n",
    "{summary_1}\n",
    "\n",
    "### {method_2}\n",
    "{summary_2}\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(text))\n",
    "\n",
    "\n",
    "print_random_prediction(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416149a",
   "metadata": {},
   "source": [
    "# Comments on Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac710f",
   "metadata": {},
   "source": [
    "### `2511.21398v1`\n",
    "\n",
    "#### Lead N\n",
    "\n",
    "---\n",
    "\n",
    "Web automation enables the completion of high-level tasks, such as booking flights or shopping online, through intelligent agents that mimic human interaction on webpages. These agents achieve this by interpreting high-level tasks, breaking them down into low-level sub-tasks, and seamlessly interacting with web elements. Recently, large language models (LLMs) have demonstrated impressive capabilities in autonomous web navigation through their strong reasoning and decision-making abilities ( yao2022webshop ; deng2023mind2web ) . Current web agents approaches fall into three main categories: 1) Textual HTML/DOM-based ( yao2022webshop ; song2024beyond ) , 2) Visual Screenshot-based ( lin2024showui ; cheng2024seeclick ) , and 3) Multi-modal-based ( he2024webvoyager ; zheng2024gpt ) . Visual screenshots provide an intuitive, human-like understanding of webpage state, making them effective for reasoning about low-level sub-tasks.\n",
    "\n",
    "---\n",
    "\n",
    "- If not given access to the abstract, it just captures the general picture from the introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc0c4d",
   "metadata": {},
   "source": [
    "### `2511.21471v1`\n",
    "\n",
    "#### Lead N\n",
    "\n",
    "---\n",
    "\n",
    "In daily life, human can effortlessly integrate spatial information from their surroundings, with a capability known as spatial cognition.\n",
    "This ability extends beyond mere object recognition, serving as a cognitive bridge between perceptual inputs and higher-level functions such as reasoning and navigation.\n",
    "With the rapid advancements of large language models (LLMs) [ brown2020language , vicuna2023 , driess2023palm , gilardi2023chatgpt , rohan2023alpaca , touvron2023llama ] , multimodal large language models (MLLMs) have recently emerged as a major step toward general-purpose visual–linguistic intelligence [ bai2023qwen , dai2023instructblip , li2023blip , zhang2023internlm , brooks2023instructpix2pix , black2023training , li2023llavamed , zhu2023minigpt , zhang2023gpt4roi , liu2023llava , liu2023improvedllava , ye2023mplug , He2024malmm , Zhang2024groundhog , Chen2023internvl , Yuan2023osprey , Dong2024dreamllm , Cha2023honeybee , qwq32b ] .\n",
    "By jointly aligning visual and textual modalities within a shared semantic space, MLLMs have moved beyond abstract visual representations, integrating linguistic context to interpret scenes in a more structured and human-like manner.\n",
    "Recent advances show that MLLMs have exhibited spatial reasoning abilities [ Cai2025spatial , Cheng2024spatial , Chen2024spatial , Han2025video , Li2024top , Yamada2024evaluating , zhu2024llava , kumar2025does , yang2023set , tang2024sparkle , wu2025spatial , li2025llava , liu2025oryx ] , and several benchmarks have been introduced to quantify these capabilities [ azuma2022scanqa , ma2023sqa , yang2025thinking , li2025spatial , yin2025spatial , tong2024cambrian , zhang2025from , li2025view ] .\n",
    "\n",
    "---\n",
    "\n",
    "- Unlike _BM25_ or _TF-IDF_, this method results in a coherent paragraph.\n",
    "- The idea is suddenly interrupted at the end of the paragraph, the point being built never gets to land.\n",
    "- It is not a summary, the paragraph paints the landscape in which this research is relevant but entirely misses any meaningful presentation of what is unique for this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d1f7a",
   "metadata": {},
   "source": [
    "### `2511.21460v1`\n",
    "\n",
    "#### Embedding-based w/ Llama 3.1\n",
    "\n",
    "---\n",
    "\n",
    "The text discusses a study on enhancing the safety awareness of artificial intelligence (AI) agents in interactive environments, particularly in household tasks. The researchers propose a few key contributions:\n",
    "\n",
    "1. MADRA (Multi-Agent Debate Risk Assessment): a method that uses a critical evaluator to drive iterative refinement and consensus voting, reducing single-agent bias and false rejections.\n",
    "2. Task Hierarchical Planning Framework: a framework that integrates safety, memory, planning, and reflection modules to achieve self-evolution.\n",
    "3. SafeAware-VH Dataset: a dataset containing safety and unsafe instructions to test the safety awareness of agents in VirtualHome.\n",
    "\n",
    "The study compares the performance of different safety enhancement methods, including:\n",
    "\n",
    "1. Single-Agent Safety CoT: a method that uses a Chain-of-Thought (CoT) safety reminder technique to enable the model to function as a safety detector.\n",
    "2. MADRA vs Safety CoT: a comparison of the two methods shows that MADRA achieves a higher rejection rate for unsafe tasks while maintaining a lower rejection rate for safe tasks.\n",
    "\n",
    "The researchers also evaluate the performance of their methods in two embodied environments: VirtualHome and AI2-THOR. The results show that:\n",
    "\n",
    "1. MADRA achieves a high rejection rate for unsafe tasks: up to 90% in some cases.\n",
    "2. MADRA maintains a low rejection rate for safe tasks: less than 10% in some cases.\n",
    "3. The critical agent's capability plays a crucial role in MADRA: using a more powerful model as the critical agent yields optimal results.\n",
    "\n",
    "Overall, the study demonstrates the effectiveness of MADRA in enhancing the safety awareness of AI agents in interactive environments and highlights the importance of using a multi-agent approach to mitigate the problem of over-rejection in single-agent risk assessment.\n",
    "\n",
    "---\n",
    "\n",
    "- Very structured, which results in a clear and easy to read summary.\n",
    "- The summary seems to represent the information from the paper accurately, I cannot find any factual errors in the output.\n",
    "- The summary follows closely the information presented on the abstract, it presents all of the information the authors deemed relevant.\n",
    "\n",
    "#### Abstract\n",
    "\n",
    "---\n",
    "\n",
    "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.\n",
    "\n",
    "---\n",
    "\n",
    "#### Embedding-based w/ Longformer Encoder-Decoder\n",
    "\n",
    "---\n",
    "\n",
    "The authors of this paper propose a new method for evaluating the safety risk awareness of LLM agents in VirtualHome by feeding a safety awareness prompt directly into the LLM . They use a chain-of-Thought technique to provide a safety reminder and perform risk assessments through step-by-step reasoning. The main contributions of this study are summarized as follows: We propose MADRA: a multi-agent debate framework where a critical evaluator drives iterative refinement and consensus voting, curbing single-llm bias and cutting false rejections.We have designed a task hierarchical planning framework based on cognitive collaboration that integrates safety, memory, planning, and reflection to achieve self-evolution. Our method is inherently training-free, demonstrating universality and flexibility as a plug-and-play module that can be easily applied across different scenarios and domains.2. MADRA vs Safety CoT In order to further explore the safety awareness of agents in virtual home, the authors develop a dataset called SafeAware-VH. This dataset contains both safe and unsafe instructions. Their safety enhancement techniques are shown in Table 2. Overall, the results suggest that different safety enhancement methods are effective in improving various language models, measured by their rejection rates for safe and unsafe content.3. Critical Agent role in Multi-Agent Debate Risk Assessment MADRA provides a critical role in detecting unsafe tasks and over-rejects safe instructions.4. Over-rejection refers to the tendency for safe instructions to be incorrectly flagged as unsafe.5. We propose a risk assessment method based on multi-labor debate (MADRA) and apply it as a universal safety module to any task.6. Various approaches to evaluating agent safety are used in this study. For example, using a less capable model such as Llama3 as the Critical Agent leads to a significant increase in the number of unsafe tasks, while for more powerful models like GPT-3/GPT-4o the Critical Assessor yields near-perfect safe task success rates.7. Critical Model vs Safety Checker MADRA uses a Chain-of Thought technique to serve as a safety detector and performs risk assessment step by step reasoning.8. Safety-CoT pushes the unsafe task rejection rate to 80%–93% , a 20-56 percentage point gain over the raw model, but achieves respectable success rates on safe tasks up to 70.3% in AI2-THOR.9. A key finding is the critical role of the Critical Antagonist's capability within the Multi-Agile Decisions Risk Assessment module.10. R-Judge is a good for serving as a starting point for assessing safety risks in an interactive environment, but lacks household tasks. Therefore, they establish a dataset named SafeAaware-Vh, which contains safety and unsafe instruction.11. MADE vs Safety Countermeasures MADRA + CoT improves all eight language models tested in this field by feeding prompts directly into LLLM and feeding them with chains of thought. The experimental results presented in table 2 demonstrate the effectiveness of different safety enhancing methods across various languages models, demonstrated by their failure rates for safely and unsafe content.12. Single-agent Safety CoT yields the highest absolute rejection rate of unsafe prompts, but simultaneously over-rebels safe instructions alike.13. While heaps of effort is required to train neural nets specifically for security detection, free-training and directly using LLMs for single-agent security detection can easily lead to the problem of overrojection, making it difficult to effectively enhance safety awareness.14. To address the issue of over-overrejection by a single LLM agent, we propose a threat assessment method Based on Multagent Debate Method MADRA, wherein a \"risk assessment method\" driven by consensus voting is used to weed out bias and false rejectings.15. The method does not require training since it is machine learning-free.16. The overall success rate of our method is very high, indicating that the performance of the planning system is excellent and it can execute the actions of security task instructions as successfully as possible.17. Determining the appropriateness of items in a non-interactive environment requires large computational costs.18. Free-training also requires huge computational costs because of its reliance on artificial intelligence.19. Reversal refers to correctly refusing unsafe tasks.20. Overtaking refers to correcting incorrect errors or flagging unsafe tasks as unsafe instead of calling them unsafe.21. Overeating refers to correcting mistakes or misinterpreting instructions incorrectly.22. Overworking refers to fixing problems caused by mistake detection.23. Overturning refers to preventing people from entering dangerous situations.24. Dangerous house tasks are relatively scarce due to the existence of no datasets for dangerous home tasks. As such, they created Safe Aware-VHD.\n",
    "\n",
    "---\n",
    "\n",
    "- There is so much text that should be cut off, but nothing more than this reference to a table we don't get to see: _Their safety enhancement techniques are shown in Table 2._\n",
    "- The formatting is poor, there are listed elements lost in the middle of the _paragraph_ that start counting from the second item. Even more, the numbers unnecessarily continue all the way til the end.\n",
    "- It compromises the results by mixing different point from the paper in a way that not only misses the relevant point, but makes it factually incorrect: _8. Safety-CoT pushes the unsafe task rejection rate to 80%–93% , a 20-56 percentage point gain over the raw model, but achieves respectable success rates on safe tasks up to 70.3% in AI2-THOR._ In the example sentence, the main comparison between Safety-CoT and MADRAS is the missing safety rejection rate. Furthermore, the mention of AI2-THOR does not belong here and confuses the methodology in the paper.\n",
    "- Another example: _MADE vs Safety Countermeasures MADRA + CoT_. There is no reference to any object, algorithm, or anything called _MADE_ in the paper. The whole construction makes no sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bf948",
   "metadata": {},
   "source": [
    "### `2511.21570v1`\n",
    "\n",
    "#### BM25 Summary\n",
    "\n",
    "---\n",
    "\n",
    "This is further compounded by AI’s need for vast amounts of data and high energy demands, highlighting an important challenge for responsible foresight: developing ethical, sustainable, and context-sensitive AI systems that fully support human intelligence in future decision-making.\n",
    "Nevertheless, AI’s ability to analyze extensive datasets, model complex systems, and simulate alternative futures presents a promising pathway to support responsible foresight, enabling policymakers to gain insights that extend beyond what human cognition alone can offer.\n",
    "Responsible foresight [ uruena2021foresight ] will thus require technical tools like AI to close the decision loop, but also a deep understanding of the interconnected social, economic, and environmental systems that influence future outcomes, alongside a commitment to ethical and sustainable decision-making.\n",
    "Each approach contributes unique insights: Superforecasting harnesses the wisdom of skilled forecasters to generate highly accurate predictions; prediction markets leverage collective knowledge; world simulation creates virtual environments to capture the complexity of our social, economic and environmental systems; and simulation intelligence uses AI to design control strategies within simulation worlds providing meaningful insights on efficient and resilient pathways for the future.\n",
    "Incorporating responsible foresight into policymaking increasingly will involve the use of algorithms to analyze complex data, predict potential outcomes, and offer insight into a range of possible futures.\n",
    "These complementary methods—Superforecasting, prediction markets, world simulation, simulation intelligence, scenario-building, participatory futures, futures literacy and hybrid intelligence—form a powerful toolkit for responsible computational foresight.\n",
    "To realize this potential, AI in responsible foresight must be conceived as an assistive tool—a cognitive exoskeleton that enables policymakers to navigate complexity, envision a range of desirable futures, and critically assess the impacts of various decisions.\n",
    "\n",
    "---\n",
    "\n",
    "- Although the selected sentences are meaningful, the whole summary is disjointed and full of jumps. For example, the first sentence of the BM25 summary starts as a response to a missing sentence.\n",
    "- The text keeps referring to _responsible foresight_ without ever defining the term, which in the abstract is done in the first sentence. There are other unclaryfied references like _superforecasting_ and _participatory futures_.\n",
    "- Some sentences are ridiculously long and overcomplicated, while we can blame the author of the paper this method can not produce anything beyond what is already offered by the text.\n",
    "- As a note, the _BM25_ methods was applied with a variable number of sentences $n$ equal to the number of sentences in the abstract.\n",
    "\n",
    "#### Embedding-based w/ Gemini 2.5 Summary\n",
    "\n",
    "---\n",
    "\n",
    "This text introduces \"responsible computational foresight,\" a field that leverages human-centric AI to empower policymakers.\n",
    "The central argument is that AI should augment human intelligence, not replace it, by helping explore a broad spectrum of future possibilities—including unexpected, unintended, and desirable ones—beyond mere precise predictions or historical extrapolations.\n",
    "\n",
    "This approach integrates various computational tools like forecasting, complex systems simulation, AI-assisted scenario-building, and simulation intelligence, alongside human elements such as participatory futures and futures literacy.\n",
    "These tools, combined with a deep understanding of interconnected social, economic, and environmental systems and a commitment to ethical decision-making, strengthen our ability to anticipate and prepare for a range of outcomes.\n",
    "\n",
    "Ultimately, responsible computational foresight aims to enhance ethical decision-making, build resilience against uncertainty, and enable society to navigate complexity with informed, adaptive, and inclusive strategies, thereby shaping sustainable and equitable paths for future generations.\n",
    "\n",
    "---\n",
    "\n",
    "- The LLM immediately defines the unique concept of the paper, _responsible foresight_.\n",
    "- No long or overcomplicated sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
