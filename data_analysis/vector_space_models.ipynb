{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb53c8c4",
   "metadata": {},
   "source": [
    "# Vector Space Models\n",
    "\n",
    "### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b578fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: rank_bm25 in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (0.2.2)\n",
      "Requirement already satisfied: click in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from scikit-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/shared/TU Wien/Natural Language Processing/project/.venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/guaya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk scikit-learn rank_bm25\n",
    "\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# %pip install rank_bm25 scikit-learn\n",
    "\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import nltk\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5006017",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a13292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"../data/processed/\"\n",
    "\n",
    "\n",
    "def load_json_data(directory: str=DIRECTORY) -> Dict[str, List[Any]]:\n",
    "    \"\"\"Load all JSON files from given directory.\"\"\"\n",
    "    json_data = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(\".json\"):\n",
    "\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                json_data[filename[:-5]] = {\"processed\": json.load(f)}\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92d077",
   "metadata": {},
   "source": [
    "Load processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fa488f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2511.21678v1\n",
      "2511.21636v1\n",
      "2511.21591v1\n",
      "2511.21570v1\n",
      "2511.21569v1\n",
      "2511.21522v1\n",
      "2511.21471v1\n",
      "2511.21460v1\n",
      "2511.21444v1\n",
      "2511.21398v1\n"
     ]
    }
   ],
   "source": [
    "papers = load_json_data()\n",
    "\n",
    "for identifier in papers:\n",
    "    print(identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc969b",
   "metadata": {},
   "source": [
    "### Tokenize and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cfb0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_paper(text: str) -> Dict[str, Union[str, List[List[str]]]]:\n",
    "    \"\"\"Process text into plain and lemmatized sentences.\"\"\"\n",
    "    paper = {}\n",
    "\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    sentence_split = nltk.sent_tokenize(text)\n",
    "    word_split = [\n",
    "        nltk.word_tokenize(sentence) for\n",
    "        sentence in sentence_split]\n",
    "\n",
    "    plain = []\n",
    "    lemma = []\n",
    "    for i, sentence in enumerate(word_split):\n",
    "        lemma.append([])\n",
    "        plain.append(sentence_split[i].replace('\\n', ''))\n",
    "\n",
    "        for word in sentence:\n",
    "            token = word.lower()\n",
    "            if token.isalpha() and token not in stop_words:\n",
    "                lemma[-1].append(lemmatizer.lemmatize(token))\n",
    "\n",
    "        # Discard sentences where\n",
    "        # lemmatization returns nothing\n",
    "        if not lemma[-1]:\n",
    "            lemma.pop()\n",
    "            plain.pop()\n",
    "\n",
    "    return {\"lemma\": lemma, \"plain\": plain }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458238ed",
   "metadata": {},
   "source": [
    "Pre-process the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "452abc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for identifier, data in papers.items():\n",
    "\n",
    "    paper = data[\"processed\"]\n",
    "    paragraphs = []\n",
    "    for section in paper:\n",
    "        paragraphs.append(section[\"paragraphs\"])\n",
    "\n",
    "    papers[identifier][\"sentences\"] = pre_process_paper(\"\\n\".join(paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477e2f3",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cbc202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_rank_sentences(lemmatized: str) -> List[int]:\n",
    "    \"\"\"Order index of sentences by TF-IDF similarity to whole document.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    sentences_list = [' '.join(s) for s in lemmatized]\n",
    "    sentences = vectorizer.fit_transform(sentences_list)\n",
    "    document = vectorizer.transform([' '.join(sentences_list)])\n",
    "    scores = cosine_similarity(document, sentences).flatten()\n",
    "\n",
    "    indexes = sorted(\n",
    "        range(len(scores)),\n",
    "        key=lambda i: scores[i],\n",
    "        reverse=True)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef67ad",
   "metadata": {},
   "source": [
    "Calculate TF-IDF scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06bd7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "for identifier, data in papers.items():\n",
    "    sentences = data[\"sentences\"]\n",
    "\n",
    "    papers[identifier][\"rank\"] = {}\n",
    "    papers[identifier][\"rank\"][\"TF-IDF\"] = tfidf_rank_sentences(sentences[\"lemma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de38def",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "195d59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rank_sentences(lemmatized: str) -> List[int]:\n",
    "    \"\"\"Order index of sentences by BM25 similarity to whole document.\"\"\"\n",
    "    sentences = BM25Okapi(lemmatized)\n",
    "    scores = sentences.get_scores(sum(lemmatized, []))\n",
    "\n",
    "    indexes = sorted(\n",
    "        range(len(scores)),\n",
    "        key=lambda i: scores[i],\n",
    "        reverse=True)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfcaff",
   "metadata": {},
   "source": [
    "Calculate BM25 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3839e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for identifier, data in papers.items():\n",
    "    sentences = data[\"sentences\"]\n",
    "\n",
    "    papers[identifier][\"rank\"][\"BM25\"] = bm25_rank_sentences(sentences[\"lemma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1adfc",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30b83420",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"..\", \"data\", \"results.json\")\n",
    "\n",
    "with open(filepath, \"w\") as f:\n",
    "    json.dump(papers, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8a877",
   "metadata": {},
   "source": [
    "by [Manuel Velarde](mailto:manuel@velarde.me)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
